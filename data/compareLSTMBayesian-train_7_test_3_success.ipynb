{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth='day'\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=2*forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list(rawArrayDatas[1])\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list(rawArrayDatas[1][:-forecastDay] )\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-3*forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-3*forecastDay]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-3*forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of 2*forecastDay:  rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "    testY= rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "        print('LSTM forecast :',mockForecastDictionary['LSTM'] )\n",
    "        ####Bayseian\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        print('Bayseian forecast :',mockForecastDictionary['LSTM'] )\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        ####더 좋은 알고리즘 호출\n",
    "        if nameOfBestAlgorithm is 'LSTM':\n",
    "            tf.reset_default_graph()\n",
    "            realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "\n",
    "        elif nameOfBestAlgorithm is 'Bayseian':\n",
    "            realForecastDictionary['Bayseian']=Bayseian(txsForRealForecastBayesian,forecastDay,'day')\n",
    "\n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        ####LSTM\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "        ####LSTM\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "\n",
    "\n",
    "        ####Bayseian\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        ####더 좋은 알고리즘 호출\n",
    "        if nameOfBestAlgorithm is 'LSTM':\n",
    "            tf.reset_default_graph()\n",
    "            realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "\n",
    "        elif nameOfBestAlgorithm is 'Bayseian':\n",
    "            realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    # tf.reset_default_graph()\n",
    "    # realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay, feature)\n",
    "\n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "    elif feature is 'Month_Season_Year':\n",
    "        tempxy = [list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "    return test_predict[-1].tolist()\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "\n",
    "    return nameOfBestAlgorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM(txs, forecastDay, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['ds','y']\n",
    "txs=pd.read_table('walMonth_train.csv', sep=',',header=None,names=columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>32990.7700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>22809.2850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>30103.3520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-05-01</td>\n",
       "      <td>16673.5375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-06-01</td>\n",
       "      <td>16685.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-07-01</td>\n",
       "      <td>16383.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-08-01</td>\n",
       "      <td>16144.7025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>17978.3175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>26928.9060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>23040.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>34796.7760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>17286.6475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>31440.6575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2011-03-01</td>\n",
       "      <td>20705.8350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>33011.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2011-05-01</td>\n",
       "      <td>17062.9300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011-06-01</td>\n",
       "      <td>15744.6425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2011-07-01</td>\n",
       "      <td>15771.2460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2011-08-01</td>\n",
       "      <td>14765.4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>17551.2820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>24701.7075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2011-11-01</td>\n",
       "      <td>24634.3775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>34902.4140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>17551.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>33670.8250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>22936.1080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ds           y\n",
       "0   2010-02-01  32990.7700\n",
       "1   2010-03-01  22809.2850\n",
       "2   2010-04-01  30103.3520\n",
       "3   2010-05-01  16673.5375\n",
       "4   2010-06-01  16685.1750\n",
       "5   2010-07-01  16383.0020\n",
       "6   2010-08-01  16144.7025\n",
       "7   2010-09-01  17978.3175\n",
       "8   2010-10-01  26928.9060\n",
       "9   2010-11-01  23040.3500\n",
       "10  2010-12-01  34796.7760\n",
       "11  2011-01-01  17286.6475\n",
       "12  2011-02-01  31440.6575\n",
       "13  2011-03-01  20705.8350\n",
       "14  2011-04-01  33011.3900\n",
       "15  2011-05-01  17062.9300\n",
       "16  2011-06-01  15744.6425\n",
       "17  2011-07-01  15771.2460\n",
       "18  2011-08-01  14765.4875\n",
       "19  2011-09-01  17551.2820\n",
       "20  2011-10-01  24701.7075\n",
       "21  2011-11-01  24634.3775\n",
       "22  2011-12-01  34902.4140\n",
       "23  2012-01-01  17551.3375\n",
       "24  2012-02-01  33670.8250\n",
       "25  2012-03-01  22936.1080"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(txs)\n",
    "# LSTM(txs, 7, features)\n",
    "# Bayseian(txs, 7, 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawArrayDatas=[ ['2007-12-01', '2008-01-01', '2008-02-01', '2008-03-01', '2008-04-01', '2008-05-01', '2008-06-01', '2008-07-01', '2008-08-01', '2008-09-01', '2008-10-01', '2008-11-01', '2008-12-01', '2009-01-01', '2009-02-01', '2009-03-01', '2009-04-01', '2009-05-01', '2009-06-01', '2009-07-01', '2009-08-01', '2009-09-01', '2009-10-01', '2009-11-01', '2009-12-01', '2010-01-01', '2010-02-01', '2010-03-01', '2010-04-01', '2010-05-01', '2010-06-01', '2010-07-01', '2010-08-01', '2010-09-01', '2010-10-01', '2010-11-01', '2010-12-01', '2011-01-01', '2011-02-01', '2011-03-01', '2011-04-01', '2011-05-01', '2011-06-01', '2011-07-01', '2011-08-01', '2011-09-01', '2011-10-01', '2011-11-01', '2011-12-01', '2012-01-01', '2012-02-01', '2012-03-01', '2012-04-01', '2012-05-01', '2012-06-01', '2012-07-01', '2012-08-01', '2012-09-01', '2012-10-01', '2012-11-01', '2012-12-01', '2013-01-01', '2013-02-01', '2013-03-01', '2013-04-01', '2013-05-01', '2013-06-01', '2013-07-01', '2013-08-01', '2013-09-01', '2013-10-01', '2013-11-01', '2013-12-01', '2014-01-01', '2014-02-01', '2014-03-01', '2014-04-01', '2014-05-01', '2014-06-01', '2014-07-01', '2014-08-01', '2014-09-01', '2014-10-01', '2014-11-01', '2014-12-01', '2015-01-01', '2015-02-01', '2015-03-01', '2015-04-01', '2015-05-01', '2015-06-01', '2015-07-01', '2015-08-01', '2015-09-01', '2015-10-01', '2015-11-01', '2015-12-01', '2016-01-01'] , [4958.0, 8359.2999999999993, 12178.14286, 1735.5357140000001, 2178.5666670000001, 1424.419355, 1237.7142859999999, 1343.4545449999998, 2100.580645, 3135.5999999999999, 2644.517241, 2929.0, 2971.4838709999999, 4590.8709680000002, 2453.4642859999999, 1603.2903229999999, 1760.5, 1483.4838710000001, 1376.5333330000001, 1725.9354840000001, 2844.6451609999999, 5786.4615380000005, 6445.6428569999998, 6952.2142860000004, 7556.0322580000002, 13760.172409999999, 12469.23077, 2277.3225809999999, 2503.9666670000001, 1698.451613, 1425.5357140000001, 1848.6153850000001, 3838.3225810000004, 9605.4666670000006, 6985.6774189999996, 8732.5, 7899.0645159999995, 9540.9032260000004, 3907.1071430000002, 2085.8709680000002, 3015.5666670000001, 1886.193548, 1695.9000000000001, 3470.6451609999999, 4622.2258060000004, 9086.6896550000001, 6864.8666670000002, 7461.9666670000006, 7556.3103449999999, 19610.45161, 25008.793099999999, 24771.03226, 3943.3103450000003, 3136.3870969999998, 2406.8666670000002, 3127.6451609999999, 5017.7741939999996, 13273.733329999999, 9823.8064519999989, 7519.6666670000004, 9169.16129, 13726.903230000002, 4432.5, 2776.5161290000001, 2286.6333329999998, 1913.451613, 1650.9333329999999, 2040.0333329999999, 2734.0645159999999, 13740.266669999999, 10564.35484, 7608.3999999999996, 8706.1935480000011, 26366.56667, 24504.89286, 2169.0322579999997, 1879.0, 2449.5483870000003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecastDay=int(len(rawArrayDatas[1])*0.2)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 78.11444854736328\n",
      "[step: 1] loss: 76.57946014404297\n",
      "[step: 2] loss: 75.05852508544922\n",
      "[step: 3] loss: 73.52033233642578\n",
      "[step: 4] loss: 71.92718505859375\n",
      "[step: 5] loss: 70.23699188232422\n",
      "[step: 6] loss: 68.40721893310547\n",
      "[step: 7] loss: 66.39246368408203\n",
      "[step: 8] loss: 64.14389038085938\n",
      "[step: 9] loss: 61.61175537109375\n",
      "[step: 10] loss: 58.749969482421875\n",
      "[step: 11] loss: 55.52494812011719\n",
      "[step: 12] loss: 51.93253707885742\n",
      "[step: 13] loss: 48.029212951660156\n",
      "[step: 14] loss: 43.98804473876953\n",
      "[step: 15] loss: 40.19749450683594\n",
      "[step: 16] loss: 37.395626068115234\n",
      "[step: 17] loss: 36.592567443847656\n",
      "[step: 18] loss: 37.73310089111328\n",
      "[step: 19] loss: 38.38792419433594\n",
      "[step: 20] loss: 37.178863525390625\n",
      "[step: 21] loss: 34.90205001831055\n",
      "[step: 22] loss: 32.76488494873047\n",
      "[step: 23] loss: 31.424392700195312\n",
      "[step: 24] loss: 30.92165756225586\n",
      "[step: 25] loss: 30.9703369140625\n",
      "[step: 26] loss: 31.235294342041016\n",
      "[step: 27] loss: 31.473222732543945\n",
      "[step: 28] loss: 31.56106185913086\n",
      "[step: 29] loss: 31.473430633544922\n",
      "[step: 30] loss: 31.250232696533203\n",
      "[step: 31] loss: 30.968582153320312\n",
      "[step: 32] loss: 30.71719741821289\n",
      "[step: 33] loss: 30.568193435668945\n",
      "[step: 34] loss: 30.54737663269043\n",
      "[step: 35] loss: 30.61705780029297\n",
      "[step: 36] loss: 30.692440032958984\n",
      "[step: 37] loss: 30.69232177734375\n",
      "[step: 38] loss: 30.58852767944336\n",
      "[step: 39] loss: 30.414356231689453\n",
      "[step: 40] loss: 30.232177734375\n",
      "[step: 41] loss: 30.093273162841797\n",
      "[step: 42] loss: 30.016996383666992\n",
      "[step: 43] loss: 29.99287986755371\n",
      "[step: 44] loss: 29.994903564453125\n",
      "[step: 45] loss: 29.996158599853516\n",
      "[step: 46] loss: 29.97824478149414\n",
      "[step: 47] loss: 29.934778213500977\n",
      "[step: 48] loss: 29.870458602905273\n",
      "[step: 49] loss: 29.797428131103516\n",
      "[step: 50] loss: 29.73011016845703\n",
      "[step: 51] loss: 29.67991828918457\n",
      "[step: 52] loss: 29.65102195739746\n",
      "[step: 53] loss: 29.638959884643555\n",
      "[step: 54] loss: 29.633045196533203\n",
      "[step: 55] loss: 29.62166976928711\n",
      "[step: 56] loss: 29.597990036010742\n",
      "[step: 57] loss: 29.56254005432129\n",
      "[step: 58] loss: 29.52169418334961\n",
      "[step: 59] loss: 29.483360290527344\n",
      "[step: 60] loss: 29.452816009521484\n",
      "[step: 61] loss: 29.43082618713379\n",
      "[step: 62] loss: 29.414262771606445\n",
      "[step: 63] loss: 29.398361206054688\n",
      "[step: 64] loss: 29.3790283203125\n",
      "[step: 65] loss: 29.35443115234375\n",
      "[step: 66] loss: 29.32530403137207\n",
      "[step: 67] loss: 29.294174194335938\n",
      "[step: 68] loss: 29.26400375366211\n",
      "[step: 69] loss: 29.236757278442383\n",
      "[step: 70] loss: 29.212560653686523\n",
      "[step: 71] loss: 29.189857482910156\n",
      "[step: 72] loss: 29.166345596313477\n",
      "[step: 73] loss: 29.14019775390625\n",
      "[step: 74] loss: 29.110984802246094\n",
      "[step: 75] loss: 29.079662322998047\n",
      "[step: 76] loss: 29.047870635986328\n",
      "[step: 77] loss: 29.0169734954834\n",
      "[step: 78] loss: 28.987394332885742\n",
      "[step: 79] loss: 28.958581924438477\n",
      "[step: 80] loss: 28.929458618164062\n",
      "[step: 81] loss: 28.899023056030273\n",
      "[step: 82] loss: 28.866836547851562\n",
      "[step: 83] loss: 28.833105087280273\n",
      "[step: 84] loss: 28.79842758178711\n",
      "[step: 85] loss: 28.763412475585938\n",
      "[step: 86] loss: 28.7282772064209\n",
      "[step: 87] loss: 28.692764282226562\n",
      "[step: 88] loss: 28.656286239624023\n",
      "[step: 89] loss: 28.618295669555664\n",
      "[step: 90] loss: 28.57855796813965\n",
      "[step: 91] loss: 28.537221908569336\n",
      "[step: 92] loss: 28.494644165039062\n",
      "[step: 93] loss: 28.451108932495117\n",
      "[step: 94] loss: 28.406639099121094\n",
      "[step: 95] loss: 28.360963821411133\n",
      "[step: 96] loss: 28.313739776611328\n",
      "[step: 97] loss: 28.26468276977539\n",
      "[step: 98] loss: 28.213741302490234\n",
      "[step: 99] loss: 28.161006927490234\n",
      "[step: 100] loss: 28.106578826904297\n",
      "[step: 101] loss: 28.05044937133789\n",
      "[step: 102] loss: 27.99237823486328\n",
      "[step: 103] loss: 27.932106018066406\n",
      "[step: 104] loss: 27.869426727294922\n",
      "[step: 105] loss: 27.80428123474121\n",
      "[step: 106] loss: 27.736682891845703\n",
      "[step: 107] loss: 27.666555404663086\n",
      "[step: 108] loss: 27.593677520751953\n",
      "[step: 109] loss: 27.51775360107422\n",
      "[step: 110] loss: 27.438596725463867\n",
      "[step: 111] loss: 27.356098175048828\n",
      "[step: 112] loss: 27.27013397216797\n",
      "[step: 113] loss: 27.180423736572266\n",
      "[step: 114] loss: 27.086647033691406\n",
      "[step: 115] loss: 26.988605499267578\n",
      "[step: 116] loss: 26.8862361907959\n",
      "[step: 117] loss: 26.779422760009766\n",
      "[step: 118] loss: 26.667861938476562\n",
      "[step: 119] loss: 26.55136489868164\n",
      "[step: 120] loss: 26.43001937866211\n",
      "[step: 121] loss: 26.303972244262695\n",
      "[step: 122] loss: 26.173221588134766\n",
      "[step: 123] loss: 26.038089752197266\n",
      "[step: 124] loss: 25.89935874938965\n",
      "[step: 125] loss: 25.757829666137695\n",
      "[step: 126] loss: 25.61457061767578\n",
      "[step: 127] loss: 25.47127914428711\n",
      "[step: 128] loss: 25.32965850830078\n",
      "[step: 129] loss: 25.191511154174805\n",
      "[step: 130] loss: 25.058734893798828\n",
      "[step: 131] loss: 24.932283401489258\n",
      "[step: 132] loss: 24.812665939331055\n",
      "[step: 133] loss: 24.69911003112793\n",
      "[step: 134] loss: 24.590017318725586\n",
      "[step: 135] loss: 24.483163833618164\n",
      "[step: 136] loss: 24.376808166503906\n",
      "[step: 137] loss: 24.27017593383789\n",
      "[step: 138] loss: 24.164213180541992\n",
      "[step: 139] loss: 24.06078338623047\n",
      "[step: 140] loss: 23.961856842041016\n",
      "[step: 141] loss: 23.867420196533203\n",
      "[step: 142] loss: 23.774452209472656\n",
      "[step: 143] loss: 23.67815589904785\n",
      "[step: 144] loss: 23.574899673461914\n",
      "[step: 145] loss: 23.464305877685547\n",
      "[step: 146] loss: 23.349411010742188\n",
      "[step: 147] loss: 23.235057830810547\n",
      "[step: 148] loss: 23.125490188598633\n",
      "[step: 149] loss: 23.022764205932617\n",
      "[step: 150] loss: 22.92630958557129\n",
      "[step: 151] loss: 22.833986282348633\n",
      "[step: 152] loss: 22.743633270263672\n",
      "[step: 153] loss: 22.6542911529541\n",
      "[step: 154] loss: 22.5665225982666\n",
      "[step: 155] loss: 22.481760025024414\n",
      "[step: 156] loss: 22.401147842407227\n",
      "[step: 157] loss: 22.32452392578125\n",
      "[step: 158] loss: 22.250286102294922\n",
      "[step: 159] loss: 22.176288604736328\n",
      "[step: 160] loss: 22.101213455200195\n",
      "[step: 161] loss: 22.025287628173828\n",
      "[step: 162] loss: 21.950101852416992\n",
      "[step: 163] loss: 21.877954483032227\n",
      "[step: 164] loss: 21.813072204589844\n",
      "[step: 165] loss: 21.757530212402344\n",
      "[step: 166] loss: 21.693328857421875\n",
      "[step: 167] loss: 21.57891845703125\n",
      "[step: 168] loss: 21.47466468811035\n",
      "[step: 169] loss: 21.411712646484375\n",
      "[step: 170] loss: 21.326982498168945\n",
      "[step: 171] loss: 21.211376190185547\n",
      "[step: 172] loss: 21.116769790649414\n",
      "[step: 173] loss: 21.036808013916016\n",
      "[step: 174] loss: 20.928466796875\n",
      "[step: 175] loss: 20.807735443115234\n",
      "[step: 176] loss: 20.710067749023438\n",
      "[step: 177] loss: 20.60594940185547\n",
      "[step: 178] loss: 20.474422454833984\n",
      "[step: 179] loss: 20.35004234313965\n",
      "[step: 180] loss: 20.234886169433594\n",
      "[step: 181] loss: 20.101303100585938\n",
      "[step: 182] loss: 19.953927993774414\n",
      "[step: 183] loss: 19.809852600097656\n",
      "[step: 184] loss: 19.669666290283203\n",
      "[step: 185] loss: 19.52263069152832\n",
      "[step: 186] loss: 19.360702514648438\n",
      "[step: 187] loss: 19.191120147705078\n",
      "[step: 188] loss: 19.01844596862793\n",
      "[step: 189] loss: 18.8460750579834\n",
      "[step: 190] loss: 18.673973083496094\n",
      "[step: 191] loss: 18.503681182861328\n",
      "[step: 192] loss: 18.345083236694336\n",
      "[step: 193] loss: 18.21147346496582\n",
      "[step: 194] loss: 18.13850975036621\n",
      "[step: 195] loss: 17.93643569946289\n",
      "[step: 196] loss: 17.667776107788086\n",
      "[step: 197] loss: 17.456281661987305\n",
      "[step: 198] loss: 17.389665603637695\n",
      "[step: 199] loss: 17.280704498291016\n",
      "[step: 200] loss: 16.997730255126953\n",
      "[step: 201] loss: 16.885128021240234\n",
      "[step: 202] loss: 16.847461700439453\n",
      "[step: 203] loss: 16.60588836669922\n",
      "[step: 204] loss: 16.445127487182617\n",
      "[step: 205] loss: 16.402488708496094\n",
      "[step: 206] loss: 16.25242042541504\n",
      "[step: 207] loss: 16.072643280029297\n",
      "[step: 208] loss: 15.973012924194336\n",
      "[step: 209] loss: 15.89808177947998\n",
      "[step: 210] loss: 15.774166107177734\n",
      "[step: 211] loss: 15.612068176269531\n",
      "[step: 212] loss: 15.509798049926758\n",
      "[step: 213] loss: 15.43943977355957\n",
      "[step: 214] loss: 15.315677642822266\n",
      "[step: 215] loss: 15.171340942382812\n",
      "[step: 216] loss: 15.048833847045898\n",
      "[step: 217] loss: 14.961372375488281\n",
      "[step: 218] loss: 14.877436637878418\n",
      "[step: 219] loss: 14.758010864257812\n",
      "[step: 220] loss: 14.62889575958252\n",
      "[step: 221] loss: 14.510208129882812\n",
      "[step: 222] loss: 14.414983749389648\n",
      "[step: 223] loss: 14.333976745605469\n",
      "[step: 224] loss: 14.24825668334961\n",
      "[step: 225] loss: 14.159037590026855\n",
      "[step: 226] loss: 14.052244186401367\n",
      "[step: 227] loss: 13.947463989257812\n",
      "[step: 228] loss: 13.845259666442871\n",
      "[step: 229] loss: 13.753517150878906\n",
      "[step: 230] loss: 13.671815872192383\n",
      "[step: 231] loss: 13.597923278808594\n",
      "[step: 232] loss: 13.531883239746094\n",
      "[step: 233] loss: 13.470907211303711\n",
      "[step: 234] loss: 13.425743103027344\n",
      "[step: 235] loss: 13.369913101196289\n",
      "[step: 236] loss: 13.325117111206055\n",
      "[step: 237] loss: 13.219863891601562\n",
      "[step: 238] loss: 13.111051559448242\n",
      "[step: 239] loss: 13.006319046020508\n",
      "[step: 240] loss: 12.941574096679688\n",
      "[step: 241] loss: 12.903402328491211\n",
      "[step: 242] loss: 12.849105834960938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 243] loss: 12.77586555480957\n",
      "[step: 244] loss: 12.676218032836914\n",
      "[step: 245] loss: 12.593024253845215\n",
      "[step: 246] loss: 12.5343656539917\n",
      "[step: 247] loss: 12.48331069946289\n",
      "[step: 248] loss: 12.425052642822266\n",
      "[step: 249] loss: 12.344083786010742\n",
      "[step: 250] loss: 12.261795043945312\n",
      "[step: 251] loss: 12.18899917602539\n",
      "[step: 252] loss: 12.129934310913086\n",
      "[step: 253] loss: 12.079739570617676\n",
      "[step: 254] loss: 12.023150444030762\n",
      "[step: 255] loss: 11.96636962890625\n",
      "[step: 256] loss: 11.8942289352417\n",
      "[step: 257] loss: 11.829404830932617\n",
      "[step: 258] loss: 11.778553009033203\n",
      "[step: 259] loss: 11.737251281738281\n",
      "[step: 260] loss: 11.694928169250488\n",
      "[step: 261] loss: 11.602041244506836\n",
      "[step: 262] loss: 11.497659683227539\n",
      "[step: 263] loss: 11.405752182006836\n",
      "[step: 264] loss: 11.349365234375\n",
      "[step: 265] loss: 11.311564445495605\n",
      "[step: 266] loss: 11.252788543701172\n",
      "[step: 267] loss: 11.177560806274414\n",
      "[step: 268] loss: 11.086441993713379\n",
      "[step: 269] loss: 11.01856517791748\n",
      "[step: 270] loss: 10.990686416625977\n",
      "[step: 271] loss: 11.019170761108398\n",
      "[step: 272] loss: 11.055365562438965\n",
      "[step: 273] loss: 10.951560974121094\n",
      "[step: 274] loss: 10.686531066894531\n",
      "[step: 275] loss: 10.656805038452148\n",
      "[step: 276] loss: 10.686363220214844\n",
      "[step: 277] loss: 10.493452072143555\n",
      "[step: 278] loss: 10.383367538452148\n",
      "[step: 279] loss: 10.411420822143555\n",
      "[step: 280] loss: 10.276642799377441\n",
      "[step: 281] loss: 10.138185501098633\n",
      "[step: 282] loss: 10.137720108032227\n",
      "[step: 283] loss: 10.031292915344238\n",
      "[step: 284] loss: 9.895034790039062\n",
      "[step: 285] loss: 9.868735313415527\n",
      "[step: 286] loss: 9.77470874786377\n",
      "[step: 287] loss: 9.645575523376465\n",
      "[step: 288] loss: 9.60094165802002\n",
      "[step: 289] loss: 9.5255126953125\n",
      "[step: 290] loss: 9.42758560180664\n",
      "[step: 291] loss: 9.481433868408203\n",
      "[step: 292] loss: 9.646883964538574\n",
      "[step: 293] loss: 10.088357925415039\n",
      "[step: 294] loss: 9.471074104309082\n",
      "[step: 295] loss: 9.004045486450195\n",
      "[step: 296] loss: 9.025566101074219\n",
      "[step: 297] loss: 9.163448333740234\n",
      "[step: 298] loss: 8.90266227722168\n",
      "[step: 299] loss: 8.624078750610352\n",
      "[step: 300] loss: 8.762596130371094\n",
      "[step: 301] loss: 8.764477729797363\n",
      "[step: 302] loss: 8.46370792388916\n",
      "[step: 303] loss: 8.327984809875488\n",
      "[step: 304] loss: 8.429171562194824\n",
      "[step: 305] loss: 8.300804138183594\n",
      "[step: 306] loss: 8.002823829650879\n",
      "[step: 307] loss: 7.999146461486816\n",
      "[step: 308] loss: 8.008037567138672\n",
      "[step: 309] loss: 7.90726900100708\n",
      "[step: 310] loss: 7.7294721603393555\n",
      "[step: 311] loss: 7.570226669311523\n",
      "[step: 312] loss: 7.577838897705078\n",
      "[step: 313] loss: 7.571260452270508\n",
      "[step: 314] loss: 7.515721321105957\n",
      "[step: 315] loss: 7.405431747436523\n",
      "[step: 316] loss: 7.247228145599365\n",
      "[step: 317] loss: 7.101036548614502\n",
      "[step: 318] loss: 7.023216724395752\n",
      "[step: 319] loss: 6.939231872558594\n",
      "[step: 320] loss: 6.909456729888916\n",
      "[step: 321] loss: 6.95765495300293\n",
      "[step: 322] loss: 7.0802483558654785\n",
      "[step: 323] loss: 7.607760906219482\n",
      "[step: 324] loss: 7.646991729736328\n",
      "[step: 325] loss: 7.387062072753906\n",
      "[step: 326] loss: 6.52601957321167\n",
      "[step: 327] loss: 6.8221282958984375\n",
      "[step: 328] loss: 7.377897262573242\n",
      "[step: 329] loss: 6.449692726135254\n",
      "[step: 330] loss: 6.5049238204956055\n",
      "[step: 331] loss: 7.054653167724609\n",
      "[step: 332] loss: 6.237894535064697\n",
      "[step: 333] loss: 6.38134765625\n",
      "[step: 334] loss: 6.7400126457214355\n",
      "[step: 335] loss: 6.040383338928223\n",
      "[step: 336] loss: 6.358180999755859\n",
      "[step: 337] loss: 6.530512809753418\n",
      "[step: 338] loss: 5.925512313842773\n",
      "[step: 339] loss: 6.3157453536987305\n",
      "[step: 340] loss: 6.3781962394714355\n",
      "[step: 341] loss: 5.816152572631836\n",
      "[step: 342] loss: 6.2308759689331055\n",
      "[step: 343] loss: 6.2135396003723145\n",
      "[step: 344] loss: 5.677788734436035\n",
      "[step: 345] loss: 6.185357093811035\n",
      "[step: 346] loss: 5.957449913024902\n",
      "[step: 347] loss: 5.588140487670898\n",
      "[step: 348] loss: 6.09144401550293\n",
      "[step: 349] loss: 5.641054153442383\n",
      "[step: 350] loss: 5.580873489379883\n",
      "[step: 351] loss: 5.83864688873291\n",
      "[step: 352] loss: 5.43055534362793\n",
      "[step: 353] loss: 5.5223894119262695\n",
      "[step: 354] loss: 5.563319206237793\n",
      "[step: 355] loss: 5.318443298339844\n",
      "[step: 356] loss: 5.380947113037109\n",
      "[step: 357] loss: 5.37859582901001\n",
      "[step: 358] loss: 5.196646690368652\n",
      "[step: 359] loss: 5.259982109069824\n",
      "[step: 360] loss: 5.2248101234436035\n",
      "[step: 361] loss: 5.091400146484375\n",
      "[step: 362] loss: 5.156495094299316\n",
      "[step: 363] loss: 5.0907063484191895\n",
      "[step: 364] loss: 5.019698143005371\n",
      "[step: 365] loss: 5.038351535797119\n",
      "[step: 366] loss: 4.99804162979126\n",
      "[step: 367] loss: 4.9506402015686035\n",
      "[step: 368] loss: 4.9228835105896\n",
      "[step: 369] loss: 4.922824382781982\n",
      "[step: 370] loss: 4.8833909034729\n",
      "[step: 371] loss: 4.830907821655273\n",
      "[step: 372] loss: 4.8331427574157715\n",
      "[step: 373] loss: 4.815614223480225\n",
      "[step: 374] loss: 4.774423122406006\n",
      "[step: 375] loss: 4.743547439575195\n",
      "[step: 376] loss: 4.720951080322266\n",
      "[step: 377] loss: 4.714163303375244\n",
      "[step: 378] loss: 4.693347930908203\n",
      "[step: 379] loss: 4.6540327072143555\n",
      "[step: 380] loss: 4.62819242477417\n",
      "[step: 381] loss: 4.605786323547363\n",
      "[step: 382] loss: 4.589856147766113\n",
      "[step: 383] loss: 4.580005645751953\n",
      "[step: 384] loss: 4.559543609619141\n",
      "[step: 385] loss: 4.538389205932617\n",
      "[step: 386] loss: 4.516543865203857\n",
      "[step: 387] loss: 4.49193811416626\n",
      "[step: 388] loss: 4.469346523284912\n",
      "[step: 389] loss: 4.451539993286133\n",
      "[step: 390] loss: 4.43320369720459\n",
      "[step: 391] loss: 4.424343585968018\n",
      "[step: 392] loss: 4.428223609924316\n",
      "[step: 393] loss: 4.470959663391113\n",
      "[step: 394] loss: 4.57381010055542\n",
      "[step: 395] loss: 4.901979923248291\n",
      "[step: 396] loss: 5.250662803649902\n",
      "[step: 397] loss: 5.9566216468811035\n",
      "[step: 398] loss: 4.900012969970703\n",
      "[step: 399] loss: 4.289490699768066\n",
      "[step: 400] loss: 4.498444557189941\n",
      "[step: 401] loss: 4.851867198944092\n",
      "[step: 402] loss: 4.818894386291504\n",
      "[step: 403] loss: 4.242135047912598\n",
      "[step: 404] loss: 4.409716606140137\n",
      "[step: 405] loss: 4.824376106262207\n",
      "[step: 406] loss: 4.33719539642334\n",
      "[step: 407] loss: 4.179924011230469\n",
      "[step: 408] loss: 4.490827560424805\n",
      "[step: 409] loss: 4.33702278137207\n",
      "[step: 410] loss: 4.109274387359619\n",
      "[step: 411] loss: 4.181877136230469\n",
      "[step: 412] loss: 4.26962947845459\n",
      "[step: 413] loss: 4.150635719299316\n",
      "[step: 414] loss: 4.035248279571533\n",
      "[step: 415] loss: 4.1197004318237305\n",
      "[step: 416] loss: 4.1805901527404785\n",
      "[step: 417] loss: 4.029179573059082\n",
      "[step: 418] loss: 3.9721364974975586\n",
      "[step: 419] loss: 4.0432281494140625\n",
      "[step: 420] loss: 4.041181564331055\n",
      "[step: 421] loss: 3.958873748779297\n",
      "[step: 422] loss: 3.8967108726501465\n",
      "[step: 423] loss: 3.91750431060791\n",
      "[step: 424] loss: 3.955963134765625\n",
      "[step: 425] loss: 3.9166927337646484\n",
      "[step: 426] loss: 3.8527510166168213\n",
      "[step: 427] loss: 3.8095805644989014\n",
      "[step: 428] loss: 3.812495231628418\n",
      "[step: 429] loss: 3.832479953765869\n",
      "[step: 430] loss: 3.8281240463256836\n",
      "[step: 431] loss: 3.803987979888916\n",
      "[step: 432] loss: 3.7577695846557617\n",
      "[step: 433] loss: 3.7172961235046387\n",
      "[step: 434] loss: 3.6884591579437256\n",
      "[step: 435] loss: 3.6715657711029053\n",
      "[step: 436] loss: 3.664863348007202\n",
      "[step: 437] loss: 3.665640354156494\n",
      "[step: 438] loss: 3.6827292442321777\n",
      "[step: 439] loss: 3.7156312465667725\n",
      "[step: 440] loss: 3.8229331970214844\n",
      "[step: 441] loss: 3.9652814865112305\n",
      "[step: 442] loss: 4.379143714904785\n",
      "[step: 443] loss: 4.429250240325928\n",
      "[step: 444] loss: 4.687831878662109\n",
      "[step: 445] loss: 3.9174256324768066\n",
      "[step: 446] loss: 3.5226993560791016\n",
      "[step: 447] loss: 3.5951719284057617\n",
      "[step: 448] loss: 3.890393018722534\n",
      "[step: 449] loss: 4.1396589279174805\n",
      "[step: 450] loss: 3.6998038291931152\n",
      "[step: 451] loss: 3.437135696411133\n",
      "[step: 452] loss: 3.505955934524536\n",
      "[step: 453] loss: 3.691211700439453\n",
      "[step: 454] loss: 3.7699084281921387\n",
      "[step: 455] loss: 3.492051601409912\n",
      "[step: 456] loss: 3.3540844917297363\n",
      "[step: 457] loss: 3.4343082904815674\n",
      "[step: 458] loss: 3.5402677059173584\n",
      "[step: 459] loss: 3.557281494140625\n",
      "[step: 460] loss: 3.383965492248535\n",
      "[step: 461] loss: 3.279078960418701\n",
      "[step: 462] loss: 3.294114828109741\n",
      "[step: 463] loss: 3.3675694465637207\n",
      "[step: 464] loss: 3.432791233062744\n",
      "[step: 465] loss: 3.3686139583587646\n",
      "[step: 466] loss: 3.289445400238037\n",
      "[step: 467] loss: 3.203580379486084\n",
      "[step: 468] loss: 3.164498805999756\n",
      "[step: 469] loss: 3.170311212539673\n",
      "[step: 470] loss: 3.201510429382324\n",
      "[step: 471] loss: 3.2585396766662598\n",
      "[step: 472] loss: 3.3007915019989014\n",
      "[step: 473] loss: 3.398418426513672\n",
      "[step: 474] loss: 3.4163503646850586\n",
      "[step: 475] loss: 3.526240348815918\n",
      "[step: 476] loss: 3.4471869468688965\n",
      "[step: 477] loss: 3.4437899589538574\n",
      "[step: 478] loss: 3.263826370239258\n",
      "[step: 479] loss: 3.147953510284424\n",
      "[step: 480] loss: 3.037349224090576\n",
      "[step: 481] loss: 2.981175422668457\n",
      "[step: 482] loss: 2.9658780097961426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 483] loss: 2.9819672107696533\n",
      "[step: 484] loss: 3.0314905643463135\n",
      "[step: 485] loss: 3.1064653396606445\n",
      "[step: 486] loss: 3.2871806621551514\n",
      "[step: 487] loss: 3.433027744293213\n",
      "[step: 488] loss: 3.8095364570617676\n",
      "[step: 489] loss: 3.615952491760254\n",
      "[step: 490] loss: 3.5132250785827637\n",
      "[step: 491] loss: 3.054818630218506\n",
      "[step: 492] loss: 2.8481626510620117\n",
      "[step: 493] loss: 2.899414300918579\n",
      "[step: 494] loss: 3.0861434936523438\n",
      "[step: 495] loss: 3.304426670074463\n",
      "[step: 496] loss: 3.155911445617676\n",
      "[step: 497] loss: 2.9785280227661133\n",
      "[step: 498] loss: 2.796476364135742\n",
      "[step: 499] loss: 2.776444435119629\n",
      "[step: 500] loss: 2.8809194564819336\n",
      "[step: 501] loss: 2.9660964012145996\n",
      "[step: 502] loss: 3.0134236812591553\n",
      "[step: 503] loss: 2.88824200630188\n",
      "[step: 504] loss: 2.7739076614379883\n",
      "[step: 505] loss: 2.6928882598876953\n",
      "[step: 506] loss: 2.6824822425842285\n",
      "[step: 507] loss: 2.7257726192474365\n",
      "[step: 508] loss: 2.7824201583862305\n",
      "[step: 509] loss: 2.8584179878234863\n",
      "[step: 510] loss: 2.8715617656707764\n",
      "[step: 511] loss: 2.9127860069274902\n",
      "[step: 512] loss: 2.855646848678589\n",
      "[step: 513] loss: 2.8302712440490723\n",
      "[step: 514] loss: 2.7440104484558105\n",
      "[step: 515] loss: 2.690415620803833\n",
      "[step: 516] loss: 2.6284050941467285\n",
      "[step: 517] loss: 2.5897204875946045\n",
      "[step: 518] loss: 2.5583438873291016\n",
      "[step: 519] loss: 2.5372581481933594\n",
      "[step: 520] loss: 2.5208828449249268\n",
      "[step: 521] loss: 2.510065793991089\n",
      "[step: 522] loss: 2.504385232925415\n",
      "[step: 523] loss: 2.5124051570892334\n",
      "[step: 524] loss: 2.5474798679351807\n",
      "[step: 525] loss: 2.6760125160217285\n",
      "[step: 526] loss: 2.964956760406494\n",
      "[step: 527] loss: 3.8536131381988525\n",
      "[step: 528] loss: 4.250592231750488\n",
      "[step: 529] loss: 4.975903511047363\n",
      "[step: 530] loss: 2.808053493499756\n",
      "[step: 531] loss: 2.75283145904541\n",
      "[step: 532] loss: 4.173038482666016\n",
      "[step: 533] loss: 3.1270012855529785\n",
      "[step: 534] loss: 2.444124937057495\n",
      "[step: 535] loss: 3.1011552810668945\n",
      "[step: 536] loss: 3.0260214805603027\n",
      "[step: 537] loss: 2.504584789276123\n",
      "[step: 538] loss: 2.586893081665039\n",
      "[step: 539] loss: 2.831717014312744\n",
      "[step: 540] loss: 2.647355556488037\n",
      "[step: 541] loss: 2.3988125324249268\n",
      "[step: 542] loss: 2.7040011882781982\n",
      "[step: 543] loss: 2.696078300476074\n",
      "[step: 544] loss: 2.349595069885254\n",
      "[step: 545] loss: 2.600088119506836\n",
      "[step: 546] loss: 2.6709892749786377\n",
      "[step: 547] loss: 2.3654322624206543\n",
      "[step: 548] loss: 2.4654123783111572\n",
      "[step: 549] loss: 2.647284984588623\n",
      "[step: 550] loss: 2.3624587059020996\n",
      "[step: 551] loss: 2.339259147644043\n",
      "[step: 552] loss: 2.5558042526245117\n",
      "[step: 553] loss: 2.3598761558532715\n",
      "[step: 554] loss: 2.2636630535125732\n",
      "[step: 555] loss: 2.3776392936706543\n",
      "[step: 556] loss: 2.3693604469299316\n",
      "[step: 557] loss: 2.2432937622070312\n",
      "[step: 558] loss: 2.2230513095855713\n",
      "[step: 559] loss: 2.311535596847534\n",
      "[step: 560] loss: 2.282045841217041\n",
      "[step: 561] loss: 2.185555934906006\n",
      "[step: 562] loss: 2.181943893432617\n",
      "[step: 563] loss: 2.2373437881469727\n",
      "[step: 564] loss: 2.227475881576538\n",
      "[step: 565] loss: 2.1509037017822266\n",
      "[step: 566] loss: 2.121363401412964\n",
      "[step: 567] loss: 2.140401840209961\n",
      "[step: 568] loss: 2.1712613105773926\n",
      "[step: 569] loss: 2.157581329345703\n",
      "[step: 570] loss: 2.111286163330078\n",
      "[step: 571] loss: 2.0744757652282715\n",
      "[step: 572] loss: 2.05839204788208\n",
      "[step: 573] loss: 2.0678696632385254\n",
      "[step: 574] loss: 2.085404396057129\n",
      "[step: 575] loss: 2.0955138206481934\n",
      "[step: 576] loss: 2.095790386199951\n",
      "[step: 577] loss: 2.0937204360961914\n",
      "[step: 578] loss: 2.0787224769592285\n",
      "[step: 579] loss: 2.0741662979125977\n",
      "[step: 580] loss: 2.0641918182373047\n",
      "[step: 581] loss: 2.0749564170837402\n",
      "[step: 582] loss: 2.0873775482177734\n",
      "[step: 583] loss: 2.1401214599609375\n",
      "[step: 584] loss: 2.2124319076538086\n",
      "[step: 585] loss: 2.3961212635040283\n",
      "[step: 586] loss: 2.5426928997039795\n",
      "[step: 587] loss: 2.8729846477508545\n",
      "[step: 588] loss: 2.7142529487609863\n",
      "[step: 589] loss: 2.5759425163269043\n",
      "[step: 590] loss: 2.1325273513793945\n",
      "[step: 591] loss: 1.9168164730072021\n",
      "[step: 592] loss: 1.9684425592422485\n",
      "[step: 593] loss: 2.1583762168884277\n",
      "[step: 594] loss: 2.328153610229492\n",
      "[step: 595] loss: 2.1836905479431152\n",
      "[step: 596] loss: 1.9917099475860596\n",
      "[step: 597] loss: 1.8694491386413574\n",
      "[step: 598] loss: 1.9140441417694092\n",
      "[step: 599] loss: 2.0386297702789307\n",
      "[step: 600] loss: 2.0615220069885254\n",
      "[step: 601] loss: 1.9961885213851929\n",
      "[step: 602] loss: 1.873874306678772\n",
      "[step: 603] loss: 1.8231770992279053\n",
      "[step: 604] loss: 1.8574228286743164\n",
      "[step: 605] loss: 1.9165198802947998\n",
      "[step: 606] loss: 1.9511319398880005\n",
      "[step: 607] loss: 1.9096895456314087\n",
      "[step: 608] loss: 1.8484859466552734\n",
      "[step: 609] loss: 1.7919480800628662\n",
      "[step: 610] loss: 1.770442008972168\n",
      "[step: 611] loss: 1.7807674407958984\n",
      "[step: 612] loss: 1.807352066040039\n",
      "[step: 613] loss: 1.839571475982666\n",
      "[step: 614] loss: 1.8555247783660889\n",
      "[step: 615] loss: 1.8740370273590088\n",
      "[step: 616] loss: 1.8688961267471313\n",
      "[step: 617] loss: 1.875488519668579\n",
      "[step: 618] loss: 1.8621494770050049\n",
      "[step: 619] loss: 1.8688596487045288\n",
      "[step: 620] loss: 1.8601264953613281\n",
      "[step: 621] loss: 1.8794734477996826\n",
      "[step: 622] loss: 1.8851659297943115\n",
      "[step: 623] loss: 1.931563138961792\n",
      "[step: 624] loss: 1.9519197940826416\n",
      "[step: 625] loss: 2.0264458656311035\n",
      "[step: 626] loss: 2.029538154602051\n",
      "[step: 627] loss: 2.0754776000976562\n",
      "[step: 628] loss: 1.99184250831604\n",
      "[step: 629] loss: 1.9247183799743652\n",
      "[step: 630] loss: 1.789670705795288\n",
      "[step: 631] loss: 1.6924608945846558\n",
      "[step: 632] loss: 1.632603406906128\n",
      "[step: 633] loss: 1.621253252029419\n",
      "[step: 634] loss: 1.6468935012817383\n",
      "[step: 635] loss: 1.6907424926757812\n",
      "[step: 636] loss: 1.7475258111953735\n",
      "[step: 637] loss: 1.7811784744262695\n",
      "[step: 638] loss: 1.8208574056625366\n",
      "[step: 639] loss: 1.804517149925232\n",
      "[step: 640] loss: 1.7935558557510376\n",
      "[step: 641] loss: 1.7349865436553955\n",
      "[step: 642] loss: 1.6881948709487915\n",
      "[step: 643] loss: 1.6318628787994385\n",
      "[step: 644] loss: 1.5919076204299927\n",
      "[step: 645] loss: 1.5615931749343872\n",
      "[step: 646] loss: 1.5428202152252197\n",
      "[step: 647] loss: 1.531970739364624\n",
      "[step: 648] loss: 1.5267831087112427\n",
      "[step: 649] loss: 1.5261471271514893\n",
      "[step: 650] loss: 1.530688762664795\n",
      "[step: 651] loss: 1.5453473329544067\n",
      "[step: 652] loss: 1.5773448944091797\n",
      "[step: 653] loss: 1.6576579809188843\n",
      "[step: 654] loss: 1.8071165084838867\n",
      "[step: 655] loss: 2.1730523109436035\n",
      "[step: 656] loss: 2.566153049468994\n",
      "[step: 657] loss: 3.254140615463257\n",
      "[step: 658] loss: 2.5846381187438965\n",
      "[step: 659] loss: 1.90968656539917\n",
      "[step: 660] loss: 1.4958447217941284\n",
      "[step: 661] loss: 1.743622899055481\n",
      "[step: 662] loss: 2.2176616191864014\n",
      "[step: 663] loss: 1.971407413482666\n",
      "[step: 664] loss: 1.5611751079559326\n",
      "[step: 665] loss: 1.5081837177276611\n",
      "[step: 666] loss: 1.7670092582702637\n",
      "[step: 667] loss: 1.8458945751190186\n",
      "[step: 668] loss: 1.5404655933380127\n",
      "[step: 669] loss: 1.4708768129348755\n",
      "[step: 670] loss: 1.6565732955932617\n",
      "[step: 671] loss: 1.6677818298339844\n",
      "[step: 672] loss: 1.4927623271942139\n",
      "[step: 673] loss: 1.4352433681488037\n",
      "[step: 674] loss: 1.5461921691894531\n",
      "[step: 675] loss: 1.5967741012573242\n",
      "[step: 676] loss: 1.4666144847869873\n",
      "[step: 677] loss: 1.4031312465667725\n",
      "[step: 678] loss: 1.4668742418289185\n",
      "[step: 679] loss: 1.5138309001922607\n",
      "[step: 680] loss: 1.4595260620117188\n",
      "[step: 681] loss: 1.382585048675537\n",
      "[step: 682] loss: 1.3842254877090454\n",
      "[step: 683] loss: 1.436345100402832\n",
      "[step: 684] loss: 1.444013237953186\n",
      "[step: 685] loss: 1.4026659727096558\n",
      "[step: 686] loss: 1.352379560470581\n",
      "[step: 687] loss: 1.3405017852783203\n",
      "[step: 688] loss: 1.3610079288482666\n",
      "[step: 689] loss: 1.3829479217529297\n",
      "[step: 690] loss: 1.3865042924880981\n",
      "[step: 691] loss: 1.3642768859863281\n",
      "[step: 692] loss: 1.3345023393630981\n",
      "[step: 693] loss: 1.3096040487289429\n",
      "[step: 694] loss: 1.2959039211273193\n",
      "[step: 695] loss: 1.2943296432495117\n",
      "[step: 696] loss: 1.2995498180389404\n",
      "[step: 697] loss: 1.3109824657440186\n",
      "[step: 698] loss: 1.3253886699676514\n",
      "[step: 699] loss: 1.350631833076477\n",
      "[step: 700] loss: 1.3815792798995972\n",
      "[step: 701] loss: 1.4454231262207031\n",
      "[step: 702] loss: 1.526931881904602\n",
      "[step: 703] loss: 1.6925667524337769\n",
      "[step: 704] loss: 1.8220183849334717\n",
      "[step: 705] loss: 2.0421602725982666\n",
      "[step: 706] loss: 1.9482178688049316\n",
      "[step: 707] loss: 1.8058149814605713\n",
      "[step: 708] loss: 1.4543895721435547\n",
      "[step: 709] loss: 1.252805471420288\n",
      "[step: 710] loss: 1.2585053443908691\n",
      "[step: 711] loss: 1.4042834043502808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 712] loss: 1.548382043838501\n",
      "[step: 713] loss: 1.4984078407287598\n",
      "[step: 714] loss: 1.3591487407684326\n",
      "[step: 715] loss: 1.2255122661590576\n",
      "[step: 716] loss: 1.2200692892074585\n",
      "[step: 717] loss: 1.3078243732452393\n",
      "[step: 718] loss: 1.365070104598999\n",
      "[step: 719] loss: 1.3442481756210327\n",
      "[step: 720] loss: 1.2495633363723755\n",
      "[step: 721] loss: 1.1852834224700928\n",
      "[step: 722] loss: 1.1890082359313965\n",
      "[step: 723] loss: 1.2351765632629395\n",
      "[step: 724] loss: 1.2738876342773438\n",
      "[step: 725] loss: 1.2608426809310913\n",
      "[step: 726] loss: 1.2192436456680298\n",
      "[step: 727] loss: 1.1707830429077148\n",
      "[step: 728] loss: 1.1469051837921143\n",
      "[step: 729] loss: 1.1514177322387695\n",
      "[step: 730] loss: 1.172203540802002\n",
      "[step: 731] loss: 1.196118950843811\n",
      "[step: 732] loss: 1.2083287239074707\n",
      "[step: 733] loss: 1.213756799697876\n",
      "[step: 734] loss: 1.20309317111969\n",
      "[step: 735] loss: 1.1918853521347046\n",
      "[step: 736] loss: 1.1739811897277832\n",
      "[step: 737] loss: 1.1607987880706787\n",
      "[step: 738] loss: 1.1475176811218262\n",
      "[step: 739] loss: 1.1403563022613525\n",
      "[step: 740] loss: 1.1356679201126099\n",
      "[step: 741] loss: 1.1389143466949463\n",
      "[step: 742] loss: 1.1484049558639526\n",
      "[step: 743] loss: 1.1762175559997559\n",
      "[step: 744] loss: 1.222284197807312\n",
      "[step: 745] loss: 1.3238691091537476\n",
      "[step: 746] loss: 1.4619337320327759\n",
      "[step: 747] loss: 1.733851432800293\n",
      "[step: 748] loss: 1.9023106098175049\n",
      "[step: 749] loss: 2.108675956726074\n",
      "[step: 750] loss: 1.758169174194336\n",
      "[step: 751] loss: 1.3755877017974854\n",
      "[step: 752] loss: 1.089417815208435\n",
      "[step: 753] loss: 1.1192413568496704\n",
      "[step: 754] loss: 1.3476438522338867\n",
      "[step: 755] loss: 1.4417954683303833\n",
      "[step: 756] loss: 1.3414349555969238\n",
      "[step: 757] loss: 1.120283842086792\n",
      "[step: 758] loss: 1.0513005256652832\n",
      "[step: 759] loss: 1.1573184728622437\n",
      "[step: 760] loss: 1.2449748516082764\n",
      "[step: 761] loss: 1.2040187120437622\n",
      "[step: 762] loss: 1.0751628875732422\n",
      "[step: 763] loss: 1.0262045860290527\n",
      "[step: 764] loss: 1.084838628768921\n",
      "[step: 765] loss: 1.1434850692749023\n",
      "[step: 766] loss: 1.1245567798614502\n",
      "[step: 767] loss: 1.0441974401474\n",
      "[step: 768] loss: 0.9987990260124207\n",
      "[step: 769] loss: 1.0210235118865967\n",
      "[step: 770] loss: 1.0646617412567139\n",
      "[step: 771] loss: 1.0751299858093262\n",
      "[step: 772] loss: 1.0370299816131592\n",
      "[step: 773] loss: 0.99235999584198\n",
      "[step: 774] loss: 0.9728912115097046\n",
      "[step: 775] loss: 0.9823720455169678\n",
      "[step: 776] loss: 1.0018094778060913\n",
      "[step: 777] loss: 1.0126842260360718\n",
      "[step: 778] loss: 1.0130581855773926\n",
      "[step: 779] loss: 1.0019333362579346\n",
      "[step: 780] loss: 0.9884785413742065\n",
      "[step: 781] loss: 0.970569372177124\n",
      "[step: 782] loss: 0.9534271955490112\n",
      "[step: 783] loss: 0.9389314651489258\n",
      "[step: 784] loss: 0.9298804402351379\n",
      "[step: 785] loss: 0.9260079264640808\n",
      "[step: 786] loss: 0.9252485036849976\n",
      "[step: 787] loss: 0.9258129596710205\n",
      "[step: 788] loss: 0.9277164936065674\n",
      "[step: 789] loss: 0.9341388940811157\n",
      "[step: 790] loss: 0.9498046636581421\n",
      "[step: 791] loss: 0.9884498715400696\n",
      "[step: 792] loss: 1.0638225078582764\n",
      "[step: 793] loss: 1.2345783710479736\n",
      "[step: 794] loss: 1.502115249633789\n",
      "[step: 795] loss: 2.0282444953918457\n",
      "[step: 796] loss: 2.222043037414551\n",
      "[step: 797] loss: 2.2721798419952393\n",
      "[step: 798] loss: 1.4238656759262085\n",
      "[step: 799] loss: 0.9450390338897705\n",
      "[step: 800] loss: 1.0979580879211426\n",
      "[step: 801] loss: 1.4490668773651123\n",
      "[step: 802] loss: 1.4897288084030151\n",
      "[step: 803] loss: 1.0819381475448608\n",
      "[step: 804] loss: 0.9939948916435242\n",
      "[step: 805] loss: 1.1881808042526245\n",
      "[step: 806] loss: 1.1605066061019897\n",
      "[step: 807] loss: 1.0022847652435303\n",
      "[step: 808] loss: 1.0115795135498047\n",
      "[step: 809] loss: 1.0616331100463867\n",
      "[step: 810] loss: 0.9958245754241943\n",
      "[step: 811] loss: 0.9523544311523438\n",
      "[step: 812] loss: 1.0111385583877563\n",
      "[step: 813] loss: 0.9667072296142578\n",
      "[step: 814] loss: 0.8928098082542419\n",
      "[step: 815] loss: 0.9466787576675415\n",
      "[step: 816] loss: 0.9705329537391663\n",
      "[step: 817] loss: 0.8697342276573181\n",
      "[step: 818] loss: 0.8688526153564453\n",
      "[step: 819] loss: 0.9342790842056274\n",
      "[step: 820] loss: 0.8886235952377319\n",
      "[step: 821] loss: 0.8290933966636658\n",
      "[step: 822] loss: 0.8683105707168579\n",
      "[step: 823] loss: 0.8782898783683777\n",
      "[step: 824] loss: 0.8309389352798462\n",
      "[step: 825] loss: 0.8276833295822144\n",
      "[step: 826] loss: 0.8520278334617615\n",
      "[step: 827] loss: 0.8258901238441467\n",
      "[step: 828] loss: 0.8017870187759399\n",
      "[step: 829] loss: 0.8205957412719727\n",
      "[step: 830] loss: 0.8256195783615112\n",
      "[step: 831] loss: 0.7982257008552551\n",
      "[step: 832] loss: 0.7921230792999268\n",
      "[step: 833] loss: 0.8023977279663086\n",
      "[step: 834] loss: 0.7916371822357178\n",
      "[step: 835] loss: 0.77554851770401\n",
      "[step: 836] loss: 0.7801264524459839\n",
      "[step: 837] loss: 0.7853564023971558\n",
      "[step: 838] loss: 0.7742291688919067\n",
      "[step: 839] loss: 0.7652462124824524\n",
      "[step: 840] loss: 0.768983006477356\n",
      "[step: 841] loss: 0.7696262001991272\n",
      "[step: 842] loss: 0.7599540948867798\n",
      "[step: 843] loss: 0.7549352049827576\n",
      "[step: 844] loss: 0.7583267688751221\n",
      "[step: 845] loss: 0.7602764964103699\n",
      "[step: 846] loss: 0.7584543824195862\n",
      "[step: 847] loss: 0.7651681900024414\n",
      "[step: 848] loss: 0.7872241139411926\n",
      "[step: 849] loss: 0.8321516513824463\n",
      "[step: 850] loss: 0.9084113836288452\n",
      "[step: 851] loss: 1.081136703491211\n",
      "[step: 852] loss: 1.3548747301101685\n",
      "[step: 853] loss: 1.8542898893356323\n",
      "[step: 854] loss: 2.0831336975097656\n",
      "[step: 855] loss: 2.1270079612731934\n",
      "[step: 856] loss: 1.3198411464691162\n",
      "[step: 857] loss: 0.7735522985458374\n",
      "[step: 858] loss: 0.8618087768554688\n",
      "[step: 859] loss: 1.2740825414657593\n",
      "[step: 860] loss: 1.4139556884765625\n",
      "[step: 861] loss: 0.948346734046936\n",
      "[step: 862] loss: 0.7306773662567139\n",
      "[step: 863] loss: 0.9677711725234985\n",
      "[step: 864] loss: 1.1046369075775146\n",
      "[step: 865] loss: 0.8886191844940186\n",
      "[step: 866] loss: 0.7186600565910339\n",
      "[step: 867] loss: 0.8720542192459106\n",
      "[step: 868] loss: 0.9845592975616455\n",
      "[step: 869] loss: 0.8010269403457642\n",
      "[step: 870] loss: 0.7117546796798706\n",
      "[step: 871] loss: 0.8390743732452393\n",
      "[step: 872] loss: 0.8734735250473022\n",
      "[step: 873] loss: 0.7473466992378235\n",
      "[step: 874] loss: 0.6994577646255493\n",
      "[step: 875] loss: 0.7886170148849487\n",
      "[step: 876] loss: 0.8142725825309753\n",
      "[step: 877] loss: 0.7175488471984863\n",
      "[step: 878] loss: 0.6825621128082275\n",
      "[step: 879] loss: 0.744365930557251\n",
      "[step: 880] loss: 0.7666073441505432\n",
      "[step: 881] loss: 0.706825852394104\n",
      "[step: 882] loss: 0.664800763130188\n",
      "[step: 883] loss: 0.6926788091659546\n",
      "[step: 884] loss: 0.7273719310760498\n",
      "[step: 885] loss: 0.7029176950454712\n",
      "[step: 886] loss: 0.6614563465118408\n",
      "[step: 887] loss: 0.6507123708724976\n",
      "[step: 888] loss: 0.6722521781921387\n",
      "[step: 889] loss: 0.6850675344467163\n",
      "[step: 890] loss: 0.6710437536239624\n",
      "[step: 891] loss: 0.6451162099838257\n",
      "[step: 892] loss: 0.6321340799331665\n",
      "[step: 893] loss: 0.6360746026039124\n",
      "[step: 894] loss: 0.6493288278579712\n",
      "[step: 895] loss: 0.6516093611717224\n",
      "[step: 896] loss: 0.642455518245697\n",
      "[step: 897] loss: 0.6250436305999756\n",
      "[step: 898] loss: 0.6142207384109497\n",
      "[step: 899] loss: 0.6111354231834412\n",
      "[step: 900] loss: 0.6147957444190979\n",
      "[step: 901] loss: 0.6189723610877991\n",
      "[step: 902] loss: 0.6213006973266602\n",
      "[step: 903] loss: 0.6205027103424072\n",
      "[step: 904] loss: 0.615572452545166\n",
      "[step: 905] loss: 0.6086281538009644\n",
      "[step: 906] loss: 0.6010315418243408\n",
      "[step: 907] loss: 0.594818115234375\n",
      "[step: 908] loss: 0.5891973972320557\n",
      "[step: 909] loss: 0.5847606062889099\n",
      "[step: 910] loss: 0.580579400062561\n",
      "[step: 911] loss: 0.5774134397506714\n",
      "[step: 912] loss: 0.5743964910507202\n",
      "[step: 913] loss: 0.5719764232635498\n",
      "[step: 914] loss: 0.569282054901123\n",
      "[step: 915] loss: 0.5668796896934509\n",
      "[step: 916] loss: 0.564480185508728\n",
      "[step: 917] loss: 0.5626362562179565\n",
      "[step: 918] loss: 0.5614013671875\n",
      "[step: 919] loss: 0.5614832043647766\n",
      "[step: 920] loss: 0.5645018815994263\n",
      "[step: 921] loss: 0.573989987373352\n",
      "[step: 922] loss: 0.5996659994125366\n",
      "[step: 923] loss: 0.6586229801177979\n",
      "[step: 924] loss: 0.805858314037323\n",
      "[step: 925] loss: 1.0970747470855713\n",
      "[step: 926] loss: 1.7406256198883057\n",
      "[step: 927] loss: 2.2453835010528564\n",
      "[step: 928] loss: 2.6575870513916016\n",
      "[step: 929] loss: 1.4397531747817993\n",
      "[step: 930] loss: 0.679654598236084\n",
      "[step: 931] loss: 0.9455665946006775\n",
      "[step: 932] loss: 1.405669093132019\n",
      "[step: 933] loss: 1.3439087867736816\n",
      "[step: 934] loss: 0.7884371876716614\n",
      "[step: 935] loss: 0.8768561482429504\n",
      "[step: 936] loss: 1.1154375076293945\n",
      "[step: 937] loss: 0.9203853607177734\n",
      "[step: 938] loss: 0.7829230427742004\n",
      "[step: 939] loss: 0.8555612564086914\n",
      "[step: 940] loss: 0.7972192764282227\n",
      "[step: 941] loss: 0.8119492530822754\n",
      "[step: 942] loss: 0.7048081159591675\n",
      "[step: 943] loss: 0.6749670505523682\n",
      "[step: 944] loss: 0.7654961347579956\n",
      "[step: 945] loss: 0.682607114315033\n",
      "[step: 946] loss: 0.570544958114624\n",
      "[step: 947] loss: 0.723487138748169\n",
      "[step: 948] loss: 0.6535793542861938\n",
      "[step: 949] loss: 0.5442662239074707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 950] loss: 0.6509965658187866\n",
      "[step: 951] loss: 0.6321827173233032\n",
      "[step: 952] loss: 0.5304298400878906\n",
      "[step: 953] loss: 0.6061193346977234\n",
      "[step: 954] loss: 0.5839805603027344\n",
      "[step: 955] loss: 0.5380548238754272\n",
      "[step: 956] loss: 0.5725740194320679\n",
      "[step: 957] loss: 0.5448795557022095\n",
      "[step: 958] loss: 0.5234650373458862\n",
      "[step: 959] loss: 0.560758650302887\n",
      "[step: 960] loss: 0.5190956592559814\n",
      "[step: 961] loss: 0.5032616853713989\n",
      "[step: 962] loss: 0.535912275314331\n",
      "[step: 963] loss: 0.5136353969573975\n",
      "[step: 964] loss: 0.4914332628250122\n",
      "[step: 965] loss: 0.5101022720336914\n",
      "[step: 966] loss: 0.49612441658973694\n",
      "[step: 967] loss: 0.48865002393722534\n",
      "[step: 968] loss: 0.49779775738716125\n",
      "[step: 969] loss: 0.4822554588317871\n",
      "[step: 970] loss: 0.47224509716033936\n",
      "[step: 971] loss: 0.48574045300483704\n",
      "[step: 972] loss: 0.47791996598243713\n",
      "[step: 973] loss: 0.4668048620223999\n",
      "[step: 974] loss: 0.4702133238315582\n",
      "[step: 975] loss: 0.4659409523010254\n",
      "[step: 976] loss: 0.45714521408081055\n",
      "[step: 977] loss: 0.4621747136116028\n",
      "[step: 978] loss: 0.46110600233078003\n",
      "[step: 979] loss: 0.45193618535995483\n",
      "[step: 980] loss: 0.4501149654388428\n",
      "[step: 981] loss: 0.45031091570854187\n",
      "[step: 982] loss: 0.4437592923641205\n",
      "[step: 983] loss: 0.44181519746780396\n",
      "[step: 984] loss: 0.4435276389122009\n",
      "[step: 985] loss: 0.4402892589569092\n",
      "[step: 986] loss: 0.4358469247817993\n",
      "[step: 987] loss: 0.4355679154396057\n",
      "[step: 988] loss: 0.4334283173084259\n",
      "[step: 989] loss: 0.42862996459007263\n",
      "[step: 990] loss: 0.42630648612976074\n",
      "[step: 991] loss: 0.42542093992233276\n",
      "[step: 992] loss: 0.4222332835197449\n",
      "[step: 993] loss: 0.4190272092819214\n",
      "[step: 994] loss: 0.4179987907409668\n",
      "[step: 995] loss: 0.41645604372024536\n",
      "[step: 996] loss: 0.41355401277542114\n",
      "[step: 997] loss: 0.41151008009910583\n",
      "[step: 998] loss: 0.41067415475845337\n",
      "[step: 999] loss: 0.40905052423477173\n",
      "LSTM forecast : [2591.44921875, 4695.6826171875, 2028.2352294921875, 3090.92724609375, 2311.189453125, 2208.249755859375, 4268.1259765625, 8428.5009765625, 11327.142578125, 6599.13623046875, 5359.42724609375, 9131.234375, 4585.8896484375, -1421.4248046875, 3927.302978515625, -598.705810546875, 4728.048828125, -447.82525634765625, 5715.71875, 8458.900390625, 6039.75439453125, 7343.369140625, 17429.630859375, 24709.115234375, 25183.888671875, 7431.068359375, 9037.96484375, 2229.97314453125, 6786.25146484375, -1120.5400390625, 9642.8916015625, 7937.8994140625, 15437.486328125, 10262.5859375, 8680.73046875, 3881.254638671875, 3238.70263671875, 693.42138671875, -2675.36669921875, -861.8938598632812]\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "Bayseian forecast : [2591.44921875, 4695.6826171875, 2028.2352294921875, 3090.92724609375, 2311.189453125, 2208.249755859375, 4268.1259765625, 8428.5009765625, 11327.142578125, 6599.13623046875, 5359.42724609375, 9131.234375, 4585.8896484375, -1421.4248046875, 3927.302978515625, -598.705810546875, 4728.048828125, -447.82525634765625, 5715.71875, 8458.900390625, 6039.75439453125, 7343.369140625, 17429.630859375, 24709.115234375, 25183.888671875, 7431.068359375, 9037.96484375, 2229.97314453125, 6786.25146484375, -1120.5400390625, 9642.8916015625, 7937.8994140625, 15437.486328125, 10262.5859375, 8680.73046875, 3881.254638671875, 3238.70263671875, 693.42138671875, -2675.36669921875, -861.8938598632812]\n",
      "[step: 0] loss: 112.49259185791016\n",
      "[step: 1] loss: 108.9278564453125\n",
      "[step: 2] loss: 105.52372741699219\n",
      "[step: 3] loss: 102.19232177734375\n",
      "[step: 4] loss: 98.8428955078125\n",
      "[step: 5] loss: 95.38925170898438\n",
      "[step: 6] loss: 91.75623321533203\n",
      "[step: 7] loss: 87.88089752197266\n",
      "[step: 8] loss: 83.71199035644531\n",
      "[step: 9] loss: 79.21298217773438\n",
      "[step: 10] loss: 74.37387084960938\n",
      "[step: 11] loss: 69.2374267578125\n",
      "[step: 12] loss: 63.948219299316406\n",
      "[step: 13] loss: 58.83446502685547\n",
      "[step: 14] loss: 54.5201301574707\n",
      "[step: 15] loss: 51.960628509521484\n",
      "[step: 16] loss: 51.93655776977539\n",
      "[step: 17] loss: 53.543846130371094\n",
      "[step: 18] loss: 54.32756805419922\n",
      "[step: 19] loss: 53.20195007324219\n",
      "[step: 20] loss: 50.94164276123047\n",
      "[step: 21] loss: 48.75703811645508\n",
      "[step: 22] loss: 47.37852096557617\n",
      "[step: 23] loss: 46.91542434692383\n",
      "[step: 24] loss: 47.098602294921875\n",
      "[step: 25] loss: 47.564353942871094\n",
      "[step: 26] loss: 48.022071838378906\n",
      "[step: 27] loss: 48.302650451660156\n",
      "[step: 28] loss: 48.34349822998047\n",
      "[step: 29] loss: 48.15718078613281\n",
      "[step: 30] loss: 47.80311584472656\n",
      "[step: 31] loss: 47.365901947021484\n",
      "[step: 32] loss: 46.93687057495117\n",
      "[step: 33] loss: 46.595401763916016\n",
      "[step: 34] loss: 46.38957977294922\n",
      "[step: 35] loss: 46.320716857910156\n",
      "[step: 36] loss: 46.34126281738281\n",
      "[step: 37] loss: 46.37438201904297\n",
      "[step: 38] loss: 46.349822998046875\n",
      "[step: 39] loss: 46.23612976074219\n",
      "[step: 40] loss: 46.04903030395508\n",
      "[step: 41] loss: 45.83434295654297\n",
      "[step: 42] loss: 45.6405029296875\n",
      "[step: 43] loss: 45.498077392578125\n",
      "[step: 44] loss: 45.4131965637207\n",
      "[step: 45] loss: 45.372642517089844\n",
      "[step: 46] loss: 45.35385513305664\n",
      "[step: 47] loss: 45.33447265625\n",
      "[step: 48] loss: 45.29859924316406\n",
      "[step: 49] loss: 45.23953628540039\n",
      "[step: 50] loss: 45.15949249267578\n",
      "[step: 51] loss: 45.06713104248047\n",
      "[step: 52] loss: 44.973960876464844\n",
      "[step: 53] loss: 44.890159606933594\n",
      "[step: 54] loss: 44.821144104003906\n",
      "[step: 55] loss: 44.76581573486328\n",
      "[step: 56] loss: 44.71715545654297\n",
      "[step: 57] loss: 44.66538619995117\n",
      "[step: 58] loss: 44.60222625732422\n",
      "[step: 59] loss: 44.52428436279297\n",
      "[step: 60] loss: 44.43394470214844\n",
      "[step: 61] loss: 44.33744812011719\n",
      "[step: 62] loss: 44.24155044555664\n",
      "[step: 63] loss: 44.150516510009766\n",
      "[step: 64] loss: 44.06487274169922\n",
      "[step: 65] loss: 43.981849670410156\n",
      "[step: 66] loss: 43.897254943847656\n",
      "[step: 67] loss: 43.80729293823242\n",
      "[step: 68] loss: 43.709999084472656\n",
      "[step: 69] loss: 43.60554504394531\n",
      "[step: 70] loss: 43.49570083618164\n",
      "[step: 71] loss: 43.382667541503906\n",
      "[step: 72] loss: 43.267887115478516\n",
      "[step: 73] loss: 43.15141296386719\n",
      "[step: 74] loss: 43.031951904296875\n",
      "[step: 75] loss: 42.907779693603516\n",
      "[step: 76] loss: 42.77764129638672\n",
      "[step: 77] loss: 42.641502380371094\n",
      "[step: 78] loss: 42.50033187866211\n",
      "[step: 79] loss: 42.3555908203125\n",
      "[step: 80] loss: 42.208641052246094\n",
      "[step: 81] loss: 42.06061935424805\n",
      "[step: 82] loss: 41.91264724731445\n",
      "[step: 83] loss: 41.766029357910156\n",
      "[step: 84] loss: 41.62208557128906\n",
      "[step: 85] loss: 41.481834411621094\n",
      "[step: 86] loss: 41.34587860107422\n",
      "[step: 87] loss: 41.21491622924805\n",
      "[step: 88] loss: 41.090232849121094\n",
      "[step: 89] loss: 40.973331451416016\n",
      "[step: 90] loss: 40.864356994628906\n",
      "[step: 91] loss: 40.761314392089844\n",
      "[step: 92] loss: 40.661258697509766\n",
      "[step: 93] loss: 40.56210708618164\n",
      "[step: 94] loss: 40.46272659301758\n",
      "[step: 95] loss: 40.361671447753906\n",
      "[step: 96] loss: 40.257286071777344\n",
      "[step: 97] loss: 40.149112701416016\n",
      "[step: 98] loss: 40.03814697265625\n",
      "[step: 99] loss: 39.92543029785156\n",
      "[step: 100] loss: 39.811771392822266\n",
      "[step: 101] loss: 39.69832992553711\n",
      "[step: 102] loss: 39.58649826049805\n",
      "[step: 103] loss: 39.47698211669922\n",
      "[step: 104] loss: 39.36982727050781\n",
      "[step: 105] loss: 39.26537322998047\n",
      "[step: 106] loss: 39.164302825927734\n",
      "[step: 107] loss: 39.06671142578125\n",
      "[step: 108] loss: 38.971832275390625\n",
      "[step: 109] loss: 38.879188537597656\n",
      "[step: 110] loss: 38.78900146484375\n",
      "[step: 111] loss: 38.70138931274414\n",
      "[step: 112] loss: 38.61606216430664\n",
      "[step: 113] loss: 38.533203125\n",
      "[step: 114] loss: 38.45351791381836\n",
      "[step: 115] loss: 38.37725830078125\n",
      "[step: 116] loss: 38.30437469482422\n",
      "[step: 117] loss: 38.23536682128906\n",
      "[step: 118] loss: 38.17055892944336\n",
      "[step: 119] loss: 38.10954666137695\n",
      "[step: 120] loss: 38.05213928222656\n",
      "[step: 121] loss: 37.99810028076172\n",
      "[step: 122] loss: 37.94651412963867\n",
      "[step: 123] loss: 37.896759033203125\n",
      "[step: 124] loss: 37.848228454589844\n",
      "[step: 125] loss: 37.800045013427734\n",
      "[step: 126] loss: 37.751953125\n",
      "[step: 127] loss: 37.70367431640625\n",
      "[step: 128] loss: 37.65496826171875\n",
      "[step: 129] loss: 37.60603332519531\n",
      "[step: 130] loss: 37.556793212890625\n",
      "[step: 131] loss: 37.50743103027344\n",
      "[step: 132] loss: 37.458133697509766\n",
      "[step: 133] loss: 37.40888977050781\n",
      "[step: 134] loss: 37.35987091064453\n",
      "[step: 135] loss: 37.31105041503906\n",
      "[step: 136] loss: 37.262474060058594\n",
      "[step: 137] loss: 37.21414566040039\n",
      "[step: 138] loss: 37.16596603393555\n",
      "[step: 139] loss: 37.1179313659668\n",
      "[step: 140] loss: 37.069984436035156\n",
      "[step: 141] loss: 37.02204132080078\n",
      "[step: 142] loss: 36.97406005859375\n",
      "[step: 143] loss: 36.9259033203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 144] loss: 36.8775634765625\n",
      "[step: 145] loss: 36.82897186279297\n",
      "[step: 146] loss: 36.78010559082031\n",
      "[step: 147] loss: 36.73091125488281\n",
      "[step: 148] loss: 36.68134307861328\n",
      "[step: 149] loss: 36.63135528564453\n",
      "[step: 150] loss: 36.58087921142578\n",
      "[step: 151] loss: 36.52984619140625\n",
      "[step: 152] loss: 36.47819519042969\n",
      "[step: 153] loss: 36.42582702636719\n",
      "[step: 154] loss: 36.372711181640625\n",
      "[step: 155] loss: 36.31881332397461\n",
      "[step: 156] loss: 36.26410675048828\n",
      "[step: 157] loss: 36.208580017089844\n",
      "[step: 158] loss: 36.15221405029297\n",
      "[step: 159] loss: 36.095008850097656\n",
      "[step: 160] loss: 36.03691864013672\n",
      "[step: 161] loss: 35.97791290283203\n",
      "[step: 162] loss: 35.917930603027344\n",
      "[step: 163] loss: 35.85694885253906\n",
      "[step: 164] loss: 35.79490661621094\n",
      "[step: 165] loss: 35.73175048828125\n",
      "[step: 166] loss: 35.667449951171875\n",
      "[step: 167] loss: 35.601966857910156\n",
      "[step: 168] loss: 35.53527069091797\n",
      "[step: 169] loss: 35.467342376708984\n",
      "[step: 170] loss: 35.398155212402344\n",
      "[step: 171] loss: 35.327720642089844\n",
      "[step: 172] loss: 35.256038665771484\n",
      "[step: 173] loss: 35.183128356933594\n",
      "[step: 174] loss: 35.10900115966797\n",
      "[step: 175] loss: 35.03367614746094\n",
      "[step: 176] loss: 34.957191467285156\n",
      "[step: 177] loss: 34.87958526611328\n",
      "[step: 178] loss: 34.80091857910156\n",
      "[step: 179] loss: 34.721256256103516\n",
      "[step: 180] loss: 34.64066696166992\n",
      "[step: 181] loss: 34.559288024902344\n",
      "[step: 182] loss: 34.477203369140625\n",
      "[step: 183] loss: 34.39454650878906\n",
      "[step: 184] loss: 34.31145477294922\n",
      "[step: 185] loss: 34.22809600830078\n",
      "[step: 186] loss: 34.144622802734375\n",
      "[step: 187] loss: 34.061187744140625\n",
      "[step: 188] loss: 33.97801208496094\n",
      "[step: 189] loss: 33.89525604248047\n",
      "[step: 190] loss: 33.813114166259766\n",
      "[step: 191] loss: 33.731781005859375\n",
      "[step: 192] loss: 33.65140151977539\n",
      "[step: 193] loss: 33.57215881347656\n",
      "[step: 194] loss: 33.49415588378906\n",
      "[step: 195] loss: 33.41749572753906\n",
      "[step: 196] loss: 33.34225082397461\n",
      "[step: 197] loss: 33.26844024658203\n",
      "[step: 198] loss: 33.196075439453125\n",
      "[step: 199] loss: 33.12507247924805\n",
      "[step: 200] loss: 33.05535125732422\n",
      "[step: 201] loss: 32.98680114746094\n",
      "[step: 202] loss: 32.91926574707031\n",
      "[step: 203] loss: 32.85258483886719\n",
      "[step: 204] loss: 32.78656768798828\n",
      "[step: 205] loss: 32.721126556396484\n",
      "[step: 206] loss: 32.65639114379883\n",
      "[step: 207] loss: 32.59531784057617\n",
      "[step: 208] loss: 32.56360626220703\n",
      "[step: 209] loss: 32.69134521484375\n",
      "[step: 210] loss: 32.86957931518555\n",
      "[step: 211] loss: 32.418983459472656\n",
      "[step: 212] loss: 32.483314514160156\n",
      "[step: 213] loss: 32.4569091796875\n",
      "[step: 214] loss: 32.214752197265625\n",
      "[step: 215] loss: 32.36813735961914\n",
      "[step: 216] loss: 32.07777404785156\n",
      "[step: 217] loss: 32.21320343017578\n",
      "[step: 218] loss: 31.988750457763672\n",
      "[step: 219] loss: 32.04185485839844\n",
      "[step: 220] loss: 31.90492057800293\n",
      "[step: 221] loss: 31.87682342529297\n",
      "[step: 222] loss: 31.813692092895508\n",
      "[step: 223] loss: 31.72699737548828\n",
      "[step: 224] loss: 31.71084213256836\n",
      "[step: 225] loss: 31.589750289916992\n",
      "[step: 226] loss: 31.596508026123047\n",
      "[step: 227] loss: 31.464336395263672\n",
      "[step: 228] loss: 31.47031021118164\n",
      "[step: 229] loss: 31.348331451416016\n",
      "[step: 230] loss: 31.333894729614258\n",
      "[step: 231] loss: 31.237594604492188\n",
      "[step: 232] loss: 31.191381454467773\n",
      "[step: 233] loss: 31.126651763916016\n",
      "[step: 234] loss: 31.049766540527344\n",
      "[step: 235] loss: 31.008010864257812\n",
      "[step: 236] loss: 30.91437530517578\n",
      "[step: 237] loss: 30.8774356842041\n",
      "[step: 238] loss: 30.787485122680664\n",
      "[step: 239] loss: 30.73479461669922\n",
      "[step: 240] loss: 30.664291381835938\n",
      "[step: 241] loss: 30.588600158691406\n",
      "[step: 242] loss: 30.53404998779297\n",
      "[step: 243] loss: 30.44828987121582\n",
      "[step: 244] loss: 30.390623092651367\n",
      "[step: 245] loss: 30.314420700073242\n",
      "[step: 246] loss: 30.23943519592285\n",
      "[step: 247] loss: 30.176128387451172\n",
      "[step: 248] loss: 30.092802047729492\n",
      "[step: 249] loss: 30.024991989135742\n",
      "[step: 250] loss: 29.950592041015625\n",
      "[step: 251] loss: 29.86937713623047\n",
      "[step: 252] loss: 29.799774169921875\n",
      "[step: 253] loss: 29.718780517578125\n",
      "[step: 254] loss: 29.638999938964844\n",
      "[step: 255] loss: 29.564451217651367\n",
      "[step: 256] loss: 29.480134963989258\n",
      "[step: 257] loss: 29.39864158630371\n",
      "[step: 258] loss: 29.31926155090332\n",
      "[step: 259] loss: 29.232210159301758\n",
      "[step: 260] loss: 29.146575927734375\n",
      "[step: 261] loss: 29.062480926513672\n",
      "[step: 262] loss: 28.972312927246094\n",
      "[step: 263] loss: 28.880847930908203\n",
      "[step: 264] loss: 28.790935516357422\n",
      "[step: 265] loss: 28.697107315063477\n",
      "[step: 266] loss: 28.599082946777344\n",
      "[step: 267] loss: 28.50088882446289\n",
      "[step: 268] loss: 28.401538848876953\n",
      "[step: 269] loss: 28.29782485961914\n",
      "[step: 270] loss: 28.190223693847656\n",
      "[step: 271] loss: 28.080595016479492\n",
      "[step: 272] loss: 27.968997955322266\n",
      "[step: 273] loss: 27.85411834716797\n",
      "[step: 274] loss: 27.734935760498047\n",
      "[step: 275] loss: 27.611557006835938\n",
      "[step: 276] loss: 27.484140396118164\n",
      "[step: 277] loss: 27.352893829345703\n",
      "[step: 278] loss: 27.217731475830078\n",
      "[step: 279] loss: 27.07886505126953\n",
      "[step: 280] loss: 26.93724822998047\n",
      "[step: 281] loss: 26.7979793548584\n",
      "[step: 282] loss: 26.684490203857422\n",
      "[step: 283] loss: 26.70686149597168\n",
      "[step: 284] loss: 27.201515197753906\n",
      "[step: 285] loss: 27.825515747070312\n",
      "[step: 286] loss: 26.536334991455078\n",
      "[step: 287] loss: 26.20235824584961\n",
      "[step: 288] loss: 26.8221492767334\n",
      "[step: 289] loss: 25.747512817382812\n",
      "[step: 290] loss: 26.219749450683594\n",
      "[step: 291] loss: 25.88764190673828\n",
      "[step: 292] loss: 25.490707397460938\n",
      "[step: 293] loss: 25.842823028564453\n",
      "[step: 294] loss: 25.103105545043945\n",
      "[step: 295] loss: 25.418384552001953\n",
      "[step: 296] loss: 24.999101638793945\n",
      "[step: 297] loss: 24.881805419921875\n",
      "[step: 298] loss: 24.94155502319336\n",
      "[step: 299] loss: 24.469425201416016\n",
      "[step: 300] loss: 24.671709060668945\n",
      "[step: 301] loss: 24.352827072143555\n",
      "[step: 302] loss: 24.15458106994629\n",
      "[step: 303] loss: 24.260231018066406\n",
      "[step: 304] loss: 23.882829666137695\n",
      "[step: 305] loss: 23.798416137695312\n",
      "[step: 306] loss: 23.829641342163086\n",
      "[step: 307] loss: 23.514564514160156\n",
      "[step: 308] loss: 23.36623191833496\n",
      "[step: 309] loss: 23.42190933227539\n",
      "[step: 310] loss: 23.260425567626953\n",
      "[step: 311] loss: 22.97515869140625\n",
      "[step: 312] loss: 22.899295806884766\n",
      "[step: 313] loss: 22.942867279052734\n",
      "[step: 314] loss: 22.863811492919922\n",
      "[step: 315] loss: 22.67254638671875\n",
      "[step: 316] loss: 22.438945770263672\n",
      "[step: 317] loss: 22.27497673034668\n",
      "[step: 318] loss: 22.193140029907227\n",
      "[step: 319] loss: 22.1798152923584\n",
      "[step: 320] loss: 22.275495529174805\n",
      "[step: 321] loss: 22.522016525268555\n",
      "[step: 322] loss: 23.108779907226562\n",
      "[step: 323] loss: 23.05891227722168\n",
      "[step: 324] loss: 22.370559692382812\n",
      "[step: 325] loss: 21.505142211914062\n",
      "[step: 326] loss: 21.94051742553711\n",
      "[step: 327] loss: 22.344451904296875\n",
      "[step: 328] loss: 21.433414459228516\n",
      "[step: 329] loss: 21.377002716064453\n",
      "[step: 330] loss: 21.833511352539062\n",
      "[step: 331] loss: 21.200857162475586\n",
      "[step: 332] loss: 21.078495025634766\n",
      "[step: 333] loss: 21.40383529663086\n",
      "[step: 334] loss: 20.92528533935547\n",
      "[step: 335] loss: 20.823984146118164\n",
      "[step: 336] loss: 21.043792724609375\n",
      "[step: 337] loss: 20.675048828125\n",
      "[step: 338] loss: 20.568222045898438\n",
      "[step: 339] loss: 20.724056243896484\n",
      "[step: 340] loss: 20.46143341064453\n",
      "[step: 341] loss: 20.303329467773438\n",
      "[step: 342] loss: 20.407352447509766\n",
      "[step: 343] loss: 20.278846740722656\n",
      "[step: 344] loss: 20.071517944335938\n",
      "[step: 345] loss: 20.068538665771484\n",
      "[step: 346] loss: 20.078510284423828\n",
      "[step: 347] loss: 19.939409255981445\n",
      "[step: 348] loss: 19.786602020263672\n",
      "[step: 349] loss: 19.766101837158203\n",
      "[step: 350] loss: 19.776714324951172\n",
      "[step: 351] loss: 19.684770584106445\n",
      "[step: 352] loss: 19.546009063720703\n",
      "[step: 353] loss: 19.44213104248047\n",
      "[step: 354] loss: 19.403751373291016\n",
      "[step: 355] loss: 19.395721435546875\n",
      "[step: 356] loss: 19.369007110595703\n",
      "[step: 357] loss: 19.325979232788086\n",
      "[step: 358] loss: 19.245840072631836\n",
      "[step: 359] loss: 19.165857315063477\n",
      "[step: 360] loss: 19.07497215270996\n",
      "[step: 361] loss: 18.99722671508789\n",
      "[step: 362] loss: 18.923080444335938\n",
      "[step: 363] loss: 18.865596771240234\n",
      "[step: 364] loss: 18.82101821899414\n",
      "[step: 365] loss: 18.81591796875\n",
      "[step: 366] loss: 18.862003326416016\n",
      "[step: 367] loss: 19.072864532470703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 368] loss: 19.397876739501953\n",
      "[step: 369] loss: 20.04914093017578\n",
      "[step: 370] loss: 19.741514205932617\n",
      "[step: 371] loss: 18.994300842285156\n",
      "[step: 372] loss: 18.27975082397461\n",
      "[step: 373] loss: 18.695247650146484\n",
      "[step: 374] loss: 19.116695404052734\n",
      "[step: 375] loss: 18.35279083251953\n",
      "[step: 376] loss: 18.167510986328125\n",
      "[step: 377] loss: 18.611324310302734\n",
      "[step: 378] loss: 18.246286392211914\n",
      "[step: 379] loss: 17.937503814697266\n",
      "[step: 380] loss: 18.196910858154297\n",
      "[step: 381] loss: 18.058807373046875\n",
      "[step: 382] loss: 17.771461486816406\n",
      "[step: 383] loss: 17.875484466552734\n",
      "[step: 384] loss: 17.868480682373047\n",
      "[step: 385] loss: 17.63646125793457\n",
      "[step: 386] loss: 17.60134506225586\n",
      "[step: 387] loss: 17.664196014404297\n",
      "[step: 388] loss: 17.53753089904785\n",
      "[step: 389] loss: 17.388795852661133\n",
      "[step: 390] loss: 17.41407012939453\n",
      "[step: 391] loss: 17.416757583618164\n",
      "[step: 392] loss: 17.273893356323242\n",
      "[step: 393] loss: 17.17082405090332\n",
      "[step: 394] loss: 17.1684627532959\n",
      "[step: 395] loss: 17.154781341552734\n",
      "[step: 396] loss: 17.076805114746094\n",
      "[step: 397] loss: 16.96605110168457\n",
      "[step: 398] loss: 16.89413833618164\n",
      "[step: 399] loss: 16.870988845825195\n",
      "[step: 400] loss: 16.85640525817871\n",
      "[step: 401] loss: 16.817808151245117\n",
      "[step: 402] loss: 16.738618850708008\n",
      "[step: 403] loss: 16.651344299316406\n",
      "[step: 404] loss: 16.57176971435547\n",
      "[step: 405] loss: 16.50631332397461\n",
      "[step: 406] loss: 16.452411651611328\n",
      "[step: 407] loss: 16.410518646240234\n",
      "[step: 408] loss: 16.381885528564453\n",
      "[step: 409] loss: 16.366865158081055\n",
      "[step: 410] loss: 16.38998794555664\n",
      "[step: 411] loss: 16.469715118408203\n",
      "[step: 412] loss: 16.71539306640625\n",
      "[step: 413] loss: 17.010169982910156\n",
      "[step: 414] loss: 17.46050262451172\n",
      "[step: 415] loss: 17.10454750061035\n",
      "[step: 416] loss: 16.535606384277344\n",
      "[step: 417] loss: 15.923789978027344\n",
      "[step: 418] loss: 16.090381622314453\n",
      "[step: 419] loss: 16.532611846923828\n",
      "[step: 420] loss: 16.14894676208496\n",
      "[step: 421] loss: 15.711686134338379\n",
      "[step: 422] loss: 15.840411186218262\n",
      "[step: 423] loss: 15.980053901672363\n",
      "[step: 424] loss: 15.753809928894043\n",
      "[step: 425] loss: 15.535524368286133\n",
      "[step: 426] loss: 15.636387825012207\n",
      "[step: 427] loss: 15.68746566772461\n",
      "[step: 428] loss: 15.476253509521484\n",
      "[step: 429] loss: 15.351785659790039\n",
      "[step: 430] loss: 15.406025886535645\n",
      "[step: 431] loss: 15.407550811767578\n",
      "[step: 432] loss: 15.267985343933105\n",
      "[step: 433] loss: 15.133679389953613\n",
      "[step: 434] loss: 15.15505599975586\n",
      "[step: 435] loss: 15.190621376037598\n",
      "[step: 436] loss: 15.080196380615234\n",
      "[step: 437] loss: 14.94874382019043\n",
      "[step: 438] loss: 14.911725997924805\n",
      "[step: 439] loss: 14.921547889709473\n",
      "[step: 440] loss: 14.903066635131836\n",
      "[step: 441] loss: 14.831402778625488\n",
      "[step: 442] loss: 14.74052619934082\n",
      "[step: 443] loss: 14.658987045288086\n",
      "[step: 444] loss: 14.621786117553711\n",
      "[step: 445] loss: 14.615747451782227\n",
      "[step: 446] loss: 14.597463607788086\n",
      "[step: 447] loss: 14.562311172485352\n",
      "[step: 448] loss: 14.510248184204102\n",
      "[step: 449] loss: 14.45753002166748\n",
      "[step: 450] loss: 14.391422271728516\n",
      "[step: 451] loss: 14.322072982788086\n",
      "[step: 452] loss: 14.257987022399902\n",
      "[step: 453] loss: 14.204168319702148\n",
      "[step: 454] loss: 14.151925086975098\n",
      "[step: 455] loss: 14.098278045654297\n",
      "[step: 456] loss: 14.045829772949219\n",
      "[step: 457] loss: 13.997726440429688\n",
      "[step: 458] loss: 13.95196533203125\n",
      "[step: 459] loss: 13.905183792114258\n",
      "[step: 460] loss: 13.856700897216797\n",
      "[step: 461] loss: 13.809407234191895\n",
      "[step: 462] loss: 13.768400192260742\n",
      "[step: 463] loss: 13.743985176086426\n",
      "[step: 464] loss: 13.76042366027832\n",
      "[step: 465] loss: 13.904382705688477\n",
      "[step: 466] loss: 14.338871002197266\n",
      "[step: 467] loss: 15.579168319702148\n",
      "[step: 468] loss: 16.618663787841797\n",
      "[step: 469] loss: 17.023208618164062\n",
      "[step: 470] loss: 14.10993480682373\n",
      "[step: 471] loss: 14.169410705566406\n",
      "[step: 472] loss: 15.242317199707031\n",
      "[step: 473] loss: 13.928387641906738\n",
      "[step: 474] loss: 13.97195816040039\n",
      "[step: 475] loss: 14.148397445678711\n",
      "[step: 476] loss: 13.77479362487793\n",
      "[step: 477] loss: 13.60482406616211\n",
      "[step: 478] loss: 13.773653030395508\n",
      "[step: 479] loss: 13.463171005249023\n",
      "[step: 480] loss: 13.38433837890625\n",
      "[step: 481] loss: 13.640453338623047\n",
      "[step: 482] loss: 13.03237533569336\n",
      "[step: 483] loss: 13.538091659545898\n",
      "[step: 484] loss: 13.098715782165527\n",
      "[step: 485] loss: 13.101082801818848\n",
      "[step: 486] loss: 13.189204216003418\n",
      "[step: 487] loss: 12.89436149597168\n",
      "[step: 488] loss: 12.973081588745117\n",
      "[step: 489] loss: 12.874628067016602\n",
      "[step: 490] loss: 12.807035446166992\n",
      "[step: 491] loss: 12.708284378051758\n",
      "[step: 492] loss: 12.783966064453125\n",
      "[step: 493] loss: 12.561530113220215\n",
      "[step: 494] loss: 12.604628562927246\n",
      "[step: 495] loss: 12.57963752746582\n",
      "[step: 496] loss: 12.405263900756836\n",
      "[step: 497] loss: 12.470829010009766\n",
      "[step: 498] loss: 12.370226860046387\n",
      "[step: 499] loss: 12.316875457763672\n",
      "[step: 500] loss: 12.27126693725586\n",
      "[step: 501] loss: 12.231124877929688\n",
      "[step: 502] loss: 12.202118873596191\n",
      "[step: 503] loss: 12.090597152709961\n",
      "[step: 504] loss: 12.095539093017578\n",
      "[step: 505] loss: 12.064823150634766\n",
      "[step: 506] loss: 11.957637786865234\n",
      "[step: 507] loss: 11.937742233276367\n",
      "[step: 508] loss: 11.90279769897461\n",
      "[step: 509] loss: 11.843866348266602\n",
      "[step: 510] loss: 11.800431251525879\n",
      "[step: 511] loss: 11.730693817138672\n",
      "[step: 512] loss: 11.693103790283203\n",
      "[step: 513] loss: 11.673308372497559\n",
      "[step: 514] loss: 11.604850769042969\n",
      "[step: 515] loss: 11.539804458618164\n",
      "[step: 516] loss: 11.503143310546875\n",
      "[step: 517] loss: 11.455801010131836\n",
      "[step: 518] loss: 11.40590763092041\n",
      "[step: 519] loss: 11.36990737915039\n",
      "[step: 520] loss: 11.326244354248047\n",
      "[step: 521] loss: 11.264066696166992\n",
      "[step: 522] loss: 11.207931518554688\n",
      "[step: 523] loss: 11.161428451538086\n",
      "[step: 524] loss: 11.110443115234375\n",
      "[step: 525] loss: 11.053241729736328\n",
      "[step: 526] loss: 11.000794410705566\n",
      "[step: 527] loss: 10.954862594604492\n",
      "[step: 528] loss: 10.90746784210205\n",
      "[step: 529] loss: 10.854706764221191\n",
      "[step: 530] loss: 10.800054550170898\n",
      "[step: 531] loss: 10.748623847961426\n",
      "[step: 532] loss: 10.70302963256836\n",
      "[step: 533] loss: 10.668734550476074\n",
      "[step: 534] loss: 10.671407699584961\n",
      "[step: 535] loss: 10.840169906616211\n",
      "[step: 536] loss: 11.54488754272461\n",
      "[step: 537] loss: 14.507511138916016\n",
      "[step: 538] loss: 15.841747283935547\n",
      "[step: 539] loss: 17.296167373657227\n",
      "[step: 540] loss: 11.495567321777344\n",
      "[step: 541] loss: 15.816803932189941\n",
      "[step: 542] loss: 15.50467300415039\n",
      "[step: 543] loss: 13.358266830444336\n",
      "[step: 544] loss: 14.384324073791504\n",
      "[step: 545] loss: 12.389059066772461\n",
      "[step: 546] loss: 12.863907814025879\n",
      "[step: 547] loss: 12.327219009399414\n",
      "[step: 548] loss: 12.12459945678711\n",
      "[step: 549] loss: 12.02490234375\n",
      "[step: 550] loss: 11.375335693359375\n",
      "[step: 551] loss: 12.073058128356934\n",
      "[step: 552] loss: 10.747997283935547\n",
      "[step: 553] loss: 11.940549850463867\n",
      "[step: 554] loss: 10.474275588989258\n",
      "[step: 555] loss: 11.542228698730469\n",
      "[step: 556] loss: 10.520833969116211\n",
      "[step: 557] loss: 11.015565872192383\n",
      "[step: 558] loss: 10.581670761108398\n",
      "[step: 559] loss: 10.583169937133789\n",
      "[step: 560] loss: 10.633150100708008\n",
      "[step: 561] loss: 10.224285125732422\n",
      "[step: 562] loss: 10.57844066619873\n",
      "[step: 563] loss: 10.043628692626953\n",
      "[step: 564] loss: 10.420551300048828\n",
      "[step: 565] loss: 9.890257835388184\n",
      "[step: 566] loss: 10.285896301269531\n",
      "[step: 567] loss: 9.787010192871094\n",
      "[step: 568] loss: 10.084251403808594\n",
      "[step: 569] loss: 9.708267211914062\n",
      "[step: 570] loss: 9.92198371887207\n",
      "[step: 571] loss: 9.632219314575195\n",
      "[step: 572] loss: 9.745328903198242\n",
      "[step: 573] loss: 9.55414867401123\n",
      "[step: 574] loss: 9.61134147644043\n",
      "[step: 575] loss: 9.452276229858398\n",
      "[step: 576] loss: 9.496856689453125\n",
      "[step: 577] loss: 9.338037490844727\n",
      "[step: 578] loss: 9.40384292602539\n",
      "[step: 579] loss: 9.224849700927734\n",
      "[step: 580] loss: 9.2999906539917\n",
      "[step: 581] loss: 9.134153366088867\n",
      "[step: 582] loss: 9.179082870483398\n",
      "[step: 583] loss: 9.063212394714355\n",
      "[step: 584] loss: 9.050291061401367\n",
      "[step: 585] loss: 8.990756034851074\n",
      "[step: 586] loss: 8.935068130493164\n",
      "[step: 587] loss: 8.910970687866211\n",
      "[step: 588] loss: 8.829793930053711\n",
      "[step: 589] loss: 8.82685661315918\n",
      "[step: 590] loss: 8.738336563110352\n",
      "[step: 591] loss: 8.718276977539062\n",
      "[step: 592] loss: 8.670230865478516\n",
      "[step: 593] loss: 8.601776123046875\n",
      "[step: 594] loss: 8.589913368225098\n",
      "[step: 595] loss: 8.51799201965332\n",
      "[step: 596] loss: 8.480457305908203\n",
      "[step: 597] loss: 8.447869300842285\n",
      "[step: 598] loss: 8.383148193359375\n",
      "[step: 599] loss: 8.35363483428955\n",
      "[step: 600] loss: 8.306674003601074\n",
      "[step: 601] loss: 8.255265235900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 602] loss: 8.222126960754395\n",
      "[step: 603] loss: 8.171756744384766\n",
      "[step: 604] loss: 8.125272750854492\n",
      "[step: 605] loss: 8.092018127441406\n",
      "[step: 606] loss: 8.040943145751953\n",
      "[step: 607] loss: 7.994179725646973\n",
      "[step: 608] loss: 7.960443496704102\n",
      "[step: 609] loss: 7.915337562561035\n",
      "[step: 610] loss: 7.864945888519287\n",
      "[step: 611] loss: 7.827633380889893\n",
      "[step: 612] loss: 7.789102554321289\n",
      "[step: 613] loss: 7.741637229919434\n",
      "[step: 614] loss: 7.697465896606445\n",
      "[step: 615] loss: 7.660174369812012\n",
      "[step: 616] loss: 7.6189165115356445\n",
      "[step: 617] loss: 7.573670387268066\n",
      "[step: 618] loss: 7.531538963317871\n",
      "[step: 619] loss: 7.493630409240723\n",
      "[step: 620] loss: 7.453948020935059\n",
      "[step: 621] loss: 7.410836219787598\n",
      "[step: 622] loss: 7.3679914474487305\n",
      "[step: 623] loss: 7.328219413757324\n",
      "[step: 624] loss: 7.289896011352539\n",
      "[step: 625] loss: 7.250720977783203\n",
      "[step: 626] loss: 7.210028648376465\n",
      "[step: 627] loss: 7.169411659240723\n",
      "[step: 628] loss: 7.130028247833252\n",
      "[step: 629] loss: 7.092294692993164\n",
      "[step: 630] loss: 7.055707931518555\n",
      "[step: 631] loss: 7.020411491394043\n",
      "[step: 632] loss: 6.987448692321777\n",
      "[step: 633] loss: 6.961654186248779\n",
      "[step: 634] loss: 6.95177698135376\n",
      "[step: 635] loss: 6.99171257019043\n",
      "[step: 636] loss: 7.130803108215332\n",
      "[step: 637] loss: 7.640748023986816\n",
      "[step: 638] loss: 8.454183578491211\n",
      "[step: 639] loss: 10.895941734313965\n",
      "[step: 640] loss: 9.509037017822266\n",
      "[step: 641] loss: 8.439431190490723\n",
      "[step: 642] loss: 6.844366550445557\n",
      "[step: 643] loss: 7.688271522521973\n",
      "[step: 644] loss: 9.19636344909668\n",
      "[step: 645] loss: 6.998086452484131\n",
      "[step: 646] loss: 7.348850727081299\n",
      "[step: 647] loss: 8.812433242797852\n",
      "[step: 648] loss: 6.6816864013671875\n",
      "[step: 649] loss: 7.81882905960083\n",
      "[step: 650] loss: 8.86536979675293\n",
      "[step: 651] loss: 6.554103851318359\n",
      "[step: 652] loss: 9.044608116149902\n",
      "[step: 653] loss: 8.633697509765625\n",
      "[step: 654] loss: 7.121834754943848\n",
      "[step: 655] loss: 9.629984855651855\n",
      "[step: 656] loss: 6.960738182067871\n",
      "[step: 657] loss: 7.90798282623291\n",
      "[step: 658] loss: 7.190090179443359\n",
      "[step: 659] loss: 6.797537803649902\n",
      "[step: 660] loss: 7.468897819519043\n",
      "[step: 661] loss: 6.487698078155518\n",
      "[step: 662] loss: 7.542316436767578\n",
      "[step: 663] loss: 6.285630226135254\n",
      "[step: 664] loss: 7.066253185272217\n",
      "[step: 665] loss: 6.266772270202637\n",
      "[step: 666] loss: 6.764901161193848\n",
      "[step: 667] loss: 6.454019069671631\n",
      "[step: 668] loss: 6.415177822113037\n",
      "[step: 669] loss: 6.5023393630981445\n",
      "[step: 670] loss: 6.103745460510254\n",
      "[step: 671] loss: 6.448978424072266\n",
      "[step: 672] loss: 5.990678787231445\n",
      "[step: 673] loss: 6.362634658813477\n",
      "[step: 674] loss: 6.010919570922852\n",
      "[step: 675] loss: 6.111523628234863\n",
      "[step: 676] loss: 6.042750358581543\n",
      "[step: 677] loss: 5.8909502029418945\n",
      "[step: 678] loss: 6.03938102722168\n",
      "[step: 679] loss: 5.784577369689941\n",
      "[step: 680] loss: 5.947443008422852\n",
      "[step: 681] loss: 5.799275875091553\n",
      "[step: 682] loss: 5.769363880157471\n",
      "[step: 683] loss: 5.816010475158691\n",
      "[step: 684] loss: 5.64500617980957\n",
      "[step: 685] loss: 5.72667121887207\n",
      "[step: 686] loss: 5.63487434387207\n",
      "[step: 687] loss: 5.588993072509766\n",
      "[step: 688] loss: 5.625731468200684\n",
      "[step: 689] loss: 5.5138421058654785\n",
      "[step: 690] loss: 5.5417938232421875\n",
      "[step: 691] loss: 5.517838954925537\n",
      "[step: 692] loss: 5.438085079193115\n",
      "[step: 693] loss: 5.47174596786499\n",
      "[step: 694] loss: 5.420025825500488\n",
      "[step: 695] loss: 5.366812705993652\n",
      "[step: 696] loss: 5.391853332519531\n",
      "[step: 697] loss: 5.339045524597168\n",
      "[step: 698] loss: 5.296881675720215\n",
      "[step: 699] loss: 5.311387062072754\n",
      "[step: 700] loss: 5.26549768447876\n",
      "[step: 701] loss: 5.223946571350098\n",
      "[step: 702] loss: 5.231545448303223\n",
      "[step: 703] loss: 5.198402404785156\n",
      "[step: 704] loss: 5.154373645782471\n",
      "[step: 705] loss: 5.15474796295166\n",
      "[step: 706] loss: 5.13504695892334\n",
      "[step: 707] loss: 5.09077787399292\n",
      "[step: 708] loss: 5.0799455642700195\n",
      "[step: 709] loss: 5.071093559265137\n",
      "[step: 710] loss: 5.034082412719727\n",
      "[step: 711] loss: 5.011931419372559\n",
      "[step: 712] loss: 5.008833408355713\n",
      "[step: 713] loss: 4.990834712982178\n",
      "[step: 714] loss: 4.976276874542236\n",
      "[step: 715] loss: 4.993729591369629\n",
      "[step: 716] loss: 5.047065734863281\n",
      "[step: 717] loss: 5.164651393890381\n",
      "[step: 718] loss: 5.428427696228027\n",
      "[step: 719] loss: 5.836625099182129\n",
      "[step: 720] loss: 6.460353851318359\n",
      "[step: 721] loss: 6.32705020904541\n",
      "[step: 722] loss: 5.703010559082031\n",
      "[step: 723] loss: 4.8802289962768555\n",
      "[step: 724] loss: 4.920958518981934\n",
      "[step: 725] loss: 5.485818386077881\n",
      "[step: 726] loss: 5.4256391525268555\n",
      "[step: 727] loss: 4.905611991882324\n",
      "[step: 728] loss: 4.738744735717773\n",
      "[step: 729] loss: 5.070102691650391\n",
      "[step: 730] loss: 5.147190093994141\n",
      "[step: 731] loss: 4.765698432922363\n",
      "[step: 732] loss: 4.6903839111328125\n",
      "[step: 733] loss: 4.896491050720215\n",
      "[step: 734] loss: 4.841303825378418\n",
      "[step: 735] loss: 4.649211883544922\n",
      "[step: 736] loss: 4.631874084472656\n",
      "[step: 737] loss: 4.72859525680542\n",
      "[step: 738] loss: 4.6960368156433105\n",
      "[step: 739] loss: 4.570714473724365\n",
      "[step: 740] loss: 4.5644731521606445\n",
      "[step: 741] loss: 4.606056213378906\n",
      "[step: 742] loss: 4.566357612609863\n",
      "[step: 743] loss: 4.505722999572754\n",
      "[step: 744] loss: 4.473479747772217\n",
      "[step: 745] loss: 4.483970642089844\n",
      "[step: 746] loss: 4.488829612731934\n",
      "[step: 747] loss: 4.437268257141113\n",
      "[step: 748] loss: 4.390360355377197\n",
      "[step: 749] loss: 4.3858747482299805\n",
      "[step: 750] loss: 4.398943901062012\n",
      "[step: 751] loss: 4.378426551818848\n",
      "[step: 752] loss: 4.322653293609619\n",
      "[step: 753] loss: 4.297195911407471\n",
      "[step: 754] loss: 4.310930252075195\n",
      "[step: 755] loss: 4.306100368499756\n",
      "[step: 756] loss: 4.271267890930176\n",
      "[step: 757] loss: 4.232072353363037\n",
      "[step: 758] loss: 4.221382141113281\n",
      "[step: 759] loss: 4.223061561584473\n",
      "[step: 760] loss: 4.209510803222656\n",
      "[step: 761] loss: 4.18567419052124\n",
      "[step: 762] loss: 4.162200927734375\n",
      "[step: 763] loss: 4.145707607269287\n",
      "[step: 764] loss: 4.130501747131348\n",
      "[step: 765] loss: 4.116127967834473\n",
      "[step: 766] loss: 4.104768753051758\n",
      "[step: 767] loss: 4.094015121459961\n",
      "[step: 768] loss: 4.077859401702881\n",
      "[step: 769] loss: 4.056371688842773\n",
      "[step: 770] loss: 4.035262584686279\n",
      "[step: 771] loss: 4.0195465087890625\n",
      "[step: 772] loss: 4.007931709289551\n",
      "[step: 773] loss: 3.996060609817505\n",
      "[step: 774] loss: 3.982799530029297\n",
      "[step: 775] loss: 3.969433546066284\n",
      "[step: 776] loss: 3.9571430683135986\n",
      "[step: 777] loss: 3.945514440536499\n",
      "[step: 778] loss: 3.932138204574585\n",
      "[step: 779] loss: 3.9185445308685303\n",
      "[step: 780] loss: 3.9047391414642334\n",
      "[step: 781] loss: 3.8938064575195312\n",
      "[step: 782] loss: 3.8845977783203125\n",
      "[step: 783] loss: 3.879390001296997\n",
      "[step: 784] loss: 3.8772501945495605\n",
      "[step: 785] loss: 3.8874971866607666\n",
      "[step: 786] loss: 3.9092016220092773\n",
      "[step: 787] loss: 3.9753947257995605\n",
      "[step: 788] loss: 4.06265115737915\n",
      "[step: 789] loss: 4.295186996459961\n",
      "[step: 790] loss: 4.473642826080322\n",
      "[step: 791] loss: 4.949676990509033\n",
      "[step: 792] loss: 4.809995651245117\n",
      "[step: 793] loss: 4.7852654457092285\n",
      "[step: 794] loss: 4.115708827972412\n",
      "[step: 795] loss: 3.7399868965148926\n",
      "[step: 796] loss: 3.758476734161377\n",
      "[step: 797] loss: 4.024391174316406\n",
      "[step: 798] loss: 4.290371417999268\n",
      "[step: 799] loss: 4.061034679412842\n",
      "[step: 800] loss: 3.800917863845825\n",
      "[step: 801] loss: 3.648592233657837\n",
      "[step: 802] loss: 3.732516288757324\n",
      "[step: 803] loss: 3.8861138820648193\n",
      "[step: 804] loss: 3.8562729358673096\n",
      "[step: 805] loss: 3.760326385498047\n",
      "[step: 806] loss: 3.626255512237549\n",
      "[step: 807] loss: 3.60658860206604\n",
      "[step: 808] loss: 3.652876615524292\n",
      "[step: 809] loss: 3.673797845840454\n",
      "[step: 810] loss: 3.668062686920166\n",
      "[step: 811] loss: 3.5932798385620117\n",
      "[step: 812] loss: 3.536914348602295\n",
      "[step: 813] loss: 3.5125741958618164\n",
      "[step: 814] loss: 3.525041103363037\n",
      "[step: 815] loss: 3.5563297271728516\n",
      "[step: 816] loss: 3.546053409576416\n",
      "[step: 817] loss: 3.510061264038086\n",
      "[step: 818] loss: 3.448962688446045\n",
      "[step: 819] loss: 3.413562536239624\n",
      "[step: 820] loss: 3.414212226867676\n",
      "[step: 821] loss: 3.4319028854370117\n",
      "[step: 822] loss: 3.444148063659668\n",
      "[step: 823] loss: 3.424631118774414\n",
      "[step: 824] loss: 3.3983325958251953\n",
      "[step: 825] loss: 3.3676326274871826\n",
      "[step: 826] loss: 3.3472790718078613\n",
      "[step: 827] loss: 3.3321433067321777\n",
      "[step: 828] loss: 3.3175621032714844\n",
      "[step: 829] loss: 3.303112268447876\n",
      "[step: 830] loss: 3.2914209365844727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 831] loss: 3.2859177589416504\n",
      "[step: 832] loss: 3.2847561836242676\n",
      "[step: 833] loss: 3.2874016761779785\n",
      "[step: 834] loss: 3.287259340286255\n",
      "[step: 835] loss: 3.2908732891082764\n",
      "[step: 836] loss: 3.292680025100708\n",
      "[step: 837] loss: 3.3131113052368164\n",
      "[step: 838] loss: 3.3322219848632812\n",
      "[step: 839] loss: 3.3987631797790527\n",
      "[step: 840] loss: 3.441761016845703\n",
      "[step: 841] loss: 3.5883026123046875\n",
      "[step: 842] loss: 3.6212830543518066\n",
      "[step: 843] loss: 3.81996750831604\n",
      "[step: 844] loss: 3.713852882385254\n",
      "[step: 845] loss: 3.742431402206421\n",
      "[step: 846] loss: 3.472991943359375\n",
      "[step: 847] loss: 3.292815685272217\n",
      "[step: 848] loss: 3.1338913440704346\n",
      "[step: 849] loss: 3.083726406097412\n",
      "[step: 850] loss: 3.1270699501037598\n",
      "[step: 851] loss: 3.21067476272583\n",
      "[step: 852] loss: 3.3202035427093506\n",
      "[step: 853] loss: 3.319389581680298\n",
      "[step: 854] loss: 3.3194758892059326\n",
      "[step: 855] loss: 3.1967804431915283\n",
      "[step: 856] loss: 3.1026341915130615\n",
      "[step: 857] loss: 3.0219459533691406\n",
      "[step: 858] loss: 2.9935030937194824\n",
      "[step: 859] loss: 3.0058858394622803\n",
      "[step: 860] loss: 3.0364537239074707\n",
      "[step: 861] loss: 3.073840856552124\n",
      "[step: 862] loss: 3.078688621520996\n",
      "[step: 863] loss: 3.0872421264648438\n",
      "[step: 864] loss: 3.049391746520996\n",
      "[step: 865] loss: 3.0243043899536133\n",
      "[step: 866] loss: 2.9769227504730225\n",
      "[step: 867] loss: 2.9424867630004883\n",
      "[step: 868] loss: 2.908461570739746\n",
      "[step: 869] loss: 2.8842051029205322\n",
      "[step: 870] loss: 2.867616653442383\n",
      "[step: 871] loss: 2.8583831787109375\n",
      "[step: 872] loss: 2.85508394241333\n",
      "[step: 873] loss: 2.855846405029297\n",
      "[step: 874] loss: 2.862316608428955\n",
      "[step: 875] loss: 2.8703815937042236\n",
      "[step: 876] loss: 2.8937249183654785\n",
      "[step: 877] loss: 2.917257308959961\n",
      "[step: 878] loss: 2.986388683319092\n",
      "[step: 879] loss: 3.042597770690918\n",
      "[step: 880] loss: 3.216953754425049\n",
      "[step: 881] loss: 3.2883031368255615\n",
      "[step: 882] loss: 3.577425479888916\n",
      "[step: 883] loss: 3.4780266284942627\n",
      "[step: 884] loss: 3.5629239082336426\n",
      "[step: 885] loss: 3.2040412425994873\n",
      "[step: 886] loss: 2.962559700012207\n",
      "[step: 887] loss: 2.7679295539855957\n",
      "[step: 888] loss: 2.733138084411621\n",
      "[step: 889] loss: 2.8344216346740723\n",
      "[step: 890] loss: 2.9496448040008545\n",
      "[step: 891] loss: 3.0932819843292236\n",
      "[step: 892] loss: 2.9976046085357666\n",
      "[step: 893] loss: 2.9049036502838135\n",
      "[step: 894] loss: 2.7333903312683105\n",
      "[step: 895] loss: 2.6453070640563965\n",
      "[step: 896] loss: 2.64803147315979\n",
      "[step: 897] loss: 2.7086009979248047\n",
      "[step: 898] loss: 2.786109209060669\n",
      "[step: 899] loss: 2.7850308418273926\n",
      "[step: 900] loss: 2.765928268432617\n",
      "[step: 901] loss: 2.6784462928771973\n",
      "[step: 902] loss: 2.614598035812378\n",
      "[step: 903] loss: 2.5780935287475586\n",
      "[step: 904] loss: 2.577716827392578\n",
      "[step: 905] loss: 2.598257303237915\n",
      "[step: 906] loss: 2.619093894958496\n",
      "[step: 907] loss: 2.6360435485839844\n",
      "[step: 908] loss: 2.624795436859131\n",
      "[step: 909] loss: 2.6124911308288574\n",
      "[step: 910] loss: 2.5777435302734375\n",
      "[step: 911] loss: 2.553142547607422\n",
      "[step: 912] loss: 2.527149200439453\n",
      "[step: 913] loss: 2.5140416622161865\n",
      "[step: 914] loss: 2.5057921409606934\n",
      "[step: 915] loss: 2.506552219390869\n",
      "[step: 916] loss: 2.507767915725708\n",
      "[step: 917] loss: 2.5157833099365234\n",
      "[step: 918] loss: 2.518404960632324\n",
      "[step: 919] loss: 2.5285089015960693\n",
      "[step: 920] loss: 2.5280797481536865\n",
      "[step: 921] loss: 2.537292003631592\n",
      "[step: 922] loss: 2.529994487762451\n",
      "[step: 923] loss: 2.5326385498046875\n",
      "[step: 924] loss: 2.5141944885253906\n",
      "[step: 925] loss: 2.504570960998535\n",
      "[step: 926] loss: 2.474961757659912\n",
      "[step: 927] loss: 2.45320463180542\n",
      "[step: 928] loss: 2.42094087600708\n",
      "[step: 929] loss: 2.3975636959075928\n",
      "[step: 930] loss: 2.3754374980926514\n",
      "[step: 931] loss: 2.367069721221924\n",
      "[step: 932] loss: 2.3715648651123047\n",
      "[step: 933] loss: 2.4120965003967285\n",
      "[step: 934] loss: 2.4886178970336914\n",
      "[step: 935] loss: 2.7137858867645264\n",
      "[step: 936] loss: 2.971898078918457\n",
      "[step: 937] loss: 3.769768238067627\n",
      "[step: 938] loss: 3.7411601543426514\n",
      "[step: 939] loss: 4.3152079582214355\n",
      "[step: 940] loss: 3.0315308570861816\n",
      "[step: 941] loss: 2.391735792160034\n",
      "[step: 942] loss: 2.4011898040771484\n",
      "[step: 943] loss: 2.844392776489258\n",
      "[step: 944] loss: 3.4805195331573486\n",
      "[step: 945] loss: 2.8341519832611084\n",
      "[step: 946] loss: 2.3665363788604736\n",
      "[step: 947] loss: 2.2616631984710693\n",
      "[step: 948] loss: 2.550990581512451\n",
      "[step: 949] loss: 2.9015047550201416\n",
      "[step: 950] loss: 2.5692169666290283\n",
      "[step: 951] loss: 2.279777765274048\n",
      "[step: 952] loss: 2.2392325401306152\n",
      "[step: 953] loss: 2.42197847366333\n",
      "[step: 954] loss: 2.5616350173950195\n",
      "[step: 955] loss: 2.3415422439575195\n",
      "[step: 956] loss: 2.179591178894043\n",
      "[step: 957] loss: 2.216167449951172\n",
      "[step: 958] loss: 2.33461856842041\n",
      "[step: 959] loss: 2.3777027130126953\n",
      "[step: 960] loss: 2.2321457862854004\n",
      "[step: 961] loss: 2.129211902618408\n",
      "[step: 962] loss: 2.150739908218384\n",
      "[step: 963] loss: 2.2193374633789062\n",
      "[step: 964] loss: 2.24704647064209\n",
      "[step: 965] loss: 2.168036460876465\n",
      "[step: 966] loss: 2.0963783264160156\n",
      "[step: 967] loss: 2.0909502506256104\n",
      "[step: 968] loss: 2.129056692123413\n",
      "[step: 969] loss: 2.156644344329834\n",
      "[step: 970] loss: 2.121432065963745\n",
      "[step: 971] loss: 2.0718331336975098\n",
      "[step: 972] loss: 2.0418753623962402\n",
      "[step: 973] loss: 2.0475940704345703\n",
      "[step: 974] loss: 2.0673208236694336\n",
      "[step: 975] loss: 2.0702733993530273\n",
      "[step: 976] loss: 2.0531420707702637\n",
      "[step: 977] loss: 2.0216541290283203\n",
      "[step: 978] loss: 1.999173879623413\n",
      "[step: 979] loss: 1.9925146102905273\n",
      "[step: 980] loss: 1.998044490814209\n",
      "[step: 981] loss: 2.0042524337768555\n",
      "[step: 982] loss: 2.001155376434326\n",
      "[step: 983] loss: 1.9901248216629028\n",
      "[step: 984] loss: 1.9730198383331299\n",
      "[step: 985] loss: 1.957854986190796\n",
      "[step: 986] loss: 1.9490786790847778\n",
      "[step: 987] loss: 1.948163390159607\n",
      "[step: 988] loss: 1.9554197788238525\n",
      "[step: 989] loss: 1.968571424484253\n",
      "[step: 990] loss: 1.9970052242279053\n",
      "[step: 991] loss: 2.0329318046569824\n",
      "[step: 992] loss: 2.120131492614746\n",
      "[step: 993] loss: 2.2193832397460938\n",
      "[step: 994] loss: 2.465475559234619\n",
      "[step: 995] loss: 2.620713472366333\n",
      "[step: 996] loss: 2.972532272338867\n",
      "[step: 997] loss: 2.7588956356048584\n",
      "[step: 998] loss: 2.525991678237915\n",
      "[step: 999] loss: 2.0431408882141113\n"
     ]
    }
   ],
   "source": [
    "LearningModuleRunner(rawArrayDatas,7,forecastDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Bayseian(txs, forecastDay, unit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
