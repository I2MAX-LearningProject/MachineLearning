{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=2*forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list(rawArrayDatas[1])\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list(rawArrayDatas[1][:-forecastDay] )\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-3*forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-3*forecastDay]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-3*forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of 2*forecastDay:  rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "    testY= rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        ####더 좋은 알고리즘 호출\n",
    "        if nameOfBestAlgorithm is 'LSTM':\n",
    "            tf.reset_default_graph()\n",
    "            realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "\n",
    "        elif nameOfBestAlgorithm is 'Bayseian':\n",
    "            realForecastDictionary['Bayseian']=Bayseian(txsForRealForecastBayesian,forecastDay,'day')\n",
    "\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        ####더 좋은 알고리즘 호출\n",
    "        if nameOfBestAlgorithm is 'LSTM':\n",
    "            tf.reset_default_graph()\n",
    "            realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "\n",
    "        elif nameOfBestAlgorithm is 'Bayseian':\n",
    "     \n",
    "            realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        ####더 좋은 알고리즘 호출\n",
    "        if nameOfBestAlgorithm is 'LSTM':\n",
    "            tf.reset_default_graph()\n",
    "            realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "\n",
    "        elif nameOfBestAlgorithm is 'Bayseian':\n",
    "            realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    # tf.reset_default_graph()\n",
    "    # realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay, feature)\n",
    "\n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "    return test_predict[-1].tolist()\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM forecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian forecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 9\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('webMonth.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.1)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4958.0,\n",
       " 8359.2999999999993,\n",
       " 12178.14286,\n",
       " 1735.5357140000001,\n",
       " 2178.5666670000001,\n",
       " 1424.419355,\n",
       " 1237.7142859999999,\n",
       " 1343.4545449999998,\n",
       " 2100.580645,\n",
       " 3135.5999999999999,\n",
       " 2644.517241,\n",
       " 2929.0,\n",
       " 2971.4838709999999,\n",
       " 4590.8709680000002,\n",
       " 2453.4642859999999,\n",
       " 1603.2903229999999,\n",
       " 1760.5,\n",
       " 1483.4838710000001,\n",
       " 1376.5333330000001,\n",
       " 1725.9354840000001,\n",
       " 2844.6451609999999,\n",
       " 5786.4615380000005,\n",
       " 6445.6428569999998,\n",
       " 6952.2142860000004,\n",
       " 7556.0322580000002,\n",
       " 13760.172409999999,\n",
       " 12469.23077,\n",
       " 2277.3225809999999,\n",
       " 2503.9666670000001,\n",
       " 1698.451613,\n",
       " 1425.5357140000001,\n",
       " 1848.6153850000001,\n",
       " 3838.3225810000004,\n",
       " 9605.4666670000006,\n",
       " 6985.6774189999996,\n",
       " 8732.5,\n",
       " 7899.0645159999995,\n",
       " 9540.9032260000004,\n",
       " 3907.1071430000002,\n",
       " 2085.8709680000002,\n",
       " 3015.5666670000001,\n",
       " 1886.193548,\n",
       " 1695.9000000000001,\n",
       " 3470.6451609999999,\n",
       " 4622.2258060000004,\n",
       " 9086.6896550000001,\n",
       " 6864.8666670000002,\n",
       " 7461.9666670000006,\n",
       " 7556.3103449999999,\n",
       " 19610.45161,\n",
       " 25008.793099999999,\n",
       " 24771.03226,\n",
       " 3943.3103450000003,\n",
       " 3136.3870969999998,\n",
       " 2406.8666670000002,\n",
       " 3127.6451609999999,\n",
       " 5017.7741939999996,\n",
       " 13273.733329999999,\n",
       " 9823.8064519999989,\n",
       " 7519.6666670000004,\n",
       " 9169.16129,\n",
       " 13726.903230000002,\n",
       " 4432.5,\n",
       " 2776.5161290000001,\n",
       " 2286.6333329999998,\n",
       " 1913.451613,\n",
       " 1650.9333329999999,\n",
       " 2040.0333329999999,\n",
       " 2734.0645159999999,\n",
       " 13740.266669999999,\n",
       " 10564.35484,\n",
       " 7608.3999999999996,\n",
       " 8706.1935480000011,\n",
       " 26366.56667,\n",
       " 24504.89286,\n",
       " 2169.0322579999997,\n",
       " 1879.0,\n",
       " 2449.5483870000003,\n",
       " 1487.9000000000001,\n",
       " 1903.580645,\n",
       " 3121.333333,\n",
       " 8773.6333329999998,\n",
       " 9429.2903230000011,\n",
       " 4370.5666670000001,\n",
       " 4122.7096770000007,\n",
       " 7644.7419349999991,\n",
       " 4147.0740740000001,\n",
       " 1799.7096770000001,\n",
       " 1405.5,\n",
       " 1648.193548,\n",
       " 1090.5666670000001,\n",
       " 1282.2903229999999,\n",
       " 1804.548387,\n",
       " 4573.1999999999998,\n",
       " 3113.6999999999998,\n",
       " 10477.299999999999,\n",
       " 6659.8709680000002,\n",
       " 6043.3000000000002]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArrayDatas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns=['ds','y']\n",
    "# txs=pd.read_table('walMonth_train.csv', sep=',',header=None,names=columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(txs)\n",
    "# LSTM(txs, 7, features)\n",
    "# Bayseian(txs, 7, 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 85.34269714355469\n",
      "[step: 1] loss: 82.27372741699219\n",
      "[step: 2] loss: 79.27803039550781\n",
      "[step: 3] loss: 76.28092193603516\n",
      "[step: 4] loss: 73.20899963378906\n",
      "[step: 5] loss: 70.00135040283203\n",
      "[step: 6] loss: 66.61135864257812\n",
      "[step: 7] loss: 63.006370544433594\n",
      "[step: 8] loss: 59.174720764160156\n",
      "[step: 9] loss: 55.14341735839844\n",
      "[step: 10] loss: 51.01003646850586\n",
      "[step: 11] loss: 46.99449157714844\n",
      "[step: 12] loss: 43.51350402832031\n",
      "[step: 13] loss: 41.235565185546875\n",
      "[step: 14] loss: 40.80485916137695\n",
      "[step: 15] loss: 41.739646911621094\n",
      "[step: 16] loss: 42.238494873046875\n",
      "[step: 17] loss: 41.29499053955078\n",
      "[step: 18] loss: 39.41954803466797\n",
      "[step: 19] loss: 37.58209991455078\n",
      "[step: 20] loss: 36.416778564453125\n",
      "[step: 21] loss: 36.05183410644531\n",
      "[step: 22] loss: 36.273292541503906\n",
      "[step: 23] loss: 36.75611114501953\n",
      "[step: 24] loss: 37.224727630615234\n",
      "[step: 25] loss: 37.5161247253418\n",
      "[step: 26] loss: 37.57550048828125\n",
      "[step: 27] loss: 37.42603302001953\n",
      "[step: 28] loss: 37.13750076293945\n",
      "[step: 29] loss: 36.79955291748047\n",
      "[step: 30] loss: 36.497657775878906\n",
      "[step: 31] loss: 36.28990936279297\n",
      "[step: 32] loss: 36.18898010253906\n",
      "[step: 33] loss: 36.15922164916992\n",
      "[step: 34] loss: 36.13737106323242\n",
      "[step: 35] loss: 36.06928253173828\n",
      "[step: 36] loss: 35.93873977661133\n",
      "[step: 37] loss: 35.769447326660156\n",
      "[step: 38] loss: 35.603599548339844\n",
      "[step: 39] loss: 35.47640609741211\n",
      "[step: 40] loss: 35.401798248291016\n",
      "[step: 41] loss: 35.37248611450195\n",
      "[step: 42] loss: 35.36859893798828\n",
      "[step: 43] loss: 35.36790466308594\n",
      "[step: 44] loss: 35.35331726074219\n",
      "[step: 45] loss: 35.31654739379883\n",
      "[step: 46] loss: 35.25834274291992\n",
      "[step: 47] loss: 35.18633270263672\n",
      "[step: 48] loss: 35.11142349243164\n",
      "[step: 49] loss: 35.04368591308594\n",
      "[step: 50] loss: 34.98870086669922\n",
      "[step: 51] loss: 34.9456787109375\n",
      "[step: 52] loss: 34.908058166503906\n",
      "[step: 53] loss: 34.86680603027344\n",
      "[step: 54] loss: 34.814735412597656\n",
      "[step: 55] loss: 34.74974060058594\n",
      "[step: 56] loss: 34.67523956298828\n",
      "[step: 57] loss: 34.597694396972656\n",
      "[step: 58] loss: 34.5230598449707\n",
      "[step: 59] loss: 34.45418167114258\n",
      "[step: 60] loss: 34.390174865722656\n",
      "[step: 61] loss: 34.327579498291016\n",
      "[step: 62] loss: 34.26240539550781\n",
      "[step: 63] loss: 34.191932678222656\n",
      "[step: 64] loss: 34.11548614501953\n",
      "[step: 65] loss: 34.03419494628906\n",
      "[step: 66] loss: 33.94990539550781\n",
      "[step: 67] loss: 33.863834381103516\n",
      "[step: 68] loss: 33.77581787109375\n",
      "[step: 69] loss: 33.68437194824219\n",
      "[step: 70] loss: 33.587623596191406\n",
      "[step: 71] loss: 33.484458923339844\n",
      "[step: 72] loss: 33.375\n",
      "[step: 73] loss: 33.260215759277344\n",
      "[step: 74] loss: 33.140968322753906\n",
      "[step: 75] loss: 33.01734161376953\n",
      "[step: 76] loss: 32.88886260986328\n",
      "[step: 77] loss: 32.755279541015625\n",
      "[step: 78] loss: 32.617183685302734\n",
      "[step: 79] loss: 32.47581481933594\n",
      "[step: 80] loss: 32.33219909667969\n",
      "[step: 81] loss: 32.18669128417969\n",
      "[step: 82] loss: 32.03968048095703\n",
      "[step: 83] loss: 31.892902374267578\n",
      "[step: 84] loss: 31.749523162841797\n",
      "[step: 85] loss: 31.612361907958984\n",
      "[step: 86] loss: 31.48258399963379\n",
      "[step: 87] loss: 31.361225128173828\n",
      "[step: 88] loss: 31.251148223876953\n",
      "[step: 89] loss: 31.154937744140625\n",
      "[step: 90] loss: 31.071369171142578\n",
      "[step: 91] loss: 30.997447967529297\n",
      "[step: 92] loss: 30.931629180908203\n",
      "[step: 93] loss: 30.870731353759766\n",
      "[step: 94] loss: 30.809484481811523\n",
      "[step: 95] loss: 30.745460510253906\n",
      "[step: 96] loss: 30.6773738861084\n",
      "[step: 97] loss: 30.602951049804688\n",
      "[step: 98] loss: 30.523296356201172\n",
      "[step: 99] loss: 30.44165802001953\n",
      "[step: 100] loss: 30.359745025634766\n",
      "[step: 101] loss: 30.279802322387695\n",
      "[step: 102] loss: 30.204669952392578\n",
      "[step: 103] loss: 30.135231018066406\n",
      "[step: 104] loss: 30.071752548217773\n",
      "[step: 105] loss: 30.01478385925293\n",
      "[step: 106] loss: 29.963470458984375\n",
      "[step: 107] loss: 29.916324615478516\n",
      "[step: 108] loss: 29.87267303466797\n",
      "[step: 109] loss: 29.831748962402344\n",
      "[step: 110] loss: 29.792617797851562\n",
      "[step: 111] loss: 29.754756927490234\n",
      "[step: 112] loss: 29.717639923095703\n",
      "[step: 113] loss: 29.681034088134766\n",
      "[step: 114] loss: 29.645172119140625\n",
      "[step: 115] loss: 29.60995101928711\n",
      "[step: 116] loss: 29.575441360473633\n",
      "[step: 117] loss: 29.542081832885742\n",
      "[step: 118] loss: 29.509862899780273\n",
      "[step: 119] loss: 29.47878646850586\n",
      "[step: 120] loss: 29.448701858520508\n",
      "[step: 121] loss: 29.419212341308594\n",
      "[step: 122] loss: 29.390178680419922\n",
      "[step: 123] loss: 29.361202239990234\n",
      "[step: 124] loss: 29.33200454711914\n",
      "[step: 125] loss: 29.302555084228516\n",
      "[step: 126] loss: 29.272781372070312\n",
      "[step: 127] loss: 29.242815017700195\n",
      "[step: 128] loss: 29.212669372558594\n",
      "[step: 129] loss: 29.182510375976562\n",
      "[step: 130] loss: 29.152503967285156\n",
      "[step: 131] loss: 29.12265396118164\n",
      "[step: 132] loss: 29.093032836914062\n",
      "[step: 133] loss: 29.063575744628906\n",
      "[step: 134] loss: 29.034278869628906\n",
      "[step: 135] loss: 29.005037307739258\n",
      "[step: 136] loss: 28.97579574584961\n",
      "[step: 137] loss: 28.946548461914062\n",
      "[step: 138] loss: 28.917280197143555\n",
      "[step: 139] loss: 28.888015747070312\n",
      "[step: 140] loss: 28.858715057373047\n",
      "[step: 141] loss: 28.82941436767578\n",
      "[step: 142] loss: 28.800079345703125\n",
      "[step: 143] loss: 28.77068519592285\n",
      "[step: 144] loss: 28.741207122802734\n",
      "[step: 145] loss: 28.711641311645508\n",
      "[step: 146] loss: 28.68197250366211\n",
      "[step: 147] loss: 28.652198791503906\n",
      "[step: 148] loss: 28.622329711914062\n",
      "[step: 149] loss: 28.592365264892578\n",
      "[step: 150] loss: 28.562320709228516\n",
      "[step: 151] loss: 28.532184600830078\n",
      "[step: 152] loss: 28.501968383789062\n",
      "[step: 153] loss: 28.471649169921875\n",
      "[step: 154] loss: 28.44120216369629\n",
      "[step: 155] loss: 28.410606384277344\n",
      "[step: 156] loss: 28.379865646362305\n",
      "[step: 157] loss: 28.348968505859375\n",
      "[step: 158] loss: 28.317930221557617\n",
      "[step: 159] loss: 28.28675079345703\n",
      "[step: 160] loss: 28.25543975830078\n",
      "[step: 161] loss: 28.224008560180664\n",
      "[step: 162] loss: 28.19244384765625\n",
      "[step: 163] loss: 28.16073989868164\n",
      "[step: 164] loss: 28.128889083862305\n",
      "[step: 165] loss: 28.096881866455078\n",
      "[step: 166] loss: 28.064701080322266\n",
      "[step: 167] loss: 28.032337188720703\n",
      "[step: 168] loss: 27.999799728393555\n",
      "[step: 169] loss: 27.967073440551758\n",
      "[step: 170] loss: 27.93415641784668\n",
      "[step: 171] loss: 27.90104866027832\n",
      "[step: 172] loss: 27.86772346496582\n",
      "[step: 173] loss: 27.834182739257812\n",
      "[step: 174] loss: 27.800418853759766\n",
      "[step: 175] loss: 27.766407012939453\n",
      "[step: 176] loss: 27.732162475585938\n",
      "[step: 177] loss: 27.697656631469727\n",
      "[step: 178] loss: 27.662879943847656\n",
      "[step: 179] loss: 27.627819061279297\n",
      "[step: 180] loss: 27.59246063232422\n",
      "[step: 181] loss: 27.556779861450195\n",
      "[step: 182] loss: 27.520771026611328\n",
      "[step: 183] loss: 27.484405517578125\n",
      "[step: 184] loss: 27.44766616821289\n",
      "[step: 185] loss: 27.410520553588867\n",
      "[step: 186] loss: 27.372941970825195\n",
      "[step: 187] loss: 27.334896087646484\n",
      "[step: 188] loss: 27.296358108520508\n",
      "[step: 189] loss: 27.25726890563965\n",
      "[step: 190] loss: 27.217609405517578\n",
      "[step: 191] loss: 27.177318572998047\n",
      "[step: 192] loss: 27.1363582611084\n",
      "[step: 193] loss: 27.09467315673828\n",
      "[step: 194] loss: 27.05221176147461\n",
      "[step: 195] loss: 27.008901596069336\n",
      "[step: 196] loss: 26.964702606201172\n",
      "[step: 197] loss: 26.919567108154297\n",
      "[step: 198] loss: 26.873615264892578\n",
      "[step: 199] loss: 26.827381134033203\n",
      "[step: 200] loss: 26.784318923950195\n",
      "[step: 201] loss: 26.757991790771484\n",
      "[step: 202] loss: 26.77821159362793\n",
      "[step: 203] loss: 26.78302001953125\n",
      "[step: 204] loss: 26.636184692382812\n",
      "[step: 205] loss: 26.54146385192871\n",
      "[step: 206] loss: 26.578758239746094\n",
      "[step: 207] loss: 26.470943450927734\n",
      "[step: 208] loss: 26.38840103149414\n",
      "[step: 209] loss: 26.397512435913086\n",
      "[step: 210] loss: 26.283432006835938\n",
      "[step: 211] loss: 26.23257064819336\n",
      "[step: 212] loss: 26.204505920410156\n",
      "[step: 213] loss: 26.093505859375\n",
      "[step: 214] loss: 26.059717178344727\n",
      "[step: 215] loss: 25.99742889404297\n",
      "[step: 216] loss: 25.9005069732666\n",
      "[step: 217] loss: 25.863834381103516\n",
      "[step: 218] loss: 25.782358169555664\n",
      "[step: 219] loss: 25.69581413269043\n",
      "[step: 220] loss: 25.648052215576172\n",
      "[step: 221] loss: 25.5604248046875\n",
      "[step: 222] loss: 25.475399017333984\n",
      "[step: 223] loss: 25.417816162109375\n",
      "[step: 224] loss: 25.33148193359375\n",
      "[step: 225] loss: 25.242393493652344\n",
      "[step: 226] loss: 25.176990509033203\n",
      "[step: 227] loss: 25.097850799560547\n",
      "[step: 228] loss: 25.005727767944336\n",
      "[step: 229] loss: 24.92935562133789\n",
      "[step: 230] loss: 24.859663009643555\n",
      "[step: 231] loss: 24.777889251708984\n",
      "[step: 232] loss: 24.69083023071289\n",
      "[step: 233] loss: 24.613109588623047\n",
      "[step: 234] loss: 24.544158935546875\n",
      "[step: 235] loss: 24.476028442382812\n",
      "[step: 236] loss: 24.405261993408203\n",
      "[step: 237] loss: 24.332660675048828\n",
      "[step: 238] loss: 24.260696411132812\n",
      "[step: 239] loss: 24.191925048828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 240] loss: 24.12952995300293\n",
      "[step: 241] loss: 24.08083724975586\n",
      "[step: 242] loss: 24.058242797851562\n",
      "[step: 243] loss: 24.079835891723633\n",
      "[step: 244] loss: 24.097179412841797\n",
      "[step: 245] loss: 23.99040985107422\n",
      "[step: 246] loss: 23.753414154052734\n",
      "[step: 247] loss: 23.645673751831055\n",
      "[step: 248] loss: 23.699792861938477\n",
      "[step: 249] loss: 23.67043685913086\n",
      "[step: 250] loss: 23.49062728881836\n",
      "[step: 251] loss: 23.386672973632812\n",
      "[step: 252] loss: 23.410259246826172\n",
      "[step: 253] loss: 23.362802505493164\n",
      "[step: 254] loss: 23.214773178100586\n",
      "[step: 255] loss: 23.135652542114258\n",
      "[step: 256] loss: 23.13543701171875\n",
      "[step: 257] loss: 23.07528305053711\n",
      "[step: 258] loss: 22.950536727905273\n",
      "[step: 259] loss: 22.879436492919922\n",
      "[step: 260] loss: 22.859811782836914\n",
      "[step: 261] loss: 22.79844093322754\n",
      "[step: 262] loss: 22.69306755065918\n",
      "[step: 263] loss: 22.611509323120117\n",
      "[step: 264] loss: 22.570812225341797\n",
      "[step: 265] loss: 22.524248123168945\n",
      "[step: 266] loss: 22.44049835205078\n",
      "[step: 267] loss: 22.345012664794922\n",
      "[step: 268] loss: 22.266681671142578\n",
      "[step: 269] loss: 22.20915985107422\n",
      "[step: 270] loss: 22.159198760986328\n",
      "[step: 271] loss: 22.103458404541016\n",
      "[step: 272] loss: 22.04412078857422\n",
      "[step: 273] loss: 21.977405548095703\n",
      "[step: 274] loss: 21.91702651977539\n",
      "[step: 275] loss: 21.856733322143555\n",
      "[step: 276] loss: 21.817317962646484\n",
      "[step: 277] loss: 21.781972885131836\n",
      "[step: 278] loss: 21.780590057373047\n",
      "[step: 279] loss: 21.732139587402344\n",
      "[step: 280] loss: 21.660253524780273\n",
      "[step: 281] loss: 21.461593627929688\n",
      "[step: 282] loss: 21.269912719726562\n",
      "[step: 283] loss: 21.142820358276367\n",
      "[step: 284] loss: 21.102069854736328\n",
      "[step: 285] loss: 21.125858306884766\n",
      "[step: 286] loss: 21.178905487060547\n",
      "[step: 287] loss: 21.300790786743164\n",
      "[step: 288] loss: 21.23957061767578\n",
      "[step: 289] loss: 21.060924530029297\n",
      "[step: 290] loss: 20.68600082397461\n",
      "[step: 291] loss: 20.527170181274414\n",
      "[step: 292] loss: 20.613649368286133\n",
      "[step: 293] loss: 20.705780029296875\n",
      "[step: 294] loss: 20.68846321105957\n",
      "[step: 295] loss: 20.387617111206055\n",
      "[step: 296] loss: 20.139347076416016\n",
      "[step: 297] loss: 20.07795524597168\n",
      "[step: 298] loss: 20.14797592163086\n",
      "[step: 299] loss: 20.24359130859375\n",
      "[step: 300] loss: 20.15381622314453\n",
      "[step: 301] loss: 19.980472564697266\n",
      "[step: 302] loss: 19.714319229125977\n",
      "[step: 303] loss: 19.548431396484375\n",
      "[step: 304] loss: 19.513349533081055\n",
      "[step: 305] loss: 19.56233787536621\n",
      "[step: 306] loss: 19.694303512573242\n",
      "[step: 307] loss: 19.779050827026367\n",
      "[step: 308] loss: 19.850101470947266\n",
      "[step: 309] loss: 19.47870635986328\n",
      "[step: 310] loss: 19.086872100830078\n",
      "[step: 311] loss: 18.896263122558594\n",
      "[step: 312] loss: 18.982450485229492\n",
      "[step: 313] loss: 19.151491165161133\n",
      "[step: 314] loss: 19.055139541625977\n",
      "[step: 315] loss: 18.796772003173828\n",
      "[step: 316] loss: 18.512943267822266\n",
      "[step: 317] loss: 18.442651748657227\n",
      "[step: 318] loss: 18.525394439697266\n",
      "[step: 319] loss: 18.55364990234375\n",
      "[step: 320] loss: 18.466825485229492\n",
      "[step: 321] loss: 18.223779678344727\n",
      "[step: 322] loss: 18.02471923828125\n",
      "[step: 323] loss: 17.93703842163086\n",
      "[step: 324] loss: 17.94255828857422\n",
      "[step: 325] loss: 17.98777198791504\n",
      "[step: 326] loss: 17.98888397216797\n",
      "[step: 327] loss: 17.96385955810547\n",
      "[step: 328] loss: 17.80979347229004\n",
      "[step: 329] loss: 17.62482452392578\n",
      "[step: 330] loss: 17.434377670288086\n",
      "[step: 331] loss: 17.31436538696289\n",
      "[step: 332] loss: 17.26649284362793\n",
      "[step: 333] loss: 17.266727447509766\n",
      "[step: 334] loss: 17.30288314819336\n",
      "[step: 335] loss: 17.335596084594727\n",
      "[step: 336] loss: 17.386919021606445\n",
      "[step: 337] loss: 17.30075454711914\n",
      "[step: 338] loss: 17.14040756225586\n",
      "[step: 339] loss: 16.874603271484375\n",
      "[step: 340] loss: 16.69095802307129\n",
      "[step: 341] loss: 16.643707275390625\n",
      "[step: 342] loss: 16.68374252319336\n",
      "[step: 343] loss: 16.728748321533203\n",
      "[step: 344] loss: 16.666751861572266\n",
      "[step: 345] loss: 16.530101776123047\n",
      "[step: 346] loss: 16.344219207763672\n",
      "[step: 347] loss: 16.21833038330078\n",
      "[step: 348] loss: 16.17588233947754\n",
      "[step: 349] loss: 16.18122100830078\n",
      "[step: 350] loss: 16.191627502441406\n",
      "[step: 351] loss: 16.150529861450195\n",
      "[step: 352] loss: 16.074108123779297\n",
      "[step: 353] loss: 15.942340850830078\n",
      "[step: 354] loss: 15.811168670654297\n",
      "[step: 355] loss: 15.697997093200684\n",
      "[step: 356] loss: 15.618852615356445\n",
      "[step: 357] loss: 15.569137573242188\n",
      "[step: 358] loss: 15.541296005249023\n",
      "[step: 359] loss: 15.539934158325195\n",
      "[step: 360] loss: 15.570005416870117\n",
      "[step: 361] loss: 15.679105758666992\n",
      "[step: 362] loss: 15.809281349182129\n",
      "[step: 363] loss: 15.997062683105469\n",
      "[step: 364] loss: 15.826070785522461\n",
      "[step: 365] loss: 15.461954116821289\n",
      "[step: 366] loss: 15.039810180664062\n",
      "[step: 367] loss: 14.990032196044922\n",
      "[step: 368] loss: 15.203084945678711\n",
      "[step: 369] loss: 15.244140625\n",
      "[step: 370] loss: 15.027252197265625\n",
      "[step: 371] loss: 14.735267639160156\n",
      "[step: 372] loss: 14.700838088989258\n",
      "[step: 373] loss: 14.828472137451172\n",
      "[step: 374] loss: 14.801702499389648\n",
      "[step: 375] loss: 14.612543106079102\n",
      "[step: 376] loss: 14.436478614807129\n",
      "[step: 377] loss: 14.423373222351074\n",
      "[step: 378] loss: 14.485177993774414\n",
      "[step: 379] loss: 14.454643249511719\n",
      "[step: 380] loss: 14.32919979095459\n",
      "[step: 381] loss: 14.182657241821289\n",
      "[step: 382] loss: 14.111519813537598\n",
      "[step: 383] loss: 14.113438606262207\n",
      "[step: 384] loss: 14.129021644592285\n",
      "[step: 385] loss: 14.118582725524902\n",
      "[step: 386] loss: 14.051445007324219\n",
      "[step: 387] loss: 13.958568572998047\n",
      "[step: 388] loss: 13.85324764251709\n",
      "[step: 389] loss: 13.764208793640137\n",
      "[step: 390] loss: 13.697453498840332\n",
      "[step: 391] loss: 13.650320053100586\n",
      "[step: 392] loss: 13.618778228759766\n",
      "[step: 393] loss: 13.605091094970703\n",
      "[step: 394] loss: 13.626213073730469\n",
      "[step: 395] loss: 13.711835861206055\n",
      "[step: 396] loss: 13.934858322143555\n",
      "[step: 397] loss: 14.268838882446289\n",
      "[step: 398] loss: 14.625335693359375\n",
      "[step: 399] loss: 14.295120239257812\n",
      "[step: 400] loss: 13.599387168884277\n",
      "[step: 401] loss: 13.212047576904297\n",
      "[step: 402] loss: 13.56108283996582\n",
      "[step: 403] loss: 13.875007629394531\n",
      "[step: 404] loss: 13.39474105834961\n",
      "[step: 405] loss: 13.049884796142578\n",
      "[step: 406] loss: 13.284523010253906\n",
      "[step: 407] loss: 13.378438949584961\n",
      "[step: 408] loss: 13.079246520996094\n",
      "[step: 409] loss: 12.908319473266602\n",
      "[step: 410] loss: 13.084501266479492\n",
      "[step: 411] loss: 13.100188255310059\n",
      "[step: 412] loss: 12.83336353302002\n",
      "[step: 413] loss: 12.759912490844727\n",
      "[step: 414] loss: 12.904699325561523\n",
      "[step: 415] loss: 12.840574264526367\n",
      "[step: 416] loss: 12.633525848388672\n",
      "[step: 417] loss: 12.601043701171875\n",
      "[step: 418] loss: 12.6893310546875\n",
      "[step: 419] loss: 12.645475387573242\n",
      "[step: 420] loss: 12.484434127807617\n",
      "[step: 421] loss: 12.415216445922852\n",
      "[step: 422] loss: 12.459522247314453\n",
      "[step: 423] loss: 12.468698501586914\n",
      "[step: 424] loss: 12.369733810424805\n",
      "[step: 425] loss: 12.255003929138184\n",
      "[step: 426] loss: 12.215499877929688\n",
      "[step: 427] loss: 12.233129501342773\n",
      "[step: 428] loss: 12.226156234741211\n",
      "[step: 429] loss: 12.182950973510742\n",
      "[step: 430] loss: 12.103645324707031\n",
      "[step: 431] loss: 12.025172233581543\n",
      "[step: 432] loss: 11.961758613586426\n",
      "[step: 433] loss: 11.934213638305664\n",
      "[step: 434] loss: 11.920083045959473\n",
      "[step: 435] loss: 11.91756534576416\n",
      "[step: 436] loss: 11.918085098266602\n",
      "[step: 437] loss: 11.92764663696289\n",
      "[step: 438] loss: 11.949772834777832\n",
      "[step: 439] loss: 11.97244930267334\n",
      "[step: 440] loss: 11.996140480041504\n",
      "[step: 441] loss: 11.976716995239258\n",
      "[step: 442] loss: 11.908447265625\n",
      "[step: 443] loss: 11.760953903198242\n",
      "[step: 444] loss: 11.601564407348633\n",
      "[step: 445] loss: 11.476582527160645\n",
      "[step: 446] loss: 11.426186561584473\n",
      "[step: 447] loss: 11.4354248046875\n",
      "[step: 448] loss: 11.469365119934082\n",
      "[step: 449] loss: 11.501651763916016\n",
      "[step: 450] loss: 11.506999969482422\n",
      "[step: 451] loss: 11.473005294799805\n",
      "[step: 452] loss: 11.385819435119629\n",
      "[step: 453] loss: 11.280745506286621\n",
      "[step: 454] loss: 11.177446365356445\n",
      "[step: 455] loss: 11.097591400146484\n",
      "[step: 456] loss: 11.048437118530273\n",
      "[step: 457] loss: 11.028116226196289\n",
      "[step: 458] loss: 11.024389266967773\n",
      "[step: 459] loss: 11.035221099853516\n",
      "[step: 460] loss: 11.074249267578125\n",
      "[step: 461] loss: 11.147998809814453\n",
      "[step: 462] loss: 11.27664852142334\n",
      "[step: 463] loss: 11.41130542755127\n",
      "[step: 464] loss: 11.525680541992188\n",
      "[step: 465] loss: 11.407402038574219\n",
      "[step: 466] loss: 11.092283248901367\n",
      "[step: 467] loss: 10.738593101501465\n",
      "[step: 468] loss: 10.620915412902832\n",
      "[step: 469] loss: 10.745790481567383\n",
      "[step: 470] loss: 10.893796920776367\n",
      "[step: 471] loss: 10.880496978759766\n",
      "[step: 472] loss: 10.667795181274414\n",
      "[step: 473] loss: 10.465167999267578\n",
      "[step: 474] loss: 10.429173469543457\n",
      "[step: 475] loss: 10.518795013427734\n",
      "[step: 476] loss: 10.579352378845215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 477] loss: 10.502227783203125\n",
      "[step: 478] loss: 10.352712631225586\n",
      "[step: 479] loss: 10.234315872192383\n",
      "[step: 480] loss: 10.209009170532227\n",
      "[step: 481] loss: 10.247021675109863\n",
      "[step: 482] loss: 10.278398513793945\n",
      "[step: 483] loss: 10.26546573638916\n",
      "[step: 484] loss: 10.194235801696777\n",
      "[step: 485] loss: 10.100150108337402\n",
      "[step: 486] loss: 10.006471633911133\n",
      "[step: 487] loss: 9.938323974609375\n",
      "[step: 488] loss: 9.898298263549805\n",
      "[step: 489] loss: 9.879769325256348\n",
      "[step: 490] loss: 9.879962921142578\n",
      "[step: 491] loss: 9.903888702392578\n",
      "[step: 492] loss: 9.976554870605469\n",
      "[step: 493] loss: 10.122981071472168\n",
      "[step: 494] loss: 10.430543899536133\n",
      "[step: 495] loss: 10.79255485534668\n",
      "[step: 496] loss: 11.127605438232422\n",
      "[step: 497] loss: 10.701409339904785\n",
      "[step: 498] loss: 9.928866386413574\n",
      "[step: 499] loss: 9.529972076416016\n",
      "[step: 500] loss: 9.849979400634766\n",
      "[step: 501] loss: 10.203743934631348\n",
      "[step: 502] loss: 9.883443832397461\n",
      "[step: 503] loss: 9.426491737365723\n",
      "[step: 504] loss: 9.50731086730957\n",
      "[step: 505] loss: 9.765767097473145\n",
      "[step: 506] loss: 9.605634689331055\n",
      "[step: 507] loss: 9.289795875549316\n",
      "[step: 508] loss: 9.333381652832031\n",
      "[step: 509] loss: 9.49715805053711\n",
      "[step: 510] loss: 9.37488842010498\n",
      "[step: 511] loss: 9.157485008239746\n",
      "[step: 512] loss: 9.143797874450684\n",
      "[step: 513] loss: 9.248527526855469\n",
      "[step: 514] loss: 9.224438667297363\n",
      "[step: 515] loss: 9.05087947845459\n",
      "[step: 516] loss: 8.954940795898438\n",
      "[step: 517] loss: 8.998041152954102\n",
      "[step: 518] loss: 9.038740158081055\n",
      "[step: 519] loss: 8.989032745361328\n",
      "[step: 520] loss: 8.8697509765625\n",
      "[step: 521] loss: 8.779287338256836\n",
      "[step: 522] loss: 8.754068374633789\n",
      "[step: 523] loss: 8.773319244384766\n",
      "[step: 524] loss: 8.797004699707031\n",
      "[step: 525] loss: 8.790008544921875\n",
      "[step: 526] loss: 8.7635498046875\n",
      "[step: 527] loss: 8.709939956665039\n",
      "[step: 528] loss: 8.655675888061523\n",
      "[step: 529] loss: 8.593006134033203\n",
      "[step: 530] loss: 8.542596817016602\n",
      "[step: 531] loss: 8.497629165649414\n",
      "[step: 532] loss: 8.464227676391602\n",
      "[step: 533] loss: 8.442361831665039\n",
      "[step: 534] loss: 8.448297500610352\n",
      "[step: 535] loss: 8.4883394241333\n",
      "[step: 536] loss: 8.625192642211914\n",
      "[step: 537] loss: 8.870672225952148\n",
      "[step: 538] loss: 9.387063026428223\n",
      "[step: 539] loss: 9.750020027160645\n",
      "[step: 540] loss: 9.889854431152344\n",
      "[step: 541] loss: 8.910211563110352\n",
      "[step: 542] loss: 8.129075050354004\n",
      "[step: 543] loss: 8.281429290771484\n",
      "[step: 544] loss: 8.788593292236328\n",
      "[step: 545] loss: 8.694656372070312\n",
      "[step: 546] loss: 8.075439453125\n",
      "[step: 547] loss: 8.0427827835083\n",
      "[step: 548] loss: 8.423084259033203\n",
      "[step: 549] loss: 8.268760681152344\n",
      "[step: 550] loss: 7.877957344055176\n",
      "[step: 551] loss: 7.941684246063232\n",
      "[step: 552] loss: 8.143560409545898\n",
      "[step: 553] loss: 7.9717912673950195\n",
      "[step: 554] loss: 7.72914457321167\n",
      "[step: 555] loss: 7.8087897300720215\n",
      "[step: 556] loss: 7.921204090118408\n",
      "[step: 557] loss: 7.771999359130859\n",
      "[step: 558] loss: 7.6038126945495605\n",
      "[step: 559] loss: 7.618274688720703\n",
      "[step: 560] loss: 7.695052146911621\n",
      "[step: 561] loss: 7.6642374992370605\n",
      "[step: 562] loss: 7.517963409423828\n",
      "[step: 563] loss: 7.427270889282227\n",
      "[step: 564] loss: 7.446859359741211\n",
      "[step: 565] loss: 7.4912638664245605\n",
      "[step: 566] loss: 7.4863505363464355\n",
      "[step: 567] loss: 7.403919696807861\n",
      "[step: 568] loss: 7.310395240783691\n",
      "[step: 569] loss: 7.23824405670166\n",
      "[step: 570] loss: 7.2053704261779785\n",
      "[step: 571] loss: 7.205826759338379\n",
      "[step: 572] loss: 7.220907211303711\n",
      "[step: 573] loss: 7.24448299407959\n",
      "[step: 574] loss: 7.266946792602539\n",
      "[step: 575] loss: 7.318838119506836\n",
      "[step: 576] loss: 7.3539958000183105\n",
      "[step: 577] loss: 7.44456148147583\n",
      "[step: 578] loss: 7.4776177406311035\n",
      "[step: 579] loss: 7.548402786254883\n",
      "[step: 580] loss: 7.454712867736816\n",
      "[step: 581] loss: 7.328462600708008\n",
      "[step: 582] loss: 7.075204849243164\n",
      "[step: 583] loss: 6.876158237457275\n",
      "[step: 584] loss: 6.777695655822754\n",
      "[step: 585] loss: 6.794860363006592\n",
      "[step: 586] loss: 6.875533580780029\n",
      "[step: 587] loss: 6.9290080070495605\n",
      "[step: 588] loss: 6.93851900100708\n",
      "[step: 589] loss: 6.844376564025879\n",
      "[step: 590] loss: 6.72693395614624\n",
      "[step: 591] loss: 6.611777305603027\n",
      "[step: 592] loss: 6.545738697052002\n",
      "[step: 593] loss: 6.530004501342773\n",
      "[step: 594] loss: 6.546526908874512\n",
      "[step: 595] loss: 6.579131126403809\n",
      "[step: 596] loss: 6.6024675369262695\n",
      "[step: 597] loss: 6.634271144866943\n",
      "[step: 598] loss: 6.632630348205566\n",
      "[step: 599] loss: 6.650238037109375\n",
      "[step: 600] loss: 6.6242265701293945\n",
      "[step: 601] loss: 6.622689247131348\n",
      "[step: 602] loss: 6.569627285003662\n",
      "[step: 603] loss: 6.53733491897583\n",
      "[step: 604] loss: 6.454951286315918\n",
      "[step: 605] loss: 6.388685703277588\n",
      "[step: 606] loss: 6.296218395233154\n",
      "[step: 607] loss: 6.21998405456543\n",
      "[step: 608] loss: 6.145974636077881\n",
      "[step: 609] loss: 6.087698936462402\n",
      "[step: 610] loss: 6.0403313636779785\n",
      "[step: 611] loss: 6.002908706665039\n",
      "[step: 612] loss: 5.972021102905273\n",
      "[step: 613] loss: 5.945703983306885\n",
      "[step: 614] loss: 5.923619270324707\n",
      "[step: 615] loss: 5.9073805809021\n",
      "[step: 616] loss: 5.903797149658203\n",
      "[step: 617] loss: 5.924371242523193\n",
      "[step: 618] loss: 6.012574195861816\n",
      "[step: 619] loss: 6.217479228973389\n",
      "[step: 620] loss: 6.779603481292725\n",
      "[step: 621] loss: 7.5508599281311035\n",
      "[step: 622] loss: 8.953892707824707\n",
      "[step: 623] loss: 7.978903770446777\n",
      "[step: 624] loss: 6.466004371643066\n",
      "[step: 625] loss: 5.706814289093018\n",
      "[step: 626] loss: 6.6097331047058105\n",
      "[step: 627] loss: 7.1372456550598145\n",
      "[step: 628] loss: 5.9276442527771\n",
      "[step: 629] loss: 5.874942779541016\n",
      "[step: 630] loss: 6.630317211151123\n",
      "[step: 631] loss: 6.013851165771484\n",
      "[step: 632] loss: 5.702494144439697\n",
      "[step: 633] loss: 6.174398422241211\n",
      "[step: 634] loss: 5.90260648727417\n",
      "[step: 635] loss: 5.619846343994141\n",
      "[step: 636] loss: 5.856610298156738\n",
      "[step: 637] loss: 5.792490005493164\n",
      "[step: 638] loss: 5.514352798461914\n",
      "[step: 639] loss: 5.623477935791016\n",
      "[step: 640] loss: 5.686087131500244\n",
      "[step: 641] loss: 5.399170398712158\n",
      "[step: 642] loss: 5.425945281982422\n",
      "[step: 643] loss: 5.577731609344482\n",
      "[step: 644] loss: 5.305525302886963\n",
      "[step: 645] loss: 5.260922908782959\n",
      "[step: 646] loss: 5.4291582107543945\n",
      "[step: 647] loss: 5.266677379608154\n",
      "[step: 648] loss: 5.1624226570129395\n",
      "[step: 649] loss: 5.226467609405518\n",
      "[step: 650] loss: 5.182806015014648\n",
      "[step: 651] loss: 5.145493984222412\n",
      "[step: 652] loss: 5.10235595703125\n",
      "[step: 653] loss: 5.022145748138428\n",
      "[step: 654] loss: 5.0513081550598145\n",
      "[step: 655] loss: 5.067708492279053\n",
      "[step: 656] loss: 4.974000453948975\n",
      "[step: 657] loss: 4.929403781890869\n",
      "[step: 658] loss: 4.927159786224365\n",
      "[step: 659] loss: 4.901409149169922\n",
      "[step: 660] loss: 4.896735191345215\n",
      "[step: 661] loss: 4.874492645263672\n",
      "[step: 662] loss: 4.809706211090088\n",
      "[step: 663] loss: 4.779649257659912\n",
      "[step: 664] loss: 4.776819229125977\n",
      "[step: 665] loss: 4.7535505294799805\n",
      "[step: 666] loss: 4.736567497253418\n",
      "[step: 667] loss: 4.722103118896484\n",
      "[step: 668] loss: 4.680546760559082\n",
      "[step: 669] loss: 4.643905162811279\n",
      "[step: 670] loss: 4.625625133514404\n",
      "[step: 671] loss: 4.60128927230835\n",
      "[step: 672] loss: 4.579195022583008\n",
      "[step: 673] loss: 4.569719314575195\n",
      "[step: 674] loss: 4.552412986755371\n",
      "[step: 675] loss: 4.527442932128906\n",
      "[step: 676] loss: 4.50993537902832\n",
      "[step: 677] loss: 4.48983097076416\n",
      "[step: 678] loss: 4.463649272918701\n",
      "[step: 679] loss: 4.4415507316589355\n",
      "[step: 680] loss: 4.42417049407959\n",
      "[step: 681] loss: 4.403182029724121\n",
      "[step: 682] loss: 4.38619327545166\n",
      "[step: 683] loss: 4.376832962036133\n",
      "[step: 684] loss: 4.376432418823242\n",
      "[step: 685] loss: 4.384975433349609\n",
      "[step: 686] loss: 4.430364608764648\n",
      "[step: 687] loss: 4.511967182159424\n",
      "[step: 688] loss: 4.701426029205322\n",
      "[step: 689] loss: 4.932943344116211\n",
      "[step: 690] loss: 5.400489807128906\n",
      "[step: 691] loss: 5.483810901641846\n",
      "[step: 692] loss: 5.515674114227295\n",
      "[step: 693] loss: 4.767707824707031\n",
      "[step: 694] loss: 4.218082427978516\n",
      "[step: 695] loss: 4.176171779632568\n",
      "[step: 696] loss: 4.513863563537598\n",
      "[step: 697] loss: 4.75235652923584\n",
      "[step: 698] loss: 4.429396629333496\n",
      "[step: 699] loss: 4.083987236022949\n",
      "[step: 700] loss: 4.093197822570801\n",
      "[step: 701] loss: 4.311967849731445\n",
      "[step: 702] loss: 4.363157749176025\n",
      "[step: 703] loss: 4.109426498413086\n",
      "[step: 704] loss: 3.9515011310577393\n",
      "[step: 705] loss: 4.030182361602783\n",
      "[step: 706] loss: 4.14093017578125\n",
      "[step: 707] loss: 4.1124138832092285\n",
      "[step: 708] loss: 3.941071033477783\n",
      "[step: 709] loss: 3.8531274795532227\n",
      "[step: 710] loss: 3.8988707065582275\n",
      "[step: 711] loss: 3.9690628051757812\n",
      "[step: 712] loss: 3.9715163707733154\n",
      "[step: 713] loss: 3.8730721473693848\n",
      "[step: 714] loss: 3.779503345489502\n",
      "[step: 715] loss: 3.7434420585632324\n",
      "[step: 716] loss: 3.7657899856567383\n",
      "[step: 717] loss: 3.8030664920806885\n",
      "[step: 718] loss: 3.8050787448883057\n",
      "[step: 719] loss: 3.7730188369750977\n",
      "[step: 720] loss: 3.7099862098693848\n",
      "[step: 721] loss: 3.653134822845459\n",
      "[step: 722] loss: 3.6162524223327637\n",
      "[step: 723] loss: 3.60387921333313\n",
      "[step: 724] loss: 3.608058452606201\n",
      "[step: 725] loss: 3.6159415245056152\n",
      "[step: 726] loss: 3.6214654445648193\n",
      "[step: 727] loss: 3.613576889038086\n",
      "[step: 728] loss: 3.601255178451538\n",
      "[step: 729] loss: 3.576528549194336\n",
      "[step: 730] loss: 3.551602840423584\n",
      "[step: 731] loss: 3.5215353965759277\n",
      "[step: 732] loss: 3.4954981803894043\n",
      "[step: 733] loss: 3.469137191772461\n",
      "[step: 734] loss: 3.4468088150024414\n",
      "[step: 735] loss: 3.4256982803344727\n",
      "[step: 736] loss: 3.408747673034668\n",
      "[step: 737] loss: 3.3941807746887207\n",
      "[step: 738] loss: 3.3851747512817383\n",
      "[step: 739] loss: 3.381415367126465\n",
      "[step: 740] loss: 3.3915297985076904\n",
      "[step: 741] loss: 3.4172019958496094\n",
      "[step: 742] loss: 3.4867846965789795\n",
      "[step: 743] loss: 3.598177909851074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 744] loss: 3.841986656188965\n",
      "[step: 745] loss: 4.109197616577148\n",
      "[step: 746] loss: 4.574816703796387\n",
      "[step: 747] loss: 4.553421974182129\n",
      "[step: 748] loss: 4.382021903991699\n",
      "[step: 749] loss: 3.6426637172698975\n",
      "[step: 750] loss: 3.208861827850342\n",
      "[step: 751] loss: 3.292285919189453\n",
      "[step: 752] loss: 3.624763011932373\n",
      "[step: 753] loss: 3.7979660034179688\n",
      "[step: 754] loss: 3.476973056793213\n",
      "[step: 755] loss: 3.1601381301879883\n",
      "[step: 756] loss: 3.1614108085632324\n",
      "[step: 757] loss: 3.364987373352051\n",
      "[step: 758] loss: 3.4487991333007812\n",
      "[step: 759] loss: 3.248363971710205\n",
      "[step: 760] loss: 3.05912184715271\n",
      "[step: 761] loss: 3.0713601112365723\n",
      "[step: 762] loss: 3.1970772743225098\n",
      "[step: 763] loss: 3.244366407394409\n",
      "[step: 764] loss: 3.1301581859588623\n",
      "[step: 765] loss: 3.0009806156158447\n",
      "[step: 766] loss: 2.96488618850708\n",
      "[step: 767] loss: 3.0151898860931396\n",
      "[step: 768] loss: 3.0683434009552\n",
      "[step: 769] loss: 3.055471181869507\n",
      "[step: 770] loss: 2.997745990753174\n",
      "[step: 771] loss: 2.924323558807373\n",
      "[step: 772] loss: 2.879490852355957\n",
      "[step: 773] loss: 2.8754849433898926\n",
      "[step: 774] loss: 2.8978068828582764\n",
      "[step: 775] loss: 2.9205517768859863\n",
      "[step: 776] loss: 2.911956787109375\n",
      "[step: 777] loss: 2.878202199935913\n",
      "[step: 778] loss: 2.828726053237915\n",
      "[step: 779] loss: 2.7896111011505127\n",
      "[step: 780] loss: 2.768277645111084\n",
      "[step: 781] loss: 2.7618532180786133\n",
      "[step: 782] loss: 2.762932777404785\n",
      "[step: 783] loss: 2.7656850814819336\n",
      "[step: 784] loss: 2.768033266067505\n",
      "[step: 785] loss: 2.764390468597412\n",
      "[step: 786] loss: 2.7571213245391846\n",
      "[step: 787] loss: 2.7422902584075928\n",
      "[step: 788] loss: 2.726677656173706\n",
      "[step: 789] loss: 2.708009719848633\n",
      "[step: 790] loss: 2.692568778991699\n",
      "[step: 791] loss: 2.6763744354248047\n",
      "[step: 792] loss: 2.664411783218384\n",
      "[step: 793] loss: 2.6537609100341797\n",
      "[step: 794] loss: 2.6493730545043945\n",
      "[step: 795] loss: 2.6486318111419678\n",
      "[step: 796] loss: 2.658275604248047\n",
      "[step: 797] loss: 2.675572395324707\n",
      "[step: 798] loss: 2.7163338661193848\n",
      "[step: 799] loss: 2.7720046043395996\n",
      "[step: 800] loss: 2.879859447479248\n",
      "[step: 801] loss: 2.9874632358551025\n",
      "[step: 802] loss: 3.1637911796569824\n",
      "[step: 803] loss: 3.2114531993865967\n",
      "[step: 804] loss: 3.245417356491089\n",
      "[step: 805] loss: 3.0174381732940674\n",
      "[step: 806] loss: 2.771780252456665\n",
      "[step: 807] loss: 2.5441746711730957\n",
      "[step: 808] loss: 2.4656667709350586\n",
      "[step: 809] loss: 2.5227622985839844\n",
      "[step: 810] loss: 2.6261374950408936\n",
      "[step: 811] loss: 2.6974780559539795\n",
      "[step: 812] loss: 2.6507201194763184\n",
      "[step: 813] loss: 2.5497729778289795\n",
      "[step: 814] loss: 2.4424548149108887\n",
      "[step: 815] loss: 2.392545700073242\n",
      "[step: 816] loss: 2.4032578468322754\n",
      "[step: 817] loss: 2.4445440769195557\n",
      "[step: 818] loss: 2.4794840812683105\n",
      "[step: 819] loss: 2.473764181137085\n",
      "[step: 820] loss: 2.437582492828369\n",
      "[step: 821] loss: 2.3822524547576904\n",
      "[step: 822] loss: 2.33595609664917\n",
      "[step: 823] loss: 2.307739496231079\n",
      "[step: 824] loss: 2.2981815338134766\n",
      "[step: 825] loss: 2.2975802421569824\n",
      "[step: 826] loss: 2.2986176013946533\n",
      "[step: 827] loss: 2.29862642288208\n",
      "[step: 828] loss: 2.296741247177124\n",
      "[step: 829] loss: 2.296947479248047\n",
      "[step: 830] loss: 2.2933239936828613\n",
      "[step: 831] loss: 2.289268970489502\n",
      "[step: 832] loss: 2.2756028175354004\n",
      "[step: 833] loss: 2.258753776550293\n",
      "[step: 834] loss: 2.234834671020508\n",
      "[step: 835] loss: 2.2120654582977295\n",
      "[step: 836] loss: 2.1901683807373047\n",
      "[step: 837] loss: 2.1725172996520996\n",
      "[step: 838] loss: 2.157686710357666\n",
      "[step: 839] loss: 2.145643711090088\n",
      "[step: 840] loss: 2.1354780197143555\n",
      "[step: 841] loss: 2.1284215450286865\n",
      "[step: 842] loss: 2.1258609294891357\n",
      "[step: 843] loss: 2.1315412521362305\n",
      "[step: 844] loss: 2.1493844985961914\n",
      "[step: 845] loss: 2.188953399658203\n",
      "[step: 846] loss: 2.259219169616699\n",
      "[step: 847] loss: 2.382195472717285\n",
      "[step: 848] loss: 2.5511059761047363\n",
      "[step: 849] loss: 2.782731056213379\n",
      "[step: 850] loss: 2.9134719371795654\n",
      "[step: 851] loss: 2.981271266937256\n",
      "[step: 852] loss: 2.7571308612823486\n",
      "[step: 853] loss: 2.5796477794647217\n",
      "[step: 854] loss: 2.462735176086426\n",
      "[step: 855] loss: 2.464287757873535\n",
      "[step: 856] loss: 2.297511100769043\n",
      "[step: 857] loss: 2.109651803970337\n",
      "[step: 858] loss: 2.0583534240722656\n",
      "[step: 859] loss: 2.205472469329834\n",
      "[step: 860] loss: 2.3331565856933594\n",
      "[step: 861] loss: 2.171536684036255\n",
      "[step: 862] loss: 1.9709497690200806\n",
      "[step: 863] loss: 1.9556821584701538\n",
      "[step: 864] loss: 2.057655096054077\n",
      "[step: 865] loss: 2.0804219245910645\n",
      "[step: 866] loss: 1.9832663536071777\n",
      "[step: 867] loss: 1.9597257375717163\n",
      "[step: 868] loss: 2.0082693099975586\n",
      "[step: 869] loss: 1.984565258026123\n",
      "[step: 870] loss: 1.8932163715362549\n",
      "[step: 871] loss: 1.850714087486267\n",
      "[step: 872] loss: 1.8935723304748535\n",
      "[step: 873] loss: 1.925278663635254\n",
      "[step: 874] loss: 1.8867216110229492\n",
      "[step: 875] loss: 1.8507566452026367\n",
      "[step: 876] loss: 1.862019419670105\n",
      "[step: 877] loss: 1.8723539113998413\n",
      "[step: 878] loss: 1.8422999382019043\n",
      "[step: 879] loss: 1.7964483499526978\n",
      "[step: 880] loss: 1.7840317487716675\n",
      "[step: 881] loss: 1.7944308519363403\n",
      "[step: 882] loss: 1.7868791818618774\n",
      "[step: 883] loss: 1.762725591659546\n",
      "[step: 884] loss: 1.7484792470932007\n",
      "[step: 885] loss: 1.7542879581451416\n",
      "[step: 886] loss: 1.760209321975708\n",
      "[step: 887] loss: 1.750586986541748\n",
      "[step: 888] loss: 1.7367058992385864\n",
      "[step: 889] loss: 1.7347476482391357\n",
      "[step: 890] loss: 1.7430815696716309\n",
      "[step: 891] loss: 1.7514115571975708\n",
      "[step: 892] loss: 1.7543684244155884\n",
      "[step: 893] loss: 1.7658848762512207\n",
      "[step: 894] loss: 1.7942637205123901\n",
      "[step: 895] loss: 1.8503693342208862\n",
      "[step: 896] loss: 1.9196852445602417\n",
      "[step: 897] loss: 2.0244925022125244\n",
      "[step: 898] loss: 2.127589225769043\n",
      "[step: 899] loss: 2.2682900428771973\n",
      "[step: 900] loss: 2.315769672393799\n",
      "[step: 901] loss: 2.3076767921447754\n",
      "[step: 902] loss: 2.1100146770477295\n",
      "[step: 903] loss: 1.872504711151123\n",
      "[step: 904] loss: 1.6675207614898682\n",
      "[step: 905] loss: 1.5978457927703857\n",
      "[step: 906] loss: 1.653672218322754\n",
      "[step: 907] loss: 1.7546381950378418\n",
      "[step: 908] loss: 1.823651909828186\n",
      "[step: 909] loss: 1.797602653503418\n",
      "[step: 910] loss: 1.7142997980117798\n",
      "[step: 911] loss: 1.6175994873046875\n",
      "[step: 912] loss: 1.5611786842346191\n",
      "[step: 913] loss: 1.5583680868148804\n",
      "[step: 914] loss: 1.5902674198150635\n",
      "[step: 915] loss: 1.6289743185043335\n",
      "[step: 916] loss: 1.643385648727417\n",
      "[step: 917] loss: 1.629376769065857\n",
      "[step: 918] loss: 1.5899815559387207\n",
      "[step: 919] loss: 1.543424129486084\n",
      "[step: 920] loss: 1.5047123432159424\n",
      "[step: 921] loss: 1.4831643104553223\n",
      "[step: 922] loss: 1.4793699979782104\n",
      "[step: 923] loss: 1.4884203672409058\n",
      "[step: 924] loss: 1.502866506576538\n",
      "[step: 925] loss: 1.5143232345581055\n",
      "[step: 926] loss: 1.5178587436676025\n",
      "[step: 927] loss: 1.510384202003479\n",
      "[step: 928] loss: 1.4943903684616089\n",
      "[step: 929] loss: 1.4728920459747314\n",
      "[step: 930] loss: 1.451485514640808\n",
      "[step: 931] loss: 1.432866096496582\n",
      "[step: 932] loss: 1.4188876152038574\n",
      "[step: 933] loss: 1.4082285165786743\n",
      "[step: 934] loss: 1.3998777866363525\n",
      "[step: 935] loss: 1.3922549486160278\n",
      "[step: 936] loss: 1.3847150802612305\n",
      "[step: 937] loss: 1.3770554065704346\n",
      "[step: 938] loss: 1.3693028688430786\n",
      "[step: 939] loss: 1.3615461587905884\n",
      "[step: 940] loss: 1.3538522720336914\n",
      "[step: 941] loss: 1.3461782932281494\n",
      "[step: 942] loss: 1.3386098146438599\n",
      "[step: 943] loss: 1.3311595916748047\n",
      "[step: 944] loss: 1.3239293098449707\n",
      "[step: 945] loss: 1.3169584274291992\n",
      "[step: 946] loss: 1.3102760314941406\n",
      "[step: 947] loss: 1.3038452863693237\n",
      "[step: 948] loss: 1.2976162433624268\n",
      "[step: 949] loss: 1.291569471359253\n",
      "[step: 950] loss: 1.2857400178909302\n",
      "[step: 951] loss: 1.280280590057373\n",
      "[step: 952] loss: 1.2755732536315918\n",
      "[step: 953] loss: 1.2725493907928467\n",
      "[step: 954] loss: 1.2732058763504028\n",
      "[step: 955] loss: 1.2825125455856323\n",
      "[step: 956] loss: 1.310656189918518\n",
      "[step: 957] loss: 1.3855723142623901\n",
      "[step: 958] loss: 1.554600477218628\n",
      "[step: 959] loss: 1.9616963863372803\n",
      "[step: 960] loss: 2.6586074829101562\n",
      "[step: 961] loss: 3.939110040664673\n",
      "[step: 962] loss: 4.147098541259766\n",
      "[step: 963] loss: 3.5260207653045654\n",
      "[step: 964] loss: 1.7660062313079834\n",
      "[step: 965] loss: 1.7195827960968018\n",
      "[step: 966] loss: 2.7021026611328125\n",
      "[step: 967] loss: 2.113644599914551\n",
      "[step: 968] loss: 1.3668460845947266\n",
      "[step: 969] loss: 1.8962748050689697\n",
      "[step: 970] loss: 2.023064613342285\n",
      "[step: 971] loss: 1.3283112049102783\n",
      "[step: 972] loss: 1.5336036682128906\n",
      "[step: 973] loss: 1.8531010150909424\n",
      "[step: 974] loss: 1.3655449151992798\n",
      "[step: 975] loss: 1.3565633296966553\n",
      "[step: 976] loss: 1.6788904666900635\n",
      "[step: 977] loss: 1.3254507780075073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 978] loss: 1.2732657194137573\n",
      "[step: 979] loss: 1.5205422639846802\n",
      "[step: 980] loss: 1.2917784452438354\n",
      "[step: 981] loss: 1.2201286554336548\n",
      "[step: 982] loss: 1.3870527744293213\n",
      "[step: 983] loss: 1.2495050430297852\n",
      "[step: 984] loss: 1.1760594844818115\n",
      "[step: 985] loss: 1.3142738342285156\n",
      "[step: 986] loss: 1.210917592048645\n",
      "[step: 987] loss: 1.133631944656372\n",
      "[step: 988] loss: 1.242073893547058\n",
      "[step: 989] loss: 1.1909838914871216\n",
      "[step: 990] loss: 1.1128592491149902\n",
      "[step: 991] loss: 1.1808582544326782\n",
      "[step: 992] loss: 1.1682524681091309\n",
      "[step: 993] loss: 1.0912456512451172\n",
      "[step: 994] loss: 1.1353427171707153\n",
      "[step: 995] loss: 1.144270420074463\n",
      "[step: 996] loss: 1.0814391374588013\n",
      "[step: 997] loss: 1.0939284563064575\n",
      "[step: 998] loss: 1.1154534816741943\n",
      "[step: 999] loss: 1.071496605873108\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "testY is:  [7608.3999999999996, 8706.1935480000011, 26366.56667, 24504.89286, 2169.0322579999997, 1879.0, 2449.5483870000003, 1487.9000000000001, 1903.580645, 3121.333333, 8773.6333329999998, 9429.2903230000011, 4370.5666670000001, 4122.7096770000007, 7644.7419349999991, 4147.0740740000001, 1799.7096770000001, 1405.5]\n",
      "\n",
      "\n",
      "LSTM forecast : [9776.1484375, 11756.7431640625, 1380.7252197265625, -851.0900268554688, 3680.492431640625, 6987.78076171875, 2387.320556640625, 4439.5966796875, 5436.5498046875, 5770.05712890625, 2974.058349609375, 4779.935546875, 3539.851318359375, 14624.8271484375, 22697.845703125, 28801.88671875, 1496.692138671875, 6073.18212890625] \n",
      "@@@@@LSTM rmse:  11451.3274326\n",
      "Bayseian forecast : [5574.6306211197752, 6825.0964678457203, 15690.016379008404, 34640.291534109114, 552.68107357991425, 2903.1399223308099, 2073.372668614033, 1885.4694330370753, 3738.8512278303956, 4277.7795062398836, 7623.9433510119416, 4114.5911007904315, 4596.4310600724884, 5828.0609886824741, 14797.607636456994, 44449.632054847112, 325.7862720932709, 2589.4681973141514] \n",
      "@@@@@Bayseian rmse:  10393.9548052\n",
      "\n",
      "\n",
      "Bayseian WON!!!!!!\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,7,9,'month') #7은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2178.676251601592,\n",
       " 1461.6115387298612,\n",
       " 2651.7794099982207,\n",
       " 3007.6696959836563,\n",
       " 5874.4991051425168,\n",
       " 3859.3472809028963,\n",
       " 3301.8477645126191,\n",
       " 4105.2784993045379,\n",
       " 11852.520793829079]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
