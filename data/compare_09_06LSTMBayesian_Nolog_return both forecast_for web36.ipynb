{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list((rawArrayDatas[1]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list((rawArrayDatas[1][:-forecastDay] ))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-(mockForcastDay+forecastDay)] & np.log\n",
    "    ds = rawArrayDatas[0][:-(mockForcastDay+forecastDay)]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-(mockForcastDay+forecastDay)]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of (mockForcastDay+forecastDay)  rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "    testY= rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출       \n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'day')\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "    \n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    print('LSTM realforecast :',realForecastDictionary['LSTM'])\n",
    "    print('Bayseian realforecast :',realForecastDictionary['Bayseian'] ) \n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "#         listedLogPredict=test_predict[-1].tolist()\n",
    "#     return [np.exp(y) for y in listedLogPredict]\n",
    "    return test_predict[-1].tolist()\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM testforecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian testforecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 7\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('webMonth36.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.2)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2013-02-01',\n",
       "  '2013-03-01',\n",
       "  '2013-04-01',\n",
       "  '2013-05-01',\n",
       "  '2013-06-01',\n",
       "  '2013-07-01',\n",
       "  '2013-08-01',\n",
       "  '2013-09-01',\n",
       "  '2013-10-01',\n",
       "  '2013-11-01',\n",
       "  '2013-12-01',\n",
       "  '2014-01-01',\n",
       "  '2014-02-01',\n",
       "  '2014-03-01',\n",
       "  '2014-04-01',\n",
       "  '2014-05-01',\n",
       "  '2014-06-01',\n",
       "  '2014-07-01',\n",
       "  '2014-08-01',\n",
       "  '2014-09-01',\n",
       "  '2014-10-01',\n",
       "  '2014-11-01',\n",
       "  '2014-12-01',\n",
       "  '2015-01-01',\n",
       "  '2015-02-01',\n",
       "  '2015-03-01',\n",
       "  '2015-04-01',\n",
       "  '2015-05-01',\n",
       "  '2015-06-01',\n",
       "  '2015-07-01',\n",
       "  '2015-08-01',\n",
       "  '2015-09-01',\n",
       "  '2015-10-01',\n",
       "  '2015-11-01',\n",
       "  '2015-12-01',\n",
       "  '2016-01-01'],\n",
       " [4432.5,\n",
       "  2776.5161290000001,\n",
       "  2286.6333329999998,\n",
       "  1913.451613,\n",
       "  1650.9333329999999,\n",
       "  2040.0333329999999,\n",
       "  2734.0645159999999,\n",
       "  13740.266669999999,\n",
       "  10564.35484,\n",
       "  7608.3999999999996,\n",
       "  8706.1935480000011,\n",
       "  26366.56667,\n",
       "  24504.89286,\n",
       "  2169.0322579999997,\n",
       "  1879.0,\n",
       "  2449.5483870000003,\n",
       "  1487.9000000000001,\n",
       "  1903.580645,\n",
       "  3121.333333,\n",
       "  8773.6333329999998,\n",
       "  9429.2903230000011,\n",
       "  4370.5666670000001,\n",
       "  4122.7096770000007,\n",
       "  7644.7419349999991,\n",
       "  4147.0740740000001,\n",
       "  1799.7096770000001,\n",
       "  1405.5,\n",
       "  1648.193548,\n",
       "  1090.5666670000001,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArrayDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 5.611518859863281\n",
      "[step: 1] loss: 5.297228813171387\n",
      "[step: 2] loss: 5.009056568145752\n",
      "[step: 3] loss: 4.743826866149902\n",
      "[step: 4] loss: 4.499009609222412\n",
      "[step: 5] loss: 4.273017406463623\n",
      "[step: 6] loss: 4.065521240234375\n",
      "[step: 7] loss: 3.8776276111602783\n",
      "[step: 8] loss: 3.712122917175293\n",
      "[step: 9] loss: 3.5735976696014404\n",
      "[step: 10] loss: 3.4679834842681885\n",
      "[step: 11] loss: 3.4005162715911865\n",
      "[step: 12] loss: 3.3710978031158447\n",
      "[step: 13] loss: 3.3691771030426025\n",
      "[step: 14] loss: 3.375135898590088\n",
      "[step: 15] loss: 3.3701696395874023\n",
      "[step: 16] loss: 3.345579147338867\n",
      "[step: 17] loss: 3.3039486408233643\n",
      "[step: 18] loss: 3.25403094291687\n",
      "[step: 19] loss: 3.2048285007476807\n",
      "[step: 20] loss: 3.162043809890747\n",
      "[step: 21] loss: 3.127261161804199\n",
      "[step: 22] loss: 3.0989184379577637\n",
      "[step: 23] loss: 3.0739097595214844\n",
      "[step: 24] loss: 3.048980712890625\n",
      "[step: 25] loss: 3.0215423107147217\n",
      "[step: 26] loss: 2.9899909496307373\n",
      "[step: 27] loss: 2.953716516494751\n",
      "[step: 28] loss: 2.9129910469055176\n",
      "[step: 29] loss: 2.868802070617676\n",
      "[step: 30] loss: 2.8226563930511475\n",
      "[step: 31] loss: 2.7762868404388428\n",
      "[step: 32] loss: 2.7312233448028564\n",
      "[step: 33] loss: 2.6882076263427734\n",
      "[step: 34] loss: 2.6466190814971924\n",
      "[step: 35] loss: 2.604321002960205\n",
      "[step: 36] loss: 2.558403253555298\n",
      "[step: 37] loss: 2.5067734718322754\n",
      "[step: 38] loss: 2.44950008392334\n",
      "[step: 39] loss: 2.3885936737060547\n",
      "[step: 40] loss: 2.326106309890747\n",
      "[step: 41] loss: 2.262131929397583\n",
      "[step: 42] loss: 2.194843292236328\n",
      "[step: 43] loss: 2.123034715652466\n",
      "[step: 44] loss: 2.0488102436065674\n",
      "[step: 45] loss: 1.9768387079238892\n",
      "[step: 46] loss: 1.9083802700042725\n",
      "[step: 47] loss: 1.8383347988128662\n",
      "[step: 48] loss: 1.7674697637557983\n",
      "[step: 49] loss: 1.7059415578842163\n",
      "[step: 50] loss: 1.6530488729476929\n",
      "[step: 51] loss: 1.6044400930404663\n",
      "[step: 52] loss: 1.5726475715637207\n",
      "[step: 53] loss: 1.5471017360687256\n",
      "[step: 54] loss: 1.5317707061767578\n",
      "[step: 55] loss: 1.522031307220459\n",
      "[step: 56] loss: 1.5141834020614624\n",
      "[step: 57] loss: 1.5031152963638306\n",
      "[step: 58] loss: 1.4909486770629883\n",
      "[step: 59] loss: 1.47060227394104\n",
      "[step: 60] loss: 1.451180338859558\n",
      "[step: 61] loss: 1.425297498703003\n",
      "[step: 62] loss: 1.4032126665115356\n",
      "[step: 63] loss: 1.3785579204559326\n",
      "[step: 64] loss: 1.3587390184402466\n",
      "[step: 65] loss: 1.3394827842712402\n",
      "[step: 66] loss: 1.3247027397155762\n",
      "[step: 67] loss: 1.310941457748413\n",
      "[step: 68] loss: 1.3007761240005493\n",
      "[step: 69] loss: 1.291137933731079\n",
      "[step: 70] loss: 1.283512830734253\n",
      "[step: 71] loss: 1.2751332521438599\n",
      "[step: 72] loss: 1.2676246166229248\n",
      "[step: 73] loss: 1.2588638067245483\n",
      "[step: 74] loss: 1.250287652015686\n",
      "[step: 75] loss: 1.2402641773223877\n",
      "[step: 76] loss: 1.2302165031433105\n",
      "[step: 77] loss: 1.2190821170806885\n",
      "[step: 78] loss: 1.2077412605285645\n",
      "[step: 79] loss: 1.1960532665252686\n",
      "[step: 80] loss: 1.1841925382614136\n",
      "[step: 81] loss: 1.1729971170425415\n",
      "[step: 82] loss: 1.1615849733352661\n",
      "[step: 83] loss: 1.1509342193603516\n",
      "[step: 84] loss: 1.1406503915786743\n",
      "[step: 85] loss: 1.1302087306976318\n",
      "[step: 86] loss: 1.1201932430267334\n",
      "[step: 87] loss: 1.1104490756988525\n",
      "[step: 88] loss: 1.1003416776657104\n",
      "[step: 89] loss: 1.0898653268814087\n",
      "[step: 90] loss: 1.078951120376587\n",
      "[step: 91] loss: 1.0678203105926514\n",
      "[step: 92] loss: 1.0564113855361938\n",
      "[step: 93] loss: 1.0448381900787354\n",
      "[step: 94] loss: 1.0331546068191528\n",
      "[step: 95] loss: 1.0218864679336548\n",
      "[step: 96] loss: 1.0121819972991943\n",
      "[step: 97] loss: 1.0101088285446167\n",
      "[step: 98] loss: 1.0207388401031494\n",
      "[step: 99] loss: 1.022940993309021\n",
      "[step: 100] loss: 0.9702349901199341\n",
      "[step: 101] loss: 1.0079737901687622\n",
      "[step: 102] loss: 0.985893726348877\n",
      "[step: 103] loss: 0.9657167792320251\n",
      "[step: 104] loss: 0.978630006313324\n",
      "[step: 105] loss: 0.9326930642127991\n",
      "[step: 106] loss: 0.9561048150062561\n",
      "[step: 107] loss: 0.9184357523918152\n",
      "[step: 108] loss: 0.9420413374900818\n",
      "[step: 109] loss: 0.9097561836242676\n",
      "[step: 110] loss: 0.9175488948822021\n",
      "[step: 111] loss: 0.8980092406272888\n",
      "[step: 112] loss: 0.8952696919441223\n",
      "[step: 113] loss: 0.892051637172699\n",
      "[step: 114] loss: 0.8769096732139587\n",
      "[step: 115] loss: 0.8836604356765747\n",
      "[step: 116] loss: 0.8608202934265137\n",
      "[step: 117] loss: 0.8673269748687744\n",
      "[step: 118] loss: 0.8516114950180054\n",
      "[step: 119] loss: 0.8498126864433289\n",
      "[step: 120] loss: 0.8464813232421875\n",
      "[step: 121] loss: 0.8337863683700562\n",
      "[step: 122] loss: 0.8379510641098022\n",
      "[step: 123] loss: 0.8244309425354004\n",
      "[step: 124] loss: 0.8217556476593018\n",
      "[step: 125] loss: 0.8183020353317261\n",
      "[step: 126] loss: 0.8072837591171265\n",
      "[step: 127] loss: 0.8073866963386536\n",
      "[step: 128] loss: 0.7988240122795105\n",
      "[step: 129] loss: 0.7932143807411194\n",
      "[step: 130] loss: 0.7908580899238586\n",
      "[step: 131] loss: 0.7816746830940247\n",
      "[step: 132] loss: 0.7784765362739563\n",
      "[step: 133] loss: 0.7735033631324768\n",
      "[step: 134] loss: 0.7650380730628967\n",
      "[step: 135] loss: 0.761944055557251\n",
      "[step: 136] loss: 0.7555583119392395\n",
      "[step: 137] loss: 0.7475059032440186\n",
      "[step: 138] loss: 0.7436902523040771\n",
      "[step: 139] loss: 0.7366484999656677\n",
      "[step: 140] loss: 0.7285909652709961\n",
      "[step: 141] loss: 0.7239190340042114\n",
      "[step: 142] loss: 0.7165493369102478\n",
      "[step: 143] loss: 0.7082570791244507\n",
      "[step: 144] loss: 0.7027449607849121\n",
      "[step: 145] loss: 0.6951694488525391\n",
      "[step: 146] loss: 0.6864888072013855\n",
      "[step: 147] loss: 0.6801003813743591\n",
      "[step: 148] loss: 0.672353208065033\n",
      "[step: 149] loss: 0.663179874420166\n",
      "[step: 150] loss: 0.6557949185371399\n",
      "[step: 151] loss: 0.647883951663971\n",
      "[step: 152] loss: 0.6382587552070618\n",
      "[step: 153] loss: 0.6296495795249939\n",
      "[step: 154] loss: 0.6214139461517334\n",
      "[step: 155] loss: 0.6116654276847839\n",
      "[step: 156] loss: 0.6017720699310303\n",
      "[step: 157] loss: 0.5926079750061035\n",
      "[step: 158] loss: 0.5830233097076416\n",
      "[step: 159] loss: 0.5725658535957336\n",
      "[step: 160] loss: 0.5619205236434937\n",
      "[step: 161] loss: 0.5516331195831299\n",
      "[step: 162] loss: 0.5412895679473877\n",
      "[step: 163] loss: 0.5303213000297546\n",
      "[step: 164] loss: 0.5189229249954224\n",
      "[step: 165] loss: 0.5073605179786682\n",
      "[step: 166] loss: 0.49586036801338196\n",
      "[step: 167] loss: 0.4843871295452118\n",
      "[step: 168] loss: 0.47282925248146057\n",
      "[step: 169] loss: 0.4612487852573395\n",
      "[step: 170] loss: 0.44962453842163086\n",
      "[step: 171] loss: 0.43842893838882446\n",
      "[step: 172] loss: 0.42783746123313904\n",
      "[step: 173] loss: 0.4195640981197357\n",
      "[step: 174] loss: 0.4130808115005493\n",
      "[step: 175] loss: 0.4113428592681885\n",
      "[step: 176] loss: 0.3975607454776764\n",
      "[step: 177] loss: 0.3771563172340393\n",
      "[step: 178] loss: 0.35427239537239075\n",
      "[step: 179] loss: 0.3473651707172394\n",
      "[step: 180] loss: 0.34665948152542114\n",
      "[step: 181] loss: 0.3309459388256073\n",
      "[step: 182] loss: 0.3112581670284271\n",
      "[step: 183] loss: 0.3018871545791626\n",
      "[step: 184] loss: 0.2984446883201599\n",
      "[step: 185] loss: 0.28853604197502136\n",
      "[step: 186] loss: 0.2712598443031311\n",
      "[step: 187] loss: 0.2606525719165802\n",
      "[step: 188] loss: 0.2564954459667206\n",
      "[step: 189] loss: 0.24887828528881073\n",
      "[step: 190] loss: 0.23685264587402344\n",
      "[step: 191] loss: 0.22488799691200256\n",
      "[step: 192] loss: 0.21727792918682098\n",
      "[step: 193] loss: 0.21282914280891418\n",
      "[step: 194] loss: 0.2074158936738968\n",
      "[step: 195] loss: 0.19847235083580017\n",
      "[step: 196] loss: 0.18859072029590607\n",
      "[step: 197] loss: 0.1805158108472824\n",
      "[step: 198] loss: 0.17514055967330933\n",
      "[step: 199] loss: 0.17149735987186432\n",
      "[step: 200] loss: 0.168235182762146\n",
      "[step: 201] loss: 0.16499054431915283\n",
      "[step: 202] loss: 0.16002590954303741\n",
      "[step: 203] loss: 0.1541593223810196\n",
      "[step: 204] loss: 0.1463664323091507\n",
      "[step: 205] loss: 0.13941040635108948\n",
      "[step: 206] loss: 0.13417844474315643\n",
      "[step: 207] loss: 0.1308184117078781\n",
      "[step: 208] loss: 0.12885655462741852\n",
      "[step: 209] loss: 0.12768752872943878\n",
      "[step: 210] loss: 0.12781760096549988\n",
      "[step: 211] loss: 0.12713076174259186\n",
      "[step: 212] loss: 0.12657375633716583\n",
      "[step: 213] loss: 0.11963546276092529\n",
      "[step: 214] loss: 0.11143719404935837\n",
      "[step: 215] loss: 0.10419566929340363\n",
      "[step: 216] loss: 0.1016782596707344\n",
      "[step: 217] loss: 0.10264986008405685\n",
      "[step: 218] loss: 0.10317913442850113\n",
      "[step: 219] loss: 0.10195658355951309\n",
      "[step: 220] loss: 0.09632040560245514\n",
      "[step: 221] loss: 0.09063787013292313\n",
      "[step: 222] loss: 0.08663184195756912\n",
      "[step: 223] loss: 0.08514496684074402\n",
      "[step: 224] loss: 0.08517717570066452\n",
      "[step: 225] loss: 0.08523745834827423\n",
      "[step: 226] loss: 0.08539266884326935\n",
      "[step: 227] loss: 0.08331912010908127\n",
      "[step: 228] loss: 0.08068740367889404\n",
      "[step: 229] loss: 0.07629574835300446\n",
      "[step: 230] loss: 0.0725715309381485\n",
      "[step: 231] loss: 0.06990223377943039\n",
      "[step: 232] loss: 0.0684930607676506\n",
      "[step: 233] loss: 0.06802700459957123\n",
      "[step: 234] loss: 0.06817714869976044\n",
      "[step: 235] loss: 0.06934153288602829\n",
      "[step: 236] loss: 0.07078929990530014\n",
      "[step: 237] loss: 0.07424630224704742\n",
      "[step: 238] loss: 0.0751340314745903\n",
      "[step: 239] loss: 0.07643841952085495\n",
      "[step: 240] loss: 0.06969286501407623\n",
      "[step: 241] loss: 0.06283678859472275\n",
      "[step: 242] loss: 0.05709116905927658\n",
      "[step: 243] loss: 0.05616583302617073\n",
      "[step: 244] loss: 0.05862702429294586\n",
      "[step: 245] loss: 0.060803934931755066\n",
      "[step: 246] loss: 0.061956994235515594\n",
      "[step: 247] loss: 0.05845876410603523\n",
      "[step: 248] loss: 0.05474530905485153\n",
      "[step: 249] loss: 0.05100703984498978\n",
      "[step: 250] loss: 0.04952431842684746\n",
      "[step: 251] loss: 0.05001808702945709\n",
      "[step: 252] loss: 0.05135779455304146\n",
      "[step: 253] loss: 0.05312786623835564\n",
      "[step: 254] loss: 0.05269811674952507\n",
      "[step: 255] loss: 0.052120696753263474\n",
      "[step: 256] loss: 0.04899195209145546\n",
      "[step: 257] loss: 0.04634900763630867\n",
      "[step: 258] loss: 0.04401096701622009\n",
      "[step: 259] loss: 0.042711615562438965\n",
      "[step: 260] loss: 0.04223502054810524\n",
      "[step: 261] loss: 0.04226101189851761\n",
      "[step: 262] loss: 0.04274476319551468\n",
      "[step: 263] loss: 0.04330798238515854\n",
      "[step: 264] loss: 0.044677309691905975\n",
      "[step: 265] loss: 0.04566945135593414\n",
      "[step: 266] loss: 0.048001691699028015\n",
      "[step: 267] loss: 0.04833602160215378\n",
      "[step: 268] loss: 0.0498107448220253\n",
      "[step: 269] loss: 0.04683656990528107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 270] loss: 0.04434351250529289\n",
      "[step: 271] loss: 0.03949549421668053\n",
      "[step: 272] loss: 0.036308176815509796\n",
      "[step: 273] loss: 0.03470148146152496\n",
      "[step: 274] loss: 0.034802988171577454\n",
      "[step: 275] loss: 0.03599502146244049\n",
      "[step: 276] loss: 0.03722159191966057\n",
      "[step: 277] loss: 0.0388123095035553\n",
      "[step: 278] loss: 0.038601115345954895\n",
      "[step: 279] loss: 0.0388236828148365\n",
      "[step: 280] loss: 0.03652128949761391\n",
      "[step: 281] loss: 0.03472968935966492\n",
      "[step: 282] loss: 0.032148417085409164\n",
      "[step: 283] loss: 0.030379198491573334\n",
      "[step: 284] loss: 0.0293203666806221\n",
      "[step: 285] loss: 0.02897919900715351\n",
      "[step: 286] loss: 0.02913524955511093\n",
      "[step: 287] loss: 0.029504068195819855\n",
      "[step: 288] loss: 0.03024415299296379\n",
      "[step: 289] loss: 0.030851095914840698\n",
      "[step: 290] loss: 0.03240402042865753\n",
      "[step: 291] loss: 0.03355005010962486\n",
      "[step: 292] loss: 0.036635030061006546\n",
      "[step: 293] loss: 0.0380077101290226\n",
      "[step: 294] loss: 0.04149849712848663\n",
      "[step: 295] loss: 0.039483971893787384\n",
      "[step: 296] loss: 0.037554364651441574\n",
      "[step: 297] loss: 0.03090737946331501\n",
      "[step: 298] loss: 0.025934137403964996\n",
      "[step: 299] loss: 0.02379273809492588\n",
      "[step: 300] loss: 0.024951238185167313\n",
      "[step: 301] loss: 0.027865499258041382\n",
      "[step: 302] loss: 0.029612122103571892\n",
      "[step: 303] loss: 0.030281690880656242\n",
      "[step: 304] loss: 0.02768251672387123\n",
      "[step: 305] loss: 0.025047162547707558\n",
      "[step: 306] loss: 0.022722626104950905\n",
      "[step: 307] loss: 0.021990327164530754\n",
      "[step: 308] loss: 0.022455917671322823\n",
      "[step: 309] loss: 0.023279542103409767\n",
      "[step: 310] loss: 0.024014435708522797\n",
      "[step: 311] loss: 0.02380986511707306\n",
      "[step: 312] loss: 0.02339359000325203\n",
      "[step: 313] loss: 0.02247035875916481\n",
      "[step: 314] loss: 0.02206217683851719\n",
      "[step: 315] loss: 0.021795406937599182\n",
      "[step: 316] loss: 0.02234693430364132\n",
      "[step: 317] loss: 0.022609777748584747\n",
      "[step: 318] loss: 0.02328168787062168\n",
      "[step: 319] loss: 0.02277945540845394\n",
      "[step: 320] loss: 0.022231651470065117\n",
      "[step: 321] loss: 0.020657263696193695\n",
      "[step: 322] loss: 0.01948809064924717\n",
      "[step: 323] loss: 0.01861381344497204\n",
      "[step: 324] loss: 0.018596511334180832\n",
      "[step: 325] loss: 0.01913994364440441\n",
      "[step: 326] loss: 0.02027101442217827\n",
      "[step: 327] loss: 0.021344885230064392\n",
      "[step: 328] loss: 0.023237228393554688\n",
      "[step: 329] loss: 0.024878602474927902\n",
      "[step: 330] loss: 0.02908124029636383\n",
      "[step: 331] loss: 0.032477330416440964\n",
      "[step: 332] loss: 0.04150468111038208\n",
      "[step: 333] loss: 0.04054594784975052\n",
      "[step: 334] loss: 0.04319487139582634\n",
      "[step: 335] loss: 0.027817074209451675\n",
      "[step: 336] loss: 0.017898790538311005\n",
      "[step: 337] loss: 0.015636855736374855\n",
      "[step: 338] loss: 0.02131439931690693\n",
      "[step: 339] loss: 0.028658630326390266\n",
      "[step: 340] loss: 0.025059467181563377\n",
      "[step: 341] loss: 0.019044512882828712\n",
      "[step: 342] loss: 0.01437019556760788\n",
      "[step: 343] loss: 0.01604885421693325\n",
      "[step: 344] loss: 0.020639561116695404\n",
      "[step: 345] loss: 0.020527979359030724\n",
      "[step: 346] loss: 0.01781100034713745\n",
      "[step: 347] loss: 0.014476388692855835\n",
      "[step: 348] loss: 0.01445088442414999\n",
      "[step: 349] loss: 0.0164262056350708\n",
      "[step: 350] loss: 0.016622859984636307\n",
      "[step: 351] loss: 0.015469187870621681\n",
      "[step: 352] loss: 0.013867512345314026\n",
      "[step: 353] loss: 0.013846976682543755\n",
      "[step: 354] loss: 0.01468801312148571\n",
      "[step: 355] loss: 0.014552365988492966\n",
      "[step: 356] loss: 0.013527344912290573\n",
      "[step: 357] loss: 0.01238267868757248\n",
      "[step: 358] loss: 0.012385877780616283\n",
      "[step: 359] loss: 0.013156548142433167\n",
      "[step: 360] loss: 0.013473380357027054\n",
      "[step: 361] loss: 0.012938250787556171\n",
      "[step: 362] loss: 0.011906720697879791\n",
      "[step: 363] loss: 0.011334787122905254\n",
      "[step: 364] loss: 0.011453074403107166\n",
      "[step: 365] loss: 0.011778331361711025\n",
      "[step: 366] loss: 0.011835755780339241\n",
      "[step: 367] loss: 0.011402101255953312\n",
      "[step: 368] loss: 0.01090420875698328\n",
      "[step: 369] loss: 0.010668129660189152\n",
      "[step: 370] loss: 0.010750792920589447\n",
      "[step: 371] loss: 0.010955090634524822\n",
      "[step: 372] loss: 0.01097166445106268\n",
      "[step: 373] loss: 0.01082862913608551\n",
      "[step: 374] loss: 0.01053647231310606\n",
      "[step: 375] loss: 0.010361402295529842\n",
      "[step: 376] loss: 0.010361436754465103\n",
      "[step: 377] loss: 0.010591832920908928\n",
      "[step: 378] loss: 0.010928398929536343\n",
      "[step: 379] loss: 0.011477923020720482\n",
      "[step: 380] loss: 0.012110811658203602\n",
      "[step: 381] loss: 0.013401213102042675\n",
      "[step: 382] loss: 0.015026210807263851\n",
      "[step: 383] loss: 0.018496841192245483\n",
      "[step: 384] loss: 0.02185550332069397\n",
      "[step: 385] loss: 0.028397243469953537\n",
      "[step: 386] loss: 0.029819661751389503\n",
      "[step: 387] loss: 0.032302916049957275\n",
      "[step: 388] loss: 0.023655463010072708\n",
      "[step: 389] loss: 0.015698326751589775\n",
      "[step: 390] loss: 0.009371783584356308\n",
      "[step: 391] loss: 0.009467015974223614\n",
      "[step: 392] loss: 0.013904671184718609\n",
      "[step: 393] loss: 0.01681121625006199\n",
      "[step: 394] loss: 0.016795745119452477\n",
      "[step: 395] loss: 0.012107652612030506\n",
      "[step: 396] loss: 0.008758274838328362\n",
      "[step: 397] loss: 0.008615138940513134\n",
      "[step: 398] loss: 0.010716792196035385\n",
      "[step: 399] loss: 0.012500550597906113\n",
      "[step: 400] loss: 0.011410445906221867\n",
      "[step: 401] loss: 0.009355537593364716\n",
      "[step: 402] loss: 0.007847926579415798\n",
      "[step: 403] loss: 0.008149822242558002\n",
      "[step: 404] loss: 0.009307029657065868\n",
      "[step: 405] loss: 0.00956928450614214\n",
      "[step: 406] loss: 0.0087732607498765\n",
      "[step: 407] loss: 0.007426994852721691\n",
      "[step: 408] loss: 0.006881565786898136\n",
      "[step: 409] loss: 0.007333473302423954\n",
      "[step: 410] loss: 0.00801790039986372\n",
      "[step: 411] loss: 0.00814827997237444\n",
      "[step: 412] loss: 0.007413663901388645\n",
      "[step: 413] loss: 0.006544626783579588\n",
      "[step: 414] loss: 0.006122746039181948\n",
      "[step: 415] loss: 0.006301499903202057\n",
      "[step: 416] loss: 0.006712082773447037\n",
      "[step: 417] loss: 0.0068501862697303295\n",
      "[step: 418] loss: 0.006623010151088238\n",
      "[step: 419] loss: 0.006151643581688404\n",
      "[step: 420] loss: 0.005782266613095999\n",
      "[step: 421] loss: 0.005679292604327202\n",
      "[step: 422] loss: 0.0057822661474347115\n",
      "[step: 423] loss: 0.0059202141128480434\n",
      "[step: 424] loss: 0.005904767662286758\n",
      "[step: 425] loss: 0.00573879387229681\n",
      "[step: 426] loss: 0.005456637591123581\n",
      "[step: 427] loss: 0.005205721128731966\n",
      "[step: 428] loss: 0.005055212415754795\n",
      "[step: 429] loss: 0.005021501332521439\n",
      "[step: 430] loss: 0.005057171918451786\n",
      "[step: 431] loss: 0.005101431161165237\n",
      "[step: 432] loss: 0.005113794002681971\n",
      "[step: 433] loss: 0.005087617319077253\n",
      "[step: 434] loss: 0.00505615072324872\n",
      "[step: 435] loss: 0.005095163360238075\n",
      "[step: 436] loss: 0.005254590883851051\n",
      "[step: 437] loss: 0.005749509669840336\n",
      "[step: 438] loss: 0.006577010732144117\n",
      "[step: 439] loss: 0.008513649925589561\n",
      "[step: 440] loss: 0.011017844080924988\n",
      "[step: 441] loss: 0.016995344310998917\n",
      "[step: 442] loss: 0.021288316696882248\n",
      "[step: 443] loss: 0.032085198909044266\n",
      "[step: 444] loss: 0.027509590610861778\n",
      "[step: 445] loss: 0.025432277470827103\n",
      "[step: 446] loss: 0.011862697079777718\n",
      "[step: 447] loss: 0.005969554651528597\n",
      "[step: 448] loss: 0.00821311492472887\n",
      "[step: 449] loss: 0.01292090117931366\n",
      "[step: 450] loss: 0.01554129272699356\n",
      "[step: 451] loss: 0.009350688196718693\n",
      "[step: 452] loss: 0.00518923019990325\n",
      "[step: 453] loss: 0.006021352019160986\n",
      "[step: 454] loss: 0.009333640336990356\n",
      "[step: 455] loss: 0.009880374185740948\n",
      "[step: 456] loss: 0.0056808930821716785\n",
      "[step: 457] loss: 0.0032817835453897715\n",
      "[step: 458] loss: 0.00495419604703784\n",
      "[step: 459] loss: 0.007240416016429663\n",
      "[step: 460] loss: 0.00687360530719161\n",
      "[step: 461] loss: 0.004279252141714096\n",
      "[step: 462] loss: 0.0033038409892469645\n",
      "[step: 463] loss: 0.004546364303678274\n",
      "[step: 464] loss: 0.005180791486054659\n",
      "[step: 465] loss: 0.004269566852599382\n",
      "[step: 466] loss: 0.0029997159726917744\n",
      "[step: 467] loss: 0.0032318541780114174\n",
      "[step: 468] loss: 0.004268885124474764\n",
      "[step: 469] loss: 0.004195712506771088\n",
      "[step: 470] loss: 0.003280792385339737\n",
      "[step: 471] loss: 0.0025947026442736387\n",
      "[step: 472] loss: 0.002862051595002413\n",
      "[step: 473] loss: 0.0033634824212640524\n",
      "[step: 474] loss: 0.0031747121829539537\n",
      "[step: 475] loss: 0.002633766271173954\n",
      "[step: 476] loss: 0.0024085224140435457\n",
      "[step: 477] loss: 0.002683103084564209\n",
      "[step: 478] loss: 0.002932052593678236\n",
      "[step: 479] loss: 0.002740254858508706\n",
      "[step: 480] loss: 0.0023386769462376833\n",
      "[step: 481] loss: 0.0021484384778887033\n",
      "[step: 482] loss: 0.0022552309092134237\n",
      "[step: 483] loss: 0.0023850123398005962\n",
      "[step: 484] loss: 0.002275261329486966\n",
      "[step: 485] loss: 0.0020405263639986515\n",
      "[step: 486] loss: 0.0019003782654181123\n",
      "[step: 487] loss: 0.001943666022270918\n",
      "[step: 488] loss: 0.0020421668887138367\n",
      "[step: 489] loss: 0.0020277216099202633\n",
      "[step: 490] loss: 0.0019094336312264204\n",
      "[step: 491] loss: 0.0017869049916043878\n",
      "[step: 492] loss: 0.001759005943313241\n",
      "[step: 493] loss: 0.0018060313304886222\n",
      "[step: 494] loss: 0.0018462140578776598\n",
      "[step: 495] loss: 0.0018239705823361874\n",
      "[step: 496] loss: 0.0017680525779724121\n",
      "[step: 497] loss: 0.0017383365193381906\n",
      "[step: 498] loss: 0.0017881799722090364\n",
      "[step: 499] loss: 0.001907038502395153\n",
      "[step: 500] loss: 0.002098690951243043\n",
      "[step: 501] loss: 0.0023362201172858477\n",
      "[step: 502] loss: 0.0027501313015818596\n",
      "[step: 503] loss: 0.0033591093961149454\n",
      "[step: 504] loss: 0.004529387224465609\n",
      "[step: 505] loss: 0.006132063455879688\n",
      "[step: 506] loss: 0.009120815433561802\n",
      "[step: 507] loss: 0.012335611507296562\n",
      "[step: 508] loss: 0.018067985773086548\n",
      "[step: 509] loss: 0.0208719652146101\n",
      "[step: 510] loss: 0.02491864748299122\n",
      "[step: 511] loss: 0.019566362723708153\n",
      "[step: 512] loss: 0.013260005973279476\n",
      "[step: 513] loss: 0.0048294332809746265\n",
      "[step: 514] loss: 0.0015536302234977484\n",
      "[step: 515] loss: 0.0037929387763142586\n",
      "[step: 516] loss: 0.007833057083189487\n",
      "[step: 517] loss: 0.01022436935454607\n",
      "[step: 518] loss: 0.007395787164568901\n",
      "[step: 519] loss: 0.003632561769336462\n",
      "[step: 520] loss: 0.001523173414170742\n",
      "[step: 521] loss: 0.0025166610721498728\n",
      "[step: 522] loss: 0.004731838125735521\n",
      "[step: 523] loss: 0.005196798592805862\n",
      "[step: 524] loss: 0.004001406487077475\n",
      "[step: 525] loss: 0.002080655423924327\n",
      "[step: 526] loss: 0.0015104442136362195\n",
      "[step: 527] loss: 0.002247053664177656\n",
      "[step: 528] loss: 0.00295398011803627\n",
      "[step: 529] loss: 0.002830060198903084\n",
      "[step: 530] loss: 0.001898877788335085\n",
      "[step: 531] loss: 0.001337122288532555\n",
      "[step: 532] loss: 0.0015043208841234446\n",
      "[step: 533] loss: 0.0019337288103997707\n",
      "[step: 534] loss: 0.0019868051167577505\n",
      "[step: 535] loss: 0.0014950496843084693\n",
      "[step: 536] loss: 0.001056151115335524\n",
      "[step: 537] loss: 0.0010435099247843027\n",
      "[step: 538] loss: 0.0013526038965210319\n",
      "[step: 539] loss: 0.001537061296403408\n",
      "[step: 540] loss: 0.001330487197265029\n",
      "[step: 541] loss: 0.000956097966991365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 542] loss: 0.000734774221200496\n",
      "[step: 543] loss: 0.0008249462116509676\n",
      "[step: 544] loss: 0.0010573087492957711\n",
      "[step: 545] loss: 0.0011574135860428214\n",
      "[step: 546] loss: 0.001027655671350658\n",
      "[step: 547] loss: 0.0007737537962384522\n",
      "[step: 548] loss: 0.0006082087638787925\n",
      "[step: 549] loss: 0.0006244428222998977\n",
      "[step: 550] loss: 0.0007500886567868292\n",
      "[step: 551] loss: 0.0008436397765763104\n",
      "[step: 552] loss: 0.0008156262338161469\n",
      "[step: 553] loss: 0.0006992415874265134\n",
      "[step: 554] loss: 0.0005841598613187671\n",
      "[step: 555] loss: 0.0005408097058534622\n",
      "[step: 556] loss: 0.0005685418727807701\n",
      "[step: 557] loss: 0.0006140595069155097\n",
      "[step: 558] loss: 0.0006271435413509607\n",
      "[step: 559] loss: 0.0005907340673729777\n",
      "[step: 560] loss: 0.0005335572641342878\n",
      "[step: 561] loss: 0.000490402162540704\n",
      "[step: 562] loss: 0.0004802475159522146\n",
      "[step: 563] loss: 0.0004960047663189471\n",
      "[step: 564] loss: 0.0005133957020007074\n",
      "[step: 565] loss: 0.0005135115934535861\n",
      "[step: 566] loss: 0.000489737547468394\n",
      "[step: 567] loss: 0.00045352333108894527\n",
      "[step: 568] loss: 0.0004197971720714122\n",
      "[step: 569] loss: 0.0004003603826276958\n",
      "[step: 570] loss: 0.00039679292240180075\n",
      "[step: 571] loss: 0.0004027152026537806\n",
      "[step: 572] loss: 0.0004088534042239189\n",
      "[step: 573] loss: 0.0004080864309798926\n",
      "[step: 574] loss: 0.00039933910011313856\n",
      "[step: 575] loss: 0.00038487947313115\n",
      "[step: 576] loss: 0.0003700941742863506\n",
      "[step: 577] loss: 0.00035888608545064926\n",
      "[step: 578] loss: 0.00035337143344804645\n",
      "[step: 579] loss: 0.00035294293775223196\n",
      "[step: 580] loss: 0.00035612686770036817\n",
      "[step: 581] loss: 0.0003604299563448876\n",
      "[step: 582] loss: 0.00036554952384904027\n",
      "[step: 583] loss: 0.0003708468866534531\n",
      "[step: 584] loss: 0.000379951176000759\n",
      "[step: 585] loss: 0.00039440402179025114\n",
      "[step: 586] loss: 0.000423421966843307\n",
      "[step: 587] loss: 0.00046979301259852946\n",
      "[step: 588] loss: 0.0005558394477702677\n",
      "[step: 589] loss: 0.0006859332206659019\n",
      "[step: 590] loss: 0.0009246416157111526\n",
      "[step: 591] loss: 0.001272565801627934\n",
      "[step: 592] loss: 0.0019378249999135733\n",
      "[step: 593] loss: 0.002849199576303363\n",
      "[step: 594] loss: 0.004705856554210186\n",
      "[step: 595] loss: 0.00686534121632576\n",
      "[step: 596] loss: 0.01155885774642229\n",
      "[step: 597] loss: 0.014908729121088982\n",
      "[step: 598] loss: 0.02267550677061081\n",
      "[step: 599] loss: 0.021382272243499756\n",
      "[step: 600] loss: 0.022290507331490517\n",
      "[step: 601] loss: 0.012180009856820107\n",
      "[step: 602] loss: 0.005087380297482014\n",
      "[step: 603] loss: 0.0021677629556506872\n",
      "[step: 604] loss: 0.004736635833978653\n",
      "[step: 605] loss: 0.009361663833260536\n",
      "[step: 606] loss: 0.008681067265570164\n",
      "[step: 607] loss: 0.0057154456153512\n",
      "[step: 608] loss: 0.0014282099436968565\n",
      "[step: 609] loss: 0.0011796130565926433\n",
      "[step: 610] loss: 0.00407383544370532\n",
      "[step: 611] loss: 0.005410723853856325\n",
      "[step: 612] loss: 0.0041901590302586555\n",
      "[step: 613] loss: 0.0011916878866031766\n",
      "[step: 614] loss: 0.0003709570155479014\n",
      "[step: 615] loss: 0.0018818866228684783\n",
      "[step: 616] loss: 0.002997366013005376\n",
      "[step: 617] loss: 0.0025296967942267656\n",
      "[step: 618] loss: 0.0009125236538238823\n",
      "[step: 619] loss: 0.0004877365136053413\n",
      "[step: 620] loss: 0.0013959932839497924\n",
      "[step: 621] loss: 0.0019461663905531168\n",
      "[step: 622] loss: 0.001466349815018475\n",
      "[step: 623] loss: 0.00045468570897355676\n",
      "[step: 624] loss: 0.0002567933697719127\n",
      "[step: 625] loss: 0.0008597715641371906\n",
      "[step: 626] loss: 0.0012393456418067217\n",
      "[step: 627] loss: 0.0009770590113475919\n",
      "[step: 628] loss: 0.00040936595178209245\n",
      "[step: 629] loss: 0.00029683863976970315\n",
      "[step: 630] loss: 0.0006202275981195271\n",
      "[step: 631] loss: 0.00080248957965523\n",
      "[step: 632] loss: 0.0006048051873221993\n",
      "[step: 633] loss: 0.0002524852752685547\n",
      "[step: 634] loss: 0.000175704772118479\n",
      "[step: 635] loss: 0.00037727749440819025\n",
      "[step: 636] loss: 0.0005272274138405919\n",
      "[step: 637] loss: 0.00045000124373473227\n",
      "[step: 638] loss: 0.0002515995583962649\n",
      "[step: 639] loss: 0.0001848191168392077\n",
      "[step: 640] loss: 0.00028453581035137177\n",
      "[step: 641] loss: 0.00037856740527786314\n",
      "[step: 642] loss: 0.0003451680240686983\n",
      "[step: 643] loss: 0.0002173105749534443\n",
      "[step: 644] loss: 0.00014275041758082807\n",
      "[step: 645] loss: 0.0001745418121572584\n",
      "[step: 646] loss: 0.00024037736875470728\n",
      "[step: 647] loss: 0.00025098881451413035\n",
      "[step: 648] loss: 0.00019136253104079515\n",
      "[step: 649] loss: 0.00013337463315110654\n",
      "[step: 650] loss: 0.00013030014815740287\n",
      "[step: 651] loss: 0.00016964413225650787\n",
      "[step: 652] loss: 0.00019915202574338764\n",
      "[step: 653] loss: 0.0001856747840065509\n",
      "[step: 654] loss: 0.00014925829600542784\n",
      "[step: 655] loss: 0.00012707665155176073\n",
      "[step: 656] loss: 0.0001363879127893597\n",
      "[step: 657] loss: 0.00016105581016745418\n",
      "[step: 658] loss: 0.0001734903926262632\n",
      "[step: 659] loss: 0.0001660917914705351\n",
      "[step: 660] loss: 0.00015094364061951637\n",
      "[step: 661] loss: 0.00014881101378705353\n",
      "[step: 662] loss: 0.00016603186668362468\n",
      "[step: 663] loss: 0.00019610047456808388\n",
      "[step: 664] loss: 0.00022717025422025472\n",
      "[step: 665] loss: 0.00026111112674698234\n",
      "[step: 666] loss: 0.0003072603140026331\n",
      "[step: 667] loss: 0.0003919715527445078\n",
      "[step: 668] loss: 0.0005260747857391834\n",
      "[step: 669] loss: 0.0007512208539992571\n",
      "[step: 670] loss: 0.0010712493676692247\n",
      "[step: 671] loss: 0.001609106082469225\n",
      "[step: 672] loss: 0.0023574333172291517\n",
      "[step: 673] loss: 0.0036810303572565317\n",
      "[step: 674] loss: 0.005371635779738426\n",
      "[step: 675] loss: 0.008388550952076912\n",
      "[step: 676] loss: 0.011350350454449654\n",
      "[step: 677] loss: 0.01633855514228344\n",
      "[step: 678] loss: 0.018072273582220078\n",
      "[step: 679] loss: 0.020238317549228668\n",
      "[step: 680] loss: 0.014829812571406364\n",
      "[step: 681] loss: 0.008846664801239967\n",
      "[step: 682] loss: 0.002431117231026292\n",
      "[step: 683] loss: 0.00039044691948220134\n",
      "[step: 684] loss: 0.0025821540039032698\n",
      "[step: 685] loss: 0.005824512802064419\n",
      "[step: 686] loss: 0.007569354493170977\n",
      "[step: 687] loss: 0.005395084153860807\n",
      "[step: 688] loss: 0.0024510533548891544\n",
      "[step: 689] loss: 0.0006593980942852795\n",
      "[step: 690] loss: 0.0011642255121842027\n",
      "[step: 691] loss: 0.0027173971757292747\n",
      "[step: 692] loss: 0.003295793430879712\n",
      "[step: 693] loss: 0.0027094522956758738\n",
      "[step: 694] loss: 0.0014053188497200608\n",
      "[step: 695] loss: 0.0007780540036037564\n",
      "[step: 696] loss: 0.0009471549419686198\n",
      "[step: 697] loss: 0.0013471620623022318\n",
      "[step: 698] loss: 0.0014506129082292318\n",
      "[step: 699] loss: 0.0011232305550947785\n",
      "[step: 700] loss: 0.0008500881376676261\n",
      "[step: 701] loss: 0.000732827465981245\n",
      "[step: 702] loss: 0.0007304470054805279\n",
      "[step: 703] loss: 0.0006585022783838212\n",
      "[step: 704] loss: 0.0005507854511961341\n",
      "[step: 705] loss: 0.0005613009561784565\n",
      "[step: 706] loss: 0.0006257944041863084\n",
      "[step: 707] loss: 0.0006369966431520879\n",
      "[step: 708] loss: 0.00045914604561403394\n",
      "[step: 709] loss: 0.0002486555022187531\n",
      "[step: 710] loss: 0.0001886271347757429\n",
      "[step: 711] loss: 0.00030962954042479396\n",
      "[step: 712] loss: 0.0004683892766479403\n",
      "[step: 713] loss: 0.0004575057828333229\n",
      "[step: 714] loss: 0.00029701704625040293\n",
      "[step: 715] loss: 0.00011428073776187375\n",
      "[step: 716] loss: 6.847539043519646e-05\n",
      "[step: 717] loss: 0.0001659270201344043\n",
      "[step: 718] loss: 0.00028020868194289505\n",
      "[step: 719] loss: 0.00030268041882663965\n",
      "[step: 720] loss: 0.00020870471780654043\n",
      "[step: 721] loss: 9.913793473970145e-05\n",
      "[step: 722] loss: 5.455428618006408e-05\n",
      "[step: 723] loss: 8.811100997263566e-05\n",
      "[step: 724] loss: 0.00014516901865135878\n",
      "[step: 725] loss: 0.0001659284607740119\n",
      "[step: 726] loss: 0.00014281626499723643\n",
      "[step: 727] loss: 0.00010041750647360459\n",
      "[step: 728] loss: 7.524350803578272e-05\n",
      "[step: 729] loss: 7.398481830023229e-05\n",
      "[step: 730] loss: 8.233687549363822e-05\n",
      "[step: 731] loss: 8.364982932107523e-05\n",
      "[step: 732] loss: 7.56148510845378e-05\n",
      "[step: 733] loss: 6.796394882258028e-05\n",
      "[step: 734] loss: 6.768386083422229e-05\n",
      "[step: 735] loss: 7.317082054214552e-05\n",
      "[step: 736] loss: 7.502952212234959e-05\n",
      "[step: 737] loss: 6.77144416840747e-05\n",
      "[step: 738] loss: 5.3217856475384906e-05\n",
      "[step: 739] loss: 4.055306635564193e-05\n",
      "[step: 740] loss: 3.692293103085831e-05\n",
      "[step: 741] loss: 4.2736559407785535e-05\n",
      "[step: 742] loss: 5.2092647820245475e-05\n",
      "[step: 743] loss: 5.7565077440813184e-05\n",
      "[step: 744] loss: 5.586337283602916e-05\n",
      "[step: 745] loss: 4.84385309391655e-05\n",
      "[step: 746] loss: 4.0141741919796914e-05\n",
      "[step: 747] loss: 3.478347207419574e-05\n",
      "[step: 748] loss: 3.3446332963649184e-05\n",
      "[step: 749] loss: 3.448939969530329e-05\n",
      "[step: 750] loss: 3.556047158781439e-05\n",
      "[step: 751] loss: 3.531140828272328e-05\n",
      "[step: 752] loss: 3.392910002730787e-05\n",
      "[step: 753] loss: 3.258566721342504e-05\n",
      "[step: 754] loss: 3.2174240914173424e-05\n",
      "[step: 755] loss: 3.279839438619092e-05\n",
      "[step: 756] loss: 3.369043406564742e-05\n",
      "[step: 757] loss: 3.399417255423032e-05\n",
      "[step: 758] loss: 3.309243402327411e-05\n",
      "[step: 759] loss: 3.121022746199742e-05\n",
      "[step: 760] loss: 2.888464405259583e-05\n",
      "[step: 761] loss: 2.6866577172768302e-05\n",
      "[step: 762] loss: 2.5548100893502124e-05\n",
      "[step: 763] loss: 2.494874206604436e-05\n",
      "[step: 764] loss: 2.4754386686254293e-05\n",
      "[step: 765] loss: 2.4583268896094523e-05\n",
      "[step: 766] loss: 2.4177581508411095e-05\n",
      "[step: 767] loss: 2.3498398149968125e-05\n",
      "[step: 768] loss: 2.2677602828480303e-05\n",
      "[step: 769] loss: 2.1915608158451505e-05\n",
      "[step: 770] loss: 2.136643888661638e-05\n",
      "[step: 771] loss: 2.107945692841895e-05\n",
      "[step: 772] loss: 2.100036363117397e-05\n",
      "[step: 773] loss: 2.1024570742156357e-05\n",
      "[step: 774] loss: 2.1050696886959486e-05\n",
      "[step: 775] loss: 2.102034522977192e-05\n",
      "[step: 776] loss: 2.0949002646375448e-05\n",
      "[step: 777] loss: 2.0901024981867522e-05\n",
      "[step: 778] loss: 2.098920776916202e-05\n",
      "[step: 779] loss: 2.1341766114346683e-05\n",
      "[step: 780] loss: 2.212794788647443e-05\n",
      "[step: 781] loss: 2.3542223061667755e-05\n",
      "[step: 782] loss: 2.5924886358552612e-05\n",
      "[step: 783] loss: 2.970370405819267e-05\n",
      "[step: 784] loss: 3.580744669307023e-05\n",
      "[step: 785] loss: 4.5382977987173945e-05\n",
      "[step: 786] loss: 6.112522532930598e-05\n",
      "[step: 787] loss: 8.615961269242689e-05\n",
      "[step: 788] loss: 0.00012860090646427125\n",
      "[step: 789] loss: 0.0001968745345948264\n",
      "[step: 790] loss: 0.0003165789239574224\n",
      "[step: 791] loss: 0.0005095432279631495\n",
      "[step: 792] loss: 0.0008614441612735391\n",
      "[step: 793] loss: 0.0014191882219165564\n",
      "[step: 794] loss: 0.002483394928276539\n",
      "[step: 795] loss: 0.004070967435836792\n",
      "[step: 796] loss: 0.007240002043545246\n",
      "[step: 797] loss: 0.011215378530323505\n",
      "[step: 798] loss: 0.019303692504763603\n",
      "[step: 799] loss: 0.025090767070651054\n",
      "[step: 800] loss: 0.03602062538266182\n",
      "[step: 801] loss: 0.030775384977459908\n",
      "[step: 802] loss: 0.02434130571782589\n",
      "[step: 803] loss: 0.008211683481931686\n",
      "[step: 804] loss: 0.0009305128478445113\n",
      "[step: 805] loss: 0.005153276026248932\n",
      "[step: 806] loss: 0.011895110830664635\n",
      "[step: 807] loss: 0.013959744945168495\n",
      "[step: 808] loss: 0.005503018386662006\n",
      "[step: 809] loss: 0.0003157308092340827\n",
      "[step: 810] loss: 0.0028867751825600863\n",
      "[step: 811] loss: 0.007149119395762682\n",
      "[step: 812] loss: 0.007300302851945162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 813] loss: 0.0020715901628136635\n",
      "[step: 814] loss: 0.0002191196836065501\n",
      "[step: 815] loss: 0.0029675415717065334\n",
      "[step: 816] loss: 0.004522689152508974\n",
      "[step: 817] loss: 0.0027444795705378056\n",
      "[step: 818] loss: 0.0002596623089630157\n",
      "[step: 819] loss: 0.0010403041960671544\n",
      "[step: 820] loss: 0.002941056387498975\n",
      "[step: 821] loss: 0.002185933291912079\n",
      "[step: 822] loss: 0.00039703852962702513\n",
      "[step: 823] loss: 0.00033902956056408584\n",
      "[step: 824] loss: 0.001572462497279048\n",
      "[step: 825] loss: 0.001726931077428162\n",
      "[step: 826] loss: 0.000510465179104358\n",
      "[step: 827] loss: 0.00012924101611133665\n",
      "[step: 828] loss: 0.0009065982885658741\n",
      "[step: 829] loss: 0.001165009569376707\n",
      "[step: 830] loss: 0.0005140042048878968\n",
      "[step: 831] loss: 4.8918009269982576e-05\n",
      "[step: 832] loss: 0.0004336648853495717\n",
      "[step: 833] loss: 0.0008066933369264007\n",
      "[step: 834] loss: 0.0004554681945592165\n",
      "[step: 835] loss: 5.9509540733415633e-05\n",
      "[step: 836] loss: 0.00021230147103779018\n",
      "[step: 837] loss: 0.0005040602991357446\n",
      "[step: 838] loss: 0.0003928631485905498\n",
      "[step: 839] loss: 8.599914144724607e-05\n",
      "[step: 840] loss: 9.473121463088319e-05\n",
      "[step: 841] loss: 0.0003080439055338502\n",
      "[step: 842] loss: 0.00029600606649182737\n",
      "[step: 843] loss: 9.727075666887686e-05\n",
      "[step: 844] loss: 3.63596627721563e-05\n",
      "[step: 845] loss: 0.00016485464584548026\n",
      "[step: 846] loss: 0.00022549864661414176\n",
      "[step: 847] loss: 0.00011421682575019076\n",
      "[step: 848] loss: 2.738628063525539e-05\n",
      "[step: 849] loss: 8.115467062452808e-05\n",
      "[step: 850] loss: 0.00015131717373151332\n",
      "[step: 851] loss: 0.00011586878099478781\n",
      "[step: 852] loss: 3.705249400809407e-05\n",
      "[step: 853] loss: 3.545540675986558e-05\n",
      "[step: 854] loss: 8.98454018170014e-05\n",
      "[step: 855] loss: 9.725237759994343e-05\n",
      "[step: 856] loss: 4.877567698713392e-05\n",
      "[step: 857] loss: 1.9823628463200293e-05\n",
      "[step: 858] loss: 4.4072010496165603e-05\n",
      "[step: 859] loss: 6.964604835957289e-05\n",
      "[step: 860] loss: 5.278316166368313e-05\n",
      "[step: 861] loss: 2.1969932276988402e-05\n",
      "[step: 862] loss: 2.0391868019942194e-05\n",
      "[step: 863] loss: 4.10262327932287e-05\n",
      "[step: 864] loss: 4.6947690861998126e-05\n",
      "[step: 865] loss: 2.9117329177097417e-05\n",
      "[step: 866] loss: 1.4895780623191968e-05\n",
      "[step: 867] loss: 2.1044894310762174e-05\n",
      "[step: 868] loss: 3.28228015860077e-05\n",
      "[step: 869] loss: 3.090255268034525e-05\n",
      "[step: 870] loss: 1.8681606889003888e-05\n",
      "[step: 871] loss: 1.3504371963790618e-05\n",
      "[step: 872] loss: 1.9760798750212416e-05\n",
      "[step: 873] loss: 2.5637053113314323e-05\n",
      "[step: 874] loss: 2.2475440346170217e-05\n",
      "[step: 875] loss: 1.5537047147518024e-05\n",
      "[step: 876] loss: 1.4582043149857782e-05\n",
      "[step: 877] loss: 2.007142211368773e-05\n",
      "[step: 878] loss: 2.468740422045812e-05\n",
      "[step: 879] loss: 2.4956525521702133e-05\n",
      "[step: 880] loss: 2.5319819542346522e-05\n",
      "[step: 881] loss: 3.2187814213102683e-05\n",
      "[step: 882] loss: 4.687142063630745e-05\n",
      "[step: 883] loss: 6.797690002713352e-05\n",
      "[step: 884] loss: 9.869010682450607e-05\n",
      "[step: 885] loss: 0.00015005609020590782\n",
      "[step: 886] loss: 0.00023906036221887916\n",
      "[step: 887] loss: 0.000395133945858106\n",
      "[step: 888] loss: 0.0006514695705845952\n",
      "[step: 889] loss: 0.0011045915307477117\n",
      "[step: 890] loss: 0.001843551523052156\n",
      "[step: 891] loss: 0.003170871874317527\n",
      "[step: 892] loss: 0.005228613503277302\n",
      "[step: 893] loss: 0.008798949420452118\n",
      "[step: 894] loss: 0.013362087309360504\n",
      "[step: 895] loss: 0.02012716419994831\n",
      "[step: 896] loss: 0.02387094683945179\n",
      "[step: 897] loss: 0.025655126199126244\n",
      "[step: 898] loss: 0.016818253323435783\n",
      "[step: 899] loss: 0.00674461480230093\n",
      "[step: 900] loss: 0.0005197411519475281\n",
      "[step: 901] loss: 0.0028036711737513542\n",
      "[step: 902] loss: 0.00874584261327982\n",
      "[step: 903] loss: 0.009777259081602097\n",
      "[step: 904] loss: 0.005824921652674675\n",
      "[step: 905] loss: 0.0009614141890779138\n",
      "[step: 906] loss: 0.0011307706590741873\n",
      "[step: 907] loss: 0.004405010957270861\n",
      "[step: 908] loss: 0.005255637690424919\n",
      "[step: 909] loss: 0.0030250598210841417\n",
      "[step: 910] loss: 0.0006447079940699041\n",
      "[step: 911] loss: 0.001173473079688847\n",
      "[step: 912] loss: 0.0028092439752072096\n",
      "[step: 913] loss: 0.0026039411313831806\n",
      "[step: 914] loss: 0.0011385871330276132\n",
      "[step: 915] loss: 0.0005246225628070533\n",
      "[step: 916] loss: 0.0012902120361104608\n",
      "[step: 917] loss: 0.0016630813479423523\n",
      "[step: 918] loss: 0.0009537485311739147\n",
      "[step: 919] loss: 0.0004482462245505303\n",
      "[step: 920] loss: 0.0007590525201521814\n",
      "[step: 921] loss: 0.0010686478344723582\n",
      "[step: 922] loss: 0.0006618006736971438\n",
      "[step: 923] loss: 0.00024408570607192814\n",
      "[step: 924] loss: 0.0004672593786381185\n",
      "[step: 925] loss: 0.0007652529748156667\n",
      "[step: 926] loss: 0.0005514695076271892\n",
      "[step: 927] loss: 0.00012051351950503886\n",
      "[step: 928] loss: 0.00015020092541817576\n",
      "[step: 929] loss: 0.0004979453515261412\n",
      "[step: 930] loss: 0.0005110132624395192\n",
      "[step: 931] loss: 0.0001874829613370821\n",
      "[step: 932] loss: 1.1451352293079253e-05\n",
      "[step: 933] loss: 0.00018765314598567784\n",
      "[step: 934] loss: 0.00036073653609491885\n",
      "[step: 935] loss: 0.00024033401859924197\n",
      "[step: 936] loss: 5.2005223551532254e-05\n",
      "[step: 937] loss: 5.459884414449334e-05\n",
      "[step: 938] loss: 0.00017239557928405702\n",
      "[step: 939] loss: 0.00018709090363699943\n",
      "[step: 940] loss: 9.043583122547716e-05\n",
      "[step: 941] loss: 4.760593583341688e-05\n",
      "[step: 942] loss: 8.955228258855641e-05\n",
      "[step: 943] loss: 0.00011006662680301815\n",
      "[step: 944] loss: 6.694652984151617e-05\n",
      "[step: 945] loss: 3.505502172629349e-05\n",
      "[step: 946] loss: 6.0063026467105374e-05\n",
      "[step: 947] loss: 8.38851192384027e-05\n",
      "[step: 948] loss: 5.708043318009004e-05\n",
      "[step: 949] loss: 1.6585679986746982e-05\n",
      "[step: 950] loss: 2.0265575585654005e-05\n",
      "[step: 951] loss: 5.3741234296467155e-05\n",
      "[step: 952] loss: 5.965966920484789e-05\n",
      "[step: 953] loss: 2.8997339541092515e-05\n",
      "[step: 954] loss: 5.111863174533937e-06\n",
      "[step: 955] loss: 1.5503524991800077e-05\n",
      "[step: 956] loss: 3.626097532105632e-05\n",
      "[step: 957] loss: 3.5040229704463854e-05\n",
      "[step: 958] loss: 1.6977004634100012e-05\n",
      "[step: 959] loss: 7.699380148551427e-06\n",
      "[step: 960] loss: 1.4400532563740853e-05\n",
      "[step: 961] loss: 2.111134926963132e-05\n",
      "[step: 962] loss: 1.6924128431128338e-05\n",
      "[step: 963] loss: 9.670111467130482e-06\n",
      "[step: 964] loss: 9.589682122168597e-06\n",
      "[step: 965] loss: 1.3967863196739927e-05\n",
      "[step: 966] loss: 1.3605800631921738e-05\n",
      "[step: 967] loss: 7.90535796113545e-06\n",
      "[step: 968] loss: 4.5223587221698835e-06\n",
      "[step: 969] loss: 7.367134912783513e-06\n",
      "[step: 970] loss: 1.1409032595111057e-05\n",
      "[step: 971] loss: 1.0466429557709489e-05\n",
      "[step: 972] loss: 5.709239758289186e-06\n",
      "[step: 973] loss: 2.876226972148288e-06\n",
      "[step: 974] loss: 4.443815669219475e-06\n",
      "[step: 975] loss: 7.192610610218253e-06\n",
      "[step: 976] loss: 7.297745924006449e-06\n",
      "[step: 977] loss: 5.065224740974372e-06\n",
      "[step: 978] loss: 3.4252091154485242e-06\n",
      "[step: 979] loss: 3.7686161249439465e-06\n",
      "[step: 980] loss: 4.680405709223123e-06\n",
      "[step: 981] loss: 4.491298113862285e-06\n",
      "[step: 982] loss: 3.4627391869435087e-06\n",
      "[step: 983] loss: 2.982195383083308e-06\n",
      "[step: 984] loss: 3.4890422284661327e-06\n",
      "[step: 985] loss: 4.0185136640502606e-06\n",
      "[step: 986] loss: 3.6584510780812707e-06\n",
      "[step: 987] loss: 2.6992900075129e-06\n",
      "[step: 988] loss: 2.1488137917913264e-06\n",
      "[step: 989] loss: 2.435942406009417e-06\n",
      "[step: 990] loss: 3.011128001162433e-06\n",
      "[step: 991] loss: 3.1275549190468155e-06\n",
      "[step: 992] loss: 2.684424543986097e-06\n",
      "[step: 993] loss: 2.190477061958518e-06\n",
      "[step: 994] loss: 2.068322828563396e-06\n",
      "[step: 995] loss: 2.233772875115392e-06\n",
      "[step: 996] loss: 2.3273094029718777e-06\n",
      "[step: 997] loss: 2.1837829535797937e-06\n",
      "[step: 998] loss: 1.9674935174407437e-06\n",
      "[step: 999] loss: 1.9045351109525654e-06\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "testY is:  [4122.7096770000007, 7644.7419349999991, 4147.0740740000001, 1799.7096770000001, 1405.5, 1648.193548, 1090.5666670000001]\n",
      "\n",
      "\n",
      "LSTM testforecast : [-13329.3466796875, 15071.72265625, 44730.5703125, 7056.5927734375, 9614.6904296875, 6601.41259765625, 3353.59912109375] \n",
      "@@@@@LSTM rmse:  17449.6655838\n",
      "Bayseian testforecast : [734.03919080076446, 1894.5132882945629, 13344240.389051739, 65.965064237998888, 156.69537789750424, 2934.0792310475513, 467.71113202047519] \n",
      "@@@@@Bayseian rmse:  5042082.06438\n",
      "\n",
      "\n",
      "LSTM WON!!!!!!\n",
      "[step: 0] loss: 7.655303955078125\n",
      "[step: 1] loss: 7.111428737640381\n",
      "[step: 2] loss: 6.617077350616455\n",
      "[step: 3] loss: 6.164812088012695\n",
      "[step: 4] loss: 5.7514543533325195\n",
      "[step: 5] loss: 5.37705659866333\n",
      "[step: 6] loss: 5.04478120803833\n",
      "[step: 7] loss: 4.761127948760986\n",
      "[step: 8] loss: 4.535430431365967\n",
      "[step: 9] loss: 4.37683629989624\n",
      "[step: 10] loss: 4.286830425262451\n",
      "[step: 11] loss: 4.250372886657715\n",
      "[step: 12] loss: 4.237537384033203\n",
      "[step: 13] loss: 4.219117641448975\n",
      "[step: 14] loss: 4.180923938751221\n",
      "[step: 15] loss: 4.125813007354736\n",
      "[step: 16] loss: 4.066070556640625\n",
      "[step: 17] loss: 4.014023303985596\n",
      "[step: 18] loss: 3.9762158393859863\n",
      "[step: 19] loss: 3.952261209487915\n",
      "[step: 20] loss: 3.9370055198669434\n",
      "[step: 21] loss: 3.9238579273223877\n",
      "[step: 22] loss: 3.907378673553467\n",
      "[step: 23] loss: 3.884413957595825\n",
      "[step: 24] loss: 3.8540821075439453\n",
      "[step: 25] loss: 3.8172447681427\n",
      "[step: 26] loss: 3.7758710384368896\n",
      "[step: 27] loss: 3.732473373413086\n",
      "[step: 28] loss: 3.689634084701538\n",
      "[step: 29] loss: 3.6495893001556396\n",
      "[step: 30] loss: 3.6138408184051514\n",
      "[step: 31] loss: 3.582815170288086\n",
      "[step: 32] loss: 3.555645227432251\n",
      "[step: 33] loss: 3.530228853225708\n",
      "[step: 34] loss: 3.503685235977173\n",
      "[step: 35] loss: 3.473191261291504\n",
      "[step: 36] loss: 3.436910629272461\n",
      "[step: 37] loss: 3.3945937156677246\n",
      "[step: 38] loss: 3.3475072383880615\n",
      "[step: 39] loss: 3.2977161407470703\n",
      "[step: 40] loss: 3.2470550537109375\n",
      "[step: 41] loss: 3.1962475776672363\n",
      "[step: 42] loss: 3.1446235179901123\n",
      "[step: 43] loss: 3.090622901916504\n",
      "[step: 44] loss: 3.032939910888672\n",
      "[step: 45] loss: 2.9717628955841064\n",
      "[step: 46] loss: 2.9092869758605957\n",
      "[step: 47] loss: 2.8488094806671143\n",
      "[step: 48] loss: 2.7925548553466797\n",
      "[step: 49] loss: 2.7403457164764404\n",
      "[step: 50] loss: 2.692227840423584\n",
      "[step: 51] loss: 2.652470588684082\n",
      "[step: 52] loss: 2.6258857250213623\n",
      "[step: 53] loss: 2.6082231998443604\n",
      "[step: 54] loss: 2.589554786682129\n",
      "[step: 55] loss: 2.5662126541137695\n",
      "[step: 56] loss: 2.536302328109741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 57] loss: 2.4962024688720703\n",
      "[step: 58] loss: 2.4505655765533447\n",
      "[step: 59] loss: 2.407627820968628\n",
      "[step: 60] loss: 2.3695504665374756\n",
      "[step: 61] loss: 2.336663007736206\n",
      "[step: 62] loss: 2.310565948486328\n",
      "[step: 63] loss: 2.2900216579437256\n",
      "[step: 64] loss: 2.270960569381714\n",
      "[step: 65] loss: 2.2511446475982666\n",
      "[step: 66] loss: 2.2301721572875977\n",
      "[step: 67] loss: 2.206718683242798\n",
      "[step: 68] loss: 2.179894208908081\n",
      "[step: 69] loss: 2.1514296531677246\n",
      "[step: 70] loss: 2.123506546020508\n",
      "[step: 71] loss: 2.096935749053955\n",
      "[step: 72] loss: 2.0735206604003906\n",
      "[step: 73] loss: 2.0545854568481445\n",
      "[step: 74] loss: 2.037886619567871\n",
      "[step: 75] loss: 2.021355628967285\n",
      "[step: 76] loss: 2.0032732486724854\n",
      "[step: 77] loss: 1.9824376106262207\n",
      "[step: 78] loss: 1.9609599113464355\n",
      "[step: 79] loss: 1.9401494264602661\n",
      "[step: 80] loss: 1.9211300611495972\n",
      "[step: 81] loss: 1.9042385816574097\n",
      "[step: 82] loss: 1.8878525495529175\n",
      "[step: 83] loss: 1.871283769607544\n",
      "[step: 84] loss: 1.8536057472229004\n",
      "[step: 85] loss: 1.8348969221115112\n",
      "[step: 86] loss: 1.816082239151001\n",
      "[step: 87] loss: 1.7974581718444824\n",
      "[step: 88] loss: 1.7798019647598267\n",
      "[step: 89] loss: 1.762550711631775\n",
      "[step: 90] loss: 1.7453665733337402\n",
      "[step: 91] loss: 1.7276160717010498\n",
      "[step: 92] loss: 1.7094508409500122\n",
      "[step: 93] loss: 1.6911664009094238\n",
      "[step: 94] loss: 1.673250436782837\n",
      "[step: 95] loss: 1.65582275390625\n",
      "[step: 96] loss: 1.6387627124786377\n",
      "[step: 97] loss: 1.6217283010482788\n",
      "[step: 98] loss: 1.6045724153518677\n",
      "[step: 99] loss: 1.587254285812378\n",
      "[step: 100] loss: 1.5700997114181519\n",
      "[step: 101] loss: 1.5532232522964478\n",
      "[step: 102] loss: 1.5367381572723389\n",
      "[step: 103] loss: 1.5203360319137573\n",
      "[step: 104] loss: 1.5039632320404053\n",
      "[step: 105] loss: 1.4875879287719727\n",
      "[step: 106] loss: 1.4712841510772705\n",
      "[step: 107] loss: 1.4552847146987915\n",
      "[step: 108] loss: 1.439601182937622\n",
      "[step: 109] loss: 1.4241775274276733\n",
      "[step: 110] loss: 1.4090650081634521\n",
      "[step: 111] loss: 1.3943018913269043\n",
      "[step: 112] loss: 1.3799034357070923\n",
      "[step: 113] loss: 1.365949273109436\n",
      "[step: 114] loss: 1.352485179901123\n",
      "[step: 115] loss: 1.3395882844924927\n",
      "[step: 116] loss: 1.3273111581802368\n",
      "[step: 117] loss: 1.3156675100326538\n",
      "[step: 118] loss: 1.304701805114746\n",
      "[step: 119] loss: 1.2946439981460571\n",
      "[step: 120] loss: 1.2869069576263428\n",
      "[step: 121] loss: 1.289834976196289\n",
      "[step: 122] loss: 1.3102167844772339\n",
      "[step: 123] loss: 1.3355776071548462\n",
      "[step: 124] loss: 1.2551326751708984\n",
      "[step: 125] loss: 1.3366175889968872\n",
      "[step: 126] loss: 1.2955842018127441\n",
      "[step: 127] loss: 1.2946443557739258\n",
      "[step: 128] loss: 1.2657766342163086\n",
      "[step: 129] loss: 1.2597920894622803\n",
      "[step: 130] loss: 1.257980227470398\n",
      "[step: 131] loss: 1.2551088333129883\n",
      "[step: 132] loss: 1.2316339015960693\n",
      "[step: 133] loss: 1.24290132522583\n",
      "[step: 134] loss: 1.222965121269226\n",
      "[step: 135] loss: 1.2355204820632935\n",
      "[step: 136] loss: 1.2051995992660522\n",
      "[step: 137] loss: 1.2281521558761597\n",
      "[step: 138] loss: 1.1978005170822144\n",
      "[step: 139] loss: 1.2156528234481812\n",
      "[step: 140] loss: 1.1891965866088867\n",
      "[step: 141] loss: 1.2058773040771484\n",
      "[step: 142] loss: 1.1821397542953491\n",
      "[step: 143] loss: 1.1924543380737305\n",
      "[step: 144] loss: 1.1774966716766357\n",
      "[step: 145] loss: 1.1807056665420532\n",
      "[step: 146] loss: 1.170481562614441\n",
      "[step: 147] loss: 1.1699457168579102\n",
      "[step: 148] loss: 1.164887547492981\n",
      "[step: 149] loss: 1.1594209671020508\n",
      "[step: 150] loss: 1.1575512886047363\n",
      "[step: 151] loss: 1.1508368253707886\n",
      "[step: 152] loss: 1.1501127481460571\n",
      "[step: 153] loss: 1.1419777870178223\n",
      "[step: 154] loss: 1.1422637701034546\n",
      "[step: 155] loss: 1.134140968322754\n",
      "[step: 156] loss: 1.13389253616333\n",
      "[step: 157] loss: 1.1260172128677368\n",
      "[step: 158] loss: 1.125590443611145\n",
      "[step: 159] loss: 1.1180835962295532\n",
      "[step: 160] loss: 1.1168746948242188\n",
      "[step: 161] loss: 1.1099268198013306\n",
      "[step: 162] loss: 1.1082439422607422\n",
      "[step: 163] loss: 1.1015900373458862\n",
      "[step: 164] loss: 1.0992629528045654\n",
      "[step: 165] loss: 1.092984914779663\n",
      "[step: 166] loss: 1.0902180671691895\n",
      "[step: 167] loss: 1.084088683128357\n",
      "[step: 168] loss: 1.0808348655700684\n",
      "[step: 169] loss: 1.074874758720398\n",
      "[step: 170] loss: 1.0712379217147827\n",
      "[step: 171] loss: 1.0653070211410522\n",
      "[step: 172] loss: 1.061238408088684\n",
      "[step: 173] loss: 1.05534029006958\n",
      "[step: 174] loss: 1.0508843660354614\n",
      "[step: 175] loss: 1.0449738502502441\n",
      "[step: 176] loss: 1.040062665939331\n",
      "[step: 177] loss: 1.0341259241104126\n",
      "[step: 178] loss: 1.0287578105926514\n",
      "[step: 179] loss: 1.0227999687194824\n",
      "[step: 180] loss: 1.0169332027435303\n",
      "[step: 181] loss: 1.0108973979949951\n",
      "[step: 182] loss: 1.0045404434204102\n",
      "[step: 183] loss: 0.9983747601509094\n",
      "[step: 184] loss: 0.991587221622467\n",
      "[step: 185] loss: 0.9851492643356323\n",
      "[step: 186] loss: 0.9780261516571045\n",
      "[step: 187] loss: 0.9711588621139526\n",
      "[step: 188] loss: 0.9638023972511292\n",
      "[step: 189] loss: 0.9563833475112915\n",
      "[step: 190] loss: 0.9488146901130676\n",
      "[step: 191] loss: 0.9408438205718994\n",
      "[step: 192] loss: 0.9329242706298828\n",
      "[step: 193] loss: 0.9245296120643616\n",
      "[step: 194] loss: 0.9160627126693726\n",
      "[step: 195] loss: 0.907337486743927\n",
      "[step: 196] loss: 0.898260235786438\n",
      "[step: 197] loss: 0.8890992403030396\n",
      "[step: 198] loss: 0.8795400857925415\n",
      "[step: 199] loss: 0.8697458505630493\n",
      "[step: 200] loss: 0.8597532510757446\n",
      "[step: 201] loss: 0.849359929561615\n",
      "[step: 202] loss: 0.8387418985366821\n",
      "[step: 203] loss: 0.8278639912605286\n",
      "[step: 204] loss: 0.816600501537323\n",
      "[step: 205] loss: 0.8050791621208191\n",
      "[step: 206] loss: 0.7932909727096558\n",
      "[step: 207] loss: 0.7811354398727417\n",
      "[step: 208] loss: 0.7686774730682373\n",
      "[step: 209] loss: 0.7559640407562256\n",
      "[step: 210] loss: 0.7429421544075012\n",
      "[step: 211] loss: 0.7296015024185181\n",
      "[step: 212] loss: 0.7159851789474487\n",
      "[step: 213] loss: 0.7021323442459106\n",
      "[step: 214] loss: 0.6880471110343933\n",
      "[step: 215] loss: 0.673723578453064\n",
      "[step: 216] loss: 0.6591942310333252\n",
      "[step: 217] loss: 0.6444869041442871\n",
      "[step: 218] loss: 0.6296486258506775\n",
      "[step: 219] loss: 0.6147122979164124\n",
      "[step: 220] loss: 0.5997252464294434\n",
      "[step: 221] loss: 0.58473140001297\n",
      "[step: 222] loss: 0.5698119401931763\n",
      "[step: 223] loss: 0.5551143884658813\n",
      "[step: 224] loss: 0.541228711605072\n",
      "[step: 225] loss: 0.5302808880805969\n",
      "[step: 226] loss: 0.5321967005729675\n",
      "[step: 227] loss: 0.5543748736381531\n",
      "[step: 228] loss: 0.6240620613098145\n",
      "[step: 229] loss: 0.49299708008766174\n",
      "[step: 230] loss: 0.4982978403568268\n",
      "[step: 231] loss: 0.5646699070930481\n",
      "[step: 232] loss: 0.4492328464984894\n",
      "[step: 233] loss: 0.5332207679748535\n",
      "[step: 234] loss: 0.5522506833076477\n",
      "[step: 235] loss: 0.45590007305145264\n",
      "[step: 236] loss: 0.5809023380279541\n",
      "[step: 237] loss: 0.42609769105911255\n",
      "[step: 238] loss: 0.49485015869140625\n",
      "[step: 239] loss: 0.3966064751148224\n",
      "[step: 240] loss: 0.46678873896598816\n",
      "[step: 241] loss: 0.40967246890068054\n",
      "[step: 242] loss: 0.4355662167072296\n",
      "[step: 243] loss: 0.3774910271167755\n",
      "[step: 244] loss: 0.4110510051250458\n",
      "[step: 245] loss: 0.3746718466281891\n",
      "[step: 246] loss: 0.3921743929386139\n",
      "[step: 247] loss: 0.35118603706359863\n",
      "[step: 248] loss: 0.37524744868278503\n",
      "[step: 249] loss: 0.343952476978302\n",
      "[step: 250] loss: 0.3593193292617798\n",
      "[step: 251] loss: 0.32862937450408936\n",
      "[step: 252] loss: 0.34627047181129456\n",
      "[step: 253] loss: 0.32105445861816406\n",
      "[step: 254] loss: 0.33225017786026\n",
      "[step: 255] loss: 0.31106671690940857\n",
      "[step: 256] loss: 0.32104218006134033\n",
      "[step: 257] loss: 0.303930401802063\n",
      "[step: 258] loss: 0.3095375597476959\n",
      "[step: 259] loss: 0.2966141998767853\n",
      "[step: 260] loss: 0.30001410841941833\n",
      "[step: 261] loss: 0.29009896516799927\n",
      "[step: 262] loss: 0.2909944951534271\n",
      "[step: 263] loss: 0.2838360369205475\n",
      "[step: 264] loss: 0.2830779254436493\n",
      "[step: 265] loss: 0.27768486738204956\n",
      "[step: 266] loss: 0.2759321331977844\n",
      "[step: 267] loss: 0.2718212306499481\n",
      "[step: 268] loss: 0.26936766505241394\n",
      "[step: 269] loss: 0.26599448919296265\n",
      "[step: 270] loss: 0.26350921392440796\n",
      "[step: 271] loss: 0.2604626417160034\n",
      "[step: 272] loss: 0.25799694657325745\n",
      "[step: 273] loss: 0.25495681166648865\n",
      "[step: 274] loss: 0.25299155712127686\n",
      "[step: 275] loss: 0.24978956580162048\n",
      "[step: 276] loss: 0.24824242293834686\n",
      "[step: 277] loss: 0.2447100132703781\n",
      "[step: 278] loss: 0.24372868239879608\n",
      "[step: 279] loss: 0.24004042148590088\n",
      "[step: 280] loss: 0.23941579461097717\n",
      "[step: 281] loss: 0.23562383651733398\n",
      "[step: 282] loss: 0.23508022725582123\n",
      "[step: 283] loss: 0.231601744890213\n",
      "[step: 284] loss: 0.23086270689964294\n",
      "[step: 285] loss: 0.22794793546199799\n",
      "[step: 286] loss: 0.22664202749729156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 287] loss: 0.22450785338878632\n",
      "[step: 288] loss: 0.2225727140903473\n",
      "[step: 289] loss: 0.2211959958076477\n",
      "[step: 290] loss: 0.21882732212543488\n",
      "[step: 291] loss: 0.21783587336540222\n",
      "[step: 292] loss: 0.2154659479856491\n",
      "[step: 293] loss: 0.21432755887508392\n",
      "[step: 294] loss: 0.21244584023952484\n",
      "[step: 295] loss: 0.21084913611412048\n",
      "[step: 296] loss: 0.20954985916614532\n",
      "[step: 297] loss: 0.20764116942882538\n",
      "[step: 298] loss: 0.20654666423797607\n",
      "[step: 299] loss: 0.20480193197727203\n",
      "[step: 300] loss: 0.20343637466430664\n",
      "[step: 301] loss: 0.20213951170444489\n",
      "[step: 302] loss: 0.20050033926963806\n",
      "[step: 303] loss: 0.19937115907669067\n",
      "[step: 304] loss: 0.19788701832294464\n",
      "[step: 305] loss: 0.19653785228729248\n",
      "[step: 306] loss: 0.19537323713302612\n",
      "[step: 307] loss: 0.19391606748104095\n",
      "[step: 308] loss: 0.19274236261844635\n",
      "[step: 309] loss: 0.19151706993579865\n",
      "[step: 310] loss: 0.19016942381858826\n",
      "[step: 311] loss: 0.18906110525131226\n",
      "[step: 312] loss: 0.18783175945281982\n",
      "[step: 313] loss: 0.18658079206943512\n",
      "[step: 314] loss: 0.18549786508083344\n",
      "[step: 315] loss: 0.1843043565750122\n",
      "[step: 316] loss: 0.18311737477779388\n",
      "[step: 317] loss: 0.18205349147319794\n",
      "[step: 318] loss: 0.18091142177581787\n",
      "[step: 319] loss: 0.1797657161951065\n",
      "[step: 320] loss: 0.1787196546792984\n",
      "[step: 321] loss: 0.1776328831911087\n",
      "[step: 322] loss: 0.17651964724063873\n",
      "[step: 323] loss: 0.1754837930202484\n",
      "[step: 324] loss: 0.17444844543933868\n",
      "[step: 325] loss: 0.17337243258953094\n",
      "[step: 326] loss: 0.1723378300666809\n",
      "[step: 327] loss: 0.17133796215057373\n",
      "[step: 328] loss: 0.17031048238277435\n",
      "[step: 329] loss: 0.16928157210350037\n",
      "[step: 330] loss: 0.1682906597852707\n",
      "[step: 331] loss: 0.1673087626695633\n",
      "[step: 332] loss: 0.16630922257900238\n",
      "[step: 333] loss: 0.16531580686569214\n",
      "[step: 334] loss: 0.16434815526008606\n",
      "[step: 335] loss: 0.1633879542350769\n",
      "[step: 336] loss: 0.16241757571697235\n",
      "[step: 337] loss: 0.16144846379756927\n",
      "[step: 338] loss: 0.1604948788881302\n",
      "[step: 339] loss: 0.15955303609371185\n",
      "[step: 340] loss: 0.15861071646213531\n",
      "[step: 341] loss: 0.1576644331216812\n",
      "[step: 342] loss: 0.15672163665294647\n",
      "[step: 343] loss: 0.15578822791576385\n",
      "[step: 344] loss: 0.15486308932304382\n",
      "[step: 345] loss: 0.15394148230552673\n",
      "[step: 346] loss: 0.15301966667175293\n",
      "[step: 347] loss: 0.15209829807281494\n",
      "[step: 348] loss: 0.15117934346199036\n",
      "[step: 349] loss: 0.15026485919952393\n",
      "[step: 350] loss: 0.1493554413318634\n",
      "[step: 351] loss: 0.14845068752765656\n",
      "[step: 352] loss: 0.14755024015903473\n",
      "[step: 353] loss: 0.1466534584760666\n",
      "[step: 354] loss: 0.1457609385251999\n",
      "[step: 355] loss: 0.1448732614517212\n",
      "[step: 356] loss: 0.14399278163909912\n",
      "[step: 357] loss: 0.14312365651130676\n",
      "[step: 358] loss: 0.14227768778800964\n",
      "[step: 359] loss: 0.14147809147834778\n",
      "[step: 360] loss: 0.14079998433589935\n",
      "[step: 361] loss: 0.14040523767471313\n",
      "[step: 362] loss: 0.14090022444725037\n",
      "[step: 363] loss: 0.14346593618392944\n",
      "[step: 364] loss: 0.1537301391363144\n",
      "[step: 365] loss: 0.17431843280792236\n",
      "[step: 366] loss: 0.23847928643226624\n",
      "[step: 367] loss: 0.2146695852279663\n",
      "[step: 368] loss: 0.1982765942811966\n",
      "[step: 369] loss: 0.13798017799854279\n",
      "[step: 370] loss: 0.1696036010980606\n",
      "[step: 371] loss: 0.2260238230228424\n",
      "[step: 372] loss: 0.14312152564525604\n",
      "[step: 373] loss: 0.16063465178012848\n",
      "[step: 374] loss: 0.2169618010520935\n",
      "[step: 375] loss: 0.13560545444488525\n",
      "[step: 376] loss: 0.17278891801834106\n",
      "[step: 377] loss: 0.2124985307455063\n",
      "[step: 378] loss: 0.1308387815952301\n",
      "[step: 379] loss: 0.21664315462112427\n",
      "[step: 380] loss: 0.20683880150318146\n",
      "[step: 381] loss: 0.1514723300933838\n",
      "[step: 382] loss: 0.23975403606891632\n",
      "[step: 383] loss: 0.14662383496761322\n",
      "[step: 384] loss: 0.18347874283790588\n",
      "[step: 385] loss: 0.15318472683429718\n",
      "[step: 386] loss: 0.14631175994873047\n",
      "[step: 387] loss: 0.16467571258544922\n",
      "[step: 388] loss: 0.13538730144500732\n",
      "[step: 389] loss: 0.16793853044509888\n",
      "[step: 390] loss: 0.12485521286725998\n",
      "[step: 391] loss: 0.1543070524930954\n",
      "[step: 392] loss: 0.12428615242242813\n",
      "[step: 393] loss: 0.1481204628944397\n",
      "[step: 394] loss: 0.12833556532859802\n",
      "[step: 395] loss: 0.1380360871553421\n",
      "[step: 396] loss: 0.12797512114048004\n",
      "[step: 397] loss: 0.13060349225997925\n",
      "[step: 398] loss: 0.12949427962303162\n",
      "[step: 399] loss: 0.12607209384441376\n",
      "[step: 400] loss: 0.12861114740371704\n",
      "[step: 401] loss: 0.12122456729412079\n",
      "[step: 402] loss: 0.12720288336277008\n",
      "[step: 403] loss: 0.11907058954238892\n",
      "[step: 404] loss: 0.12582357227802277\n",
      "[step: 405] loss: 0.11699945479631424\n",
      "[step: 406] loss: 0.12296053767204285\n",
      "[step: 407] loss: 0.11595030128955841\n",
      "[step: 408] loss: 0.12063867598772049\n",
      "[step: 409] loss: 0.11554605513811111\n",
      "[step: 410] loss: 0.11778147518634796\n",
      "[step: 411] loss: 0.1149510070681572\n",
      "[step: 412] loss: 0.11521559208631516\n",
      "[step: 413] loss: 0.11455606669187546\n",
      "[step: 414] loss: 0.11306837201118469\n",
      "[step: 415] loss: 0.11383257806301117\n",
      "[step: 416] loss: 0.11122290045022964\n",
      "[step: 417] loss: 0.11268457770347595\n",
      "[step: 418] loss: 0.11000870168209076\n",
      "[step: 419] loss: 0.11129492521286011\n",
      "[step: 420] loss: 0.10922771692276001\n",
      "[step: 421] loss: 0.10955633223056793\n",
      "[step: 422] loss: 0.10860085487365723\n",
      "[step: 423] loss: 0.1078944206237793\n",
      "[step: 424] loss: 0.10791770368814468\n",
      "[step: 425] loss: 0.10656100511550903\n",
      "[step: 426] loss: 0.1069641187787056\n",
      "[step: 427] loss: 0.10559078305959702\n",
      "[step: 428] loss: 0.10568487644195557\n",
      "[step: 429] loss: 0.1048794761300087\n",
      "[step: 430] loss: 0.10436256974935532\n",
      "[step: 431] loss: 0.10417065024375916\n",
      "[step: 432] loss: 0.10324432700872421\n",
      "[step: 433] loss: 0.10324932634830475\n",
      "[step: 434] loss: 0.10241252928972244\n",
      "[step: 435] loss: 0.10212641954421997\n",
      "[step: 436] loss: 0.10170453786849976\n",
      "[step: 437] loss: 0.101057268679142\n",
      "[step: 438] loss: 0.10088331252336502\n",
      "[step: 439] loss: 0.1002027615904808\n",
      "[step: 440] loss: 0.09991145879030228\n",
      "[step: 441] loss: 0.09948056191205978\n",
      "[step: 442] loss: 0.0989449992775917\n",
      "[step: 443] loss: 0.09869365394115448\n",
      "[step: 444] loss: 0.09813405573368073\n",
      "[step: 445] loss: 0.09778550267219543\n",
      "[step: 446] loss: 0.09740988910198212\n",
      "[step: 447] loss: 0.09690941870212555\n",
      "[step: 448] loss: 0.0966126024723053\n",
      "[step: 449] loss: 0.09615613520145416\n",
      "[step: 450] loss: 0.09575584530830383\n",
      "[step: 451] loss: 0.09542333334684372\n",
      "[step: 452] loss: 0.09496626257896423\n",
      "[step: 453] loss: 0.09462228417396545\n",
      "[step: 454] loss: 0.09424642473459244\n",
      "[step: 455] loss: 0.093824602663517\n",
      "[step: 456] loss: 0.09349316358566284\n",
      "[step: 457] loss: 0.09309772402048111\n",
      "[step: 458] loss: 0.09271033853292465\n",
      "[step: 459] loss: 0.09237276017665863\n",
      "[step: 460] loss: 0.09197729080915451\n",
      "[step: 461] loss: 0.09161174297332764\n",
      "[step: 462] loss: 0.09126568585634232\n",
      "[step: 463] loss: 0.09087909013032913\n",
      "[step: 464] loss: 0.09052474796772003\n",
      "[step: 465] loss: 0.0901741087436676\n",
      "[step: 466] loss: 0.08979757130146027\n",
      "[step: 467] loss: 0.08944817632436752\n",
      "[step: 468] loss: 0.08909643441438675\n",
      "[step: 469] loss: 0.08872838318347931\n",
      "[step: 470] loss: 0.08838113397359848\n",
      "[step: 471] loss: 0.08803077787160873\n",
      "[step: 472] loss: 0.08766905218362808\n",
      "[step: 473] loss: 0.08732259273529053\n",
      "[step: 474] loss: 0.08697446435689926\n",
      "[step: 475] loss: 0.08661764860153198\n",
      "[step: 476] loss: 0.08627151697874069\n",
      "[step: 477] loss: 0.08592567592859268\n",
      "[step: 478] loss: 0.08557271212339401\n",
      "[step: 479] loss: 0.0852266401052475\n",
      "[step: 480] loss: 0.08488276600837708\n",
      "[step: 481] loss: 0.08453311771154404\n",
      "[step: 482] loss: 0.08418714255094528\n",
      "[step: 483] loss: 0.08384459465742111\n",
      "[step: 484] loss: 0.08349798619747162\n",
      "[step: 485] loss: 0.08315210789442062\n",
      "[step: 486] loss: 0.08281015604734421\n",
      "[step: 487] loss: 0.08246613293886185\n",
      "[step: 488] loss: 0.08212123811244965\n",
      "[step: 489] loss: 0.0817793607711792\n",
      "[step: 490] loss: 0.08143734931945801\n",
      "[step: 491] loss: 0.08109372854232788\n",
      "[step: 492] loss: 0.0807516872882843\n",
      "[step: 493] loss: 0.08041118085384369\n",
      "[step: 494] loss: 0.08006927371025085\n",
      "[step: 495] loss: 0.07972749322652817\n",
      "[step: 496] loss: 0.0793873593211174\n",
      "[step: 497] loss: 0.07904711365699768\n",
      "[step: 498] loss: 0.07870640605688095\n",
      "[step: 499] loss: 0.07836635410785675\n",
      "[step: 500] loss: 0.07802718132734299\n",
      "[step: 501] loss: 0.07768788933753967\n",
      "[step: 502] loss: 0.07734835147857666\n",
      "[step: 503] loss: 0.07700971513986588\n",
      "[step: 504] loss: 0.07667131721973419\n",
      "[step: 505] loss: 0.07633284479379654\n",
      "[step: 506] loss: 0.07599441707134247\n",
      "[step: 507] loss: 0.07565663009881973\n",
      "[step: 508] loss: 0.07531905919313431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 509] loss: 0.07498130947351456\n",
      "[step: 510] loss: 0.07464370876550674\n",
      "[step: 511] loss: 0.07430645823478699\n",
      "[step: 512] loss: 0.07396921515464783\n",
      "[step: 513] loss: 0.07363186776638031\n",
      "[step: 514] loss: 0.07329446822404861\n",
      "[step: 515] loss: 0.07295721024274826\n",
      "[step: 516] loss: 0.07261984795331955\n",
      "[step: 517] loss: 0.0722821056842804\n",
      "[step: 518] loss: 0.07194439321756363\n",
      "[step: 519] loss: 0.07160631567239761\n",
      "[step: 520] loss: 0.07126791775226593\n",
      "[step: 521] loss: 0.07092909514904022\n",
      "[step: 522] loss: 0.07058980315923691\n",
      "[step: 523] loss: 0.07024998217821121\n",
      "[step: 524] loss: 0.06990958750247955\n",
      "[step: 525] loss: 0.06956847012042999\n",
      "[step: 526] loss: 0.06922657787799835\n",
      "[step: 527] loss: 0.06888367235660553\n",
      "[step: 528] loss: 0.06853986531496048\n",
      "[step: 529] loss: 0.06819502264261246\n",
      "[step: 530] loss: 0.06784899532794952\n",
      "[step: 531] loss: 0.06750169396400452\n",
      "[step: 532] loss: 0.06715299934148788\n",
      "[step: 533] loss: 0.06680278480052948\n",
      "[step: 534] loss: 0.06645100563764572\n",
      "[step: 535] loss: 0.06609749794006348\n",
      "[step: 536] loss: 0.06574219465255737\n",
      "[step: 537] loss: 0.06538496911525726\n",
      "[step: 538] loss: 0.0650257095694542\n",
      "[step: 539] loss: 0.06466422975063324\n",
      "[step: 540] loss: 0.06430051475763321\n",
      "[step: 541] loss: 0.06393438577651978\n",
      "[step: 542] loss: 0.06356574594974518\n",
      "[step: 543] loss: 0.06319444626569748\n",
      "[step: 544] loss: 0.06282038986682892\n",
      "[step: 545] loss: 0.06244349107146263\n",
      "[step: 546] loss: 0.062063612043857574\n",
      "[step: 547] loss: 0.06168061122298241\n",
      "[step: 548] loss: 0.061294443905353546\n",
      "[step: 549] loss: 0.06090492382645607\n",
      "[step: 550] loss: 0.06051214039325714\n",
      "[step: 551] loss: 0.06011580675840378\n",
      "[step: 552] loss: 0.05971613526344299\n",
      "[step: 553] loss: 0.059313420206308365\n",
      "[step: 554] loss: 0.05890859663486481\n",
      "[step: 555] loss: 0.058503758162260056\n",
      "[step: 556] loss: 0.0581052266061306\n",
      "[step: 557] loss: 0.0577281154692173\n",
      "[step: 558] loss: 0.05741522088646889\n",
      "[step: 559] loss: 0.057267479598522186\n",
      "[step: 560] loss: 0.05760566517710686\n",
      "[step: 561] loss: 0.05906547233462334\n",
      "[step: 562] loss: 0.06411416828632355\n",
      "[step: 563] loss: 0.07441335171461105\n",
      "[step: 564] loss: 0.10284090042114258\n",
      "[step: 565] loss: 0.11021479964256287\n",
      "[step: 566] loss: 0.1265692561864853\n",
      "[step: 567] loss: 0.08453419059515\n",
      "[step: 568] loss: 0.08511810004711151\n",
      "[step: 569] loss: 0.07454116642475128\n",
      "[step: 570] loss: 0.0634685754776001\n",
      "[step: 571] loss: 0.08501490205526352\n",
      "[step: 572] loss: 0.0742209330201149\n",
      "[step: 573] loss: 0.054398320615291595\n",
      "[step: 574] loss: 0.05767315998673439\n",
      "[step: 575] loss: 0.0646747574210167\n",
      "[step: 576] loss: 0.06039976328611374\n",
      "[step: 577] loss: 0.058041010051965714\n",
      "[step: 578] loss: 0.057691581547260284\n",
      "[step: 579] loss: 0.05145980790257454\n",
      "[step: 580] loss: 0.05761291831731796\n",
      "[step: 581] loss: 0.059754546731710434\n",
      "[step: 582] loss: 0.04869188368320465\n",
      "[step: 583] loss: 0.05279141664505005\n",
      "[step: 584] loss: 0.05497146397829056\n",
      "[step: 585] loss: 0.050815630704164505\n",
      "[step: 586] loss: 0.05251903459429741\n",
      "[step: 587] loss: 0.04697533696889877\n",
      "[step: 588] loss: 0.04823595657944679\n",
      "[step: 589] loss: 0.052360791712999344\n",
      "[step: 590] loss: 0.04564112424850464\n",
      "[step: 591] loss: 0.04547150433063507\n",
      "[step: 592] loss: 0.04622042179107666\n",
      "[step: 593] loss: 0.04456905275583267\n",
      "[step: 594] loss: 0.04619288071990013\n",
      "[step: 595] loss: 0.04273508861660957\n",
      "[step: 596] loss: 0.041044022887945175\n",
      "[step: 597] loss: 0.04281061887741089\n",
      "[step: 598] loss: 0.04138887673616409\n",
      "[step: 599] loss: 0.04105269908905029\n",
      "[step: 600] loss: 0.04005153104662895\n",
      "[step: 601] loss: 0.03807955980300903\n",
      "[step: 602] loss: 0.03855149447917938\n",
      "[step: 603] loss: 0.038240738213062286\n",
      "[step: 604] loss: 0.037420351058244705\n",
      "[step: 605] loss: 0.03728287294507027\n",
      "[step: 606] loss: 0.036024075001478195\n",
      "[step: 607] loss: 0.03513725474476814\n",
      "[step: 608] loss: 0.034904155880212784\n",
      "[step: 609] loss: 0.03406672179698944\n",
      "[step: 610] loss: 0.033875539898872375\n",
      "[step: 611] loss: 0.033690158277750015\n",
      "[step: 612] loss: 0.032982178032398224\n",
      "[step: 613] loss: 0.032734740525484085\n",
      "[step: 614] loss: 0.032125432044267654\n",
      "[step: 615] loss: 0.031331319361925125\n",
      "[step: 616] loss: 0.03090105950832367\n",
      "[step: 617] loss: 0.03018186241388321\n",
      "[step: 618] loss: 0.029556352645158768\n",
      "[step: 619] loss: 0.0291599128395319\n",
      "[step: 620] loss: 0.028522875159978867\n",
      "[step: 621] loss: 0.02811533585190773\n",
      "[step: 622] loss: 0.027883678674697876\n",
      "[step: 623] loss: 0.027727678418159485\n",
      "[step: 624] loss: 0.028192277997732162\n",
      "[step: 625] loss: 0.02930358611047268\n",
      "[step: 626] loss: 0.03136579692363739\n",
      "[step: 627] loss: 0.03397388756275177\n",
      "[step: 628] loss: 0.03635725751519203\n",
      "[step: 629] loss: 0.03823767229914665\n",
      "[step: 630] loss: 0.038534779101610184\n",
      "[step: 631] loss: 0.04122458025813103\n",
      "[step: 632] loss: 0.03602079302072525\n",
      "[step: 633] loss: 0.031169135123491287\n",
      "[step: 634] loss: 0.024737533181905746\n",
      "[step: 635] loss: 0.022194234654307365\n",
      "[step: 636] loss: 0.02247445285320282\n",
      "[step: 637] loss: 0.02335192821919918\n",
      "[step: 638] loss: 0.024639317765831947\n",
      "[step: 639] loss: 0.025458991527557373\n",
      "[step: 640] loss: 0.026056349277496338\n",
      "[step: 641] loss: 0.02473849058151245\n",
      "[step: 642] loss: 0.02166423387825489\n",
      "[step: 643] loss: 0.018967069685459137\n",
      "[step: 644] loss: 0.018250664696097374\n",
      "[step: 645] loss: 0.018771180883049965\n",
      "[step: 646] loss: 0.01928379386663437\n",
      "[step: 647] loss: 0.01941799931228161\n",
      "[step: 648] loss: 0.019313687458634377\n",
      "[step: 649] loss: 0.01913081295788288\n",
      "[step: 650] loss: 0.017947111278772354\n",
      "[step: 651] loss: 0.016551543027162552\n",
      "[step: 652] loss: 0.01552680041640997\n",
      "[step: 653] loss: 0.015316350385546684\n",
      "[step: 654] loss: 0.015549673698842525\n",
      "[step: 655] loss: 0.01565428450703621\n",
      "[step: 656] loss: 0.01557454839348793\n",
      "[step: 657] loss: 0.015356103889644146\n",
      "[step: 658] loss: 0.015269380994141102\n",
      "[step: 659] loss: 0.015123213641345501\n",
      "[step: 660] loss: 0.014752781949937344\n",
      "[step: 661] loss: 0.014171821996569633\n",
      "[step: 662] loss: 0.013548956252634525\n",
      "[step: 663] loss: 0.01309289038181305\n",
      "[step: 664] loss: 0.012829356826841831\n",
      "[step: 665] loss: 0.012619087472558022\n",
      "[step: 666] loss: 0.012382544577121735\n",
      "[step: 667] loss: 0.012127871625125408\n",
      "[step: 668] loss: 0.011931969784200191\n",
      "[step: 669] loss: 0.011839275248348713\n",
      "[step: 670] loss: 0.011814119294285774\n",
      "[step: 671] loss: 0.011808512732386589\n",
      "[step: 672] loss: 0.011761924251914024\n",
      "[step: 673] loss: 0.0117337042465806\n",
      "[step: 674] loss: 0.011728222481906414\n",
      "[step: 675] loss: 0.011859641410410404\n",
      "[step: 676] loss: 0.012109385803341866\n",
      "[step: 677] loss: 0.012544757686555386\n",
      "[step: 678] loss: 0.01312825083732605\n",
      "[step: 679] loss: 0.013965114951133728\n",
      "[step: 680] loss: 0.014944089576601982\n",
      "[step: 681] loss: 0.01642674393951893\n",
      "[step: 682] loss: 0.017724720761179924\n",
      "[step: 683] loss: 0.019863808527588844\n",
      "[step: 684] loss: 0.02048659324645996\n",
      "[step: 685] loss: 0.021797867491841316\n",
      "[step: 686] loss: 0.01993478089570999\n",
      "[step: 687] loss: 0.018210938200354576\n",
      "[step: 688] loss: 0.014476058073341846\n",
      "[step: 689] loss: 0.01146638859063387\n",
      "[step: 690] loss: 0.009430356323719025\n",
      "[step: 691] loss: 0.008984526619315147\n",
      "[step: 692] loss: 0.009830847382545471\n",
      "[step: 693] loss: 0.011128975078463554\n",
      "[step: 694] loss: 0.012220099568367004\n",
      "[step: 695] loss: 0.012131249532103539\n",
      "[step: 696] loss: 0.011339276097714901\n",
      "[step: 697] loss: 0.009881950914859772\n",
      "[step: 698] loss: 0.008715900592505932\n",
      "[step: 699] loss: 0.008196881040930748\n",
      "[step: 700] loss: 0.00837812852114439\n",
      "[step: 701] loss: 0.008933409117162228\n",
      "[step: 702] loss: 0.009371266700327396\n",
      "[step: 703] loss: 0.009488427080214024\n",
      "[step: 704] loss: 0.009073938243091106\n",
      "[step: 705] loss: 0.008480350486934185\n",
      "[step: 706] loss: 0.007885967381298542\n",
      "[step: 707] loss: 0.007553976960480213\n",
      "[step: 708] loss: 0.007523879408836365\n",
      "[step: 709] loss: 0.007691110484302044\n",
      "[step: 710] loss: 0.00789652206003666\n",
      "[step: 711] loss: 0.00796912144869566\n",
      "[step: 712] loss: 0.007898632436990738\n",
      "[step: 713] loss: 0.007658972404897213\n",
      "[step: 714] loss: 0.007373183034360409\n",
      "[step: 715] loss: 0.007103626616299152\n",
      "[step: 716] loss: 0.006922941189259291\n",
      "[step: 717] loss: 0.006845412310212851\n",
      "[step: 718] loss: 0.006847667042165995\n",
      "[step: 719] loss: 0.006886020302772522\n",
      "[step: 720] loss: 0.006912670563906431\n",
      "[step: 721] loss: 0.006907327100634575\n",
      "[step: 722] loss: 0.006846215110272169\n",
      "[step: 723] loss: 0.00675111124292016\n",
      "[step: 724] loss: 0.006622477434575558\n",
      "[step: 725] loss: 0.006491863634437323\n",
      "[step: 726] loss: 0.006369263399392366\n",
      "[step: 727] loss: 0.006268620491027832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 728] loss: 0.006191725376993418\n",
      "[step: 729] loss: 0.006136439274996519\n",
      "[step: 730] loss: 0.006097504869103432\n",
      "[step: 731] loss: 0.006068509072065353\n",
      "[step: 732] loss: 0.006043832749128342\n",
      "[step: 733] loss: 0.006017602514475584\n",
      "[step: 734] loss: 0.005988866090774536\n",
      "[step: 735] loss: 0.005953904706984758\n",
      "[step: 736] loss: 0.005916525609791279\n",
      "[step: 737] loss: 0.005873286165297031\n",
      "[step: 738] loss: 0.005830179899930954\n",
      "[step: 739] loss: 0.00578340794891119\n",
      "[step: 740] loss: 0.00573942344635725\n",
      "[step: 741] loss: 0.005694231484085321\n",
      "[step: 742] loss: 0.005653929430991411\n",
      "[step: 743] loss: 0.005614497698843479\n",
      "[step: 744] loss: 0.005582223646342754\n",
      "[step: 745] loss: 0.005552995018661022\n",
      "[step: 746] loss: 0.005534997675567865\n",
      "[step: 747] loss: 0.005523262079805136\n",
      "[step: 748] loss: 0.005530651658773422\n",
      "[step: 749] loss: 0.0055505214259028435\n",
      "[step: 750] loss: 0.005606491584330797\n",
      "[step: 751] loss: 0.005688084755092859\n",
      "[step: 752] loss: 0.00584407290443778\n",
      "[step: 753] loss: 0.006052018608897924\n",
      "[step: 754] loss: 0.006422760896384716\n",
      "[step: 755] loss: 0.006890767719596624\n",
      "[step: 756] loss: 0.007725289091467857\n",
      "[step: 757] loss: 0.00869785062968731\n",
      "[step: 758] loss: 0.010470019653439522\n",
      "[step: 759] loss: 0.012223576195538044\n",
      "[step: 760] loss: 0.015488659031689167\n",
      "[step: 761] loss: 0.01763571985065937\n",
      "[step: 762] loss: 0.021702762693166733\n",
      "[step: 763] loss: 0.021593116223812103\n",
      "[step: 764] loss: 0.022257361561059952\n",
      "[step: 765] loss: 0.017025701701641083\n",
      "[step: 766] loss: 0.012136943638324738\n",
      "[step: 767] loss: 0.006919148378074169\n",
      "[step: 768] loss: 0.004675162490457296\n",
      "[step: 769] loss: 0.005551489535719156\n",
      "[step: 770] loss: 0.008073055185377598\n",
      "[step: 771] loss: 0.010439748875796795\n",
      "[step: 772] loss: 0.010176739655435085\n",
      "[step: 773] loss: 0.008483718149363995\n",
      "[step: 774] loss: 0.005867972504347563\n",
      "[step: 775] loss: 0.00449716579169035\n",
      "[step: 776] loss: 0.004859574139118195\n",
      "[step: 777] loss: 0.00615586107596755\n",
      "[step: 778] loss: 0.007189529482275248\n",
      "[step: 779] loss: 0.006863054353743792\n",
      "[step: 780] loss: 0.0057854498736560345\n",
      "[step: 781] loss: 0.0046247318387031555\n",
      "[step: 782] loss: 0.004236107226461172\n",
      "[step: 783] loss: 0.004651729017496109\n",
      "[step: 784] loss: 0.0053029232658445835\n",
      "[step: 785] loss: 0.0056387740187346935\n",
      "[step: 786] loss: 0.0053214323706924915\n",
      "[step: 787] loss: 0.004717408679425716\n",
      "[step: 788] loss: 0.004188844934105873\n",
      "[step: 789] loss: 0.004048080649226904\n",
      "[step: 790] loss: 0.0042605153284966946\n",
      "[step: 791] loss: 0.004566780291497707\n",
      "[step: 792] loss: 0.004731754772365093\n",
      "[step: 793] loss: 0.0046085393987596035\n",
      "[step: 794] loss: 0.0043280464597046375\n",
      "[step: 795] loss: 0.004029094707220793\n",
      "[step: 796] loss: 0.003870145883411169\n",
      "[step: 797] loss: 0.003882497549057007\n",
      "[step: 798] loss: 0.003999464213848114\n",
      "[step: 799] loss: 0.004118824377655983\n",
      "[step: 800] loss: 0.0041483324021101\n",
      "[step: 801] loss: 0.004083923529833555\n",
      "[step: 802] loss: 0.003944174386560917\n",
      "[step: 803] loss: 0.0038010054267942905\n",
      "[step: 804] loss: 0.003699827939271927\n",
      "[step: 805] loss: 0.003663075855001807\n",
      "[step: 806] loss: 0.0036794934421777725\n",
      "[step: 807] loss: 0.003718458814546466\n",
      "[step: 808] loss: 0.0037497486919164658\n",
      "[step: 809] loss: 0.0037494527641683817\n",
      "[step: 810] loss: 0.0037179577630013227\n",
      "[step: 811] loss: 0.003658883273601532\n",
      "[step: 812] loss: 0.0035921481903642416\n",
      "[step: 813] loss: 0.003530025016516447\n",
      "[step: 814] loss: 0.003483385546132922\n",
      "[step: 815] loss: 0.00345503818243742\n",
      "[step: 816] loss: 0.0034423714969307184\n",
      "[step: 817] loss: 0.003439344000071287\n",
      "[step: 818] loss: 0.0034392322413623333\n",
      "[step: 819] loss: 0.0034372289665043354\n",
      "[step: 820] loss: 0.003429037518799305\n",
      "[step: 821] loss: 0.0034146804828196764\n",
      "[step: 822] loss: 0.0033932849764823914\n",
      "[step: 823] loss: 0.0033679029438644648\n",
      "[step: 824] loss: 0.003339155111461878\n",
      "[step: 825] loss: 0.003309941850602627\n",
      "[step: 826] loss: 0.00328084803186357\n",
      "[step: 827] loss: 0.003253364469856024\n",
      "[step: 828] loss: 0.003227515844628215\n",
      "[step: 829] loss: 0.0032036935444921255\n",
      "[step: 830] loss: 0.0031815767288208008\n",
      "[step: 831] loss: 0.003160977503284812\n",
      "[step: 832] loss: 0.003141578286886215\n",
      "[step: 833] loss: 0.003123131114989519\n",
      "[step: 834] loss: 0.003105341223999858\n",
      "[step: 835] loss: 0.00308811804279685\n",
      "[step: 836] loss: 0.0030712950974702835\n",
      "[step: 837] loss: 0.0030548316426575184\n",
      "[step: 838] loss: 0.0030387078877538443\n",
      "[step: 839] loss: 0.003022996708750725\n",
      "[step: 840] loss: 0.0030077812261879444\n",
      "[step: 841] loss: 0.0029932358302176\n",
      "[step: 842] loss: 0.002979687415063381\n",
      "[step: 843] loss: 0.0029675986152142286\n",
      "[step: 844] loss: 0.0029577927198261023\n",
      "[step: 845] loss: 0.0029513221234083176\n",
      "[step: 846] loss: 0.002950527938082814\n",
      "[step: 847] loss: 0.00295816152356565\n",
      "[step: 848] loss: 0.002980738179758191\n",
      "[step: 849] loss: 0.0030256505124270916\n",
      "[step: 850] loss: 0.0031122718937695026\n",
      "[step: 851] loss: 0.0032603973522782326\n",
      "[step: 852] loss: 0.003531253430992365\n",
      "[step: 853] loss: 0.003974484279751778\n",
      "[step: 854] loss: 0.004792813677340746\n",
      "[step: 855] loss: 0.0060793389566242695\n",
      "[step: 856] loss: 0.00851381104439497\n",
      "[step: 857] loss: 0.011991805396974087\n",
      "[step: 858] loss: 0.01861463487148285\n",
      "[step: 859] loss: 0.025722268968820572\n",
      "[step: 860] loss: 0.03819071874022484\n",
      "[step: 861] loss: 0.04126490280032158\n",
      "[step: 862] loss: 0.04416407272219658\n",
      "[step: 863] loss: 0.02635890617966652\n",
      "[step: 864] loss: 0.010613339021801949\n",
      "[step: 865] loss: 0.0029836406465619802\n",
      "[step: 866] loss: 0.0085215475410223\n",
      "[step: 867] loss: 0.018484260886907578\n",
      "[step: 868] loss: 0.01765037141740322\n",
      "[step: 869] loss: 0.010233430191874504\n",
      "[step: 870] loss: 0.0032080390956252813\n",
      "[step: 871] loss: 0.0052634249441325665\n",
      "[step: 872] loss: 0.011253910139203072\n",
      "[step: 873] loss: 0.010518981143832207\n",
      "[step: 874] loss: 0.0054728370159864426\n",
      "[step: 875] loss: 0.0028133129235357046\n",
      "[step: 876] loss: 0.005527832079678774\n",
      "[step: 877] loss: 0.008274883031845093\n",
      "[step: 878] loss: 0.006105193868279457\n",
      "[step: 879] loss: 0.0031126518733799458\n",
      "[step: 880] loss: 0.0032967820297926664\n",
      "[step: 881] loss: 0.00535973533987999\n",
      "[step: 882] loss: 0.005691697355359793\n",
      "[step: 883] loss: 0.003692959900945425\n",
      "[step: 884] loss: 0.002622547559440136\n",
      "[step: 885] loss: 0.003602455835789442\n",
      "[step: 886] loss: 0.0045569101348519325\n",
      "[step: 887] loss: 0.003972948528826237\n",
      "[step: 888] loss: 0.0027274321764707565\n",
      "[step: 889] loss: 0.0026105644647032022\n",
      "[step: 890] loss: 0.003439789405092597\n",
      "[step: 891] loss: 0.003701594891026616\n",
      "[step: 892] loss: 0.00303470017388463\n",
      "[step: 893] loss: 0.002401829231530428\n",
      "[step: 894] loss: 0.002593761309981346\n",
      "[step: 895] loss: 0.003101295791566372\n",
      "[step: 896] loss: 0.0030686480458825827\n",
      "[step: 897] loss: 0.0025843337643891573\n",
      "[step: 898] loss: 0.002299639629200101\n",
      "[step: 899] loss: 0.0024859346449375153\n",
      "[step: 900] loss: 0.0027479694690555334\n",
      "[step: 901] loss: 0.0026761724147945642\n",
      "[step: 902] loss: 0.002389095025137067\n",
      "[step: 903] loss: 0.0022304770536720753\n",
      "[step: 904] loss: 0.0023249404039233923\n",
      "[step: 905] loss: 0.0024707268457859755\n",
      "[step: 906] loss: 0.0024442514404654503\n",
      "[step: 907] loss: 0.002278841333463788\n",
      "[step: 908] loss: 0.0021559023298323154\n",
      "[step: 909] loss: 0.002182518132030964\n",
      "[step: 910] loss: 0.002274591475725174\n",
      "[step: 911] loss: 0.002286080503836274\n",
      "[step: 912] loss: 0.0021947917994111776\n",
      "[step: 913] loss: 0.002094283001497388\n",
      "[step: 914] loss: 0.0020771543495357037\n",
      "[step: 915] loss: 0.002125128637999296\n",
      "[step: 916] loss: 0.002151872031390667\n",
      "[step: 917] loss: 0.002115249400958419\n",
      "[step: 918] loss: 0.0020474845077842474\n",
      "[step: 919] loss: 0.0020080844406038523\n",
      "[step: 920] loss: 0.002013217890635133\n",
      "[step: 921] loss: 0.0020327167585492134\n",
      "[step: 922] loss: 0.0020312173292040825\n",
      "[step: 923] loss: 0.0020001637749373913\n",
      "[step: 924] loss: 0.001960629364475608\n",
      "[step: 925] loss: 0.0019369564251974225\n",
      "[step: 926] loss: 0.0019354729447513819\n",
      "[step: 927] loss: 0.0019417146686464548\n",
      "[step: 928] loss: 0.0019366859924048185\n",
      "[step: 929] loss: 0.0019150908337906003\n",
      "[step: 930] loss: 0.0018874593079090118\n",
      "[step: 931] loss: 0.001868180581368506\n",
      "[step: 932] loss: 0.0018615428125485778\n",
      "[step: 933] loss: 0.00186017039231956\n",
      "[step: 934] loss: 0.001854232745245099\n",
      "[step: 935] loss: 0.0018400107510387897\n",
      "[step: 936] loss: 0.0018213096773251891\n",
      "[step: 937] loss: 0.0018042410956695676\n",
      "[step: 938] loss: 0.001792381634004414\n",
      "[step: 939] loss: 0.0017850592266768217\n",
      "[step: 940] loss: 0.0017786893295124173\n",
      "[step: 941] loss: 0.001769772614352405\n",
      "[step: 942] loss: 0.0017570400377735496\n",
      "[step: 943] loss: 0.001742383698001504\n",
      "[step: 944] loss: 0.0017287004739046097\n",
      "[step: 945] loss: 0.0017177305417135358\n",
      "[step: 946] loss: 0.0017090670298784971\n",
      "[step: 947] loss: 0.0017008736031129956\n",
      "[step: 948] loss: 0.0016915587475523353\n",
      "[step: 949] loss: 0.0016806600615382195\n",
      "[step: 950] loss: 0.0016687692841514945\n",
      "[step: 951] loss: 0.0016569121507927775\n",
      "[step: 952] loss: 0.0016458086902275681\n",
      "[step: 953] loss: 0.0016357292188331485\n",
      "[step: 954] loss: 0.00162645080126822\n",
      "[step: 955] loss: 0.001617372501641512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 956] loss: 0.0016078801127150655\n",
      "[step: 957] loss: 0.0015977324219420552\n",
      "[step: 958] loss: 0.0015871268697082996\n",
      "[step: 959] loss: 0.0015764321433380246\n",
      "[step: 960] loss: 0.0015660362550988793\n",
      "[step: 961] loss: 0.0015560663305222988\n",
      "[step: 962] loss: 0.0015464633470401168\n",
      "[step: 963] loss: 0.0015370827168226242\n",
      "[step: 964] loss: 0.0015277510974556208\n",
      "[step: 965] loss: 0.0015183056239038706\n",
      "[step: 966] loss: 0.0015087034553289413\n",
      "[step: 967] loss: 0.0014989293413236737\n",
      "[step: 968] loss: 0.0014891078462824225\n",
      "[step: 969] loss: 0.0014793297741562128\n",
      "[step: 970] loss: 0.0014696929138153791\n",
      "[step: 971] loss: 0.0014602026203647256\n",
      "[step: 972] loss: 0.0014508459717035294\n",
      "[step: 973] loss: 0.001441590371541679\n",
      "[step: 974] loss: 0.0014323971699923277\n",
      "[step: 975] loss: 0.0014232475077733397\n",
      "[step: 976] loss: 0.001414117286913097\n",
      "[step: 977] loss: 0.0014049519086256623\n",
      "[step: 978] loss: 0.0013957963092252612\n",
      "[step: 979] loss: 0.001386631978675723\n",
      "[step: 980] loss: 0.0013774940744042397\n",
      "[step: 981] loss: 0.0013683862052857876\n",
      "[step: 982] loss: 0.0013593189651146531\n",
      "[step: 983] loss: 0.0013502941001206636\n",
      "[step: 984] loss: 0.001341333263553679\n",
      "[step: 985] loss: 0.0013323973398655653\n",
      "[step: 986] loss: 0.001323535805568099\n",
      "[step: 987] loss: 0.0013147309655323625\n",
      "[step: 988] loss: 0.0013059655902907252\n",
      "[step: 989] loss: 0.0012972354888916016\n",
      "[step: 990] loss: 0.0012885457836091518\n",
      "[step: 991] loss: 0.001279903226532042\n",
      "[step: 992] loss: 0.001271304558031261\n",
      "[step: 993] loss: 0.0012627302203327417\n",
      "[step: 994] loss: 0.0012542210752144456\n",
      "[step: 995] loss: 0.001245735795237124\n",
      "[step: 996] loss: 0.0012373122153803706\n",
      "[step: 997] loss: 0.001228927867487073\n",
      "[step: 998] loss: 0.0012205931125208735\n",
      "[step: 999] loss: 0.0012123348424211144\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "LSTM realforecast : [10258.6474609375, 12691.853515625, -739.6880493164062, -589.4653930664062, 5872.23388671875, 5039.17236328125, -4128.912109375]\n",
      "Bayseian realforecast : [934.67590488609881, 1660.8676990102022, 4777.8656352160351, 5393.0442755984013, 2664.1705383767676, 2745.4086336296377, 5034.9435872385957]\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,0,forecastDay,'month') #0은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10258.6474609375,\n",
       " 12691.853515625,\n",
       " -739.6880493164062,\n",
       " -589.4653930664062,\n",
       " 5872.23388671875,\n",
       " 5039.17236328125,\n",
       " -4128.912109375]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawArrayDatas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
