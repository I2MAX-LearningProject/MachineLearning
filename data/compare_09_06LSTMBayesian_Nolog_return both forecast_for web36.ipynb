{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list((rawArrayDatas[1]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list((rawArrayDatas[1][:-forecastDay] ))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-(mockForcastDay+forecastDay)] & np.log\n",
    "    ds = rawArrayDatas[0][:-(mockForcastDay+forecastDay)]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-(mockForcastDay+forecastDay)]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of (mockForcastDay+forecastDay)  rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "    testY= rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출       \n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'day')\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "    \n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    print('LSTM realforecast :',realForecastDictionary['LSTM'])\n",
    "    print('Bayseian realforecast :',realForecastDictionary['Bayseian'] ) \n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "#         listedLogPredict=test_predict[-1].tolist()\n",
    "#     return [np.exp(y) for y in listedLogPredict]\n",
    "    return test_predict[-1].tolist()\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM testforecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian testforecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 28\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('walWeek.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.2)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Date\": ['2010-02-05', '2010-02-12', '2010-02-19', '2010-02-26', '2010-03-05', '2010-03-12', '2010-03-19', '2010-03-26', '2010-04-02', '2010-04-09', '2010-04-16', '2010-04-23', '2010-04-30', '2010-05-07', '2010-05-14', '2010-05-21', '2010-05-28', '2010-06-04', '2010-06-11', '2010-06-18', '2010-06-25', '2010-07-02', '2010-07-09', '2010-07-16', '2010-07-23', '2010-07-30', '2010-08-06', '2010-08-13', '2010-08-20', '2010-08-27', '2010-09-03', '2010-09-10', '2010-09-17', '2010-09-24', '2010-10-01', '2010-10-08', '2010-10-15', '2010-10-22', '2010-10-29', '2010-11-05', '2010-11-12', '2010-11-19', '2010-11-26', '2010-12-03', '2010-12-10', '2010-12-17', '2010-12-24', '2010-12-31', '2011-01-07', '2011-01-14', '2011-01-21', '2011-01-28', '2011-02-04', '2011-02-11', '2011-02-18', '2011-02-25', '2011-03-04', '2011-03-11', '2011-03-18', '2011-03-25', '2011-04-01', '2011-04-08', '2011-04-15', '2011-04-22', '2011-04-29', '2011-05-06', '2011-05-13', '2011-05-20', '2011-05-27', '2011-06-03', '2011-06-10', '2011-06-17', '2011-06-24', '2011-07-01', '2011-07-08', '2011-07-15', '2011-07-22', '2011-07-29', '2011-08-05', '2011-08-12', '2011-08-19', '2011-08-26', '2011-09-02', '2011-09-09', '2011-09-16', '2011-09-23', '2011-09-30', '2011-10-07', '2011-10-14', '2011-10-21', '2011-10-28', '2011-11-04', '2011-11-11', '2011-11-18', '2011-11-25', '2011-12-02', '2011-12-09', '2011-12-16', '2011-12-23', '2011-12-30', '2012-01-06', '2012-01-13', '2012-01-20', '2012-01-27', '2012-02-03', '2012-02-10', '2012-02-17', '2012-02-24', '2012-03-02', '2012-03-09', '2012-03-16', '2012-03-23', '2012-03-30', '2012-04-06', '2012-04-13', '2012-04-20', '2012-04-27', '2012-05-04', '2012-05-11', '2012-05-18', '2012-05-25', '2012-06-01', '2012-06-08', '2012-06-15', '2012-06-22', '2012-06-29', '2012-07-06', '2012-07-13', '2012-07-20', '2012-07-27', '2012-08-03', '2012-08-10', '2012-08-17', '2012-08-24', '2012-08-31', '2012-09-07', '2012-09-14', '2012-09-21', '2012-09-28', '2012-10-05', '2012-10-12', '2012-10-19', '2012-10-26'] ,\"Data\": [24924.5, 46039.489999999998, 41595.550000000003, 19403.540000000001, 21827.900000000001, 21043.389999999999, 22136.639999999999, 26229.209999999999, 57258.43, 42960.910000000003, 17596.959999999999, 16145.35, 16555.110000000001, 17413.939999999999, 18926.740000000002, 14773.040000000001, 15580.43, 17558.09, 16637.619999999999, 16216.27, 16328.719999999999, 16333.139999999999, 17688.759999999998, 17150.84, 15360.450000000001, 15381.82, 17508.41, 15536.4, 15740.129999999999, 15793.870000000001, 16241.780000000001, 18194.740000000002, 19354.23, 18122.52, 20094.189999999999, 23388.029999999999, 26978.34, 25543.040000000001, 38640.93, 34238.879999999997, 19549.389999999999, 19552.84, 18820.290000000001, 22517.560000000001, 31497.650000000001, 44912.860000000001, 55931.230000000003, 19124.580000000002, 15984.24, 17359.700000000001, 17341.470000000001, 18461.18, 21665.759999999998, 37887.169999999998, 46845.870000000003, 19363.830000000002, 20327.610000000001, 21280.400000000001, 20334.23, 20881.099999999999, 20398.09, 23873.790000000001, 28762.369999999999, 50510.309999999998, 41512.389999999999, 20138.189999999999, 17235.150000000001, 15136.780000000001, 15741.6, 16434.150000000001, 15883.52, 14978.09, 15682.809999999999, 15363.5, 16148.870000000001, 15654.85, 15766.6, 15922.41, 15295.549999999999, 14539.790000000001, 14689.24, 14537.370000000001, 15277.27, 17746.68, 18535.48, 17859.299999999999, 18337.68, 20797.580000000002, 23077.549999999999, 23351.799999999999, 31579.900000000001, 39886.059999999998, 18689.540000000001, 19050.66, 20911.25, 25293.490000000002, 33305.919999999998, 45773.029999999999, 46788.75, 23350.880000000001, 16567.689999999999, 16894.400000000001, 18365.099999999999, 18378.16, 23510.490000000002, 36988.489999999998, 54060.099999999999, 20124.220000000001, 20113.029999999999, 21140.07, 22366.880000000001, 22107.700000000001, 28952.860000000001, 57592.120000000003, 34684.209999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] ,\"Day\": 28 }\n"
     ]
    }
   ],
   "source": [
    "print('{\"Date\":',rawArrayDatas[0],',\"Data\":',rawArrayDatas[1],',\"Day\":',forecastDay,'}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 284.3252258300781\n",
      "[step: 1] loss: 273.5332946777344\n",
      "[step: 2] loss: 263.53515625\n",
      "[step: 3] loss: 254.09426879882812\n",
      "[step: 4] loss: 245.02047729492188\n",
      "[step: 5] loss: 236.13613891601562\n",
      "[step: 6] loss: 227.2740936279297\n",
      "[step: 7] loss: 218.28106689453125\n",
      "[step: 8] loss: 209.0157012939453\n",
      "[step: 9] loss: 199.3433837890625\n",
      "[step: 10] loss: 189.13375854492188\n",
      "[step: 11] loss: 178.2645721435547\n",
      "[step: 12] loss: 166.6344451904297\n",
      "[step: 13] loss: 154.1919403076172\n",
      "[step: 14] loss: 140.98886108398438\n",
      "[step: 15] loss: 127.27042388916016\n",
      "[step: 16] loss: 113.60594177246094\n",
      "[step: 17] loss: 101.03141784667969\n",
      "[step: 18] loss: 91.02660369873047\n",
      "[step: 19] loss: 84.91893005371094\n",
      "[step: 20] loss: 82.43571472167969\n",
      "[step: 21] loss: 80.8990478515625\n",
      "[step: 22] loss: 77.31671142578125\n",
      "[step: 23] loss: 71.1046142578125\n",
      "[step: 24] loss: 63.857017517089844\n",
      "[step: 25] loss: 57.6027946472168\n",
      "[step: 26] loss: 53.583900451660156\n",
      "[step: 27] loss: 51.939292907714844\n",
      "[step: 28] loss: 52.01591491699219\n",
      "[step: 29] loss: 52.89597702026367\n",
      "[step: 30] loss: 53.81140899658203\n",
      "[step: 31] loss: 54.323524475097656\n",
      "[step: 32] loss: 54.31711959838867\n",
      "[step: 33] loss: 53.90357208251953\n",
      "[step: 34] loss: 53.29693603515625\n",
      "[step: 35] loss: 52.69526672363281\n",
      "[step: 36] loss: 52.19574737548828\n",
      "[step: 37] loss: 51.77322769165039\n",
      "[step: 38] loss: 51.328895568847656\n",
      "[step: 39] loss: 50.775489807128906\n",
      "[step: 40] loss: 50.0997314453125\n",
      "[step: 41] loss: 49.366302490234375\n",
      "[step: 42] loss: 48.67511749267578\n",
      "[step: 43] loss: 48.10942840576172\n",
      "[step: 44] loss: 47.70561981201172\n",
      "[step: 45] loss: 47.45158004760742\n",
      "[step: 46] loss: 47.30406951904297\n",
      "[step: 47] loss: 47.211307525634766\n",
      "[step: 48] loss: 47.13048553466797\n",
      "[step: 49] loss: 47.03666305541992\n",
      "[step: 50] loss: 46.923362731933594\n",
      "[step: 51] loss: 46.79749298095703\n",
      "[step: 52] loss: 46.67156982421875\n",
      "[step: 53] loss: 46.55614471435547\n",
      "[step: 54] loss: 46.45464324951172\n",
      "[step: 55] loss: 46.362396240234375\n",
      "[step: 56] loss: 46.269290924072266\n",
      "[step: 57] loss: 46.16499328613281\n",
      "[step: 58] loss: 46.04378128051758\n",
      "[step: 59] loss: 45.906978607177734\n",
      "[step: 60] loss: 45.761985778808594\n",
      "[step: 61] loss: 45.61884307861328\n",
      "[step: 62] loss: 45.486324310302734\n",
      "[step: 63] loss: 45.36913299560547\n",
      "[step: 64] loss: 45.26732635498047\n",
      "[step: 65] loss: 45.177490234375\n",
      "[step: 66] loss: 45.095008850097656\n",
      "[step: 67] loss: 45.01600646972656\n",
      "[step: 68] loss: 44.93860626220703\n",
      "[step: 69] loss: 44.862876892089844\n",
      "[step: 70] loss: 44.790164947509766\n",
      "[step: 71] loss: 44.72174072265625\n",
      "[step: 72] loss: 44.65789794921875\n",
      "[step: 73] loss: 44.59757995605469\n",
      "[step: 74] loss: 44.53868865966797\n",
      "[step: 75] loss: 44.47900390625\n",
      "[step: 76] loss: 44.41698455810547\n",
      "[step: 77] loss: 44.352378845214844\n",
      "[step: 78] loss: 44.28615951538086\n",
      "[step: 79] loss: 44.21995162963867\n",
      "[step: 80] loss: 44.155372619628906\n",
      "[step: 81] loss: 44.09343719482422\n",
      "[step: 82] loss: 44.034332275390625\n",
      "[step: 83] loss: 43.97755813598633\n",
      "[step: 84] loss: 43.9223518371582\n",
      "[step: 85] loss: 43.868072509765625\n",
      "[step: 86] loss: 43.81438446044922\n",
      "[step: 87] loss: 43.76136016845703\n",
      "[step: 88] loss: 43.709251403808594\n",
      "[step: 89] loss: 43.65829086303711\n",
      "[step: 90] loss: 43.6085205078125\n",
      "[step: 91] loss: 43.55971145629883\n",
      "[step: 92] loss: 43.511451721191406\n",
      "[step: 93] loss: 43.46335983276367\n",
      "[step: 94] loss: 43.41519546508789\n",
      "[step: 95] loss: 43.36697006225586\n",
      "[step: 96] loss: 43.318870544433594\n",
      "[step: 97] loss: 43.271148681640625\n",
      "[step: 98] loss: 43.22400665283203\n",
      "[step: 99] loss: 43.1774787902832\n",
      "[step: 100] loss: 43.13145446777344\n",
      "[step: 101] loss: 43.08579635620117\n",
      "[step: 102] loss: 43.040313720703125\n",
      "[step: 103] loss: 42.99492263793945\n",
      "[step: 104] loss: 42.949615478515625\n",
      "[step: 105] loss: 42.90446090698242\n",
      "[step: 106] loss: 42.85951232910156\n",
      "[step: 107] loss: 42.814788818359375\n",
      "[step: 108] loss: 42.77018737792969\n",
      "[step: 109] loss: 42.725650787353516\n",
      "[step: 110] loss: 42.68101501464844\n",
      "[step: 111] loss: 42.63623809814453\n",
      "[step: 112] loss: 42.59126281738281\n",
      "[step: 113] loss: 42.54611587524414\n",
      "[step: 114] loss: 42.50077819824219\n",
      "[step: 115] loss: 42.45521926879883\n",
      "[step: 116] loss: 42.409400939941406\n",
      "[step: 117] loss: 42.36322021484375\n",
      "[step: 118] loss: 42.31659698486328\n",
      "[step: 119] loss: 42.26947784423828\n",
      "[step: 120] loss: 42.22185134887695\n",
      "[step: 121] loss: 42.17372131347656\n",
      "[step: 122] loss: 42.125022888183594\n",
      "[step: 123] loss: 42.07573699951172\n",
      "[step: 124] loss: 42.025718688964844\n",
      "[step: 125] loss: 41.974937438964844\n",
      "[step: 126] loss: 41.92326354980469\n",
      "[step: 127] loss: 41.87068176269531\n",
      "[step: 128] loss: 41.81713104248047\n",
      "[step: 129] loss: 41.762535095214844\n",
      "[step: 130] loss: 41.706825256347656\n",
      "[step: 131] loss: 41.64990234375\n",
      "[step: 132] loss: 41.591651916503906\n",
      "[step: 133] loss: 41.53204345703125\n",
      "[step: 134] loss: 41.47100067138672\n",
      "[step: 135] loss: 41.40847396850586\n",
      "[step: 136] loss: 41.344444274902344\n",
      "[step: 137] loss: 41.278778076171875\n",
      "[step: 138] loss: 41.21146011352539\n",
      "[step: 139] loss: 41.14239501953125\n",
      "[step: 140] loss: 41.07157897949219\n",
      "[step: 141] loss: 40.99897384643555\n",
      "[step: 142] loss: 40.924522399902344\n",
      "[step: 143] loss: 40.84820556640625\n",
      "[step: 144] loss: 40.77007293701172\n",
      "[step: 145] loss: 40.69020080566406\n",
      "[step: 146] loss: 40.608665466308594\n",
      "[step: 147] loss: 40.525611877441406\n",
      "[step: 148] loss: 40.44116973876953\n",
      "[step: 149] loss: 40.355621337890625\n",
      "[step: 150] loss: 40.26923751831055\n",
      "[step: 151] loss: 40.18232727050781\n",
      "[step: 152] loss: 40.095359802246094\n",
      "[step: 153] loss: 40.00889587402344\n",
      "[step: 154] loss: 39.92350769042969\n",
      "[step: 155] loss: 39.83990478515625\n",
      "[step: 156] loss: 39.75883483886719\n",
      "[step: 157] loss: 39.68109893798828\n",
      "[step: 158] loss: 39.60749816894531\n",
      "[step: 159] loss: 39.53885269165039\n",
      "[step: 160] loss: 39.47586441040039\n",
      "[step: 161] loss: 39.4189453125\n",
      "[step: 162] loss: 39.368404388427734\n",
      "[step: 163] loss: 39.32400131225586\n",
      "[step: 164] loss: 39.285213470458984\n",
      "[step: 165] loss: 39.25099563598633\n",
      "[step: 166] loss: 39.219966888427734\n",
      "[step: 167] loss: 39.190521240234375\n",
      "[step: 168] loss: 39.161102294921875\n",
      "[step: 169] loss: 39.13041305541992\n",
      "[step: 170] loss: 39.097572326660156\n",
      "[step: 171] loss: 39.06224822998047\n",
      "[step: 172] loss: 39.02460861206055\n",
      "[step: 173] loss: 38.98522186279297\n",
      "[step: 174] loss: 38.944854736328125\n",
      "[step: 175] loss: 38.904361724853516\n",
      "[step: 176] loss: 38.86455535888672\n",
      "[step: 177] loss: 38.82603454589844\n",
      "[step: 178] loss: 38.78919982910156\n",
      "[step: 179] loss: 38.7542724609375\n",
      "[step: 180] loss: 38.72126007080078\n",
      "[step: 181] loss: 38.690032958984375\n",
      "[step: 182] loss: 38.660377502441406\n",
      "[step: 183] loss: 38.631996154785156\n",
      "[step: 184] loss: 38.604618072509766\n",
      "[step: 185] loss: 38.57796096801758\n",
      "[step: 186] loss: 38.55180358886719\n",
      "[step: 187] loss: 38.525978088378906\n",
      "[step: 188] loss: 38.50035095214844\n",
      "[step: 189] loss: 38.474830627441406\n",
      "[step: 190] loss: 38.449462890625\n",
      "[step: 191] loss: 38.424224853515625\n",
      "[step: 192] loss: 38.39916229248047\n",
      "[step: 193] loss: 38.374366760253906\n",
      "[step: 194] loss: 38.34991455078125\n",
      "[step: 195] loss: 38.32586669921875\n",
      "[step: 196] loss: 38.302276611328125\n",
      "[step: 197] loss: 38.2791748046875\n",
      "[step: 198] loss: 38.2565803527832\n",
      "[step: 199] loss: 38.234474182128906\n",
      "[step: 200] loss: 38.21283721923828\n",
      "[step: 201] loss: 38.191585540771484\n",
      "[step: 202] loss: 38.1706657409668\n",
      "[step: 203] loss: 38.150001525878906\n",
      "[step: 204] loss: 38.129539489746094\n",
      "[step: 205] loss: 38.109230041503906\n",
      "[step: 206] loss: 38.08903884887695\n",
      "[step: 207] loss: 38.06892395019531\n",
      "[step: 208] loss: 38.04890441894531\n",
      "[step: 209] loss: 38.028953552246094\n",
      "[step: 210] loss: 38.009117126464844\n",
      "[step: 211] loss: 37.98937225341797\n",
      "[step: 212] loss: 37.969764709472656\n",
      "[step: 213] loss: 37.950294494628906\n",
      "[step: 214] loss: 37.93096160888672\n",
      "[step: 215] loss: 37.91175079345703\n",
      "[step: 216] loss: 37.89265823364258\n",
      "[step: 217] loss: 37.87367248535156\n",
      "[step: 218] loss: 37.85475540161133\n",
      "[step: 219] loss: 37.835899353027344\n",
      "[step: 220] loss: 37.81707763671875\n",
      "[step: 221] loss: 37.79827117919922\n",
      "[step: 222] loss: 37.77946472167969\n",
      "[step: 223] loss: 37.760623931884766\n",
      "[step: 224] loss: 37.741783142089844\n",
      "[step: 225] loss: 37.7228889465332\n",
      "[step: 226] loss: 37.70396423339844\n",
      "[step: 227] loss: 37.68499755859375\n",
      "[step: 228] loss: 37.666011810302734\n",
      "[step: 229] loss: 37.6469612121582\n",
      "[step: 230] loss: 37.62786102294922\n",
      "[step: 231] loss: 37.60869598388672\n",
      "[step: 232] loss: 37.58946990966797\n",
      "[step: 233] loss: 37.570167541503906\n",
      "[step: 234] loss: 37.55076599121094\n",
      "[step: 235] loss: 37.531280517578125\n",
      "[step: 236] loss: 37.511714935302734\n",
      "[step: 237] loss: 37.492027282714844\n",
      "[step: 238] loss: 37.47222900390625\n",
      "[step: 239] loss: 37.452335357666016\n",
      "[step: 240] loss: 37.43231201171875\n",
      "[step: 241] loss: 37.41218566894531\n",
      "[step: 242] loss: 37.39194869995117\n",
      "[step: 243] loss: 37.37160110473633\n",
      "[step: 244] loss: 37.35114288330078\n",
      "[step: 245] loss: 37.33057403564453\n",
      "[step: 246] loss: 37.30990982055664\n",
      "[step: 247] loss: 37.28913497924805\n",
      "[step: 248] loss: 37.26826477050781\n",
      "[step: 249] loss: 37.2473030090332\n",
      "[step: 250] loss: 37.22624206542969\n",
      "[step: 251] loss: 37.20510482788086\n",
      "[step: 252] loss: 37.183876037597656\n",
      "[step: 253] loss: 37.16261291503906\n",
      "[step: 254] loss: 37.14125061035156\n",
      "[step: 255] loss: 37.119869232177734\n",
      "[step: 256] loss: 37.098419189453125\n",
      "[step: 257] loss: 37.07697296142578\n",
      "[step: 258] loss: 37.05548095703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 259] loss: 37.03400421142578\n",
      "[step: 260] loss: 37.01252746582031\n",
      "[step: 261] loss: 36.991058349609375\n",
      "[step: 262] loss: 36.96962356567383\n",
      "[step: 263] loss: 36.94822692871094\n",
      "[step: 264] loss: 36.926876068115234\n",
      "[step: 265] loss: 36.90558624267578\n",
      "[step: 266] loss: 36.884368896484375\n",
      "[step: 267] loss: 36.86323547363281\n",
      "[step: 268] loss: 36.8421745300293\n",
      "[step: 269] loss: 36.82121276855469\n",
      "[step: 270] loss: 36.80034637451172\n",
      "[step: 271] loss: 36.779563903808594\n",
      "[step: 272] loss: 36.758907318115234\n",
      "[step: 273] loss: 36.73835372924805\n",
      "[step: 274] loss: 36.7178955078125\n",
      "[step: 275] loss: 36.697547912597656\n",
      "[step: 276] loss: 36.67729568481445\n",
      "[step: 277] loss: 36.65715026855469\n",
      "[step: 278] loss: 36.6370735168457\n",
      "[step: 279] loss: 36.617103576660156\n",
      "[step: 280] loss: 36.59721374511719\n",
      "[step: 281] loss: 36.57740020751953\n",
      "[step: 282] loss: 36.55764389038086\n",
      "[step: 283] loss: 36.53794860839844\n",
      "[step: 284] loss: 36.518310546875\n",
      "[step: 285] loss: 36.49871063232422\n",
      "[step: 286] loss: 36.479164123535156\n",
      "[step: 287] loss: 36.459632873535156\n",
      "[step: 288] loss: 36.440128326416016\n",
      "[step: 289] loss: 36.42064666748047\n",
      "[step: 290] loss: 36.40118408203125\n",
      "[step: 291] loss: 36.38172912597656\n",
      "[step: 292] loss: 36.36229705810547\n",
      "[step: 293] loss: 36.342872619628906\n",
      "[step: 294] loss: 36.32344436645508\n",
      "[step: 295] loss: 36.30402374267578\n",
      "[step: 296] loss: 36.284637451171875\n",
      "[step: 297] loss: 36.265235900878906\n",
      "[step: 298] loss: 36.24585723876953\n",
      "[step: 299] loss: 36.22648620605469\n",
      "[step: 300] loss: 36.2071418762207\n",
      "[step: 301] loss: 36.18779373168945\n",
      "[step: 302] loss: 36.16846466064453\n",
      "[step: 303] loss: 36.14915084838867\n",
      "[step: 304] loss: 36.12986373901367\n",
      "[step: 305] loss: 36.11058807373047\n",
      "[step: 306] loss: 36.09131622314453\n",
      "[step: 307] loss: 36.07207107543945\n",
      "[step: 308] loss: 36.05284118652344\n",
      "[step: 309] loss: 36.03362274169922\n",
      "[step: 310] loss: 36.014434814453125\n",
      "[step: 311] loss: 35.9952392578125\n",
      "[step: 312] loss: 35.97605895996094\n",
      "[step: 313] loss: 35.95689392089844\n",
      "[step: 314] loss: 35.9377326965332\n",
      "[step: 315] loss: 35.91857147216797\n",
      "[step: 316] loss: 35.899410247802734\n",
      "[step: 317] loss: 35.8802604675293\n",
      "[step: 318] loss: 35.861122131347656\n",
      "[step: 319] loss: 35.84196090698242\n",
      "[step: 320] loss: 35.82281494140625\n",
      "[step: 321] loss: 35.80365753173828\n",
      "[step: 322] loss: 35.784507751464844\n",
      "[step: 323] loss: 35.76536560058594\n",
      "[step: 324] loss: 35.74620819091797\n",
      "[step: 325] loss: 35.72705078125\n",
      "[step: 326] loss: 35.70790481567383\n",
      "[step: 327] loss: 35.68876647949219\n",
      "[step: 328] loss: 35.66964340209961\n",
      "[step: 329] loss: 35.65052795410156\n",
      "[step: 330] loss: 35.63141632080078\n",
      "[step: 331] loss: 35.612327575683594\n",
      "[step: 332] loss: 35.59325408935547\n",
      "[step: 333] loss: 35.574180603027344\n",
      "[step: 334] loss: 35.5551643371582\n",
      "[step: 335] loss: 35.536155700683594\n",
      "[step: 336] loss: 35.5172119140625\n",
      "[step: 337] loss: 35.498268127441406\n",
      "[step: 338] loss: 35.4793701171875\n",
      "[step: 339] loss: 35.46051788330078\n",
      "[step: 340] loss: 35.441707611083984\n",
      "[step: 341] loss: 35.42295837402344\n",
      "[step: 342] loss: 35.404258728027344\n",
      "[step: 343] loss: 35.38560485839844\n",
      "[step: 344] loss: 35.367008209228516\n",
      "[step: 345] loss: 35.34847640991211\n",
      "[step: 346] loss: 35.329986572265625\n",
      "[step: 347] loss: 35.311580657958984\n",
      "[step: 348] loss: 35.29322814941406\n",
      "[step: 349] loss: 35.274940490722656\n",
      "[step: 350] loss: 35.25672149658203\n",
      "[step: 351] loss: 35.23857879638672\n",
      "[step: 352] loss: 35.220497131347656\n",
      "[step: 353] loss: 35.202476501464844\n",
      "[step: 354] loss: 35.18450927734375\n",
      "[step: 355] loss: 35.1666259765625\n",
      "[step: 356] loss: 35.148799896240234\n",
      "[step: 357] loss: 35.13102722167969\n",
      "[step: 358] loss: 35.11332702636719\n",
      "[step: 359] loss: 35.095672607421875\n",
      "[step: 360] loss: 35.07807159423828\n",
      "[step: 361] loss: 35.06053161621094\n",
      "[step: 362] loss: 35.04303741455078\n",
      "[step: 363] loss: 35.025596618652344\n",
      "[step: 364] loss: 35.00817108154297\n",
      "[step: 365] loss: 34.990814208984375\n",
      "[step: 366] loss: 34.97349548339844\n",
      "[step: 367] loss: 34.95620346069336\n",
      "[step: 368] loss: 34.93896484375\n",
      "[step: 369] loss: 34.92183303833008\n",
      "[step: 370] loss: 34.905029296875\n",
      "[step: 371] loss: 34.889427185058594\n",
      "[step: 372] loss: 34.87828063964844\n",
      "[step: 373] loss: 34.88026428222656\n",
      "[step: 374] loss: 34.90351104736328\n",
      "[step: 375] loss: 34.8975830078125\n",
      "[step: 376] loss: 34.83271789550781\n",
      "[step: 377] loss: 34.790985107421875\n",
      "[step: 378] loss: 34.81580352783203\n",
      "[step: 379] loss: 34.79925537109375\n",
      "[step: 380] loss: 34.74333953857422\n",
      "[step: 381] loss: 34.74977493286133\n",
      "[step: 382] loss: 34.747657775878906\n",
      "[step: 383] loss: 34.69881820678711\n",
      "[step: 384] loss: 34.69458770751953\n",
      "[step: 385] loss: 34.69392395019531\n",
      "[step: 386] loss: 34.65310287475586\n",
      "[step: 387] loss: 34.64332580566406\n",
      "[step: 388] loss: 34.641170501708984\n",
      "[step: 389] loss: 34.60695266723633\n",
      "[step: 390] loss: 34.593238830566406\n",
      "[step: 391] loss: 34.5892333984375\n",
      "[step: 392] loss: 34.560508728027344\n",
      "[step: 393] loss: 34.543479919433594\n",
      "[step: 394] loss: 34.537567138671875\n",
      "[step: 395] loss: 34.51369094848633\n",
      "[step: 396] loss: 34.49385070800781\n",
      "[step: 397] loss: 34.485595703125\n",
      "[step: 398] loss: 34.46620178222656\n",
      "[step: 399] loss: 34.44444274902344\n",
      "[step: 400] loss: 34.43280029296875\n",
      "[step: 401] loss: 34.41731262207031\n",
      "[step: 402] loss: 34.3954963684082\n",
      "[step: 403] loss: 34.37933349609375\n",
      "[step: 404] loss: 34.365814208984375\n",
      "[step: 405] loss: 34.34657669067383\n",
      "[step: 406] loss: 34.326541900634766\n",
      "[step: 407] loss: 34.31109619140625\n",
      "[step: 408] loss: 34.2951545715332\n",
      "[step: 409] loss: 34.27523422241211\n",
      "[step: 410] loss: 34.25562286376953\n",
      "[step: 411] loss: 34.23881530761719\n",
      "[step: 412] loss: 34.22174072265625\n",
      "[step: 413] loss: 34.20220184326172\n",
      "[step: 414] loss: 34.18180465698242\n",
      "[step: 415] loss: 34.162784576416016\n",
      "[step: 416] loss: 34.14470672607422\n",
      "[step: 417] loss: 34.12590026855469\n",
      "[step: 418] loss: 34.10574722290039\n",
      "[step: 419] loss: 34.084774017333984\n",
      "[step: 420] loss: 34.06388473510742\n",
      "[step: 421] loss: 34.043373107910156\n",
      "[step: 422] loss: 34.02312469482422\n",
      "[step: 423] loss: 34.00292205810547\n",
      "[step: 424] loss: 33.982696533203125\n",
      "[step: 425] loss: 33.96278762817383\n",
      "[step: 426] loss: 33.94384765625\n",
      "[step: 427] loss: 33.92777633666992\n",
      "[step: 428] loss: 33.91767501831055\n",
      "[step: 429] loss: 33.920291900634766\n",
      "[step: 430] loss: 33.934574127197266\n",
      "[step: 431] loss: 33.94500732421875\n",
      "[step: 432] loss: 33.89482498168945\n",
      "[step: 433] loss: 33.809181213378906\n",
      "[step: 434] loss: 33.76262664794922\n",
      "[step: 435] loss: 33.778236389160156\n",
      "[step: 436] loss: 33.79081726074219\n",
      "[step: 437] loss: 33.7431640625\n",
      "[step: 438] loss: 33.678863525390625\n",
      "[step: 439] loss: 33.65801239013672\n",
      "[step: 440] loss: 33.66779327392578\n",
      "[step: 441] loss: 33.65528106689453\n",
      "[step: 442] loss: 33.60477828979492\n",
      "[step: 443] loss: 33.55947494506836\n",
      "[step: 444] loss: 33.54628372192383\n",
      "[step: 445] loss: 33.545108795166016\n",
      "[step: 446] loss: 33.52649688720703\n",
      "[step: 447] loss: 33.48529815673828\n",
      "[step: 448] loss: 33.44404220581055\n",
      "[step: 449] loss: 33.41980743408203\n",
      "[step: 450] loss: 33.40911865234375\n",
      "[step: 451] loss: 33.399417877197266\n",
      "[step: 452] loss: 33.380210876464844\n",
      "[step: 453] loss: 33.349754333496094\n",
      "[step: 454] loss: 33.312931060791016\n",
      "[step: 455] loss: 33.277740478515625\n",
      "[step: 456] loss: 33.24871063232422\n",
      "[step: 457] loss: 33.22571563720703\n",
      "[step: 458] loss: 33.20758819580078\n",
      "[step: 459] loss: 33.1944465637207\n",
      "[step: 460] loss: 33.18953323364258\n",
      "[step: 461] loss: 33.19852828979492\n",
      "[step: 462] loss: 33.22222137451172\n",
      "[step: 463] loss: 33.239959716796875\n",
      "[step: 464] loss: 33.199371337890625\n",
      "[step: 465] loss: 33.09274673461914\n",
      "[step: 466] loss: 33.00852966308594\n",
      "[step: 467] loss: 33.009559631347656\n",
      "[step: 468] loss: 33.04435729980469\n",
      "[step: 469] loss: 33.02787780761719\n",
      "[step: 470] loss: 32.95016860961914\n",
      "[step: 471] loss: 32.88871765136719\n",
      "[step: 472] loss: 32.88871765136719\n",
      "[step: 473] loss: 32.90219497680664\n",
      "[step: 474] loss: 32.87142562866211\n",
      "[step: 475] loss: 32.80873107910156\n",
      "[step: 476] loss: 32.76907730102539\n",
      "[step: 477] loss: 32.76599884033203\n",
      "[step: 478] loss: 32.76406478881836\n",
      "[step: 479] loss: 32.73243713378906\n",
      "[step: 480] loss: 32.682926177978516\n",
      "[step: 481] loss: 32.64643096923828\n",
      "[step: 482] loss: 32.63239288330078\n",
      "[step: 483] loss: 32.62416076660156\n",
      "[step: 484] loss: 32.602867126464844\n",
      "[step: 485] loss: 32.56664276123047\n",
      "[step: 486] loss: 32.52690887451172\n",
      "[step: 487] loss: 32.49585723876953\n",
      "[step: 488] loss: 32.475379943847656\n",
      "[step: 489] loss: 32.459877014160156\n",
      "[step: 490] loss: 32.44286346435547\n",
      "[step: 491] loss: 32.42058563232422\n",
      "[step: 492] loss: 32.39298629760742\n",
      "[step: 493] loss: 32.361122131347656\n",
      "[step: 494] loss: 32.32802200317383\n",
      "[step: 495] loss: 32.295570373535156\n",
      "[step: 496] loss: 32.26478576660156\n",
      "[step: 497] loss: 32.235595703125\n",
      "[step: 498] loss: 32.20747375488281\n",
      "[step: 499] loss: 32.179908752441406\n",
      "[step: 500] loss: 32.15264129638672\n",
      "[step: 501] loss: 32.125640869140625\n",
      "[step: 502] loss: 32.099449157714844\n",
      "[step: 503] loss: 32.075775146484375\n",
      "[step: 504] loss: 32.05999755859375\n",
      "[step: 505] loss: 32.06864929199219\n",
      "[step: 506] loss: 32.14491271972656\n",
      "[step: 507] loss: 32.36761474609375\n",
      "[step: 508] loss: 32.60125732421875\n",
      "[step: 509] loss: 32.413333892822266\n",
      "[step: 510] loss: 31.927425384521484\n",
      "[step: 511] loss: 32.0385627746582\n",
      "[step: 512] loss: 32.25177764892578\n",
      "[step: 513] loss: 31.91598129272461\n",
      "[step: 514] loss: 31.85946273803711\n",
      "[step: 515] loss: 32.050537109375\n",
      "[step: 516] loss: 31.810827255249023\n",
      "[step: 517] loss: 31.769350051879883\n",
      "[step: 518] loss: 31.897811889648438\n",
      "[step: 519] loss: 31.695594787597656\n",
      "[step: 520] loss: 31.699934005737305\n",
      "[step: 521] loss: 31.759708404541016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 522] loss: 31.59561538696289\n",
      "[step: 523] loss: 31.627836227416992\n",
      "[step: 524] loss: 31.629241943359375\n",
      "[step: 525] loss: 31.514114379882812\n",
      "[step: 526] loss: 31.540523529052734\n",
      "[step: 527] loss: 31.513872146606445\n",
      "[step: 528] loss: 31.43914031982422\n",
      "[step: 529] loss: 31.441268920898438\n",
      "[step: 530] loss: 31.41635513305664\n",
      "[step: 531] loss: 31.35863494873047\n",
      "[step: 532] loss: 31.339059829711914\n",
      "[step: 533] loss: 31.327791213989258\n",
      "[step: 534] loss: 31.271404266357422\n",
      "[step: 535] loss: 31.240785598754883\n",
      "[step: 536] loss: 31.237895965576172\n",
      "[step: 537] loss: 31.182497024536133\n",
      "[step: 538] loss: 31.147125244140625\n",
      "[step: 539] loss: 31.141637802124023\n",
      "[step: 540] loss: 31.09527015686035\n",
      "[step: 541] loss: 31.056997299194336\n",
      "[step: 542] loss: 31.039310455322266\n",
      "[step: 543] loss: 31.00749397277832\n",
      "[step: 544] loss: 30.97039794921875\n",
      "[step: 545] loss: 30.93646240234375\n",
      "[step: 546] loss: 30.912635803222656\n",
      "[step: 547] loss: 30.884057998657227\n",
      "[step: 548] loss: 30.841712951660156\n",
      "[step: 549] loss: 30.811752319335938\n",
      "[step: 550] loss: 30.788148880004883\n",
      "[step: 551] loss: 30.753368377685547\n",
      "[step: 552] loss: 30.71766471862793\n",
      "[step: 553] loss: 30.684921264648438\n",
      "[step: 554] loss: 30.654897689819336\n",
      "[step: 555] loss: 30.626615524291992\n",
      "[step: 556] loss: 30.591907501220703\n",
      "[step: 557] loss: 30.555208206176758\n",
      "[step: 558] loss: 30.523670196533203\n",
      "[step: 559] loss: 30.492799758911133\n",
      "[step: 560] loss: 30.461252212524414\n",
      "[step: 561] loss: 30.4290771484375\n",
      "[step: 562] loss: 30.393856048583984\n",
      "[step: 563] loss: 30.3583984375\n",
      "[step: 564] loss: 30.325210571289062\n",
      "[step: 565] loss: 30.292037963867188\n",
      "[step: 566] loss: 30.259132385253906\n",
      "[step: 567] loss: 30.226951599121094\n",
      "[step: 568] loss: 30.19362449645996\n",
      "[step: 569] loss: 30.15953826904297\n",
      "[step: 570] loss: 30.125802993774414\n",
      "[step: 571] loss: 30.091838836669922\n",
      "[step: 572] loss: 30.057790756225586\n",
      "[step: 573] loss: 30.025259017944336\n",
      "[step: 574] loss: 29.99493980407715\n",
      "[step: 575] loss: 29.96927261352539\n",
      "[step: 576] loss: 29.95380401611328\n",
      "[step: 577] loss: 29.961408615112305\n",
      "[step: 578] loss: 30.007701873779297\n",
      "[step: 579] loss: 30.118776321411133\n",
      "[step: 580] loss: 30.23408317565918\n",
      "[step: 581] loss: 30.228742599487305\n",
      "[step: 582] loss: 29.943557739257812\n",
      "[step: 583] loss: 29.679080963134766\n",
      "[step: 584] loss: 29.69877815246582\n",
      "[step: 585] loss: 29.84756088256836\n",
      "[step: 586] loss: 29.816482543945312\n",
      "[step: 587] loss: 29.591012954711914\n",
      "[step: 588] loss: 29.50101089477539\n",
      "[step: 589] loss: 29.592384338378906\n",
      "[step: 590] loss: 29.596820831298828\n",
      "[step: 591] loss: 29.44761085510254\n",
      "[step: 592] loss: 29.350215911865234\n",
      "[step: 593] loss: 29.389209747314453\n",
      "[step: 594] loss: 29.40342903137207\n",
      "[step: 595] loss: 29.30170440673828\n",
      "[step: 596] loss: 29.203826904296875\n",
      "[step: 597] loss: 29.195344924926758\n",
      "[step: 598] loss: 29.207992553710938\n",
      "[step: 599] loss: 29.160724639892578\n",
      "[step: 600] loss: 29.07143783569336\n",
      "[step: 601] loss: 29.00933265686035\n",
      "[step: 602] loss: 28.993337631225586\n",
      "[step: 603] loss: 28.98694610595703\n",
      "[step: 604] loss: 28.951438903808594\n",
      "[step: 605] loss: 28.885717391967773\n",
      "[step: 606] loss: 28.812833786010742\n",
      "[step: 607] loss: 28.75844955444336\n",
      "[step: 608] loss: 28.725753784179688\n",
      "[step: 609] loss: 28.704994201660156\n",
      "[step: 610] loss: 28.68856430053711\n",
      "[step: 611] loss: 28.670392990112305\n",
      "[step: 612] loss: 28.65572738647461\n",
      "[step: 613] loss: 28.64069366455078\n",
      "[step: 614] loss: 28.633338928222656\n",
      "[step: 615] loss: 28.621074676513672\n",
      "[step: 616] loss: 28.60596466064453\n",
      "[step: 617] loss: 28.558181762695312\n",
      "[step: 618] loss: 28.48468017578125\n",
      "[step: 619] loss: 28.37076187133789\n",
      "[step: 620] loss: 28.25261878967285\n",
      "[step: 621] loss: 28.14807891845703\n",
      "[step: 622] loss: 28.071060180664062\n",
      "[step: 623] loss: 28.0177001953125\n",
      "[step: 624] loss: 27.982379913330078\n",
      "[step: 625] loss: 27.967384338378906\n",
      "[step: 626] loss: 27.992321014404297\n",
      "[step: 627] loss: 28.128280639648438\n",
      "[step: 628] loss: 28.434701919555664\n",
      "[step: 629] loss: 29.023181915283203\n",
      "[step: 630] loss: 28.832990646362305\n",
      "[step: 631] loss: 28.162540435791016\n",
      "[step: 632] loss: 27.699846267700195\n",
      "[step: 633] loss: 27.878040313720703\n",
      "[step: 634] loss: 28.186222076416016\n",
      "[step: 635] loss: 27.934511184692383\n",
      "[step: 636] loss: 27.629070281982422\n",
      "[step: 637] loss: 27.574325561523438\n",
      "[step: 638] loss: 27.670150756835938\n",
      "[step: 639] loss: 27.62945556640625\n",
      "[step: 640] loss: 27.284414291381836\n",
      "[step: 641] loss: 27.261749267578125\n",
      "[step: 642] loss: 27.460193634033203\n",
      "[step: 643] loss: 27.243106842041016\n",
      "[step: 644] loss: 26.97242546081543\n",
      "[step: 645] loss: 27.104190826416016\n",
      "[step: 646] loss: 27.09212875366211\n",
      "[step: 647] loss: 26.918453216552734\n",
      "[step: 648] loss: 26.92347526550293\n",
      "[step: 649] loss: 26.88933753967285\n",
      "[step: 650] loss: 26.663009643554688\n",
      "[step: 651] loss: 26.61313247680664\n",
      "[step: 652] loss: 26.629995346069336\n",
      "[step: 653] loss: 26.50884246826172\n",
      "[step: 654] loss: 26.465232849121094\n",
      "[step: 655] loss: 26.60618019104004\n",
      "[step: 656] loss: 26.94512176513672\n",
      "[step: 657] loss: 27.706417083740234\n",
      "[step: 658] loss: 29.286090850830078\n",
      "[step: 659] loss: 26.824222564697266\n",
      "[step: 660] loss: 26.505571365356445\n",
      "[step: 661] loss: 27.65317153930664\n",
      "[step: 662] loss: 26.946300506591797\n",
      "[step: 663] loss: 26.604534149169922\n",
      "[step: 664] loss: 26.214693069458008\n",
      "[step: 665] loss: 26.812021255493164\n",
      "[step: 666] loss: 26.36508560180664\n",
      "[step: 667] loss: 25.853736877441406\n",
      "[step: 668] loss: 26.409130096435547\n",
      "[step: 669] loss: 26.00619888305664\n",
      "[step: 670] loss: 25.944000244140625\n",
      "[step: 671] loss: 25.765613555908203\n",
      "[step: 672] loss: 25.83348846435547\n",
      "[step: 673] loss: 25.793418884277344\n",
      "[step: 674] loss: 25.444347381591797\n",
      "[step: 675] loss: 25.587574005126953\n",
      "[step: 676] loss: 25.492515563964844\n",
      "[step: 677] loss: 25.48335838317871\n",
      "[step: 678] loss: 25.349401473999023\n",
      "[step: 679] loss: 25.183801651000977\n",
      "[step: 680] loss: 25.240856170654297\n",
      "[step: 681] loss: 25.170507431030273\n",
      "[step: 682] loss: 25.22595977783203\n",
      "[step: 683] loss: 25.24253273010254\n",
      "[step: 684] loss: 25.261810302734375\n",
      "[step: 685] loss: 25.390033721923828\n",
      "[step: 686] loss: 25.558040618896484\n",
      "[step: 687] loss: 25.61913299560547\n",
      "[step: 688] loss: 25.85340690612793\n",
      "[step: 689] loss: 25.32830047607422\n",
      "[step: 690] loss: 24.986469268798828\n",
      "[step: 691] loss: 24.632165908813477\n",
      "[step: 692] loss: 24.579875946044922\n",
      "[step: 693] loss: 24.696998596191406\n",
      "[step: 694] loss: 24.868976593017578\n",
      "[step: 695] loss: 25.171009063720703\n",
      "[step: 696] loss: 24.995403289794922\n",
      "[step: 697] loss: 24.823753356933594\n",
      "[step: 698] loss: 24.488506317138672\n",
      "[step: 699] loss: 24.261459350585938\n",
      "[step: 700] loss: 24.243680953979492\n",
      "[step: 701] loss: 24.313114166259766\n",
      "[step: 702] loss: 24.49273681640625\n",
      "[step: 703] loss: 24.58484649658203\n",
      "[step: 704] loss: 24.67586898803711\n",
      "[step: 705] loss: 24.520679473876953\n",
      "[step: 706] loss: 24.315208435058594\n",
      "[step: 707] loss: 24.06065559387207\n",
      "[step: 708] loss: 23.9073543548584\n",
      "[step: 709] loss: 23.82273292541504\n",
      "[step: 710] loss: 23.835031509399414\n",
      "[step: 711] loss: 23.903993606567383\n",
      "[step: 712] loss: 24.01476287841797\n",
      "[step: 713] loss: 24.255504608154297\n",
      "[step: 714] loss: 24.43674659729004\n",
      "[step: 715] loss: 24.72394561767578\n",
      "[step: 716] loss: 24.39599609375\n",
      "[step: 717] loss: 24.024974822998047\n",
      "[step: 718] loss: 23.594680786132812\n",
      "[step: 719] loss: 23.436729431152344\n",
      "[step: 720] loss: 23.552120208740234\n",
      "[step: 721] loss: 23.765384674072266\n",
      "[step: 722] loss: 23.94298553466797\n",
      "[step: 723] loss: 23.786514282226562\n",
      "[step: 724] loss: 23.52707862854004\n",
      "[step: 725] loss: 23.263870239257812\n",
      "[step: 726] loss: 23.189613342285156\n",
      "[step: 727] loss: 23.278451919555664\n",
      "[step: 728] loss: 23.402450561523438\n",
      "[step: 729] loss: 23.476638793945312\n",
      "[step: 730] loss: 23.368331909179688\n",
      "[step: 731] loss: 23.20901107788086\n",
      "[step: 732] loss: 23.027069091796875\n",
      "[step: 733] loss: 22.923063278198242\n",
      "[step: 734] loss: 22.9007625579834\n",
      "[step: 735] loss: 22.935026168823242\n",
      "[step: 736] loss: 23.012203216552734\n",
      "[step: 737] loss: 23.09353256225586\n",
      "[step: 738] loss: 23.214767456054688\n",
      "[step: 739] loss: 23.25107192993164\n",
      "[step: 740] loss: 23.300498962402344\n",
      "[step: 741] loss: 23.15709686279297\n",
      "[step: 742] loss: 23.003250122070312\n",
      "[step: 743] loss: 22.775650024414062\n",
      "[step: 744] loss: 22.608049392700195\n",
      "[step: 745] loss: 22.506364822387695\n",
      "[step: 746] loss: 22.474166870117188\n",
      "[step: 747] loss: 22.495346069335938\n",
      "[step: 748] loss: 22.549646377563477\n",
      "[step: 749] loss: 22.642208099365234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 750] loss: 22.725990295410156\n",
      "[step: 751] loss: 22.84661102294922\n",
      "[step: 752] loss: 22.84684181213379\n",
      "[step: 753] loss: 22.835065841674805\n",
      "[step: 754] loss: 22.639360427856445\n",
      "[step: 755] loss: 22.448535919189453\n",
      "[step: 756] loss: 22.250986099243164\n",
      "[step: 757] loss: 22.12891387939453\n",
      "[step: 758] loss: 22.081453323364258\n",
      "[step: 759] loss: 22.093719482421875\n",
      "[step: 760] loss: 22.148048400878906\n",
      "[step: 761] loss: 22.219079971313477\n",
      "[step: 762] loss: 22.321300506591797\n",
      "[step: 763] loss: 22.37651252746582\n",
      "[step: 764] loss: 22.44508934020996\n",
      "[step: 765] loss: 22.373958587646484\n",
      "[step: 766] loss: 22.28668212890625\n",
      "[step: 767] loss: 22.097034454345703\n",
      "[step: 768] loss: 21.93514633178711\n",
      "[step: 769] loss: 21.800058364868164\n",
      "[step: 770] loss: 21.721284866333008\n",
      "[step: 771] loss: 21.68940544128418\n",
      "[step: 772] loss: 21.6928653717041\n",
      "[step: 773] loss: 21.726577758789062\n",
      "[step: 774] loss: 21.785110473632812\n",
      "[step: 775] loss: 21.89767074584961\n",
      "[step: 776] loss: 22.027664184570312\n",
      "[step: 777] loss: 22.257780075073242\n",
      "[step: 778] loss: 22.35409164428711\n",
      "[step: 779] loss: 22.454055786132812\n",
      "[step: 780] loss: 22.147720336914062\n",
      "[step: 781] loss: 21.81100082397461\n",
      "[step: 782] loss: 21.478139877319336\n",
      "[step: 783] loss: 21.348196029663086\n",
      "[step: 784] loss: 21.408859252929688\n",
      "[step: 785] loss: 21.56577491760254\n",
      "[step: 786] loss: 21.73052978515625\n",
      "[step: 787] loss: 21.71634292602539\n",
      "[step: 788] loss: 21.599720001220703\n",
      "[step: 789] loss: 21.36760711669922\n",
      "[step: 790] loss: 21.197668075561523\n",
      "[step: 791] loss: 21.135059356689453\n",
      "[step: 792] loss: 21.170270919799805\n",
      "[step: 793] loss: 21.26233673095703\n",
      "[step: 794] loss: 21.34814453125\n",
      "[step: 795] loss: 21.428722381591797\n",
      "[step: 796] loss: 21.40736961364746\n",
      "[step: 797] loss: 21.36545181274414\n",
      "[step: 798] loss: 21.233646392822266\n",
      "[step: 799] loss: 21.113183975219727\n",
      "[step: 800] loss: 20.991317749023438\n",
      "[step: 801] loss: 20.905181884765625\n",
      "[step: 802] loss: 20.845027923583984\n",
      "[step: 803] loss: 20.806438446044922\n",
      "[step: 804] loss: 20.78271484375\n",
      "[step: 805] loss: 20.77110481262207\n",
      "[step: 806] loss: 20.776641845703125\n",
      "[step: 807] loss: 20.811674118041992\n",
      "[step: 808] loss: 20.92298126220703\n",
      "[step: 809] loss: 21.155988693237305\n",
      "[step: 810] loss: 21.733776092529297\n",
      "[step: 811] loss: 22.35745620727539\n",
      "[step: 812] loss: 23.228940963745117\n",
      "[step: 813] loss: 22.09590721130371\n",
      "[step: 814] loss: 20.9522647857666\n",
      "[step: 815] loss: 20.5479736328125\n",
      "[step: 816] loss: 21.10181427001953\n",
      "[step: 817] loss: 21.82870101928711\n",
      "[step: 818] loss: 21.294994354248047\n",
      "[step: 819] loss: 20.575044631958008\n",
      "[step: 820] loss: 20.53104019165039\n",
      "[step: 821] loss: 20.993444442749023\n",
      "[step: 822] loss: 21.129467010498047\n",
      "[step: 823] loss: 20.570674896240234\n",
      "[step: 824] loss: 20.35651397705078\n",
      "[step: 825] loss: 20.672876358032227\n",
      "[step: 826] loss: 20.793468475341797\n",
      "[step: 827] loss: 20.53061294555664\n",
      "[step: 828] loss: 20.24454689025879\n",
      "[step: 829] loss: 20.35009765625\n",
      "[step: 830] loss: 20.56786346435547\n",
      "[step: 831] loss: 20.470645904541016\n",
      "[step: 832] loss: 20.222118377685547\n",
      "[step: 833] loss: 20.11866569519043\n",
      "[step: 834] loss: 20.223360061645508\n",
      "[step: 835] loss: 20.342971801757812\n",
      "[step: 836] loss: 20.268299102783203\n",
      "[step: 837] loss: 20.10280990600586\n",
      "[step: 838] loss: 19.986186981201172\n",
      "[step: 839] loss: 19.991867065429688\n",
      "[step: 840] loss: 20.069229125976562\n",
      "[step: 841] loss: 20.107349395751953\n",
      "[step: 842] loss: 20.09598731994629\n",
      "[step: 843] loss: 20.00143814086914\n",
      "[step: 844] loss: 19.909997940063477\n",
      "[step: 845] loss: 19.834062576293945\n",
      "[step: 846] loss: 19.793533325195312\n",
      "[step: 847] loss: 19.785507202148438\n",
      "[step: 848] loss: 19.796585083007812\n",
      "[step: 849] loss: 19.814382553100586\n",
      "[step: 850] loss: 19.81747817993164\n",
      "[step: 851] loss: 19.815025329589844\n",
      "[step: 852] loss: 19.82405662536621\n",
      "[step: 853] loss: 19.931745529174805\n",
      "[step: 854] loss: 20.155418395996094\n",
      "[step: 855] loss: 20.68140411376953\n",
      "[step: 856] loss: 21.12575340270996\n",
      "[step: 857] loss: 21.59697723388672\n",
      "[step: 858] loss: 20.729516983032227\n",
      "[step: 859] loss: 19.96640396118164\n",
      "[step: 860] loss: 19.57897186279297\n",
      "[step: 861] loss: 19.724140167236328\n",
      "[step: 862] loss: 20.114017486572266\n",
      "[step: 863] loss: 20.231428146362305\n",
      "[step: 864] loss: 20.1134090423584\n",
      "[step: 865] loss: 19.578868865966797\n",
      "[step: 866] loss: 19.37591552734375\n",
      "[step: 867] loss: 19.6361083984375\n",
      "[step: 868] loss: 19.867813110351562\n",
      "[step: 869] loss: 19.799053192138672\n",
      "[step: 870] loss: 19.424030303955078\n",
      "[step: 871] loss: 19.290847778320312\n",
      "[step: 872] loss: 19.3787841796875\n",
      "[step: 873] loss: 19.491893768310547\n",
      "[step: 874] loss: 19.561004638671875\n",
      "[step: 875] loss: 19.425783157348633\n",
      "[step: 876] loss: 19.221973419189453\n",
      "[step: 877] loss: 19.110614776611328\n",
      "[step: 878] loss: 19.169113159179688\n",
      "[step: 879] loss: 19.27420425415039\n",
      "[step: 880] loss: 19.293746948242188\n",
      "[step: 881] loss: 19.271963119506836\n",
      "[step: 882] loss: 19.185203552246094\n",
      "[step: 883] loss: 19.090076446533203\n",
      "[step: 884] loss: 18.984804153442383\n",
      "[step: 885] loss: 18.929759979248047\n",
      "[step: 886] loss: 18.926193237304688\n",
      "[step: 887] loss: 18.936290740966797\n",
      "[step: 888] loss: 18.95759391784668\n",
      "[step: 889] loss: 19.003812789916992\n",
      "[step: 890] loss: 19.11732292175293\n",
      "[step: 891] loss: 19.244779586791992\n",
      "[step: 892] loss: 19.49869155883789\n",
      "[step: 893] loss: 19.635290145874023\n",
      "[step: 894] loss: 19.93016815185547\n",
      "[step: 895] loss: 19.656633377075195\n",
      "[step: 896] loss: 19.427291870117188\n",
      "[step: 897] loss: 18.966426849365234\n",
      "[step: 898] loss: 18.707839965820312\n",
      "[step: 899] loss: 18.644044876098633\n",
      "[step: 900] loss: 18.734315872192383\n",
      "[step: 901] loss: 18.9281005859375\n",
      "[step: 902] loss: 19.070266723632812\n",
      "[step: 903] loss: 19.199954986572266\n",
      "[step: 904] loss: 19.038555145263672\n",
      "[step: 905] loss: 18.853023529052734\n",
      "[step: 906] loss: 18.616714477539062\n",
      "[step: 907] loss: 18.48322296142578\n",
      "[step: 908] loss: 18.45140838623047\n",
      "[step: 909] loss: 18.493881225585938\n",
      "[step: 910] loss: 18.585426330566406\n",
      "[step: 911] loss: 18.675739288330078\n",
      "[step: 912] loss: 18.80373764038086\n",
      "[step: 913] loss: 18.826553344726562\n",
      "[step: 914] loss: 18.89040184020996\n",
      "[step: 915] loss: 18.76656723022461\n",
      "[step: 916] loss: 18.677810668945312\n",
      "[step: 917] loss: 18.499614715576172\n",
      "[step: 918] loss: 18.378795623779297\n",
      "[step: 919] loss: 18.276687622070312\n",
      "[step: 920] loss: 18.214271545410156\n",
      "[step: 921] loss: 18.1746768951416\n",
      "[step: 922] loss: 18.14925193786621\n",
      "[step: 923] loss: 18.13385772705078\n",
      "[step: 924] loss: 18.129438400268555\n",
      "[step: 925] loss: 18.146747589111328\n",
      "[step: 926] loss: 18.200641632080078\n",
      "[step: 927] loss: 18.363006591796875\n",
      "[step: 928] loss: 18.653629302978516\n",
      "[step: 929] loss: 19.38886833190918\n",
      "[step: 930] loss: 19.839996337890625\n",
      "[step: 931] loss: 20.584484100341797\n",
      "[step: 932] loss: 19.199813842773438\n",
      "[step: 933] loss: 18.209407806396484\n",
      "[step: 934] loss: 18.047588348388672\n",
      "[step: 935] loss: 18.583505630493164\n",
      "[step: 936] loss: 19.503868103027344\n",
      "[step: 937] loss: 18.949024200439453\n",
      "[step: 938] loss: 18.19424819946289\n",
      "[step: 939] loss: 17.872573852539062\n",
      "[step: 940] loss: 18.19087791442871\n",
      "[step: 941] loss: 18.77118682861328\n",
      "[step: 942] loss: 18.50706672668457\n",
      "[step: 943] loss: 18.00852394104004\n",
      "[step: 944] loss: 17.790842056274414\n",
      "[step: 945] loss: 17.94316864013672\n",
      "[step: 946] loss: 18.294769287109375\n",
      "[step: 947] loss: 18.177112579345703\n",
      "[step: 948] loss: 17.86946678161621\n",
      "[step: 949] loss: 17.67332649230957\n",
      "[step: 950] loss: 17.691537857055664\n",
      "[step: 951] loss: 17.876646041870117\n",
      "[step: 952] loss: 17.90085220336914\n",
      "[step: 953] loss: 17.824506759643555\n",
      "[step: 954] loss: 17.653528213500977\n",
      "[step: 955] loss: 17.529104232788086\n",
      "[step: 956] loss: 17.50261116027832\n",
      "[step: 957] loss: 17.53676414489746\n",
      "[step: 958] loss: 17.609466552734375\n",
      "[step: 959] loss: 17.652359008789062\n",
      "[step: 960] loss: 17.695764541625977\n",
      "[step: 961] loss: 17.631092071533203\n",
      "[step: 962] loss: 17.581462860107422\n",
      "[step: 963] loss: 17.492847442626953\n",
      "[step: 964] loss: 17.43984603881836\n",
      "[step: 965] loss: 17.387834548950195\n",
      "[step: 966] loss: 17.35698699951172\n",
      "[step: 967] loss: 17.35715103149414\n",
      "[step: 968] loss: 17.41437530517578\n",
      "[step: 969] loss: 17.587465286254883\n",
      "[step: 970] loss: 17.946823120117188\n",
      "[step: 971] loss: 18.444705963134766\n",
      "[step: 972] loss: 18.6273136138916\n",
      "[step: 973] loss: 18.012441635131836\n",
      "[step: 974] loss: 17.563232421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 975] loss: 18.037212371826172\n",
      "[step: 976] loss: 18.800647735595703\n",
      "[step: 977] loss: 18.57923126220703\n",
      "[step: 978] loss: 18.553138732910156\n",
      "[step: 979] loss: 18.29905891418457\n",
      "[step: 980] loss: 17.758684158325195\n",
      "[step: 981] loss: 17.060434341430664\n",
      "[step: 982] loss: 17.367443084716797\n",
      "[step: 983] loss: 17.64583969116211\n",
      "[step: 984] loss: 17.557422637939453\n",
      "[step: 985] loss: 17.864025115966797\n",
      "[step: 986] loss: 17.573501586914062\n",
      "[step: 987] loss: 17.124774932861328\n",
      "[step: 988] loss: 17.118980407714844\n",
      "[step: 989] loss: 17.025253295898438\n",
      "[step: 990] loss: 17.00511932373047\n",
      "[step: 991] loss: 17.25263214111328\n",
      "[step: 992] loss: 17.31804847717285\n",
      "[step: 993] loss: 17.210058212280273\n",
      "[step: 994] loss: 17.216060638427734\n",
      "[step: 995] loss: 16.965755462646484\n",
      "[step: 996] loss: 16.84280776977539\n",
      "[step: 997] loss: 16.844757080078125\n",
      "[step: 998] loss: 16.76294708251953\n",
      "[step: 999] loss: 16.799579620361328\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "testY is:  [20797.580000000002, 23077.549999999999, 23351.799999999999, 31579.900000000001, 39886.059999999998, 18689.540000000001, 19050.66, 20911.25, 25293.490000000002, 33305.919999999998, 45773.029999999999, 46788.75, 23350.880000000001, 16567.689999999999, 16894.400000000001, 18365.099999999999, 18378.16, 23510.490000000002, 36988.489999999998, 54060.099999999999, 20124.220000000001, 20113.029999999999, 21140.07, 22366.880000000001, 22107.700000000001, 28952.860000000001, 57592.120000000003, 34684.209999999999]\n",
      "\n",
      "\n",
      "LSTM testforecast : [13719.44921875, 18650.74609375, 33551.54296875, 32803.859375, 29937.19140625, 32675.029296875, 15895.150390625, 15438.5537109375, 14348.88671875, 37060.43359375, 39363.57421875, 46263.63671875, 45466.921875, 33385.421875, 15272.0830078125, 9867.77734375, 20425.837890625, 23751.712890625, 23000.83984375, 34120.953125, 47577.09375, 36497.6015625, 21967.46484375, 15136.888671875, 10865.171875, 14227.9453125, 21149.677734375, 32218.970703125] \n",
      "@@@@@LSTM rmse:  13203.9412202\n",
      "Bayseian testforecast : [19276.115054474096, 20709.866123540029, 25207.156602802392, 31946.920190518522, 35082.806411431171, 29848.344056759026, 21619.817282688065, 17096.616126905574, 18016.841940462735, 24837.988177216746, 35949.834841629287, 42583.984202328837, 37077.438452866758, 25734.522544393629, 17337.302708278126, 13843.350784667542, 14614.663730777605, 19707.36930420341, 28560.692623620409, 35625.703235169225, 33680.549333643321, 25759.768622156789, 19691.042402673935, 18190.668570039485, 20800.864983221039, 25740.734483918361, 29550.229268191015, 29905.992621094894] \n",
      "@@@@@Bayseian rmse:  8880.25374031\n",
      "\n",
      "\n",
      "Bayseian WON!!!!!!\n",
      "[step: 0] loss: 430.44207763671875\n",
      "[step: 1] loss: 414.27435302734375\n",
      "[step: 2] loss: 399.256591796875\n",
      "[step: 3] loss: 385.0477294921875\n",
      "[step: 4] loss: 371.36724853515625\n",
      "[step: 5] loss: 357.9510498046875\n",
      "[step: 6] loss: 344.54925537109375\n",
      "[step: 7] loss: 330.92901611328125\n",
      "[step: 8] loss: 316.8726806640625\n",
      "[step: 9] loss: 302.17535400390625\n",
      "[step: 10] loss: 286.64117431640625\n",
      "[step: 11] loss: 270.0882263183594\n",
      "[step: 12] loss: 252.36776733398438\n",
      "[step: 13] loss: 233.4097900390625\n",
      "[step: 14] loss: 213.30967712402344\n",
      "[step: 15] loss: 192.4694366455078\n",
      "[step: 16] loss: 171.7985382080078\n",
      "[step: 17] loss: 152.9242706298828\n",
      "[step: 18] loss: 138.1070556640625\n",
      "[step: 19] loss: 129.22979736328125\n",
      "[step: 20] loss: 125.58290100097656\n",
      "[step: 21] loss: 122.93179321289062\n",
      "[step: 22] loss: 116.96394348144531\n",
      "[step: 23] loss: 107.14886474609375\n",
      "[step: 24] loss: 96.07144927978516\n",
      "[step: 25] loss: 86.76382446289062\n",
      "[step: 26] loss: 80.9580078125\n",
      "[step: 27] loss: 78.70890808105469\n",
      "[step: 28] loss: 78.92900848388672\n",
      "[step: 29] loss: 80.20567321777344\n",
      "[step: 30] loss: 81.41297912597656\n",
      "[step: 31] loss: 81.95388793945312\n",
      "[step: 32] loss: 81.72447204589844\n",
      "[step: 33] loss: 80.94935607910156\n",
      "[step: 34] loss: 79.98068237304688\n",
      "[step: 35] loss: 79.10884857177734\n",
      "[step: 36] loss: 78.43557739257812\n",
      "[step: 37] loss: 77.86314392089844\n",
      "[step: 38] loss: 77.20266723632812\n",
      "[step: 39] loss: 76.32425689697266\n",
      "[step: 40] loss: 75.2415771484375\n",
      "[step: 41] loss: 74.089111328125\n",
      "[step: 42] loss: 73.03480529785156\n",
      "[step: 43] loss: 72.19781494140625\n",
      "[step: 44] loss: 71.61276245117188\n",
      "[step: 45] loss: 71.24064636230469\n",
      "[step: 46] loss: 71.00422668457031\n",
      "[step: 47] loss: 70.82431030273438\n",
      "[step: 48] loss: 70.6441650390625\n",
      "[step: 49] loss: 70.43833923339844\n",
      "[step: 50] loss: 70.20919036865234\n",
      "[step: 51] loss: 69.97563934326172\n",
      "[step: 52] loss: 69.75926971435547\n",
      "[step: 53] loss: 69.5729751586914\n",
      "[step: 54] loss: 69.41508483886719\n",
      "[step: 55] loss: 69.27066802978516\n",
      "[step: 56] loss: 69.1188735961914\n",
      "[step: 57] loss: 68.94268035888672\n",
      "[step: 58] loss: 68.73605346679688\n",
      "[step: 59] loss: 68.50550842285156\n",
      "[step: 60] loss: 68.26622009277344\n",
      "[step: 61] loss: 68.03507995605469\n",
      "[step: 62] loss: 67.82414245605469\n",
      "[step: 63] loss: 67.6377944946289\n",
      "[step: 64] loss: 67.4731674194336\n",
      "[step: 65] loss: 67.32335662841797\n",
      "[step: 66] loss: 67.18130493164062\n",
      "[step: 67] loss: 67.04261016845703\n",
      "[step: 68] loss: 66.90653991699219\n",
      "[step: 69] loss: 66.77518463134766\n",
      "[step: 70] loss: 66.65129089355469\n",
      "[step: 71] loss: 66.5364990234375\n",
      "[step: 72] loss: 66.42994689941406\n",
      "[step: 73] loss: 66.32840728759766\n",
      "[step: 74] loss: 66.22749328613281\n",
      "[step: 75] loss: 66.12355041503906\n",
      "[step: 76] loss: 66.01487731933594\n",
      "[step: 77] loss: 65.90227508544922\n",
      "[step: 78] loss: 65.7882080078125\n",
      "[step: 79] loss: 65.67579650878906\n",
      "[step: 80] loss: 65.56732177734375\n",
      "[step: 81] loss: 65.46366882324219\n",
      "[step: 82] loss: 65.36428833007812\n",
      "[step: 83] loss: 65.26793670654297\n",
      "[step: 84] loss: 65.17320251464844\n",
      "[step: 85] loss: 65.07929992675781\n",
      "[step: 86] loss: 64.98629760742188\n",
      "[step: 87] loss: 64.89456176757812\n",
      "[step: 88] loss: 64.8047866821289\n",
      "[step: 89] loss: 64.71715545654297\n",
      "[step: 90] loss: 64.63136291503906\n",
      "[step: 91] loss: 64.54673767089844\n",
      "[step: 92] loss: 64.46244812011719\n",
      "[step: 93] loss: 64.3779296875\n",
      "[step: 94] loss: 64.29310607910156\n",
      "[step: 95] loss: 64.20820617675781\n",
      "[step: 96] loss: 64.12371063232422\n",
      "[step: 97] loss: 64.04006958007812\n",
      "[step: 98] loss: 63.95738983154297\n",
      "[step: 99] loss: 63.875457763671875\n",
      "[step: 100] loss: 63.793922424316406\n",
      "[step: 101] loss: 63.712432861328125\n",
      "[step: 102] loss: 63.63077926635742\n",
      "[step: 103] loss: 63.54895782470703\n",
      "[step: 104] loss: 63.46704864501953\n",
      "[step: 105] loss: 63.38520812988281\n",
      "[step: 106] loss: 63.30337905883789\n",
      "[step: 107] loss: 63.22140884399414\n",
      "[step: 108] loss: 63.13905715942383\n",
      "[step: 109] loss: 63.05607223510742\n",
      "[step: 110] loss: 62.97233581542969\n",
      "[step: 111] loss: 62.88780212402344\n",
      "[step: 112] loss: 62.802433013916016\n",
      "[step: 113] loss: 62.71617126464844\n",
      "[step: 114] loss: 62.62884521484375\n",
      "[step: 115] loss: 62.54029846191406\n",
      "[step: 116] loss: 62.450233459472656\n",
      "[step: 117] loss: 62.358524322509766\n",
      "[step: 118] loss: 62.265052795410156\n",
      "[step: 119] loss: 62.1697998046875\n",
      "[step: 120] loss: 62.07264709472656\n",
      "[step: 121] loss: 61.97349548339844\n",
      "[step: 122] loss: 61.87206268310547\n",
      "[step: 123] loss: 61.76812744140625\n",
      "[step: 124] loss: 61.66149139404297\n",
      "[step: 125] loss: 61.55206298828125\n",
      "[step: 126] loss: 61.43967056274414\n",
      "[step: 127] loss: 61.32414627075195\n",
      "[step: 128] loss: 61.20525360107422\n",
      "[step: 129] loss: 61.082740783691406\n",
      "[step: 130] loss: 60.95651626586914\n",
      "[step: 131] loss: 60.82656478881836\n",
      "[step: 132] loss: 60.69288635253906\n",
      "[step: 133] loss: 60.5554313659668\n",
      "[step: 134] loss: 60.414249420166016\n",
      "[step: 135] loss: 60.269447326660156\n",
      "[step: 136] loss: 60.12131118774414\n",
      "[step: 137] loss: 59.97010803222656\n",
      "[step: 138] loss: 59.81619644165039\n",
      "[step: 139] loss: 59.66016387939453\n",
      "[step: 140] loss: 59.502845764160156\n",
      "[step: 141] loss: 59.34526062011719\n",
      "[step: 142] loss: 59.188655853271484\n",
      "[step: 143] loss: 59.03435516357422\n",
      "[step: 144] loss: 58.884151458740234\n",
      "[step: 145] loss: 58.73976516723633\n",
      "[step: 146] loss: 58.60313415527344\n",
      "[step: 147] loss: 58.4763298034668\n",
      "[step: 148] loss: 58.361202239990234\n",
      "[step: 149] loss: 58.25918197631836\n",
      "[step: 150] loss: 58.17097473144531\n",
      "[step: 151] loss: 58.096317291259766\n",
      "[step: 152] loss: 58.03367233276367\n",
      "[step: 153] loss: 57.9803581237793\n",
      "[step: 154] loss: 57.93248748779297\n",
      "[step: 155] loss: 57.885860443115234\n",
      "[step: 156] loss: 57.836669921875\n",
      "[step: 157] loss: 57.78225326538086\n",
      "[step: 158] loss: 57.72138214111328\n",
      "[step: 159] loss: 57.654396057128906\n",
      "[step: 160] loss: 57.58281326293945\n",
      "[step: 161] loss: 57.50878143310547\n",
      "[step: 162] loss: 57.43458938598633\n",
      "[step: 163] loss: 57.36222457885742\n",
      "[step: 164] loss: 57.29325866699219\n",
      "[step: 165] loss: 57.228553771972656\n",
      "[step: 166] loss: 57.16849136352539\n",
      "[step: 167] loss: 57.112823486328125\n",
      "[step: 168] loss: 57.061119079589844\n",
      "[step: 169] loss: 57.01270294189453\n",
      "[step: 170] loss: 56.96681213378906\n",
      "[step: 171] loss: 56.922725677490234\n",
      "[step: 172] loss: 56.87983703613281\n",
      "[step: 173] loss: 56.83769226074219\n",
      "[step: 174] loss: 56.795989990234375\n",
      "[step: 175] loss: 56.75450134277344\n",
      "[step: 176] loss: 56.713294982910156\n",
      "[step: 177] loss: 56.67243576049805\n",
      "[step: 178] loss: 56.63212203979492\n",
      "[step: 179] loss: 56.59254455566406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 180] loss: 56.553977966308594\n",
      "[step: 181] loss: 56.516578674316406\n",
      "[step: 182] loss: 56.48052215576172\n",
      "[step: 183] loss: 56.44593048095703\n",
      "[step: 184] loss: 56.41276168823242\n",
      "[step: 185] loss: 56.38093566894531\n",
      "[step: 186] loss: 56.35038757324219\n",
      "[step: 187] loss: 56.32088088989258\n",
      "[step: 188] loss: 56.292259216308594\n",
      "[step: 189] loss: 56.26428985595703\n",
      "[step: 190] loss: 56.23686599731445\n",
      "[step: 191] loss: 56.209896087646484\n",
      "[step: 192] loss: 56.18334197998047\n",
      "[step: 193] loss: 56.157203674316406\n",
      "[step: 194] loss: 56.13148498535156\n",
      "[step: 195] loss: 56.10627746582031\n",
      "[step: 196] loss: 56.0816650390625\n",
      "[step: 197] loss: 56.057708740234375\n",
      "[step: 198] loss: 56.034385681152344\n",
      "[step: 199] loss: 56.011756896972656\n",
      "[step: 200] loss: 55.98982238769531\n",
      "[step: 201] loss: 55.96845245361328\n",
      "[step: 202] loss: 55.94769287109375\n",
      "[step: 203] loss: 55.92745590209961\n",
      "[step: 204] loss: 55.90769958496094\n",
      "[step: 205] loss: 55.888328552246094\n",
      "[step: 206] loss: 55.86933517456055\n",
      "[step: 207] loss: 55.85068893432617\n",
      "[step: 208] loss: 55.832427978515625\n",
      "[step: 209] loss: 55.81446075439453\n",
      "[step: 210] loss: 55.79684066772461\n",
      "[step: 211] loss: 55.779579162597656\n",
      "[step: 212] loss: 55.76264572143555\n",
      "[step: 213] loss: 55.74604034423828\n",
      "[step: 214] loss: 55.72972869873047\n",
      "[step: 215] loss: 55.7137451171875\n",
      "[step: 216] loss: 55.698020935058594\n",
      "[step: 217] loss: 55.68257141113281\n",
      "[step: 218] loss: 55.66735076904297\n",
      "[step: 219] loss: 55.6523323059082\n",
      "[step: 220] loss: 55.63747787475586\n",
      "[step: 221] loss: 55.6227912902832\n",
      "[step: 222] loss: 55.60829162597656\n",
      "[step: 223] loss: 55.59391784667969\n",
      "[step: 224] loss: 55.57969665527344\n",
      "[step: 225] loss: 55.565608978271484\n",
      "[step: 226] loss: 55.55162048339844\n",
      "[step: 227] loss: 55.53781509399414\n",
      "[step: 228] loss: 55.52404022216797\n",
      "[step: 229] loss: 55.51042938232422\n",
      "[step: 230] loss: 55.49687957763672\n",
      "[step: 231] loss: 55.48339080810547\n",
      "[step: 232] loss: 55.46997833251953\n",
      "[step: 233] loss: 55.45663070678711\n",
      "[step: 234] loss: 55.44331359863281\n",
      "[step: 235] loss: 55.43004608154297\n",
      "[step: 236] loss: 55.41680908203125\n",
      "[step: 237] loss: 55.40358352661133\n",
      "[step: 238] loss: 55.390419006347656\n",
      "[step: 239] loss: 55.37723159790039\n",
      "[step: 240] loss: 55.36408996582031\n",
      "[step: 241] loss: 55.350929260253906\n",
      "[step: 242] loss: 55.337799072265625\n",
      "[step: 243] loss: 55.32469940185547\n",
      "[step: 244] loss: 55.31159210205078\n",
      "[step: 245] loss: 55.298458099365234\n",
      "[step: 246] loss: 55.28535461425781\n",
      "[step: 247] loss: 55.272239685058594\n",
      "[step: 248] loss: 55.259124755859375\n",
      "[step: 249] loss: 55.24601364135742\n",
      "[step: 250] loss: 55.232887268066406\n",
      "[step: 251] loss: 55.219764709472656\n",
      "[step: 252] loss: 55.206626892089844\n",
      "[step: 253] loss: 55.19349670410156\n",
      "[step: 254] loss: 55.18038558959961\n",
      "[step: 255] loss: 55.16728591918945\n",
      "[step: 256] loss: 55.15416717529297\n",
      "[step: 257] loss: 55.14106369018555\n",
      "[step: 258] loss: 55.12798309326172\n",
      "[step: 259] loss: 55.114871978759766\n",
      "[step: 260] loss: 55.10179901123047\n",
      "[step: 261] loss: 55.0887565612793\n",
      "[step: 262] loss: 55.075714111328125\n",
      "[step: 263] loss: 55.06270217895508\n",
      "[step: 264] loss: 55.04973602294922\n",
      "[step: 265] loss: 55.0367431640625\n",
      "[step: 266] loss: 55.0238151550293\n",
      "[step: 267] loss: 55.01093292236328\n",
      "[step: 268] loss: 54.99807357788086\n",
      "[step: 269] loss: 54.985260009765625\n",
      "[step: 270] loss: 54.97251892089844\n",
      "[step: 271] loss: 54.959754943847656\n",
      "[step: 272] loss: 54.94709777832031\n",
      "[step: 273] loss: 54.93447494506836\n",
      "[step: 274] loss: 54.92189025878906\n",
      "[step: 275] loss: 54.90939712524414\n",
      "[step: 276] loss: 54.896949768066406\n",
      "[step: 277] loss: 54.884552001953125\n",
      "[step: 278] loss: 54.87222671508789\n",
      "[step: 279] loss: 54.859947204589844\n",
      "[step: 280] loss: 54.84775924682617\n",
      "[step: 281] loss: 54.835609436035156\n",
      "[step: 282] loss: 54.82355499267578\n",
      "[step: 283] loss: 54.81156539916992\n",
      "[step: 284] loss: 54.799625396728516\n",
      "[step: 285] loss: 54.787750244140625\n",
      "[step: 286] loss: 54.775962829589844\n",
      "[step: 287] loss: 54.76423645019531\n",
      "[step: 288] loss: 54.7525749206543\n",
      "[step: 289] loss: 54.74098587036133\n",
      "[step: 290] loss: 54.72945022583008\n",
      "[step: 291] loss: 54.718013763427734\n",
      "[step: 292] loss: 54.706626892089844\n",
      "[step: 293] loss: 54.695281982421875\n",
      "[step: 294] loss: 54.684017181396484\n",
      "[step: 295] loss: 54.6728401184082\n",
      "[step: 296] loss: 54.66169738769531\n",
      "[step: 297] loss: 54.65060806274414\n",
      "[step: 298] loss: 54.63961410522461\n",
      "[step: 299] loss: 54.6286735534668\n",
      "[step: 300] loss: 54.617767333984375\n",
      "[step: 301] loss: 54.606929779052734\n",
      "[step: 302] loss: 54.59616470336914\n",
      "[step: 303] loss: 54.585411071777344\n",
      "[step: 304] loss: 54.57474136352539\n",
      "[step: 305] loss: 54.56413269042969\n",
      "[step: 306] loss: 54.55353927612305\n",
      "[step: 307] loss: 54.54302215576172\n",
      "[step: 308] loss: 54.53254699707031\n",
      "[step: 309] loss: 54.522125244140625\n",
      "[step: 310] loss: 54.511749267578125\n",
      "[step: 311] loss: 54.50141525268555\n",
      "[step: 312] loss: 54.491119384765625\n",
      "[step: 313] loss: 54.48085403442383\n",
      "[step: 314] loss: 54.47065734863281\n",
      "[step: 315] loss: 54.460506439208984\n",
      "[step: 316] loss: 54.45037078857422\n",
      "[step: 317] loss: 54.440277099609375\n",
      "[step: 318] loss: 54.43023681640625\n",
      "[step: 319] loss: 54.42021942138672\n",
      "[step: 320] loss: 54.410240173339844\n",
      "[step: 321] loss: 54.40029525756836\n",
      "[step: 322] loss: 54.39038848876953\n",
      "[step: 323] loss: 54.38051223754883\n",
      "[step: 324] loss: 54.370643615722656\n",
      "[step: 325] loss: 54.36082077026367\n",
      "[step: 326] loss: 54.35102844238281\n",
      "[step: 327] loss: 54.341243743896484\n",
      "[step: 328] loss: 54.33147430419922\n",
      "[step: 329] loss: 54.32172775268555\n",
      "[step: 330] loss: 54.31201934814453\n",
      "[step: 331] loss: 54.30233383178711\n",
      "[step: 332] loss: 54.29264450073242\n",
      "[step: 333] loss: 54.2829704284668\n",
      "[step: 334] loss: 54.273292541503906\n",
      "[step: 335] loss: 54.263641357421875\n",
      "[step: 336] loss: 54.253990173339844\n",
      "[step: 337] loss: 54.24435043334961\n",
      "[step: 338] loss: 54.234710693359375\n",
      "[step: 339] loss: 54.22507095336914\n",
      "[step: 340] loss: 54.215423583984375\n",
      "[step: 341] loss: 54.20576095581055\n",
      "[step: 342] loss: 54.19611740112305\n",
      "[step: 343] loss: 54.18644332885742\n",
      "[step: 344] loss: 54.1767692565918\n",
      "[step: 345] loss: 54.167076110839844\n",
      "[step: 346] loss: 54.15736389160156\n",
      "[step: 347] loss: 54.14763259887695\n",
      "[step: 348] loss: 54.13789367675781\n",
      "[step: 349] loss: 54.12812805175781\n",
      "[step: 350] loss: 54.118324279785156\n",
      "[step: 351] loss: 54.10850524902344\n",
      "[step: 352] loss: 54.09864044189453\n",
      "[step: 353] loss: 54.0887565612793\n",
      "[step: 354] loss: 54.07884979248047\n",
      "[step: 355] loss: 54.06887435913086\n",
      "[step: 356] loss: 54.05889892578125\n",
      "[step: 357] loss: 54.04886245727539\n",
      "[step: 358] loss: 54.03877258300781\n",
      "[step: 359] loss: 54.02864456176758\n",
      "[step: 360] loss: 54.018470764160156\n",
      "[step: 361] loss: 54.00825119018555\n",
      "[step: 362] loss: 53.997962951660156\n",
      "[step: 363] loss: 53.987632751464844\n",
      "[step: 364] loss: 53.977264404296875\n",
      "[step: 365] loss: 53.96680450439453\n",
      "[step: 366] loss: 53.95628356933594\n",
      "[step: 367] loss: 53.945716857910156\n",
      "[step: 368] loss: 53.93507385253906\n",
      "[step: 369] loss: 53.92437744140625\n",
      "[step: 370] loss: 53.91358947753906\n",
      "[step: 371] loss: 53.902767181396484\n",
      "[step: 372] loss: 53.89183807373047\n",
      "[step: 373] loss: 53.880855560302734\n",
      "[step: 374] loss: 53.869808197021484\n",
      "[step: 375] loss: 53.85865783691406\n",
      "[step: 376] loss: 53.84741973876953\n",
      "[step: 377] loss: 53.83612823486328\n",
      "[step: 378] loss: 53.82476043701172\n",
      "[step: 379] loss: 53.81330108642578\n",
      "[step: 380] loss: 53.80176544189453\n",
      "[step: 381] loss: 53.79013442993164\n",
      "[step: 382] loss: 53.778419494628906\n",
      "[step: 383] loss: 53.76662063598633\n",
      "[step: 384] loss: 53.75474548339844\n",
      "[step: 385] loss: 53.742767333984375\n",
      "[step: 386] loss: 53.730743408203125\n",
      "[step: 387] loss: 53.71860122680664\n",
      "[step: 388] loss: 53.70635986328125\n",
      "[step: 389] loss: 53.69404220581055\n",
      "[step: 390] loss: 53.68164825439453\n",
      "[step: 391] loss: 53.66917419433594\n",
      "[step: 392] loss: 53.65659713745117\n",
      "[step: 393] loss: 53.64393615722656\n",
      "[step: 394] loss: 53.6312141418457\n",
      "[step: 395] loss: 53.61840057373047\n",
      "[step: 396] loss: 53.6055793762207\n",
      "[step: 397] loss: 53.59284973144531\n",
      "[step: 398] loss: 53.580780029296875\n",
      "[step: 399] loss: 53.571319580078125\n",
      "[step: 400] loss: 53.57061767578125\n",
      "[step: 401] loss: 53.5911865234375\n",
      "[step: 402] loss: 53.61769104003906\n",
      "[step: 403] loss: 53.590965270996094\n",
      "[step: 404] loss: 53.50848388671875\n",
      "[step: 405] loss: 53.51075744628906\n",
      "[step: 406] loss: 53.53984069824219\n",
      "[step: 407] loss: 53.482913970947266\n",
      "[step: 408] loss: 53.45640563964844\n",
      "[step: 409] loss: 53.480194091796875\n",
      "[step: 410] loss: 53.443504333496094\n",
      "[step: 411] loss: 53.41440200805664\n",
      "[step: 412] loss: 53.427860260009766\n",
      "[step: 413] loss: 53.401004791259766\n",
      "[step: 414] loss: 53.37467956542969\n",
      "[step: 415] loss: 53.38030242919922\n",
      "[step: 416] loss: 53.358882904052734\n",
      "[step: 417] loss: 53.33525085449219\n",
      "[step: 418] loss: 53.33527755737305\n",
      "[step: 419] loss: 53.31748962402344\n",
      "[step: 420] loss: 53.295753479003906\n",
      "[step: 421] loss: 53.291526794433594\n",
      "[step: 422] loss: 53.27662658691406\n",
      "[step: 423] loss: 53.256187438964844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 424] loss: 53.24851989746094\n",
      "[step: 425] loss: 53.235931396484375\n",
      "[step: 426] loss: 53.216644287109375\n",
      "[step: 427] loss: 53.20585632324219\n",
      "[step: 428] loss: 53.19492721557617\n",
      "[step: 429] loss: 53.17727279663086\n",
      "[step: 430] loss: 53.16357421875\n",
      "[step: 431] loss: 53.1531867980957\n",
      "[step: 432] loss: 53.13777160644531\n",
      "[step: 433] loss: 53.12199783325195\n",
      "[step: 434] loss: 53.11041259765625\n",
      "[step: 435] loss: 53.09733581542969\n",
      "[step: 436] loss: 53.081260681152344\n",
      "[step: 437] loss: 53.06726837158203\n",
      "[step: 438] loss: 53.054996490478516\n",
      "[step: 439] loss: 53.040489196777344\n",
      "[step: 440] loss: 53.02488708496094\n",
      "[step: 441] loss: 53.0111198425293\n",
      "[step: 442] loss: 52.99785614013672\n",
      "[step: 443] loss: 52.98305130004883\n",
      "[step: 444] loss: 52.96755599975586\n",
      "[step: 445] loss: 52.953094482421875\n",
      "[step: 446] loss: 52.93913269042969\n",
      "[step: 447] loss: 52.92431640625\n",
      "[step: 448] loss: 52.908687591552734\n",
      "[step: 449] loss: 52.89323425292969\n",
      "[step: 450] loss: 52.87830352783203\n",
      "[step: 451] loss: 52.86328887939453\n",
      "[step: 452] loss: 52.84766387939453\n",
      "[step: 453] loss: 52.831512451171875\n",
      "[step: 454] loss: 52.81525421142578\n",
      "[step: 455] loss: 52.79909133911133\n",
      "[step: 456] loss: 52.78296661376953\n",
      "[step: 457] loss: 52.76662063598633\n",
      "[step: 458] loss: 52.749874114990234\n",
      "[step: 459] loss: 52.73271942138672\n",
      "[step: 460] loss: 52.715232849121094\n",
      "[step: 461] loss: 52.69744873046875\n",
      "[step: 462] loss: 52.67938232421875\n",
      "[step: 463] loss: 52.661067962646484\n",
      "[step: 464] loss: 52.6424560546875\n",
      "[step: 465] loss: 52.623512268066406\n",
      "[step: 466] loss: 52.604217529296875\n",
      "[step: 467] loss: 52.584556579589844\n",
      "[step: 468] loss: 52.56446838378906\n",
      "[step: 469] loss: 52.54398727416992\n",
      "[step: 470] loss: 52.523048400878906\n",
      "[step: 471] loss: 52.501670837402344\n",
      "[step: 472] loss: 52.47984313964844\n",
      "[step: 473] loss: 52.457763671875\n",
      "[step: 474] loss: 52.4360237121582\n",
      "[step: 475] loss: 52.417091369628906\n",
      "[step: 476] loss: 52.41120910644531\n",
      "[step: 477] loss: 52.45204544067383\n",
      "[step: 478] loss: 52.6171989440918\n",
      "[step: 479] loss: 52.747432708740234\n",
      "[step: 480] loss: 52.60237503051758\n",
      "[step: 481] loss: 52.27635955810547\n",
      "[step: 482] loss: 52.512725830078125\n",
      "[step: 483] loss: 52.5374870300293\n",
      "[step: 484] loss: 52.2111930847168\n",
      "[step: 485] loss: 52.48454284667969\n",
      "[step: 486] loss: 52.364463806152344\n",
      "[step: 487] loss: 52.186546325683594\n",
      "[step: 488] loss: 52.40607452392578\n",
      "[step: 489] loss: 52.15031433105469\n",
      "[step: 490] loss: 52.19220733642578\n",
      "[step: 491] loss: 52.20301818847656\n",
      "[step: 492] loss: 52.03166198730469\n",
      "[step: 493] loss: 52.15045166015625\n",
      "[step: 494] loss: 51.996917724609375\n",
      "[step: 495] loss: 52.03125762939453\n",
      "[step: 496] loss: 52.007354736328125\n",
      "[step: 497] loss: 51.919673919677734\n",
      "[step: 498] loss: 51.9739990234375\n",
      "[step: 499] loss: 51.874664306640625\n",
      "[step: 500] loss: 51.872398376464844\n",
      "[step: 501] loss: 51.85791778564453\n",
      "[step: 502] loss: 51.787105560302734\n",
      "[step: 503] loss: 51.79106521606445\n",
      "[step: 504] loss: 51.749813079833984\n",
      "[step: 505] loss: 51.70518493652344\n",
      "[step: 506] loss: 51.69968795776367\n",
      "[step: 507] loss: 51.65363693237305\n",
      "[step: 508] loss: 51.62147521972656\n",
      "[step: 509] loss: 51.610687255859375\n",
      "[step: 510] loss: 51.56424331665039\n",
      "[step: 511] loss: 51.53606414794922\n",
      "[step: 512] loss: 51.52417755126953\n",
      "[step: 513] loss: 51.48055648803711\n",
      "[step: 514] loss: 51.447391510009766\n",
      "[step: 515] loss: 51.435203552246094\n",
      "[step: 516] loss: 51.40428161621094\n",
      "[step: 517] loss: 51.36354064941406\n",
      "[step: 518] loss: 51.33641815185547\n",
      "[step: 519] loss: 51.320533752441406\n",
      "[step: 520] loss: 51.30097961425781\n",
      "[step: 521] loss: 51.267852783203125\n",
      "[step: 522] loss: 51.23332214355469\n",
      "[step: 523] loss: 51.20030975341797\n",
      "[step: 524] loss: 51.172088623046875\n",
      "[step: 525] loss: 51.148216247558594\n",
      "[step: 526] loss: 51.12899398803711\n",
      "[step: 527] loss: 51.12397003173828\n",
      "[step: 528] loss: 51.153785705566406\n",
      "[step: 529] loss: 51.3017692565918\n",
      "[step: 530] loss: 51.343711853027344\n",
      "[step: 531] loss: 51.3104248046875\n",
      "[step: 532] loss: 50.98890686035156\n",
      "[step: 533] loss: 51.082950592041016\n",
      "[step: 534] loss: 51.307586669921875\n",
      "[step: 535] loss: 50.97618103027344\n",
      "[step: 536] loss: 50.93683624267578\n",
      "[step: 537] loss: 51.12516784667969\n",
      "[step: 538] loss: 50.91499328613281\n",
      "[step: 539] loss: 50.836082458496094\n",
      "[step: 540] loss: 50.95305633544922\n",
      "[step: 541] loss: 50.842254638671875\n",
      "[step: 542] loss: 50.75196075439453\n",
      "[step: 543] loss: 50.8106689453125\n",
      "[step: 544] loss: 50.774070739746094\n",
      "[step: 545] loss: 50.68531799316406\n",
      "[step: 546] loss: 50.6889762878418\n",
      "[step: 547] loss: 50.70035171508789\n",
      "[step: 548] loss: 50.640220642089844\n",
      "[step: 549] loss: 50.592193603515625\n",
      "[step: 550] loss: 50.605403900146484\n",
      "[step: 551] loss: 50.594329833984375\n",
      "[step: 552] loss: 50.53197479248047\n",
      "[step: 553] loss: 50.501495361328125\n",
      "[step: 554] loss: 50.50627517700195\n",
      "[step: 555] loss: 50.48662185668945\n",
      "[step: 556] loss: 50.43987274169922\n",
      "[step: 557] loss: 50.40331268310547\n",
      "[step: 558] loss: 50.39268493652344\n",
      "[step: 559] loss: 50.3843994140625\n",
      "[step: 560] loss: 50.356658935546875\n",
      "[step: 561] loss: 50.31833267211914\n",
      "[step: 562] loss: 50.284027099609375\n",
      "[step: 563] loss: 50.261207580566406\n",
      "[step: 564] loss: 50.24507141113281\n",
      "[step: 565] loss: 50.22857666015625\n",
      "[step: 566] loss: 50.20915222167969\n",
      "[step: 567] loss: 50.18526840209961\n",
      "[step: 568] loss: 50.16061019897461\n",
      "[step: 569] loss: 50.13334655761719\n",
      "[step: 570] loss: 50.10688018798828\n",
      "[step: 571] loss: 50.07878875732422\n",
      "[step: 572] loss: 50.052181243896484\n",
      "[step: 573] loss: 50.02617263793945\n",
      "[step: 574] loss: 50.004615783691406\n",
      "[step: 575] loss: 49.99008560180664\n",
      "[step: 576] loss: 49.991920471191406\n",
      "[step: 577] loss: 50.02320861816406\n",
      "[step: 578] loss: 50.10012435913086\n",
      "[step: 579] loss: 50.20825958251953\n",
      "[step: 580] loss: 50.219459533691406\n",
      "[step: 581] loss: 50.041831970214844\n",
      "[step: 582] loss: 49.798797607421875\n",
      "[step: 583] loss: 49.774391174316406\n",
      "[step: 584] loss: 49.90460205078125\n",
      "[step: 585] loss: 49.893455505371094\n",
      "[step: 586] loss: 49.72476577758789\n",
      "[step: 587] loss: 49.63824462890625\n",
      "[step: 588] loss: 49.70867156982422\n",
      "[step: 589] loss: 49.731788635253906\n",
      "[step: 590] loss: 49.61054611206055\n",
      "[step: 591] loss: 49.53103256225586\n",
      "[step: 592] loss: 49.56397247314453\n",
      "[step: 593] loss: 49.57374954223633\n",
      "[step: 594] loss: 49.495662689208984\n",
      "[step: 595] loss: 49.41990661621094\n",
      "[step: 596] loss: 49.420989990234375\n",
      "[step: 597] loss: 49.437583923339844\n",
      "[step: 598] loss: 49.39567947387695\n",
      "[step: 599] loss: 49.32438659667969\n",
      "[step: 600] loss: 49.28474044799805\n",
      "[step: 601] loss: 49.287567138671875\n",
      "[step: 602] loss: 49.29829406738281\n",
      "[step: 603] loss: 49.28833770751953\n",
      "[step: 604] loss: 49.3070068359375\n",
      "[step: 605] loss: 49.426536560058594\n",
      "[step: 606] loss: 49.75434112548828\n",
      "[step: 607] loss: 50.139892578125\n",
      "[step: 608] loss: 49.68426513671875\n",
      "[step: 609] loss: 49.09484100341797\n",
      "[step: 610] loss: 49.29999923706055\n",
      "[step: 611] loss: 49.55512237548828\n",
      "[step: 612] loss: 49.112335205078125\n",
      "[step: 613] loss: 49.078269958496094\n",
      "[step: 614] loss: 49.349754333496094\n",
      "[step: 615] loss: 49.027313232421875\n",
      "[step: 616] loss: 48.987571716308594\n",
      "[step: 617] loss: 49.17649459838867\n",
      "[step: 618] loss: 48.935760498046875\n",
      "[step: 619] loss: 48.90143585205078\n",
      "[step: 620] loss: 49.01731872558594\n",
      "[step: 621] loss: 48.84734344482422\n",
      "[step: 622] loss: 48.79066848754883\n",
      "[step: 623] loss: 48.8929328918457\n",
      "[step: 624] loss: 48.80784225463867\n",
      "[step: 625] loss: 48.68341827392578\n",
      "[step: 626] loss: 48.73139572143555\n",
      "[step: 627] loss: 48.774200439453125\n",
      "[step: 628] loss: 48.678367614746094\n",
      "[step: 629] loss: 48.58830261230469\n",
      "[step: 630] loss: 48.600685119628906\n",
      "[step: 631] loss: 48.640411376953125\n",
      "[step: 632] loss: 48.623802185058594\n",
      "[step: 633] loss: 48.55543518066406\n",
      "[step: 634] loss: 48.50716018676758\n",
      "[step: 635] loss: 48.51373291015625\n",
      "[step: 636] loss: 48.535213470458984\n",
      "[step: 637] loss: 48.5384407043457\n",
      "[step: 638] loss: 48.49693298339844\n",
      "[step: 639] loss: 48.46812057495117\n",
      "[step: 640] loss: 48.4593391418457\n",
      "[step: 641] loss: 48.511863708496094\n",
      "[step: 642] loss: 48.51951599121094\n",
      "[step: 643] loss: 48.50389099121094\n",
      "[step: 644] loss: 48.38249969482422\n",
      "[step: 645] loss: 48.30088424682617\n",
      "[step: 646] loss: 48.26835632324219\n",
      "[step: 647] loss: 48.274566650390625\n",
      "[step: 648] loss: 48.25072479248047\n",
      "[step: 649] loss: 48.18744659423828\n",
      "[step: 650] loss: 48.119140625\n",
      "[step: 651] loss: 48.086421966552734\n",
      "[step: 652] loss: 48.08905792236328\n",
      "[step: 653] loss: 48.099853515625\n",
      "[step: 654] loss: 48.09677505493164\n",
      "[step: 655] loss: 48.068389892578125\n",
      "[step: 656] loss: 48.025508880615234\n",
      "[step: 657] loss: 47.979000091552734\n",
      "[step: 658] loss: 47.945125579833984\n",
      "[step: 659] loss: 47.93526840209961\n",
      "[step: 660] loss: 47.97875213623047\n",
      "[step: 661] loss: 48.116966247558594\n",
      "[step: 662] loss: 48.51995086669922\n",
      "[step: 663] loss: 48.72791290283203\n",
      "[step: 664] loss: 48.722190856933594\n",
      "[step: 665] loss: 47.89446258544922\n",
      "[step: 666] loss: 47.959022521972656\n",
      "[step: 667] loss: 48.46223449707031\n",
      "[step: 668] loss: 47.92839813232422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 669] loss: 47.82582092285156\n",
      "[step: 670] loss: 48.0982666015625\n",
      "[step: 671] loss: 47.834625244140625\n",
      "[step: 672] loss: 47.68763732910156\n",
      "[step: 673] loss: 47.89250946044922\n",
      "[step: 674] loss: 47.76814270019531\n",
      "[step: 675] loss: 47.59241485595703\n",
      "[step: 676] loss: 47.68463897705078\n",
      "[step: 677] loss: 47.704376220703125\n",
      "[step: 678] loss: 47.5701904296875\n",
      "[step: 679] loss: 47.49617004394531\n",
      "[step: 680] loss: 47.57002258300781\n",
      "[step: 681] loss: 47.62236022949219\n",
      "[step: 682] loss: 47.53455352783203\n",
      "[step: 683] loss: 47.43849563598633\n",
      "[step: 684] loss: 47.42301940917969\n",
      "[step: 685] loss: 47.45448303222656\n",
      "[step: 686] loss: 47.470149993896484\n",
      "[step: 687] loss: 47.390872955322266\n",
      "[step: 688] loss: 47.312721252441406\n",
      "[step: 689] loss: 47.27191162109375\n",
      "[step: 690] loss: 47.27581024169922\n",
      "[step: 691] loss: 47.29023361206055\n",
      "[step: 692] loss: 47.268089294433594\n",
      "[step: 693] loss: 47.23090362548828\n",
      "[step: 694] loss: 47.19575881958008\n",
      "[step: 695] loss: 47.20429229736328\n",
      "[step: 696] loss: 47.26988220214844\n",
      "[step: 697] loss: 47.42007827758789\n",
      "[step: 698] loss: 47.6611328125\n",
      "[step: 699] loss: 48.01213073730469\n",
      "[step: 700] loss: 48.04484558105469\n",
      "[step: 701] loss: 47.67668533325195\n",
      "[step: 702] loss: 47.10462188720703\n",
      "[step: 703] loss: 47.184486389160156\n",
      "[step: 704] loss: 47.529197692871094\n",
      "[step: 705] loss: 47.30376434326172\n",
      "[step: 706] loss: 46.951454162597656\n",
      "[step: 707] loss: 47.09990692138672\n",
      "[step: 708] loss: 47.261634826660156\n",
      "[step: 709] loss: 47.01604461669922\n",
      "[step: 710] loss: 46.87931823730469\n",
      "[step: 711] loss: 47.0494499206543\n",
      "[step: 712] loss: 47.00010681152344\n",
      "[step: 713] loss: 46.804588317871094\n",
      "[step: 714] loss: 46.887107849121094\n",
      "[step: 715] loss: 46.94491958618164\n",
      "[step: 716] loss: 46.77940368652344\n",
      "[step: 717] loss: 46.7602653503418\n",
      "[step: 718] loss: 46.83542251586914\n",
      "[step: 719] loss: 46.742549896240234\n",
      "[step: 720] loss: 46.674766540527344\n",
      "[step: 721] loss: 46.7322883605957\n",
      "[step: 722] loss: 46.70473098754883\n",
      "[step: 723] loss: 46.61820983886719\n",
      "[step: 724] loss: 46.625064849853516\n",
      "[step: 725] loss: 46.637725830078125\n",
      "[step: 726] loss: 46.57589340209961\n",
      "[step: 727] loss: 46.54056930541992\n",
      "[step: 728] loss: 46.55922317504883\n",
      "[step: 729] loss: 46.53987121582031\n",
      "[step: 730] loss: 46.48723602294922\n",
      "[step: 731] loss: 46.4764289855957\n",
      "[step: 732] loss: 46.48243713378906\n",
      "[step: 733] loss: 46.45138168334961\n",
      "[step: 734] loss: 46.414894104003906\n",
      "[step: 735] loss: 46.40984344482422\n",
      "[step: 736] loss: 46.40730285644531\n",
      "[step: 737] loss: 46.38615417480469\n",
      "[step: 738] loss: 46.36361312866211\n",
      "[step: 739] loss: 46.372039794921875\n",
      "[step: 740] loss: 46.39446258544922\n",
      "[step: 741] loss: 46.433902740478516\n",
      "[step: 742] loss: 46.472755432128906\n",
      "[step: 743] loss: 46.59504318237305\n",
      "[step: 744] loss: 46.58180618286133\n",
      "[step: 745] loss: 46.5922966003418\n",
      "[step: 746] loss: 46.358158111572266\n",
      "[step: 747] loss: 46.20221710205078\n",
      "[step: 748] loss: 46.164886474609375\n",
      "[step: 749] loss: 46.23349380493164\n",
      "[step: 750] loss: 46.33864974975586\n",
      "[step: 751] loss: 46.31145477294922\n",
      "[step: 752] loss: 46.25556945800781\n",
      "[step: 753] loss: 46.1275520324707\n",
      "[step: 754] loss: 46.05313491821289\n",
      "[step: 755] loss: 46.04512023925781\n",
      "[step: 756] loss: 46.076866149902344\n",
      "[step: 757] loss: 46.12274932861328\n",
      "[step: 758] loss: 46.11867904663086\n",
      "[step: 759] loss: 46.1099853515625\n",
      "[step: 760] loss: 46.054908752441406\n",
      "[step: 761] loss: 45.99775314331055\n",
      "[step: 762] loss: 45.93869400024414\n",
      "[step: 763] loss: 45.894901275634766\n",
      "[step: 764] loss: 45.87482833862305\n",
      "[step: 765] loss: 45.874427795410156\n",
      "[step: 766] loss: 45.885581970214844\n",
      "[step: 767] loss: 45.89360046386719\n",
      "[step: 768] loss: 45.896278381347656\n",
      "[step: 769] loss: 45.879539489746094\n",
      "[step: 770] loss: 45.8517951965332\n",
      "[step: 771] loss: 45.80821990966797\n",
      "[step: 772] loss: 45.76527786254883\n",
      "[step: 773] loss: 45.726219177246094\n",
      "[step: 774] loss: 45.698020935058594\n",
      "[step: 775] loss: 45.679229736328125\n",
      "[step: 776] loss: 45.667606353759766\n",
      "[step: 777] loss: 45.661407470703125\n",
      "[step: 778] loss: 45.661468505859375\n",
      "[step: 779] loss: 45.671443939208984\n",
      "[step: 780] loss: 45.698062896728516\n",
      "[step: 781] loss: 45.759342193603516\n",
      "[step: 782] loss: 45.85694885253906\n",
      "[step: 783] loss: 46.02638626098633\n",
      "[step: 784] loss: 46.12919998168945\n",
      "[step: 785] loss: 46.13187026977539\n",
      "[step: 786] loss: 45.80494689941406\n",
      "[step: 787] loss: 45.516395568847656\n",
      "[step: 788] loss: 45.54340362548828\n",
      "[step: 789] loss: 45.72388458251953\n",
      "[step: 790] loss: 45.74469757080078\n",
      "[step: 791] loss: 45.52601623535156\n",
      "[step: 792] loss: 45.41954803466797\n",
      "[step: 793] loss: 45.51697540283203\n",
      "[step: 794] loss: 45.60048294067383\n",
      "[step: 795] loss: 45.51490783691406\n",
      "[step: 796] loss: 45.382057189941406\n",
      "[step: 797] loss: 45.35743713378906\n",
      "[step: 798] loss: 45.44379806518555\n",
      "[step: 799] loss: 45.501426696777344\n",
      "[step: 800] loss: 45.539833068847656\n",
      "[step: 801] loss: 45.427818298339844\n",
      "[step: 802] loss: 45.337486267089844\n",
      "[step: 803] loss: 45.271690368652344\n",
      "[step: 804] loss: 45.28543472290039\n",
      "[step: 805] loss: 45.328277587890625\n",
      "[step: 806] loss: 45.34632873535156\n",
      "[step: 807] loss: 45.302581787109375\n",
      "[step: 808] loss: 45.22578430175781\n",
      "[step: 809] loss: 45.16242218017578\n",
      "[step: 810] loss: 45.159507751464844\n",
      "[step: 811] loss: 45.19758605957031\n",
      "[step: 812] loss: 45.25658416748047\n",
      "[step: 813] loss: 45.25167465209961\n",
      "[step: 814] loss: 45.27754211425781\n",
      "[step: 815] loss: 45.30168151855469\n",
      "[step: 816] loss: 45.41259765625\n",
      "[step: 817] loss: 45.51157760620117\n",
      "[step: 818] loss: 45.559104919433594\n",
      "[step: 819] loss: 45.389564514160156\n",
      "[step: 820] loss: 45.134552001953125\n",
      "[step: 821] loss: 44.944488525390625\n",
      "[step: 822] loss: 44.95155334472656\n",
      "[step: 823] loss: 45.079139709472656\n",
      "[step: 824] loss: 45.1402587890625\n",
      "[step: 825] loss: 45.066959381103516\n",
      "[step: 826] loss: 44.91240692138672\n",
      "[step: 827] loss: 44.84081268310547\n",
      "[step: 828] loss: 44.885093688964844\n",
      "[step: 829] loss: 44.94188690185547\n",
      "[step: 830] loss: 44.92287826538086\n",
      "[step: 831] loss: 44.828575134277344\n",
      "[step: 832] loss: 44.76122283935547\n",
      "[step: 833] loss: 44.766693115234375\n",
      "[step: 834] loss: 44.801429748535156\n",
      "[step: 835] loss: 44.80165100097656\n",
      "[step: 836] loss: 44.748165130615234\n",
      "[step: 837] loss: 44.68827438354492\n",
      "[step: 838] loss: 44.66199493408203\n",
      "[step: 839] loss: 44.66969299316406\n",
      "[step: 840] loss: 44.681312561035156\n",
      "[step: 841] loss: 44.669132232666016\n",
      "[step: 842] loss: 44.63583755493164\n",
      "[step: 843] loss: 44.59585189819336\n",
      "[step: 844] loss: 44.56800842285156\n",
      "[step: 845] loss: 44.5584831237793\n",
      "[step: 846] loss: 44.562644958496094\n",
      "[step: 847] loss: 44.57667541503906\n",
      "[step: 848] loss: 44.597320556640625\n",
      "[step: 849] loss: 44.64451599121094\n",
      "[step: 850] loss: 44.72011947631836\n",
      "[step: 851] loss: 44.935157775878906\n",
      "[step: 852] loss: 45.10796356201172\n",
      "[step: 853] loss: 45.58509826660156\n",
      "[step: 854] loss: 45.23823547363281\n",
      "[step: 855] loss: 44.917144775390625\n",
      "[step: 856] loss: 44.517417907714844\n",
      "[step: 857] loss: 44.66844177246094\n",
      "[step: 858] loss: 45.138763427734375\n",
      "[step: 859] loss: 44.871070861816406\n",
      "[step: 860] loss: 44.502925872802734\n",
      "[step: 861] loss: 44.35615539550781\n",
      "[step: 862] loss: 44.54204559326172\n",
      "[step: 863] loss: 44.71274948120117\n",
      "[step: 864] loss: 44.43739318847656\n",
      "[step: 865] loss: 44.281272888183594\n",
      "[step: 866] loss: 44.3784065246582\n",
      "[step: 867] loss: 44.49412536621094\n",
      "[step: 868] loss: 44.49322509765625\n",
      "[step: 869] loss: 44.2904167175293\n",
      "[step: 870] loss: 44.20001220703125\n",
      "[step: 871] loss: 44.246742248535156\n",
      "[step: 872] loss: 44.264671325683594\n",
      "[step: 873] loss: 44.223289489746094\n",
      "[step: 874] loss: 44.13867950439453\n",
      "[step: 875] loss: 44.128726959228516\n",
      "[step: 876] loss: 44.18247985839844\n",
      "[step: 877] loss: 44.20954132080078\n",
      "[step: 878] loss: 44.177276611328125\n",
      "[step: 879] loss: 44.10065460205078\n",
      "[step: 880] loss: 44.057525634765625\n",
      "[step: 881] loss: 44.060523986816406\n",
      "[step: 882] loss: 44.075660705566406\n",
      "[step: 883] loss: 44.081539154052734\n",
      "[step: 884] loss: 44.0380859375\n",
      "[step: 885] loss: 44.002647399902344\n",
      "[step: 886] loss: 43.99319839477539\n",
      "[step: 887] loss: 44.01542663574219\n",
      "[step: 888] loss: 44.06477355957031\n",
      "[step: 889] loss: 44.10803985595703\n",
      "[step: 890] loss: 44.163307189941406\n",
      "[step: 891] loss: 44.1978759765625\n",
      "[step: 892] loss: 44.26166534423828\n",
      "[step: 893] loss: 44.26301193237305\n",
      "[step: 894] loss: 44.2279052734375\n",
      "[step: 895] loss: 44.073062896728516\n",
      "[step: 896] loss: 43.8653564453125\n",
      "[step: 897] loss: 43.739078521728516\n",
      "[step: 898] loss: 43.75562286376953\n",
      "[step: 899] loss: 43.85688400268555\n",
      "[step: 900] loss: 43.91396713256836\n",
      "[step: 901] loss: 43.86345672607422\n",
      "[step: 902] loss: 43.7352294921875\n",
      "[step: 903] loss: 43.63660430908203\n",
      "[step: 904] loss: 43.6276969909668\n",
      "[step: 905] loss: 43.6792106628418\n",
      "[step: 906] loss: 43.721797943115234\n",
      "[step: 907] loss: 43.71293258666992\n",
      "[step: 908] loss: 43.651817321777344\n",
      "[step: 909] loss: 43.57588577270508\n",
      "[step: 910] loss: 43.51747512817383\n",
      "[step: 911] loss: 43.49372100830078\n",
      "[step: 912] loss: 43.49811553955078\n",
      "[step: 913] loss: 43.51593780517578\n",
      "[step: 914] loss: 43.53343200683594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 915] loss: 43.540550231933594\n",
      "[step: 916] loss: 43.536285400390625\n",
      "[step: 917] loss: 43.5157470703125\n",
      "[step: 918] loss: 43.48530197143555\n",
      "[step: 919] loss: 43.44404602050781\n",
      "[step: 920] loss: 43.40334701538086\n",
      "[step: 921] loss: 43.36402893066406\n",
      "[step: 922] loss: 43.335899353027344\n",
      "[step: 923] loss: 43.31918716430664\n",
      "[step: 924] loss: 43.32695007324219\n",
      "[step: 925] loss: 43.37199783325195\n",
      "[step: 926] loss: 43.50353240966797\n",
      "[step: 927] loss: 43.77412414550781\n",
      "[step: 928] loss: 44.26663589477539\n",
      "[step: 929] loss: 44.69652557373047\n",
      "[step: 930] loss: 44.76420974731445\n",
      "[step: 931] loss: 43.97689437866211\n",
      "[step: 932] loss: 43.44645690917969\n",
      "[step: 933] loss: 43.649105072021484\n",
      "[step: 934] loss: 44.08007049560547\n",
      "[step: 935] loss: 44.539955139160156\n",
      "[step: 936] loss: 43.431541442871094\n",
      "[step: 937] loss: 43.5176887512207\n",
      "[step: 938] loss: 44.026432037353516\n",
      "[step: 939] loss: 43.31646728515625\n",
      "[step: 940] loss: 43.11132049560547\n",
      "[step: 941] loss: 43.56571578979492\n",
      "[step: 942] loss: 43.427528381347656\n",
      "[step: 943] loss: 43.064571380615234\n",
      "[step: 944] loss: 43.116737365722656\n",
      "[step: 945] loss: 43.26213455200195\n",
      "[step: 946] loss: 43.10504913330078\n",
      "[step: 947] loss: 42.92613983154297\n",
      "[step: 948] loss: 43.06574249267578\n",
      "[step: 949] loss: 43.146881103515625\n",
      "[step: 950] loss: 42.92320251464844\n",
      "[step: 951] loss: 42.89251708984375\n",
      "[step: 952] loss: 43.00321578979492\n",
      "[step: 953] loss: 42.91432189941406\n",
      "[step: 954] loss: 42.78948974609375\n",
      "[step: 955] loss: 42.82743453979492\n",
      "[step: 956] loss: 42.866729736328125\n",
      "[step: 957] loss: 42.79399871826172\n",
      "[step: 958] loss: 42.73155212402344\n",
      "[step: 959] loss: 42.754154205322266\n",
      "[step: 960] loss: 42.78577423095703\n",
      "[step: 961] loss: 42.73042297363281\n",
      "[step: 962] loss: 42.697998046875\n",
      "[step: 963] loss: 42.74528121948242\n",
      "[step: 964] loss: 42.82707214355469\n",
      "[step: 965] loss: 42.942604064941406\n",
      "[step: 966] loss: 43.15433120727539\n",
      "[step: 967] loss: 43.54106140136719\n",
      "[step: 968] loss: 44.10297775268555\n",
      "[step: 969] loss: 43.78668212890625\n",
      "[step: 970] loss: 43.06011962890625\n",
      "[step: 971] loss: 42.64173126220703\n",
      "[step: 972] loss: 42.81504440307617\n",
      "[step: 973] loss: 43.12687683105469\n",
      "[step: 974] loss: 42.96847152709961\n",
      "[step: 975] loss: 42.64281463623047\n",
      "[step: 976] loss: 42.66899871826172\n",
      "[step: 977] loss: 42.77423095703125\n",
      "[step: 978] loss: 42.653778076171875\n",
      "[step: 979] loss: 42.480743408203125\n",
      "[step: 980] loss: 42.49396896362305\n",
      "[step: 981] loss: 42.58619689941406\n",
      "[step: 982] loss: 42.56656265258789\n",
      "[step: 983] loss: 42.483123779296875\n",
      "[step: 984] loss: 42.36540603637695\n",
      "[step: 985] loss: 42.29026794433594\n",
      "[step: 986] loss: 42.36464309692383\n",
      "[step: 987] loss: 42.438507080078125\n",
      "[step: 988] loss: 42.378868103027344\n",
      "[step: 989] loss: 42.25458526611328\n",
      "[step: 990] loss: 42.216209411621094\n",
      "[step: 991] loss: 42.19721603393555\n",
      "[step: 992] loss: 42.179664611816406\n",
      "[step: 993] loss: 42.191375732421875\n",
      "[step: 994] loss: 42.18867111206055\n",
      "[step: 995] loss: 42.144081115722656\n",
      "[step: 996] loss: 42.08803939819336\n",
      "[step: 997] loss: 42.05189514160156\n",
      "[step: 998] loss: 42.020835876464844\n",
      "[step: 999] loss: 42.0157356262207\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "LSTM realforecast : [22893.85546875, 17932.98828125, 15893.5322265625, 16782.552734375, 18438.66015625, 22842.158203125, 25625.88671875, 27617.017578125, 26413.548828125, 24765.607421875, 19137.41015625, 18635.05078125, 18456.388671875, 17958.958984375, 18695.279296875, 21706.734375, 21970.05859375, 17274.99609375, 16291.037109375, 13130.275390625, 13246.7705078125, 15944.0927734375, 15511.1328125, 15316.9228515625, 15082.0693359375, 18212.03515625, 18699.9375, 20721.009765625]\n",
      "Bayseian realforecast : [31919.295046721436, 28023.896400626025, 24375.627162087185, 20950.305165728831, 18090.689262586988, 16492.185803541714, 16361.022539365167, 17077.627250807684, 17527.163219077185, 17181.349378622759, 16662.440893279501, 16751.377424575498, 17390.167397856108, 17771.561147530927, 17368.778954001489, 16645.151323068123, 16321.950961684437, 16471.21827486117, 16625.507399831353, 16620.610971159385, 16998.210406788887, 18293.535998663916, 20105.266830484994, 21171.956902364105, 21166.428423350382, 21814.131936555455, 25298.540713776478, 31872.984188345559]\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,0,forecastDay,'week') #0은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31919.295046721436,\n",
       " 28023.896400626025,\n",
       " 24375.627162087185,\n",
       " 20950.305165728831,\n",
       " 18090.689262586988,\n",
       " 16492.185803541714,\n",
       " 16361.022539365167,\n",
       " 17077.627250807684,\n",
       " 17527.163219077185,\n",
       " 17181.349378622759,\n",
       " 16662.440893279501,\n",
       " 16751.377424575498,\n",
       " 17390.167397856108,\n",
       " 17771.561147530927,\n",
       " 17368.778954001489,\n",
       " 16645.151323068123,\n",
       " 16321.950961684437,\n",
       " 16471.21827486117,\n",
       " 16625.507399831353,\n",
       " 16620.610971159385,\n",
       " 16998.210406788887,\n",
       " 18293.535998663916,\n",
       " 20105.266830484994,\n",
       " 21171.956902364105,\n",
       " 21166.428423350382,\n",
       " 21814.131936555455,\n",
       " 25298.540713776478,\n",
       " 31872.984188345559]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawArrayDatas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
