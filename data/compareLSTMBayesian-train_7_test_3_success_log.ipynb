{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=2*forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list(np.log(rawArrayDatas[1]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list(np.log(rawArrayDatas[1][:-forecastDay] ))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-3*forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-3*forecastDay]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-3*forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of 2*forecastDay:  rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "    testY= rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        ####더 좋은 알고리즘 호출\n",
    "        if nameOfBestAlgorithm is 'LSTM':\n",
    "            tf.reset_default_graph()\n",
    "            realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "\n",
    "        elif nameOfBestAlgorithm is 'Bayseian':\n",
    "            realForecastDictionary['Bayseian']=Bayseian(txsForRealForecastBayesian,forecastDay,'day')\n",
    "\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        ####더 좋은 알고리즘 호출\n",
    "        if nameOfBestAlgorithm is 'LSTM':\n",
    "            tf.reset_default_graph()\n",
    "            realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "\n",
    "        elif nameOfBestAlgorithm is 'Bayseian':\n",
    "     \n",
    "            realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        ####더 좋은 알고리즘 호출\n",
    "        if nameOfBestAlgorithm is 'LSTM':\n",
    "            tf.reset_default_graph()\n",
    "            realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "\n",
    "        elif nameOfBestAlgorithm is 'Bayseian':\n",
    "            realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    # tf.reset_default_graph()\n",
    "    # realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay, feature)\n",
    "\n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "        listedLogPredict=test_predict[-1].tolist()\n",
    "    return [np.exp(y) for y in listedLogPredict]\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM forecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian forecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 9\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('webMonth.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.1)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns=['ds','y']\n",
    "# txs=pd.read_table('walMonth_train.csv', sep=',',header=None,names=columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(txs)\n",
    "# LSTM(txs, 7, features)\n",
    "# Bayseian(txs, 7, 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 7.058945655822754\n",
      "[step: 1] loss: 3.811281204223633\n",
      "[step: 2] loss: 1.7838832139968872\n",
      "[step: 3] loss: 0.6835789680480957\n",
      "[step: 4] loss: 0.24877071380615234\n",
      "[step: 5] loss: 0.24200588464736938\n",
      "[step: 6] loss: 0.4317721128463745\n",
      "[step: 7] loss: 0.6298753619194031\n",
      "[step: 8] loss: 0.7370294332504272\n",
      "[step: 9] loss: 0.73935866355896\n",
      "[step: 10] loss: 0.6657836437225342\n",
      "[step: 11] loss: 0.5538766980171204\n",
      "[step: 12] loss: 0.43429720401763916\n",
      "[step: 13] loss: 0.3259832561016083\n",
      "[step: 14] loss: 0.23714883625507355\n",
      "[step: 15] loss: 0.16899918019771576\n",
      "[step: 16] loss: 0.11951932311058044\n",
      "[step: 17] loss: 0.08599355816841125\n",
      "[step: 18] loss: 0.06604373455047607\n",
      "[step: 19] loss: 0.05761495605111122\n",
      "[step: 20] loss: 0.05852103978395462\n",
      "[step: 21] loss: 0.06605970859527588\n",
      "[step: 22] loss: 0.07700095325708389\n",
      "[step: 23] loss: 0.08798571676015854\n",
      "[step: 24] loss: 0.09612330794334412\n",
      "[step: 25] loss: 0.09948892891407013\n",
      "[step: 26] loss: 0.09733263403177261\n",
      "[step: 27] loss: 0.08998505771160126\n",
      "[step: 28] loss: 0.07856752723455429\n",
      "[step: 29] loss: 0.06464435160160065\n",
      "[step: 30] loss: 0.04991200938820839\n",
      "[step: 31] loss: 0.03595617786049843\n",
      "[step: 32] loss: 0.02406812086701393\n",
      "[step: 33] loss: 0.015109848231077194\n",
      "[step: 34] loss: 0.00943615660071373\n",
      "[step: 35] loss: 0.006893508601933718\n",
      "[step: 36] loss: 0.006907254457473755\n",
      "[step: 37] loss: 0.008642584085464478\n",
      "[step: 38] loss: 0.011198002845048904\n",
      "[step: 39] loss: 0.013778037391602993\n",
      "[step: 40] loss: 0.015803731977939606\n",
      "[step: 41] loss: 0.016947133466601372\n",
      "[step: 42] loss: 0.01710531674325466\n",
      "[step: 43] loss: 0.016344577074050903\n",
      "[step: 44] loss: 0.01484238263219595\n",
      "[step: 45] loss: 0.012838740833103657\n",
      "[step: 46] loss: 0.010595117695629597\n",
      "[step: 47] loss: 0.00835641473531723\n",
      "[step: 48] loss: 0.006318645551800728\n",
      "[step: 49] loss: 0.004610445350408554\n",
      "[step: 50] loss: 0.0032938073854893446\n",
      "[step: 51] loss: 0.002380013233050704\n",
      "[step: 52] loss: 0.0018489048816263676\n",
      "[step: 53] loss: 0.0016604122938588262\n",
      "[step: 54] loss: 0.0017556584207341075\n",
      "[step: 55] loss: 0.0020540624391287565\n",
      "[step: 56] loss: 0.002455627778545022\n",
      "[step: 57] loss: 0.002853014972060919\n",
      "[step: 58] loss: 0.0031502358615398407\n",
      "[step: 59] loss: 0.0032799982000142336\n",
      "[step: 60] loss: 0.0032127690501511097\n",
      "[step: 61] loss: 0.0029561633709818125\n",
      "[step: 62] loss: 0.0025478259194642305\n",
      "[step: 63] loss: 0.0020462521351873875\n",
      "[step: 64] loss: 0.0015212539583444595\n",
      "[step: 65] loss: 0.0010433470597490668\n",
      "[step: 66] loss: 0.0006714350311085582\n",
      "[step: 67] loss: 0.0004409310349728912\n",
      "[step: 68] loss: 0.00035671971272677183\n",
      "[step: 69] loss: 0.0003944236086681485\n",
      "[step: 70] loss: 0.0005098384572193027\n",
      "[step: 71] loss: 0.000652498914860189\n",
      "[step: 72] loss: 0.0007781288004480302\n",
      "[step: 73] loss: 0.0008564871968701482\n",
      "[step: 74] loss: 0.0008741305791772902\n",
      "[step: 75] loss: 0.0008333855075761676\n",
      "[step: 76] loss: 0.000748762278817594\n",
      "[step: 77] loss: 0.0006415485986508429\n",
      "[step: 78] loss: 0.0005332895671017468\n",
      "[step: 79] loss: 0.000439932948211208\n",
      "[step: 80] loss: 0.00036882367567159235\n",
      "[step: 81] loss: 0.0003195993776898831\n",
      "[step: 82] loss: 0.00028796642436645925\n",
      "[step: 83] loss: 0.00026969617465510964\n",
      "[step: 84] loss: 0.0002624802873469889\n",
      "[step: 85] loss: 0.00026519433595240116\n",
      "[step: 86] loss: 0.00027602643240243196\n",
      "[step: 87] loss: 0.00029133932548575103\n",
      "[step: 88] loss: 0.00030615320429205894\n",
      "[step: 89] loss: 0.0003156490856781602\n",
      "[step: 90] loss: 0.0003165826783515513\n",
      "[step: 91] loss: 0.00030791517929174006\n",
      "[step: 92] loss: 0.0002907439193222672\n",
      "[step: 93] loss: 0.0002678913879208267\n",
      "[step: 94] loss: 0.00024332018801942468\n",
      "[step: 95] loss: 0.00022125328541733325\n",
      "[step: 96] loss: 0.00020506023429334164\n",
      "[step: 97] loss: 0.00019631243776530027\n",
      "[step: 98] loss: 0.0001945068797795102\n",
      "[step: 99] loss: 0.00019762440933845937\n",
      "[step: 100] loss: 0.00020313174172770232\n",
      "[step: 101] loss: 0.00020883779507130384\n",
      "[step: 102] loss: 0.00021324129193089902\n",
      "[step: 103] loss: 0.00021548016229644418\n",
      "[step: 104] loss: 0.00021519225265365094\n",
      "[step: 105] loss: 0.00021247583208605647\n",
      "[step: 106] loss: 0.00020786823006346822\n",
      "[step: 107] loss: 0.00020222766033839434\n",
      "[step: 108] loss: 0.0001964849216165021\n",
      "[step: 109] loss: 0.00019141382654197514\n",
      "[step: 110] loss: 0.00018754455959424376\n",
      "[step: 111] loss: 0.00018517952412366867\n",
      "[step: 112] loss: 0.00018441914289724082\n",
      "[step: 113] loss: 0.00018511188682168722\n",
      "[step: 114] loss: 0.00018680909124668688\n",
      "[step: 115] loss: 0.00018883215670939535\n",
      "[step: 116] loss: 0.00019045891531277448\n",
      "[step: 117] loss: 0.0001911651052068919\n",
      "[step: 118] loss: 0.0001907685655169189\n",
      "[step: 119] loss: 0.00018942645692732185\n",
      "[step: 120] loss: 0.00018752363394014537\n",
      "[step: 121] loss: 0.00018552270194049925\n",
      "[step: 122] loss: 0.00018382706912234426\n",
      "[step: 123] loss: 0.00018269465363118798\n",
      "[step: 124] loss: 0.00018219518824480474\n",
      "[step: 125] loss: 0.00018222705693915486\n",
      "[step: 126] loss: 0.00018258592172060162\n",
      "[step: 127] loss: 0.00018304699915461242\n",
      "[step: 128] loss: 0.00018343154806643724\n",
      "[step: 129] loss: 0.00018363587150815874\n",
      "[step: 130] loss: 0.00018361973343417048\n",
      "[step: 131] loss: 0.00018339678354095668\n",
      "[step: 132] loss: 0.00018301545060239732\n",
      "[step: 133] loss: 0.00018254978931508958\n",
      "[step: 134] loss: 0.00018208057736046612\n",
      "[step: 135] loss: 0.00018167559755966067\n",
      "[step: 136] loss: 0.00018137886945623904\n",
      "[step: 137] loss: 0.0001812065893318504\n",
      "[step: 138] loss: 0.00018114429258275777\n",
      "[step: 139] loss: 0.00018115973216481507\n",
      "[step: 140] loss: 0.00018121024186257273\n",
      "[step: 141] loss: 0.00018125594942830503\n",
      "[step: 142] loss: 0.00018126581562682986\n",
      "[step: 143] loss: 0.00018122344044968486\n",
      "[step: 144] loss: 0.0001811257388908416\n",
      "[step: 145] loss: 0.00018098346481565386\n",
      "[step: 146] loss: 0.0001808134256862104\n",
      "[step: 147] loss: 0.00018064129108097404\n",
      "[step: 148] loss: 0.00018048964557237923\n",
      "[step: 149] loss: 0.00018037340487353504\n",
      "[step: 150] loss: 0.00018029478087555617\n",
      "[step: 151] loss: 0.00018024721066467464\n",
      "[step: 152] loss: 0.00018021761206910014\n",
      "[step: 153] loss: 0.00018019218987319618\n",
      "[step: 154] loss: 0.00018015727982856333\n",
      "[step: 155] loss: 0.00018010695930570364\n",
      "[step: 156] loss: 0.00018003959849011153\n",
      "[step: 157] loss: 0.00017995898087974638\n",
      "[step: 158] loss: 0.0001798716839402914\n",
      "[step: 159] loss: 0.0001797847799025476\n",
      "[step: 160] loss: 0.0001797000295482576\n",
      "[step: 161] loss: 0.00017962238052859902\n",
      "[step: 162] loss: 0.00017955130897462368\n",
      "[step: 163] loss: 0.0001794870913727209\n",
      "[step: 164] loss: 0.00017943016428034753\n",
      "[step: 165] loss: 0.00017937758821062744\n",
      "[step: 166] loss: 0.000179324735654518\n",
      "[step: 167] loss: 0.0001792709226720035\n",
      "[step: 168] loss: 0.00017921194375958294\n",
      "[step: 169] loss: 0.00017914746422320604\n",
      "[step: 170] loss: 0.0001790773094398901\n",
      "[step: 171] loss: 0.00017900561215355992\n",
      "[step: 172] loss: 0.00017893346375785768\n",
      "[step: 173] loss: 0.00017886466230265796\n",
      "[step: 174] loss: 0.000178800051799044\n",
      "[step: 175] loss: 0.000178738686372526\n",
      "[step: 176] loss: 0.00017868010036181659\n",
      "[step: 177] loss: 0.00017862061213236302\n",
      "[step: 178] loss: 0.00017855979967862368\n",
      "[step: 179] loss: 0.00017849757568910718\n",
      "[step: 180] loss: 0.0001784338674042374\n",
      "[step: 181] loss: 0.00017836876213550568\n",
      "[step: 182] loss: 0.00017830388969741762\n",
      "[step: 183] loss: 0.00017823830421548337\n",
      "[step: 184] loss: 0.00017817379557527602\n",
      "[step: 185] loss: 0.0001781085884431377\n",
      "[step: 186] loss: 0.00017804448725655675\n",
      "[step: 187] loss: 0.00017797980399336666\n",
      "[step: 188] loss: 0.0001779159647412598\n",
      "[step: 189] loss: 0.0001778523001121357\n",
      "[step: 190] loss: 0.0001777892466634512\n",
      "[step: 191] loss: 0.00017772546561900526\n",
      "[step: 192] loss: 0.0001776612043613568\n",
      "[step: 193] loss: 0.00017759612819645554\n",
      "[step: 194] loss: 0.00017753071733750403\n",
      "[step: 195] loss: 0.00017746433150023222\n",
      "[step: 196] loss: 0.00017739899340085685\n",
      "[step: 197] loss: 0.0001773337717168033\n",
      "[step: 198] loss: 0.00017726756050251424\n",
      "[step: 199] loss: 0.00017720287723932415\n",
      "[step: 200] loss: 0.00017713832494337112\n",
      "[step: 201] loss: 0.00017707259394228458\n",
      "[step: 202] loss: 0.0001770069357007742\n",
      "[step: 203] loss: 0.0001769410737324506\n",
      "[step: 204] loss: 0.00017687486251816154\n",
      "[step: 205] loss: 0.00017680834571365267\n",
      "[step: 206] loss: 0.0001767423382261768\n",
      "[step: 207] loss: 0.00017667555948719382\n",
      "[step: 208] loss: 0.00017660920275375247\n",
      "[step: 209] loss: 0.00017654208932071924\n",
      "[step: 210] loss: 0.0001764753833413124\n",
      "[step: 211] loss: 0.00017640893929637969\n",
      "[step: 212] loss: 0.000176341796759516\n",
      "[step: 213] loss: 0.00017627538181841373\n",
      "[step: 214] loss: 0.00017620870494283736\n",
      "[step: 215] loss: 0.00017614057287573814\n",
      "[step: 216] loss: 0.00017607337213121355\n",
      "[step: 217] loss: 0.0001760056911734864\n",
      "[step: 218] loss: 0.0001759385340847075\n",
      "[step: 219] loss: 0.00017587012553121895\n",
      "[step: 220] loss: 0.0001758028956828639\n",
      "[step: 221] loss: 0.00017573470540810376\n",
      "[step: 222] loss: 0.00017566682072356343\n",
      "[step: 223] loss: 0.00017559794650878757\n",
      "[step: 224] loss: 0.00017553071666043252\n",
      "[step: 225] loss: 0.00017546194430906326\n",
      "[step: 226] loss: 0.00017539318650960922\n",
      "[step: 227] loss: 0.00017532477795612067\n",
      "[step: 228] loss: 0.00017525593284517527\n",
      "[step: 229] loss: 0.0001751871022861451\n",
      "[step: 230] loss: 0.00017511809710413218\n",
      "[step: 231] loss: 0.000175049266545102\n",
      "[step: 232] loss: 0.00017498002853244543\n",
      "[step: 233] loss: 0.0001749111688695848\n",
      "[step: 234] loss: 0.0001748418144416064\n",
      "[step: 235] loss: 0.00017477216897532344\n",
      "[step: 236] loss: 0.00017470316379331052\n",
      "[step: 237] loss: 0.00017463354743085802\n",
      "[step: 238] loss: 0.0001745630579534918\n",
      "[step: 239] loss: 0.0001744934997987002\n",
      "[step: 240] loss: 0.00017442312673665583\n",
      "[step: 241] loss: 0.00017435393237974495\n",
      "[step: 242] loss: 0.00017428376304451376\n",
      "[step: 243] loss: 0.00017421369557268918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 244] loss: 0.00017414274043403566\n",
      "[step: 245] loss: 0.0001740724255796522\n",
      "[step: 246] loss: 0.00017400190699845552\n",
      "[step: 247] loss: 0.00017393159214407206\n",
      "[step: 248] loss: 0.00017386094259563833\n",
      "[step: 249] loss: 0.0001737899729050696\n",
      "[step: 250] loss: 0.00017371926514897496\n",
      "[step: 251] loss: 0.00017364797531627119\n",
      "[step: 252] loss: 0.0001735776022542268\n",
      "[step: 253] loss: 0.000173506501596421\n",
      "[step: 254] loss: 0.00017343484796583652\n",
      "[step: 255] loss: 0.00017336380551569164\n",
      "[step: 256] loss: 0.00017329162801615894\n",
      "[step: 257] loss: 0.00017322083294857293\n",
      "[step: 258] loss: 0.0001731492520775646\n",
      "[step: 259] loss: 0.00017307745292782784\n",
      "[step: 260] loss: 0.00017300579929724336\n",
      "[step: 261] loss: 0.0001729338546283543\n",
      "[step: 262] loss: 0.00017286204092670232\n",
      "[step: 263] loss: 0.00017278979066759348\n",
      "[step: 264] loss: 0.0001727176713757217\n",
      "[step: 265] loss: 0.0001726456976030022\n",
      "[step: 266] loss: 0.0001725732727209106\n",
      "[step: 267] loss: 0.0001725009351503104\n",
      "[step: 268] loss: 0.0001724284520605579\n",
      "[step: 269] loss: 0.00017235575069207698\n",
      "[step: 270] loss: 0.00017228357319254428\n",
      "[step: 271] loss: 0.0001722103770589456\n",
      "[step: 272] loss: 0.00017213821411132812\n",
      "[step: 273] loss: 0.00017206527991220355\n",
      "[step: 274] loss: 0.0001719924039207399\n",
      "[step: 275] loss: 0.00017191906226798892\n",
      "[step: 276] loss: 0.0001718456915114075\n",
      "[step: 277] loss: 0.00017177262634504586\n",
      "[step: 278] loss: 0.00017169972124975175\n",
      "[step: 279] loss: 0.00017162651056423783\n",
      "[step: 280] loss: 0.00017155305249616504\n",
      "[step: 281] loss: 0.00017147991457022727\n",
      "[step: 282] loss: 0.000171406427398324\n",
      "[step: 283] loss: 0.0001713325036689639\n",
      "[step: 284] loss: 0.00017125922022387385\n",
      "[step: 285] loss: 0.00017118500545620918\n",
      "[step: 286] loss: 0.00017111147462856025\n",
      "[step: 287] loss: 0.00017103762365877628\n",
      "[step: 288] loss: 0.00017096374358516186\n",
      "[step: 289] loss: 0.00017088974709622562\n",
      "[step: 290] loss: 0.00017081547412090003\n",
      "[step: 291] loss: 0.00017074128845706582\n",
      "[step: 292] loss: 0.00017066691361833364\n",
      "[step: 293] loss: 0.00017059265519492328\n",
      "[step: 294] loss: 0.00017051867325790226\n",
      "[step: 295] loss: 0.00017044416745193303\n",
      "[step: 296] loss: 0.0001703694142634049\n",
      "[step: 297] loss: 0.0001702945155557245\n",
      "[step: 298] loss: 0.00017022019892465323\n",
      "[step: 299] loss: 0.00017014506738632917\n",
      "[step: 300] loss: 0.00017007073620334268\n",
      "[step: 301] loss: 0.0001699957938399166\n",
      "[step: 302] loss: 0.00016992099699564278\n",
      "[step: 303] loss: 0.0001698456471785903\n",
      "[step: 304] loss: 0.000169770180946216\n",
      "[step: 305] loss: 0.00016969531134236604\n",
      "[step: 306] loss: 0.00016962012159638107\n",
      "[step: 307] loss: 0.00016954471357166767\n",
      "[step: 308] loss: 0.00016947014955803752\n",
      "[step: 309] loss: 0.0001693942176643759\n",
      "[step: 310] loss: 0.00016931939171627164\n",
      "[step: 311] loss: 0.0001692435471341014\n",
      "[step: 312] loss: 0.00016916784807108343\n",
      "[step: 313] loss: 0.000169092119904235\n",
      "[step: 314] loss: 0.00016901612980291247\n",
      "[step: 315] loss: 0.0001689412019914016\n",
      "[step: 316] loss: 0.00016886560479179025\n",
      "[step: 317] loss: 0.00016878987662494183\n",
      "[step: 318] loss: 0.0001687135372776538\n",
      "[step: 319] loss: 0.00016863716882653534\n",
      "[step: 320] loss: 0.0001685611205175519\n",
      "[step: 321] loss: 0.000168485552421771\n",
      "[step: 322] loss: 0.00016840899479575455\n",
      "[step: 323] loss: 0.0001683335576672107\n",
      "[step: 324] loss: 0.00016825714556034654\n",
      "[step: 325] loss: 0.00016818050062283874\n",
      "[step: 326] loss: 0.00016810436500236392\n",
      "[step: 327] loss: 0.00016802808386273682\n",
      "[step: 328] loss: 0.0001679509732639417\n",
      "[step: 329] loss: 0.00016787517233751714\n",
      "[step: 330] loss: 0.00016779887664597481\n",
      "[step: 331] loss: 0.00016772183880675584\n",
      "[step: 332] loss: 0.0001676450192462653\n",
      "[step: 333] loss: 0.00016756808327045292\n",
      "[step: 334] loss: 0.00016749170026741922\n",
      "[step: 335] loss: 0.0001674147933954373\n",
      "[step: 336] loss: 0.0001673382066655904\n",
      "[step: 337] loss: 0.00016726047033444047\n",
      "[step: 338] loss: 0.00016718401457183063\n",
      "[step: 339] loss: 0.0001671067875577137\n",
      "[step: 340] loss: 0.00016702990978956223\n",
      "[step: 341] loss: 0.0001669532066443935\n",
      "[step: 342] loss: 0.00016687597963027656\n",
      "[step: 343] loss: 0.00016679846157785505\n",
      "[step: 344] loss: 0.00016672111814841628\n",
      "[step: 345] loss: 0.00016664405120536685\n",
      "[step: 346] loss: 0.0001665666204644367\n",
      "[step: 347] loss: 0.0001664891606196761\n",
      "[step: 348] loss: 0.00016641223919577897\n",
      "[step: 349] loss: 0.00016633473569527268\n",
      "[step: 350] loss: 0.0001662566210143268\n",
      "[step: 351] loss: 0.00016617935034446418\n",
      "[step: 352] loss: 0.00016610139573458582\n",
      "[step: 353] loss: 0.00016602347022853792\n",
      "[step: 354] loss: 0.00016594624321442097\n",
      "[step: 355] loss: 0.00016586881247349083\n",
      "[step: 356] loss: 0.00016579090151935816\n",
      "[step: 357] loss: 0.00016571326705161482\n",
      "[step: 358] loss: 0.00016563473036512733\n",
      "[step: 359] loss: 0.00016555721231270581\n",
      "[step: 360] loss: 0.0001654792868066579\n",
      "[step: 361] loss: 0.00016540134674869478\n",
      "[step: 362] loss: 0.00016532352310605347\n",
      "[step: 363] loss: 0.00016524552484042943\n",
      "[step: 364] loss: 0.0001651669153943658\n",
      "[step: 365] loss: 0.00016508897533640265\n",
      "[step: 366] loss: 0.00016501126810908318\n",
      "[step: 367] loss: 0.00016493236762471497\n",
      "[step: 368] loss: 0.00016485474770888686\n",
      "[step: 369] loss: 0.00016477624012622982\n",
      "[step: 370] loss: 0.0001646979944780469\n",
      "[step: 371] loss: 0.00016461967607028782\n",
      "[step: 372] loss: 0.000164541183039546\n",
      "[step: 373] loss: 0.00016446196241304278\n",
      "[step: 374] loss: 0.00016438461898360401\n",
      "[step: 375] loss: 0.00016430619871243834\n",
      "[step: 376] loss: 0.00016422735643573105\n",
      "[step: 377] loss: 0.00016414854326285422\n",
      "[step: 378] loss: 0.00016406997747253627\n",
      "[step: 379] loss: 0.0001639917609281838\n",
      "[step: 380] loss: 0.00016391274402849376\n",
      "[step: 381] loss: 0.00016383429465349764\n",
      "[step: 382] loss: 0.00016375572886317968\n",
      "[step: 383] loss: 0.00016367642092518508\n",
      "[step: 384] loss: 0.0001635981461731717\n",
      "[step: 385] loss: 0.00016351908561773598\n",
      "[step: 386] loss: 0.00016344035975635052\n",
      "[step: 387] loss: 0.00016336122644133866\n",
      "[step: 388] loss: 0.00016328241326846182\n",
      "[step: 389] loss: 0.00016320403665304184\n",
      "[step: 390] loss: 0.000163125223480165\n",
      "[step: 391] loss: 0.00016304643941111863\n",
      "[step: 392] loss: 0.00016296689864248037\n",
      "[step: 393] loss: 0.0001628878089832142\n",
      "[step: 394] loss: 0.00016280841373372823\n",
      "[step: 395] loss: 0.00016272941138595343\n",
      "[step: 396] loss: 0.00016265097656287253\n",
      "[step: 397] loss: 0.00016257193055935204\n",
      "[step: 398] loss: 0.00016249206964857876\n",
      "[step: 399] loss: 0.0001624134456505999\n",
      "[step: 400] loss: 0.0001623335701879114\n",
      "[step: 401] loss: 0.0001622541167307645\n",
      "[step: 402] loss: 0.00016217540542129427\n",
      "[step: 403] loss: 0.0001620960101718083\n",
      "[step: 404] loss: 0.00016201646940317005\n",
      "[step: 405] loss: 0.00016193778719753027\n",
      "[step: 406] loss: 0.00016185826098080724\n",
      "[step: 407] loss: 0.00016177905490621924\n",
      "[step: 408] loss: 0.00016169948503375053\n",
      "[step: 409] loss: 0.00016161982784979045\n",
      "[step: 410] loss: 0.0001615404908079654\n",
      "[step: 411] loss: 0.0001614605716895312\n",
      "[step: 412] loss: 0.00016138199134729803\n",
      "[step: 413] loss: 0.00016130192670971155\n",
      "[step: 414] loss: 0.00016122241504490376\n",
      "[step: 415] loss: 0.00016114258323796093\n",
      "[step: 416] loss: 0.00016106314433272928\n",
      "[step: 417] loss: 0.00016098347259685397\n",
      "[step: 418] loss: 0.00016090422286652029\n",
      "[step: 419] loss: 0.00016082474030554295\n",
      "[step: 420] loss: 0.00016074498125817627\n",
      "[step: 421] loss: 0.0001606649166205898\n",
      "[step: 422] loss: 0.0001605850993655622\n",
      "[step: 423] loss: 0.00016050526755861938\n",
      "[step: 424] loss: 0.00016042542119976133\n",
      "[step: 425] loss: 0.0001603462005732581\n",
      "[step: 426] loss: 0.00016026620869524777\n",
      "[step: 427] loss: 0.0001601861004019156\n",
      "[step: 428] loss: 0.00016010631225071847\n",
      "[step: 429] loss: 0.00016002662596292794\n",
      "[step: 430] loss: 0.00015994723071344197\n",
      "[step: 431] loss: 0.00015986694779712707\n",
      "[step: 432] loss: 0.00015978762530721724\n",
      "[step: 433] loss: 0.00015970799722708762\n",
      "[step: 434] loss: 0.00015962778707034886\n",
      "[step: 435] loss: 0.0001595477806404233\n",
      "[step: 436] loss: 0.00015946844359859824\n",
      "[step: 437] loss: 0.00015938791329972446\n",
      "[step: 438] loss: 0.00015930840163491666\n",
      "[step: 439] loss: 0.00015922795864753425\n",
      "[step: 440] loss: 0.00015914809773676097\n",
      "[step: 441] loss: 0.00015906835324130952\n",
      "[step: 442] loss: 0.00015898828860372305\n",
      "[step: 443] loss: 0.00015890786016825587\n",
      "[step: 444] loss: 0.00015882814477663487\n",
      "[step: 445] loss: 0.0001587482402101159\n",
      "[step: 446] loss: 0.00015866837929934263\n",
      "[step: 447] loss: 0.0001585882855579257\n",
      "[step: 448] loss: 0.00015850804629735649\n",
      "[step: 449] loss: 0.0001584279234521091\n",
      "[step: 450] loss: 0.0001583480043336749\n",
      "[step: 451] loss: 0.00015826788148842752\n",
      "[step: 452] loss: 0.00015818735118955374\n",
      "[step: 453] loss: 0.00015810765034984797\n",
      "[step: 454] loss: 0.00015802806592546403\n",
      "[step: 455] loss: 0.00015794720093254\n",
      "[step: 456] loss: 0.00015786731091793627\n",
      "[step: 457] loss: 0.0001577875518705696\n",
      "[step: 458] loss: 0.00015770741447340697\n",
      "[step: 459] loss: 0.00015762676775921136\n",
      "[step: 460] loss: 0.00015754673222545534\n",
      "[step: 461] loss: 0.00015746636199764907\n",
      "[step: 462] loss: 0.00015738637011963874\n",
      "[step: 463] loss: 0.00015730691666249186\n",
      "[step: 464] loss: 0.0001572260953253135\n",
      "[step: 465] loss: 0.0001571461616549641\n",
      "[step: 466] loss: 0.00015706509293522686\n",
      "[step: 467] loss: 0.00015698526112828404\n",
      "[step: 468] loss: 0.00015690538566559553\n",
      "[step: 469] loss: 0.0001568252337165177\n",
      "[step: 470] loss: 0.00015674451424274594\n",
      "[step: 471] loss: 0.00015666460967622697\n",
      "[step: 472] loss: 0.00015658477786928415\n",
      "[step: 473] loss: 0.00015650407294742763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 474] loss: 0.0001564247941132635\n",
      "[step: 475] loss: 0.00015634414739906788\n",
      "[step: 476] loss: 0.0001562640245538205\n",
      "[step: 477] loss: 0.00015618366887792945\n",
      "[step: 478] loss: 0.00015610340051352978\n",
      "[step: 479] loss: 0.00015602345229126513\n",
      "[step: 480] loss: 0.0001559431984787807\n",
      "[step: 481] loss: 0.00015586278459522873\n",
      "[step: 482] loss: 0.00015578274906147271\n",
      "[step: 483] loss: 0.000155702349729836\n",
      "[step: 484] loss: 0.0001556223287479952\n",
      "[step: 485] loss: 0.00015554176934529096\n",
      "[step: 486] loss: 0.00015546187933068722\n",
      "[step: 487] loss: 0.0001553815818624571\n",
      "[step: 488] loss: 0.00015530124073848128\n",
      "[step: 489] loss: 0.00015522098692599684\n",
      "[step: 490] loss: 0.00015514127153437585\n",
      "[step: 491] loss: 0.0001550607557874173\n",
      "[step: 492] loss: 0.00015498082211706787\n",
      "[step: 493] loss: 0.0001549001899547875\n",
      "[step: 494] loss: 0.00015482035814784467\n",
      "[step: 495] loss: 0.00015474010433536023\n",
      "[step: 496] loss: 0.00015465989417862147\n",
      "[step: 497] loss: 0.00015457873814739287\n",
      "[step: 498] loss: 0.000154499604832381\n",
      "[step: 499] loss: 0.00015441901632584631\n",
      "[step: 500] loss: 0.00015433902444783598\n",
      "[step: 501] loss: 0.00015425877063535154\n",
      "[step: 502] loss: 0.00015417860413435847\n",
      "[step: 503] loss: 0.00015409836487378925\n",
      "[step: 504] loss: 0.00015401814016513526\n",
      "[step: 505] loss: 0.00015393781359307468\n",
      "[step: 506] loss: 0.00015385803999379277\n",
      "[step: 507] loss: 0.00015377739327959716\n",
      "[step: 508] loss: 0.00015369747416116297\n",
      "[step: 509] loss: 0.0001536165364086628\n",
      "[step: 510] loss: 0.00015353719936683774\n",
      "[step: 511] loss: 0.00015345729480031878\n",
      "[step: 512] loss: 0.0001533766626380384\n",
      "[step: 513] loss: 0.00015329639427363873\n",
      "[step: 514] loss: 0.00015321691171266139\n",
      "[step: 515] loss: 0.00015313657058868557\n",
      "[step: 516] loss: 0.00015305622946470976\n",
      "[step: 517] loss: 0.00015297665959224105\n",
      "[step: 518] loss: 0.00015289672592189163\n",
      "[step: 519] loss: 0.00015281641390174627\n",
      "[step: 520] loss: 0.00015273605822585523\n",
      "[step: 521] loss: 0.00015265669208019972\n",
      "[step: 522] loss: 0.00015257630730047822\n",
      "[step: 523] loss: 0.00015249583520926535\n",
      "[step: 524] loss: 0.00015241559594869614\n",
      "[step: 525] loss: 0.00015233604062814265\n",
      "[step: 526] loss: 0.00015225580136757344\n",
      "[step: 527] loss: 0.00015217573672998697\n",
      "[step: 528] loss: 0.00015209600678645074\n",
      "[step: 529] loss: 0.00015201570931822062\n",
      "[step: 530] loss: 0.00015193625586107373\n",
      "[step: 531] loss: 0.00015185619122348726\n",
      "[step: 532] loss: 0.00015177621389739215\n",
      "[step: 533] loss: 0.00015169610560406\n",
      "[step: 534] loss: 0.00015161634655669332\n",
      "[step: 535] loss: 0.0001515362091595307\n",
      "[step: 536] loss: 0.00015145642100833356\n",
      "[step: 537] loss: 0.00015137670561671257\n",
      "[step: 538] loss: 0.00015129658277146518\n",
      "[step: 539] loss: 0.00015121680917218328\n",
      "[step: 540] loss: 0.00015113662811927497\n",
      "[step: 541] loss: 0.00015105708735063672\n",
      "[step: 542] loss: 0.00015097706636879593\n",
      "[step: 543] loss: 0.00015089787484612316\n",
      "[step: 544] loss: 0.00015081778110470623\n",
      "[step: 545] loss: 0.00015073787653818727\n",
      "[step: 546] loss: 0.00015065798652358353\n",
      "[step: 547] loss: 0.00015057808195706457\n",
      "[step: 548] loss: 0.00015049867215566337\n",
      "[step: 549] loss: 0.00015041916049085557\n",
      "[step: 550] loss: 0.00015033913950901479\n",
      "[step: 551] loss: 0.00015025974425952882\n",
      "[step: 552] loss: 0.00015017992700450122\n",
      "[step: 553] loss: 0.00015010025526862592\n",
      "[step: 554] loss: 0.00015002069994807243\n",
      "[step: 555] loss: 0.0001499410136602819\n",
      "[step: 556] loss: 0.0001498614001320675\n",
      "[step: 557] loss: 0.00014978153922129422\n",
      "[step: 558] loss: 0.00014970228949096054\n",
      "[step: 559] loss: 0.00014962295244913548\n",
      "[step: 560] loss: 0.00014954301877878606\n",
      "[step: 561] loss: 0.0001494632160756737\n",
      "[step: 562] loss: 0.0001493835006840527\n",
      "[step: 563] loss: 0.00014930471661500633\n",
      "[step: 564] loss: 0.00014922521950211376\n",
      "[step: 565] loss: 0.00014914575149305165\n",
      "[step: 566] loss: 0.00014906660362612456\n",
      "[step: 567] loss: 0.00014898681547492743\n",
      "[step: 568] loss: 0.00014890727470628917\n",
      "[step: 569] loss: 0.00014882786490488797\n",
      "[step: 570] loss: 0.0001487484114477411\n",
      "[step: 571] loss: 0.00014866924902889878\n",
      "[step: 572] loss: 0.00014858972281217575\n",
      "[step: 573] loss: 0.00014851050218567252\n",
      "[step: 574] loss: 0.00014843173266854137\n",
      "[step: 575] loss: 0.00014835249749012291\n",
      "[step: 576] loss: 0.00014827268023509532\n",
      "[step: 577] loss: 0.0001481941872043535\n",
      "[step: 578] loss: 0.0001481142535340041\n",
      "[step: 579] loss: 0.00014803488738834858\n",
      "[step: 580] loss: 0.0001479561033193022\n",
      "[step: 581] loss: 0.00014787692634854466\n",
      "[step: 582] loss: 0.000147797487443313\n",
      "[step: 583] loss: 0.00014771857240702957\n",
      "[step: 584] loss: 0.00014763977378606796\n",
      "[step: 585] loss: 0.0001475602330174297\n",
      "[step: 586] loss: 0.00014748176909051836\n",
      "[step: 587] loss: 0.0001474026357755065\n",
      "[step: 588] loss: 0.00014732380805071443\n",
      "[step: 589] loss: 0.00014724416541866958\n",
      "[step: 590] loss: 0.00014716561418026686\n",
      "[step: 591] loss: 0.00014708645176142454\n",
      "[step: 592] loss: 0.00014700760948471725\n",
      "[step: 593] loss: 0.00014692921831738204\n",
      "[step: 594] loss: 0.00014685012865811586\n",
      "[step: 595] loss: 0.00014677096623927355\n",
      "[step: 596] loss: 0.00014669210941065103\n",
      "[step: 597] loss: 0.00014661399472970515\n",
      "[step: 598] loss: 0.00014653529797215015\n",
      "[step: 599] loss: 0.00014645620831288397\n",
      "[step: 600] loss: 0.00014637724962085485\n",
      "[step: 601] loss: 0.0001462987274862826\n",
      "[step: 602] loss: 0.0001462201471440494\n",
      "[step: 603] loss: 0.00014614092651754618\n",
      "[step: 604] loss: 0.00014606268086936325\n",
      "[step: 605] loss: 0.00014598382404074073\n",
      "[step: 606] loss: 0.00014590565115213394\n",
      "[step: 607] loss: 0.00014582695439457893\n",
      "[step: 608] loss: 0.000145748199429363\n",
      "[step: 609] loss: 0.00014566988102160394\n",
      "[step: 610] loss: 0.00014559115516021848\n",
      "[step: 611] loss: 0.0001455132442060858\n",
      "[step: 612] loss: 0.00014543456200044602\n",
      "[step: 613] loss: 0.00014535634545609355\n",
      "[step: 614] loss: 0.00014527738676406443\n",
      "[step: 615] loss: 0.00014519909746013582\n",
      "[step: 616] loss: 0.00014512112829834223\n",
      "[step: 617] loss: 0.00014504275168292224\n",
      "[step: 618] loss: 0.0001449648116249591\n",
      "[step: 619] loss: 0.00014488663873635232\n",
      "[step: 620] loss: 0.00014480861136689782\n",
      "[step: 621] loss: 0.00014472981274593621\n",
      "[step: 622] loss: 0.00014465212007053196\n",
      "[step: 623] loss: 0.00014457394718192518\n",
      "[step: 624] loss: 0.00014449591981247067\n",
      "[step: 625] loss: 0.00014441742678172886\n",
      "[step: 626] loss: 0.00014433989417739213\n",
      "[step: 627] loss: 0.0001442618086002767\n",
      "[step: 628] loss: 0.00014418365026358515\n",
      "[step: 629] loss: 0.00014410597214009613\n",
      "[step: 630] loss: 0.00014402740634977818\n",
      "[step: 631] loss: 0.0001439497573301196\n",
      "[step: 632] loss: 0.00014387184637598693\n",
      "[step: 633] loss: 0.0001437944156350568\n",
      "[step: 634] loss: 0.00014371640281751752\n",
      "[step: 635] loss: 0.00014363895752467215\n",
      "[step: 636] loss: 0.00014356087194755673\n",
      "[step: 637] loss: 0.0001434832374798134\n",
      "[step: 638] loss: 0.00014340542838908732\n",
      "[step: 639] loss: 0.0001433276484021917\n",
      "[step: 640] loss: 0.0001432499848306179\n",
      "[step: 641] loss: 0.00014317268505692482\n",
      "[step: 642] loss: 0.00014309491962194443\n",
      "[step: 643] loss: 0.00014301776536740363\n",
      "[step: 644] loss: 0.00014293970889411867\n",
      "[step: 645] loss: 0.00014286240912042558\n",
      "[step: 646] loss: 0.00014278483286034316\n",
      "[step: 647] loss: 0.00014270757674239576\n",
      "[step: 648] loss: 0.00014263021876104176\n",
      "[step: 649] loss: 0.0001425529335392639\n",
      "[step: 650] loss: 0.00014247548824641854\n",
      "[step: 651] loss: 0.00014239789743442088\n",
      "[step: 652] loss: 0.00014232064131647348\n",
      "[step: 653] loss: 0.00014224345795810223\n",
      "[step: 654] loss: 0.0001421661872882396\n",
      "[step: 655] loss: 0.000142089236760512\n",
      "[step: 656] loss: 0.00014201222802512348\n",
      "[step: 657] loss: 0.00014193510287441313\n",
      "[step: 658] loss: 0.0001418577739968896\n",
      "[step: 659] loss: 0.00014178093988448381\n",
      "[step: 660] loss: 0.00014170394570101053\n",
      "[step: 661] loss: 0.0001416267768945545\n",
      "[step: 662] loss: 0.00014154960808809847\n",
      "[step: 663] loss: 0.00014147270121611655\n",
      "[step: 664] loss: 0.00014139630366116762\n",
      "[step: 665] loss: 0.00014131875650491565\n",
      "[step: 666] loss: 0.00014124262088444084\n",
      "[step: 667] loss: 0.0001411660632584244\n",
      "[step: 668] loss: 0.0001410886470694095\n",
      "[step: 669] loss: 0.00014101248234510422\n",
      "[step: 670] loss: 0.0001409356773365289\n",
      "[step: 671] loss: 0.00014085879956837744\n",
      "[step: 672] loss: 0.0001407828531228006\n",
      "[step: 673] loss: 0.0001407061645295471\n",
      "[step: 674] loss: 0.00014062946138437837\n",
      "[step: 675] loss: 0.00014055325300432742\n",
      "[step: 676] loss: 0.00014047662261873484\n",
      "[step: 677] loss: 0.00014039999223314226\n",
      "[step: 678] loss: 0.00014032403123565018\n",
      "[step: 679] loss: 0.00014024748816154897\n",
      "[step: 680] loss: 0.00014017103239893913\n",
      "[step: 681] loss: 0.0001400945766363293\n",
      "[step: 682] loss: 0.00014001836825627834\n",
      "[step: 683] loss: 0.00013994226173963398\n",
      "[step: 684] loss: 0.00013986651902087033\n",
      "[step: 685] loss: 0.00013979000505059958\n",
      "[step: 686] loss: 0.0001397137821186334\n",
      "[step: 687] loss: 0.00013963814126327634\n",
      "[step: 688] loss: 0.00013956197653897107\n",
      "[step: 689] loss: 0.00013948572450317442\n",
      "[step: 690] loss: 0.00013941028737463057\n",
      "[step: 691] loss: 0.00013933441368862987\n",
      "[step: 692] loss: 0.0001392585108987987\n",
      "[step: 693] loss: 0.0001391823752783239\n",
      "[step: 694] loss: 0.00013910673442296684\n",
      "[step: 695] loss: 0.0001390304823871702\n",
      "[step: 696] loss: 0.00013895469601266086\n",
      "[step: 697] loss: 0.0001388787932228297\n",
      "[step: 698] loss: 0.0001388034870615229\n",
      "[step: 699] loss: 0.00013872784620616585\n",
      "[step: 700] loss: 0.0001386525691486895\n",
      "[step: 701] loss: 0.00013857695739716291\n",
      "[step: 702] loss: 0.00013850124378222972\n",
      "[step: 703] loss: 0.0001384254137519747\n",
      "[step: 704] loss: 0.00013835052959620953\n",
      "[step: 705] loss: 0.00013827503426000476\n",
      "[step: 706] loss: 0.00013819916057400405\n",
      "[step: 707] loss: 0.0001381242909701541\n",
      "[step: 708] loss: 0.00013804881018586457\n",
      "[step: 709] loss: 0.00013797401334159076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 710] loss: 0.00013789829972665757\n",
      "[step: 711] loss: 0.00013782294990960509\n",
      "[step: 712] loss: 0.0001377481676172465\n",
      "[step: 713] loss: 0.00013767299242317677\n",
      "[step: 714] loss: 0.00013759805005975068\n",
      "[step: 715] loss: 0.0001375229039695114\n",
      "[step: 716] loss: 0.00013744787429459393\n",
      "[step: 717] loss: 0.00013737328117713332\n",
      "[step: 718] loss: 0.00013729778584092855\n",
      "[step: 719] loss: 0.0001372231199638918\n",
      "[step: 720] loss: 0.00013714810484088957\n",
      "[step: 721] loss: 0.00013707307516597211\n",
      "[step: 722] loss: 0.00013699846749659628\n",
      "[step: 723] loss: 0.00013692420907318592\n",
      "[step: 724] loss: 0.00013684984878636897\n",
      "[step: 725] loss: 0.00013677464448846877\n",
      "[step: 726] loss: 0.00013669989129994065\n",
      "[step: 727] loss: 0.0001366256328765303\n",
      "[step: 728] loss: 0.00013655057409778237\n",
      "[step: 729] loss: 0.00013647685409523547\n",
      "[step: 730] loss: 0.0001364022318739444\n",
      "[step: 731] loss: 0.00013632747868541628\n",
      "[step: 732] loss: 0.00013625359861180186\n",
      "[step: 733] loss: 0.00013617890363093466\n",
      "[step: 734] loss: 0.00013610502355732024\n",
      "[step: 735] loss: 0.00013603089610114694\n",
      "[step: 736] loss: 0.00013595653581432998\n",
      "[step: 737] loss: 0.00013588249566964805\n",
      "[step: 738] loss: 0.00013580784434452653\n",
      "[step: 739] loss: 0.00013573405158240348\n",
      "[step: 740] loss: 0.0001356600841972977\n",
      "[step: 741] loss: 0.00013558589853346348\n",
      "[step: 742] loss: 0.00013551174197345972\n",
      "[step: 743] loss: 0.00013543851673603058\n",
      "[step: 744] loss: 0.00013536387996282429\n",
      "[step: 745] loss: 0.00013529066927731037\n",
      "[step: 746] loss: 0.0001352167164441198\n",
      "[step: 747] loss: 0.00013514244346879423\n",
      "[step: 748] loss: 0.00013506945106200874\n",
      "[step: 749] loss: 0.0001349956582998857\n",
      "[step: 750] loss: 0.0001349221565760672\n",
      "[step: 751] loss: 0.00013484830560628325\n",
      "[step: 752] loss: 0.0001347751240245998\n",
      "[step: 753] loss: 0.00013470165140461177\n",
      "[step: 754] loss: 0.00013462812057696283\n",
      "[step: 755] loss: 0.00013455457519739866\n",
      "[step: 756] loss: 0.00013448130630422384\n",
      "[step: 757] loss: 0.00013440809561870992\n",
      "[step: 758] loss: 0.00013433466665446758\n",
      "[step: 759] loss: 0.0001342612667940557\n",
      "[step: 760] loss: 0.00013418823073152453\n",
      "[step: 761] loss: 0.0001341151655651629\n",
      "[step: 762] loss: 0.00013404204219114035\n",
      "[step: 763] loss: 0.00013396874419413507\n",
      "[step: 764] loss: 0.0001338961737928912\n",
      "[step: 765] loss: 0.00013382264296524227\n",
      "[step: 766] loss: 0.00013374999980442226\n",
      "[step: 767] loss: 0.00013367706560529768\n",
      "[step: 768] loss: 0.00013360446610022336\n",
      "[step: 769] loss: 0.0001335314300376922\n",
      "[step: 770] loss: 0.00013345829211175442\n",
      "[step: 771] loss: 0.00013338611461222172\n",
      "[step: 772] loss: 0.00013331318041309714\n",
      "[step: 773] loss: 0.0001332407700829208\n",
      "[step: 774] loss: 0.000133168141474016\n",
      "[step: 775] loss: 0.00013309568748809397\n",
      "[step: 776] loss: 0.00013302314619068056\n",
      "[step: 777] loss: 0.000132950532133691\n",
      "[step: 778] loss: 0.00013287791807670146\n",
      "[step: 779] loss: 0.00013280582788866013\n",
      "[step: 780] loss: 0.00013273354852572083\n",
      "[step: 781] loss: 0.00013266096357256174\n",
      "[step: 782] loss: 0.00013258869876153767\n",
      "[step: 783] loss: 0.00013251630298327655\n",
      "[step: 784] loss: 0.00013244414003565907\n",
      "[step: 785] loss: 0.00013237222447060049\n",
      "[step: 786] loss: 0.00013230014883447438\n",
      "[step: 787] loss: 0.0001322283933404833\n",
      "[step: 788] loss: 0.00013215612852945924\n",
      "[step: 789] loss: 0.00013208430027589202\n",
      "[step: 790] loss: 0.00013201234105508775\n",
      "[step: 791] loss: 0.00013194078928790987\n",
      "[step: 792] loss: 0.00013186890282668173\n",
      "[step: 793] loss: 0.00013179739471524954\n",
      "[step: 794] loss: 0.00013172498438507318\n",
      "[step: 795] loss: 0.00013165381096769124\n",
      "[step: 796] loss: 0.00013158198271412402\n",
      "[step: 797] loss: 0.00013151053281035274\n",
      "[step: 798] loss: 0.00013143912656232715\n",
      "[step: 799] loss: 0.00013136740017216653\n",
      "[step: 800] loss: 0.00013129600847605616\n",
      "[step: 801] loss: 0.00013122450036462396\n",
      "[step: 802] loss: 0.00013115315232425928\n",
      "[step: 803] loss: 0.0001310821098741144\n",
      "[step: 804] loss: 0.00013101041258778423\n",
      "[step: 805] loss: 0.00013093944289721549\n",
      "[step: 806] loss: 0.0001308684586547315\n",
      "[step: 807] loss: 0.0001307973579969257\n",
      "[step: 808] loss: 0.00013072606816422194\n",
      "[step: 809] loss: 0.0001306557678617537\n",
      "[step: 810] loss: 0.00013058408512733877\n",
      "[step: 811] loss: 0.00013051362475380301\n",
      "[step: 812] loss: 0.00013044255319982767\n",
      "[step: 813] loss: 0.00013037184544373304\n",
      "[step: 814] loss: 0.00013030075933784246\n",
      "[step: 815] loss: 0.0001302299788221717\n",
      "[step: 816] loss: 0.00013015922741033137\n",
      "[step: 817] loss: 0.00013008849055040628\n",
      "[step: 818] loss: 0.0001300178118981421\n",
      "[step: 819] loss: 0.00012994716234970838\n",
      "[step: 820] loss: 0.00012987661466468126\n",
      "[step: 821] loss: 0.0001298060524277389\n",
      "[step: 822] loss: 0.00012973579578101635\n",
      "[step: 823] loss: 0.0001296656992053613\n",
      "[step: 824] loss: 0.0001295956171816215\n",
      "[step: 825] loss: 0.00012952540419064462\n",
      "[step: 826] loss: 0.00012945504568051547\n",
      "[step: 827] loss: 0.0001293844688916579\n",
      "[step: 828] loss: 0.00012931438686791807\n",
      "[step: 829] loss: 0.00012924429029226303\n",
      "[step: 830] loss: 0.00012917375715915114\n",
      "[step: 831] loss: 0.0001291039225179702\n",
      "[step: 832] loss: 0.0001290343643631786\n",
      "[step: 833] loss: 0.00012896460248157382\n",
      "[step: 834] loss: 0.00012889440404251218\n",
      "[step: 835] loss: 0.00012882465671282262\n",
      "[step: 836] loss: 0.00012875456013716757\n",
      "[step: 837] loss: 0.00012868456542491913\n",
      "[step: 838] loss: 0.00012861548748333007\n",
      "[step: 839] loss: 0.00012854601664002985\n",
      "[step: 840] loss: 0.00012847637117374688\n",
      "[step: 841] loss: 0.0001284063619095832\n",
      "[step: 842] loss: 0.00012833718210458755\n",
      "[step: 843] loss: 0.00012826756574213505\n",
      "[step: 844] loss: 0.00012819815310649574\n",
      "[step: 845] loss: 0.00012812924978788942\n",
      "[step: 846] loss: 0.00012805950245819986\n",
      "[step: 847] loss: 0.00012799054093193263\n",
      "[step: 848] loss: 0.00012792125926353037\n",
      "[step: 849] loss: 0.00012785207945853472\n",
      "[step: 850] loss: 0.0001277828705497086\n",
      "[step: 851] loss: 0.0001277135161217302\n",
      "[step: 852] loss: 0.00012764448183588684\n",
      "[step: 853] loss: 0.0001275759859709069\n",
      "[step: 854] loss: 0.00012750684982165694\n",
      "[step: 855] loss: 0.0001274376263609156\n",
      "[step: 856] loss: 0.00012736876669805497\n",
      "[step: 857] loss: 0.00012730062007904053\n",
      "[step: 858] loss: 0.00012723107647616416\n",
      "[step: 859] loss: 0.00012716253695543855\n",
      "[step: 860] loss: 0.00012709427392110229\n",
      "[step: 861] loss: 0.00012702576350420713\n",
      "[step: 862] loss: 0.00012695712212007493\n",
      "[step: 863] loss: 0.00012688861170317978\n",
      "[step: 864] loss: 0.00012681969383265823\n",
      "[step: 865] loss: 0.00012675163452513516\n",
      "[step: 866] loss: 0.0001266829203814268\n",
      "[step: 867] loss: 0.00012661523942369968\n",
      "[step: 868] loss: 0.00012654662714339793\n",
      "[step: 869] loss: 0.00012647808762267232\n",
      "[step: 870] loss: 0.00012641007197089493\n",
      "[step: 871] loss: 0.00012634258018806577\n",
      "[step: 872] loss: 0.00012627386604435742\n",
      "[step: 873] loss: 0.00012620628695003688\n",
      "[step: 874] loss: 0.00012613834405783564\n",
      "[step: 875] loss: 0.00012607002281583846\n",
      "[step: 876] loss: 0.0001260021817870438\n",
      "[step: 877] loss: 0.00012593460269272327\n",
      "[step: 878] loss: 0.00012586714001372457\n",
      "[step: 879] loss: 0.00012579889153130352\n",
      "[step: 880] loss: 0.00012573144340422004\n",
      "[step: 881] loss: 0.00012566399527713656\n",
      "[step: 882] loss: 0.00012559600872918963\n",
      "[step: 883] loss: 0.00012552857515402138\n",
      "[step: 884] loss: 0.00012546109792310745\n",
      "[step: 885] loss: 0.0001253935624845326\n",
      "[step: 886] loss: 0.00012532580876722932\n",
      "[step: 887] loss: 0.0001252589572686702\n",
      "[step: 888] loss: 0.00012519166921265423\n",
      "[step: 889] loss: 0.0001251240901183337\n",
      "[step: 890] loss: 0.00012505741324275732\n",
      "[step: 891] loss: 0.00012498993601184338\n",
      "[step: 892] loss: 0.00012492321548052132\n",
      "[step: 893] loss: 0.00012485592742450535\n",
      "[step: 894] loss: 0.0001247891050297767\n",
      "[step: 895] loss: 0.00012472181697376072\n",
      "[step: 896] loss: 0.00012465518375393003\n",
      "[step: 897] loss: 0.0001245881139766425\n",
      "[step: 898] loss: 0.000124521175166592\n",
      "[step: 899] loss: 0.00012445465836208314\n",
      "[step: 900] loss: 0.00012438767589628696\n",
      "[step: 901] loss: 0.00012432118819560856\n",
      "[step: 902] loss: 0.00012425440945662558\n",
      "[step: 903] loss: 0.0001241877762367949\n",
      "[step: 904] loss: 0.00012412146315909922\n",
      "[step: 905] loss: 0.0001240550773218274\n",
      "[step: 906] loss: 0.00012398890976328403\n",
      "[step: 907] loss: 0.00012392224743962288\n",
      "[step: 908] loss: 0.00012385600712150335\n",
      "[step: 909] loss: 0.00012378951942082494\n",
      "[step: 910] loss: 0.00012372355558909476\n",
      "[step: 911] loss: 0.00012365700968075544\n",
      "[step: 912] loss: 0.00012359113316051662\n",
      "[step: 913] loss: 0.00012352538760751486\n",
      "[step: 914] loss: 0.00012345885625109076\n",
      "[step: 915] loss: 0.00012339292152319103\n",
      "[step: 916] loss: 0.00012332724872976542\n",
      "[step: 917] loss: 0.00012326092110015452\n",
      "[step: 918] loss: 0.00012319564120844007\n",
      "[step: 919] loss: 0.0001231294882018119\n",
      "[step: 920] loss: 0.00012306365533731878\n",
      "[step: 921] loss: 0.0001229982590302825\n",
      "[step: 922] loss: 0.00012293245526961982\n",
      "[step: 923] loss: 0.00012286694254726171\n",
      "[step: 924] loss: 0.00012280125520192087\n",
      "[step: 925] loss: 0.00012273635366000235\n",
      "[step: 926] loss: 0.00012267063721083105\n",
      "[step: 927] loss: 0.00012260529911145568\n",
      "[step: 928] loss: 0.00012253946624696255\n",
      "[step: 929] loss: 0.00012247409904375672\n",
      "[step: 930] loss: 0.00012240911019034684\n",
      "[step: 931] loss: 0.00012234400492161512\n",
      "[step: 932] loss: 0.00012227856495883316\n",
      "[step: 933] loss: 0.00012221340148244053\n",
      "[step: 934] loss: 0.00012214847083669156\n",
      "[step: 935] loss: 0.00012208391854073852\n",
      "[step: 936] loss: 0.00012201862409710884\n",
      "[step: 937] loss: 0.00012195392628200352\n",
      "[step: 938] loss: 0.00012188905384391546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 939] loss: 0.00012182358477730304\n",
      "[step: 940] loss: 0.00012175946176284924\n",
      "[step: 941] loss: 0.00012169477849965915\n",
      "[step: 942] loss: 0.0001216302189277485\n",
      "[step: 943] loss: 0.00012156579759903252\n",
      "[step: 944] loss: 0.00012150072143413126\n",
      "[step: 945] loss: 0.00012143670755904168\n",
      "[step: 946] loss: 0.00012137237354181707\n",
      "[step: 947] loss: 0.00012130763207096606\n",
      "[step: 948] loss: 0.00012124377826694399\n",
      "[step: 949] loss: 0.00012117883306927979\n",
      "[step: 950] loss: 0.00012111471005482599\n",
      "[step: 951] loss: 0.00012105047790100798\n",
      "[step: 952] loss: 0.00012098644947400317\n",
      "[step: 953] loss: 0.00012092229735571891\n",
      "[step: 954] loss: 0.00012085798516636714\n",
      "[step: 955] loss: 0.00012079460429958999\n",
      "[step: 956] loss: 0.00012073005927959457\n",
      "[step: 957] loss: 0.00012066659110132605\n",
      "[step: 958] loss: 0.00012060246081091464\n",
      "[step: 959] loss: 0.00012053885438945144\n",
      "[step: 960] loss: 0.00012047457130393013\n",
      "[step: 961] loss: 0.00012041134323226288\n",
      "[step: 962] loss: 0.00012034771498292685\n",
      "[step: 963] loss: 0.00012028432684019208\n",
      "[step: 964] loss: 0.00012022030568914488\n",
      "[step: 965] loss: 0.0001201569612021558\n",
      "[step: 966] loss: 0.0001200935075758025\n",
      "[step: 967] loss: 0.0001200299957417883\n",
      "[step: 968] loss: 0.00011996677494607866\n",
      "[step: 969] loss: 0.00011990369239356369\n",
      "[step: 970] loss: 0.00011984005686827004\n",
      "[step: 971] loss: 0.00011977698159171268\n",
      "[step: 972] loss: 0.00011971380445174873\n",
      "[step: 973] loss: 0.00011965083103859797\n",
      "[step: 974] loss: 0.0001195876975543797\n",
      "[step: 975] loss: 0.00011952481872867793\n",
      "[step: 976] loss: 0.00011946191807510331\n",
      "[step: 977] loss: 0.00011939827527385205\n",
      "[step: 978] loss: 0.00011933624045923352\n",
      "[step: 979] loss: 0.00011927293235203251\n",
      "[step: 980] loss: 0.00011921070836251602\n",
      "[step: 981] loss: 0.0001191479605040513\n",
      "[step: 982] loss: 0.00011908516171388328\n",
      "[step: 983] loss: 0.00011902261030627415\n",
      "[step: 984] loss: 0.00011895981151610613\n",
      "[step: 985] loss: 0.00011889716552104801\n",
      "[step: 986] loss: 0.00011883487604791299\n",
      "[step: 987] loss: 0.00011877234646817669\n",
      "[step: 988] loss: 0.00011871004244312644\n",
      "[step: 989] loss: 0.00011864777479786426\n",
      "[step: 990] loss: 0.00011858568177558482\n",
      "[step: 991] loss: 0.00011852321040350944\n",
      "[step: 992] loss: 0.00011846165580209345\n",
      "[step: 993] loss: 0.00011839930812129751\n",
      "[step: 994] loss: 0.00011833700409624726\n",
      "[step: 995] loss: 0.00011827529669972137\n",
      "[step: 996] loss: 0.00011821322550531477\n",
      "[step: 997] loss: 0.0001181509142043069\n",
      "[step: 998] loss: 0.00011808911949628964\n",
      "[step: 999] loss: 0.0001180277977255173\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "testY is:  [7608.3999999999996, 8706.1935480000011, 26366.56667, 24504.89286, 2169.0322579999997, 1879.0, 2449.5483870000003, 1487.9000000000001, 1903.580645, 3121.333333, 8773.6333329999998, 9429.2903230000011, 4370.5666670000001, 4122.7096770000007, 7644.7419349999991, 4147.0740740000001, 1799.7096770000001, 1405.5]\n",
      "\n",
      "\n",
      "LSTM forecast : [4050.7673279054275, 5184.3861050170581, 6091.7471532539894, 4402.2775867642704, 4703.6441318440766, 3826.4870946274509, 4642.8415266863813, 4564.9929856119916, 6605.1422413942018, 4582.3966074943482, 6319.1194129743417, 11846.959670044154, 9521.1970806631234, 6976.2010183939237, 4695.5856739687815, 6002.2928233476377, 3789.9876016300309, 4304.3978721280919] \n",
      "@@@@@LSTM rmse:  7303.05893431\n",
      "Bayseian forecast : [5574.6306211197752, 6825.0964678457203, 15690.016379008404, 34640.291534109114, 552.68107357991425, 2903.1399223308099, 2073.372668614033, 1885.4694330370753, 3738.8512278303956, 4277.7795062398836, 7623.9433510119416, 4114.5911007904315, 4596.4310600724884, 5828.0609886824741, 14797.607636456994, 44449.632054847112, 325.7862720932709, 2589.4681973141514] \n",
      "@@@@@Bayseian rmse:  10393.9548052\n",
      "\n",
      "\n",
      "LSTM WON!!!!!!\n",
      "[step: 0] loss: 5.600452899932861\n",
      "[step: 1] loss: 2.5085997581481934\n",
      "[step: 2] loss: 0.819968044757843\n",
      "[step: 3] loss: 0.179527148604393\n",
      "[step: 4] loss: 0.1946897804737091\n",
      "[step: 5] loss: 0.5001175999641418\n",
      "[step: 6] loss: 0.7914791703224182\n",
      "[step: 7] loss: 0.9136455655097961\n",
      "[step: 8] loss: 0.8620606660842896\n",
      "[step: 9] loss: 0.6999630331993103\n",
      "[step: 10] loss: 0.49635010957717896\n",
      "[step: 11] loss: 0.3033240735530853\n",
      "[step: 12] loss: 0.15285560488700867\n",
      "[step: 13] loss: 0.059148482978343964\n",
      "[step: 14] loss: 0.021852215752005577\n",
      "[step: 15] loss: 0.029835863038897514\n",
      "[step: 16] loss: 0.06572083383798599\n",
      "[step: 17] loss: 0.11077199876308441\n",
      "[step: 18] loss: 0.14938391745090485\n",
      "[step: 19] loss: 0.17209072411060333\n",
      "[step: 20] loss: 0.17613893747329712\n",
      "[step: 21] loss: 0.1639156937599182\n",
      "[step: 22] loss: 0.14049014449119568\n",
      "[step: 23] loss: 0.1114753782749176\n",
      "[step: 24] loss: 0.08173321187496185\n",
      "[step: 25] loss: 0.05484266206622124\n",
      "[step: 26] loss: 0.0330718569457531\n",
      "[step: 27] loss: 0.017608387395739555\n",
      "[step: 28] loss: 0.008829998783767223\n",
      "[step: 29] loss: 0.006456155329942703\n",
      "[step: 30] loss: 0.00955555122345686\n",
      "[step: 31] loss: 0.01651824079453945\n",
      "[step: 32] loss: 0.025153575465083122\n",
      "[step: 33] loss: 0.033024128526449203\n",
      "[step: 34] loss: 0.03799512982368469\n",
      "[step: 35] loss: 0.03881183639168739\n",
      "[step: 36] loss: 0.03542601317167282\n",
      "[step: 37] loss: 0.028896836563944817\n",
      "[step: 38] loss: 0.02092340774834156\n",
      "[step: 39] loss: 0.013242846354842186\n",
      "[step: 40] loss: 0.007145953364670277\n",
      "[step: 41] loss: 0.0032572464551776648\n",
      "[step: 42] loss: 0.0015796847874298692\n",
      "[step: 43] loss: 0.001697394996881485\n",
      "[step: 44] loss: 0.003003931837156415\n",
      "[step: 45] loss: 0.00487155094742775\n",
      "[step: 46] loss: 0.0067472923547029495\n",
      "[step: 47] loss: 0.008202618919312954\n",
      "[step: 48] loss: 0.00896237138658762\n",
      "[step: 49] loss: 0.008918747305870056\n",
      "[step: 50] loss: 0.008124090731143951\n",
      "[step: 51] loss: 0.006760387681424618\n",
      "[step: 52] loss: 0.0050919861532747746\n",
      "[step: 53] loss: 0.0034116210881620646\n",
      "[step: 54] loss: 0.00198608567006886\n",
      "[step: 55] loss: 0.0010066840332001448\n",
      "[step: 56] loss: 0.0005533095682039857\n",
      "[step: 57] loss: 0.0005847378051839769\n",
      "[step: 58] loss: 0.0009622169309295714\n",
      "[step: 59] loss: 0.0014987776521593332\n",
      "[step: 60] loss: 0.0020139128901064396\n",
      "[step: 61] loss: 0.002372885588556528\n",
      "[step: 62] loss: 0.002502454211935401\n",
      "[step: 63] loss: 0.0023886323906481266\n",
      "[step: 64] loss: 0.0020669938530772924\n",
      "[step: 65] loss: 0.0016109716380015016\n",
      "[step: 66] loss: 0.0011167421471327543\n",
      "[step: 67] loss: 0.0006823953008279204\n",
      "[step: 68] loss: 0.0003841831348836422\n",
      "[step: 69] loss: 0.00025752244982868433\n",
      "[step: 70] loss: 0.00029034080216661096\n",
      "[step: 71] loss: 0.00043150107376277447\n",
      "[step: 72] loss: 0.0006106633227318525\n",
      "[step: 73] loss: 0.0007618136005476117\n",
      "[step: 74] loss: 0.0008417614153586328\n",
      "[step: 75] loss: 0.00083771045319736\n",
      "[step: 76] loss: 0.0007630440522916615\n",
      "[step: 77] loss: 0.0006456623086705804\n",
      "[step: 78] loss: 0.0005156603292562068\n",
      "[step: 79] loss: 0.00039745995309203863\n",
      "[step: 80] loss: 0.0003072700055781752\n",
      "[step: 81] loss: 0.00025358711718581617\n",
      "[step: 82] loss: 0.00023797173344064504\n",
      "[step: 83] loss: 0.0002552812802605331\n",
      "[step: 84] loss: 0.0002942483115475625\n",
      "[step: 85] loss: 0.0003395360254216939\n",
      "[step: 86] loss: 0.0003754038189072162\n",
      "[step: 87] loss: 0.0003900591400451958\n",
      "[step: 88] loss: 0.00037912122206762433\n",
      "[step: 89] loss: 0.0003466967900749296\n",
      "[step: 90] loss: 0.0003032983804587275\n",
      "[step: 91] loss: 0.00026137413806281984\n",
      "[step: 92] loss: 0.00023057826911099255\n",
      "[step: 93] loss: 0.00021507356723304838\n",
      "[step: 94] loss: 0.00021376600489020348\n",
      "[step: 95] loss: 0.00022241659462451935\n",
      "[step: 96] loss: 0.00023583704023621976\n",
      "[step: 97] loss: 0.00024912998196668923\n",
      "[step: 98] loss: 0.0002582085144240409\n",
      "[step: 99] loss: 0.00026032194728031754\n",
      "[step: 100] loss: 0.00025477251620031893\n",
      "[step: 101] loss: 0.00024326634593307972\n",
      "[step: 102] loss: 0.00022937552421353757\n",
      "[step: 103] loss: 0.000217118562432006\n",
      "[step: 104] loss: 0.00020938704255968332\n",
      "[step: 105] loss: 0.0002070112677756697\n",
      "[step: 106] loss: 0.00020886395941488445\n",
      "[step: 107] loss: 0.0002127906191162765\n",
      "[step: 108] loss: 0.00021672333241440356\n",
      "[step: 109] loss: 0.0002193672553403303\n",
      "[step: 110] loss: 0.000220246976823546\n",
      "[step: 111] loss: 0.000219383102376014\n",
      "[step: 112] loss: 0.00021699721401091665\n",
      "[step: 113] loss: 0.00021348934387788177\n",
      "[step: 114] loss: 0.00020954593492206186\n",
      "[step: 115] loss: 0.00020610570209100842\n",
      "[step: 116] loss: 0.00020405215036589652\n",
      "[step: 117] loss: 0.00020378945919219404\n",
      "[step: 118] loss: 0.00020500776008702815\n",
      "[step: 119] loss: 0.00020682273316197097\n",
      "[step: 120] loss: 0.00020823928934987634\n",
      "[step: 121] loss: 0.00020864582620561123\n",
      "[step: 122] loss: 0.00020800571655854583\n",
      "[step: 123] loss: 0.0002067143068416044\n",
      "[step: 124] loss: 0.00020526337902992964\n",
      "[step: 125] loss: 0.00020399947243276983\n",
      "[step: 126] loss: 0.00020308342936914414\n",
      "[step: 127] loss: 0.00020256555580999702\n",
      "[step: 128] loss: 0.00020244198094587773\n",
      "[step: 129] loss: 0.0002026393631240353\n",
      "[step: 130] loss: 0.00020299467723816633\n",
      "[step: 131] loss: 0.000203293573576957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 132] loss: 0.00020335233421064913\n",
      "[step: 133] loss: 0.00020309702085796744\n",
      "[step: 134] loss: 0.00020259343727957457\n",
      "[step: 135] loss: 0.00020199375285301358\n",
      "[step: 136] loss: 0.00020145898452028632\n",
      "[step: 137] loss: 0.00020108347234781832\n",
      "[step: 138] loss: 0.00020088153542019427\n",
      "[step: 139] loss: 0.00020081002730876207\n",
      "[step: 140] loss: 0.00020081066759303212\n",
      "[step: 141] loss: 0.0002008293231483549\n",
      "[step: 142] loss: 0.00020081222464796156\n",
      "[step: 143] loss: 0.00020071429025847465\n",
      "[step: 144] loss: 0.00020051315368618816\n",
      "[step: 145] loss: 0.0002002297405852005\n",
      "[step: 146] loss: 0.00019992247689515352\n",
      "[step: 147] loss: 0.00019965333922300488\n",
      "[step: 148] loss: 0.0001994618069147691\n",
      "[step: 149] loss: 0.00019934419833589345\n",
      "[step: 150] loss: 0.00019926704408135265\n",
      "[step: 151] loss: 0.00019919234910048544\n",
      "[step: 152] loss: 0.00019909284310415387\n",
      "[step: 153] loss: 0.00019896180310752243\n",
      "[step: 154] loss: 0.00019880390027537942\n",
      "[step: 155] loss: 0.00019862927729263902\n",
      "[step: 156] loss: 0.0001984452537726611\n",
      "[step: 157] loss: 0.00019826223433483392\n",
      "[step: 158] loss: 0.0001980957604246214\n",
      "[step: 159] loss: 0.0001979472435778007\n",
      "[step: 160] loss: 0.00019781933224294335\n",
      "[step: 161] loss: 0.0001977026549866423\n",
      "[step: 162] loss: 0.00019758407142944634\n",
      "[step: 163] loss: 0.00019745429744943976\n",
      "[step: 164] loss: 0.0001973098114831373\n",
      "[step: 165] loss: 0.00019715222879312932\n",
      "[step: 166] loss: 0.00019699186668731272\n",
      "[step: 167] loss: 0.00019683376012835652\n",
      "[step: 168] loss: 0.0001966833951883018\n",
      "[step: 169] loss: 0.0001965405244845897\n",
      "[step: 170] loss: 0.00019640312530100346\n",
      "[step: 171] loss: 0.00019626886933110654\n",
      "[step: 172] loss: 0.00019613286713138223\n",
      "[step: 173] loss: 0.0001959930668817833\n",
      "[step: 174] loss: 0.00019584919209592044\n",
      "[step: 175] loss: 0.00019570047152228653\n",
      "[step: 176] loss: 0.00019555077597033232\n",
      "[step: 177] loss: 0.00019540121138561517\n",
      "[step: 178] loss: 0.00019525398965924978\n",
      "[step: 179] loss: 0.0001951104059116915\n",
      "[step: 180] loss: 0.00019496770983096212\n",
      "[step: 181] loss: 0.00019482550851535052\n",
      "[step: 182] loss: 0.00019468352547846735\n",
      "[step: 183] loss: 0.00019453892309684306\n",
      "[step: 184] loss: 0.00019439357856754214\n",
      "[step: 185] loss: 0.00019424600759521127\n",
      "[step: 186] loss: 0.00019409839296713471\n",
      "[step: 187] loss: 0.00019394986156839877\n",
      "[step: 188] loss: 0.00019380268349777907\n",
      "[step: 189] loss: 0.00019365661137271672\n",
      "[step: 190] loss: 0.00019351074297446758\n",
      "[step: 191] loss: 0.0001933664025273174\n",
      "[step: 192] loss: 0.0001932199957082048\n",
      "[step: 193] loss: 0.0001930726575665176\n",
      "[step: 194] loss: 0.00019292575598228723\n",
      "[step: 195] loss: 0.00019277683168184012\n",
      "[step: 196] loss: 0.0001926286640809849\n",
      "[step: 197] loss: 0.0001924801035784185\n",
      "[step: 198] loss: 0.0001923318050103262\n",
      "[step: 199] loss: 0.0001921836956171319\n",
      "[step: 200] loss: 0.00019203589181415737\n",
      "[step: 201] loss: 0.00019188651640433818\n",
      "[step: 202] loss: 0.00019173938198946416\n",
      "[step: 203] loss: 0.00019158978830091655\n",
      "[step: 204] loss: 0.0001914404856506735\n",
      "[step: 205] loss: 0.00019129100837744772\n",
      "[step: 206] loss: 0.00019114042515866458\n",
      "[step: 207] loss: 0.0001909907441586256\n",
      "[step: 208] loss: 0.00019084088853560388\n",
      "[step: 209] loss: 0.00019069093104917556\n",
      "[step: 210] loss: 0.00019054122094530612\n",
      "[step: 211] loss: 0.0001903911615954712\n",
      "[step: 212] loss: 0.00019023996719624847\n",
      "[step: 213] loss: 0.00019009012612514198\n",
      "[step: 214] loss: 0.000189939106348902\n",
      "[step: 215] loss: 0.00018978748994413763\n",
      "[step: 216] loss: 0.00018963681941386312\n",
      "[step: 217] loss: 0.00018948482465930283\n",
      "[step: 218] loss: 0.00018933348474092782\n",
      "[step: 219] loss: 0.00018918199930340052\n",
      "[step: 220] loss: 0.00018902964075095952\n",
      "[step: 221] loss: 0.00018887918849941343\n",
      "[step: 222] loss: 0.000188726102351211\n",
      "[step: 223] loss: 0.00018857460236176848\n",
      "[step: 224] loss: 0.00018842180725187063\n",
      "[step: 225] loss: 0.0001882690703496337\n",
      "[step: 226] loss: 0.0001881170755950734\n",
      "[step: 227] loss: 0.00018796416406985372\n",
      "[step: 228] loss: 0.0001878118928289041\n",
      "[step: 229] loss: 0.00018765829736366868\n",
      "[step: 230] loss: 0.0001875050802482292\n",
      "[step: 231] loss: 0.00018735171761363745\n",
      "[step: 232] loss: 0.00018719852960202843\n",
      "[step: 233] loss: 0.00018704474496189505\n",
      "[step: 234] loss: 0.00018689091666601598\n",
      "[step: 235] loss: 0.00018673748127184808\n",
      "[step: 236] loss: 0.00018658376939129084\n",
      "[step: 237] loss: 0.0001864296937128529\n",
      "[step: 238] loss: 0.00018627573444973677\n",
      "[step: 239] loss: 0.00018612158601172268\n",
      "[step: 240] loss: 0.000185966826393269\n",
      "[step: 241] loss: 0.00018581234326120466\n",
      "[step: 242] loss: 0.0001856583694461733\n",
      "[step: 243] loss: 0.0001855032314779237\n",
      "[step: 244] loss: 0.00018534853006713092\n",
      "[step: 245] loss: 0.00018519377044867724\n",
      "[step: 246] loss: 0.0001850383123382926\n",
      "[step: 247] loss: 0.00018488336354494095\n",
      "[step: 248] loss: 0.0001847288804128766\n",
      "[step: 249] loss: 0.00018457320402376354\n",
      "[step: 250] loss: 0.00018441828433424234\n",
      "[step: 251] loss: 0.00018426196766085923\n",
      "[step: 252] loss: 0.00018410698976367712\n",
      "[step: 253] loss: 0.00018395170627627522\n",
      "[step: 254] loss: 0.00018379607354290783\n",
      "[step: 255] loss: 0.00018363971321377903\n",
      "[step: 256] loss: 0.0001834836439229548\n",
      "[step: 257] loss: 0.00018332788022235036\n",
      "[step: 258] loss: 0.00018317259673494846\n",
      "[step: 259] loss: 0.00018301632371731102\n",
      "[step: 260] loss: 0.0001828593813115731\n",
      "[step: 261] loss: 0.00018270306463818997\n",
      "[step: 262] loss: 0.00018254673341289163\n",
      "[step: 263] loss: 0.00018239048949908465\n",
      "[step: 264] loss: 0.00018223350343760103\n",
      "[step: 265] loss: 0.00018207736138720065\n",
      "[step: 266] loss: 0.000181921073817648\n",
      "[step: 267] loss: 0.00018176407320424914\n",
      "[step: 268] loss: 0.00018160729086957872\n",
      "[step: 269] loss: 0.00018145034846384078\n",
      "[step: 270] loss: 0.00018129372620023787\n",
      "[step: 271] loss: 0.00018113647820428014\n",
      "[step: 272] loss: 0.00018097924476023763\n",
      "[step: 273] loss: 0.00018082238966599107\n",
      "[step: 274] loss: 0.00018066541815642267\n",
      "[step: 275] loss: 0.00018050863582175225\n",
      "[step: 276] loss: 0.0001803508203011006\n",
      "[step: 277] loss: 0.0001801941980374977\n",
      "[step: 278] loss: 0.00018003623699769378\n",
      "[step: 279] loss: 0.00017987903265748173\n",
      "[step: 280] loss: 0.00017972155183088034\n",
      "[step: 281] loss: 0.00017956392548512667\n",
      "[step: 282] loss: 0.00017940616817213595\n",
      "[step: 283] loss: 0.00017924878920894116\n",
      "[step: 284] loss: 0.00017909125017467886\n",
      "[step: 285] loss: 0.0001789335801731795\n",
      "[step: 286] loss: 0.00017877468781080097\n",
      "[step: 287] loss: 0.00017861815285868943\n",
      "[step: 288] loss: 0.00017845953698270023\n",
      "[step: 289] loss: 0.00017830204160418361\n",
      "[step: 290] loss: 0.00017814416787587106\n",
      "[step: 291] loss: 0.00017798668704926968\n",
      "[step: 292] loss: 0.00017782824579626322\n",
      "[step: 293] loss: 0.0001776702411007136\n",
      "[step: 294] loss: 0.00017751249833963811\n",
      "[step: 295] loss: 0.00017735465371515602\n",
      "[step: 296] loss: 0.00017719645984470844\n",
      "[step: 297] loss: 0.00017703855701256543\n",
      "[step: 298] loss: 0.0001768801739672199\n",
      "[step: 299] loss: 0.0001767220819601789\n",
      "[step: 300] loss: 0.00017656436830293387\n",
      "[step: 301] loss: 0.00017640534497331828\n",
      "[step: 302] loss: 0.00017624790780246258\n",
      "[step: 303] loss: 0.00017608913185540587\n",
      "[step: 304] loss: 0.0001759318693075329\n",
      "[step: 305] loss: 0.0001757731952238828\n",
      "[step: 306] loss: 0.00017561482673045248\n",
      "[step: 307] loss: 0.00017545618175063282\n",
      "[step: 308] loss: 0.00017529843898955733\n",
      "[step: 309] loss: 0.00017514024511910975\n",
      "[step: 310] loss: 0.00017498174565844238\n",
      "[step: 311] loss: 0.00017482369730714709\n",
      "[step: 312] loss: 0.00017466509598307312\n",
      "[step: 313] loss: 0.00017450671293772757\n",
      "[step: 314] loss: 0.00017434819892514497\n",
      "[step: 315] loss: 0.0001741898013278842\n",
      "[step: 316] loss: 0.00017403147649019957\n",
      "[step: 317] loss: 0.00017387326806783676\n",
      "[step: 318] loss: 0.00017371516150888056\n",
      "[step: 319] loss: 0.00017355656018480659\n",
      "[step: 320] loss: 0.00017339819169137627\n",
      "[step: 321] loss: 0.00017324091459158808\n",
      "[step: 322] loss: 0.00017308269161731005\n",
      "[step: 323] loss: 0.0001729239011183381\n",
      "[step: 324] loss: 0.00017276594007853419\n",
      "[step: 325] loss: 0.00017260762979276478\n",
      "[step: 326] loss: 0.00017244888294953853\n",
      "[step: 327] loss: 0.0001722908637020737\n",
      "[step: 328] loss: 0.00017213268438354135\n",
      "[step: 329] loss: 0.00017197472334373742\n",
      "[step: 330] loss: 0.00017181616567540914\n",
      "[step: 331] loss: 0.00017165843746624887\n",
      "[step: 332] loss: 0.00017150037456303835\n",
      "[step: 333] loss: 0.00017134181689471006\n",
      "[step: 334] loss: 0.0001711840886855498\n",
      "[step: 335] loss: 0.00017102464335039258\n",
      "[step: 336] loss: 0.00017086751176975667\n",
      "[step: 337] loss: 0.00017070888134185225\n",
      "[step: 338] loss: 0.00017055067291948944\n",
      "[step: 339] loss: 0.00017039303202182055\n",
      "[step: 340] loss: 0.00017023553664330393\n",
      "[step: 341] loss: 0.00017007689166348428\n",
      "[step: 342] loss: 0.00016991927986964583\n",
      "[step: 343] loss: 0.00016976139158941805\n",
      "[step: 344] loss: 0.00016960348875727504\n",
      "[step: 345] loss: 0.00016944532399065793\n",
      "[step: 346] loss: 0.0001692869554972276\n",
      "[step: 347] loss: 0.0001691300276434049\n",
      "[step: 348] loss: 0.00016897195018827915\n",
      "[step: 349] loss: 0.00016881446936167777\n",
      "[step: 350] loss: 0.00016865662473719567\n",
      "[step: 351] loss: 0.00016849845997057855\n",
      "[step: 352] loss: 0.00016834113921504468\n",
      "[step: 353] loss: 0.0001681838621152565\n",
      "[step: 354] loss: 0.00016802649770397693\n",
      "[step: 355] loss: 0.0001678684348007664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 356] loss: 0.00016771136142779142\n",
      "[step: 357] loss: 0.00016755354590713978\n",
      "[step: 358] loss: 0.00016739680722821504\n",
      "[step: 359] loss: 0.00016723906446713954\n",
      "[step: 360] loss: 0.00016708180191926658\n",
      "[step: 361] loss: 0.0001669246848905459\n",
      "[step: 362] loss: 0.00016676764062140137\n",
      "[step: 363] loss: 0.00016661059635225683\n",
      "[step: 364] loss: 0.00016645353753119707\n",
      "[step: 365] loss: 0.00016629601304885\n",
      "[step: 366] loss: 0.0001661391870584339\n",
      "[step: 367] loss: 0.000165982884936966\n",
      "[step: 368] loss: 0.00016582521493546665\n",
      "[step: 369] loss: 0.00016566889826208353\n",
      "[step: 370] loss: 0.00016551197040826082\n",
      "[step: 371] loss: 0.00016535501345060766\n",
      "[step: 372] loss: 0.0001651986822253093\n",
      "[step: 373] loss: 0.00016504230734426528\n",
      "[step: 374] loss: 0.00016488574328832328\n",
      "[step: 375] loss: 0.00016472850984428078\n",
      "[step: 376] loss: 0.00016457214951515198\n",
      "[step: 377] loss: 0.00016441640036646277\n",
      "[step: 378] loss: 0.0001642598072066903\n",
      "[step: 379] loss: 0.00016410378157161176\n",
      "[step: 380] loss: 0.00016394740669056773\n",
      "[step: 381] loss: 0.0001637914392631501\n",
      "[step: 382] loss: 0.00016363494796678424\n",
      "[step: 383] loss: 0.00016347919881809503\n",
      "[step: 384] loss: 0.0001633231295272708\n",
      "[step: 385] loss: 0.00016316765686497092\n",
      "[step: 386] loss: 0.00016301136929541826\n",
      "[step: 387] loss: 0.00016285583842545748\n",
      "[step: 388] loss: 0.00016270052583422512\n",
      "[step: 389] loss: 0.00016254499496426433\n",
      "[step: 390] loss: 0.00016238960961345583\n",
      "[step: 391] loss: 0.000162233947776258\n",
      "[step: 392] loss: 0.0001620787661522627\n",
      "[step: 393] loss: 0.00016192340990528464\n",
      "[step: 394] loss: 0.00016176822828128934\n",
      "[step: 395] loss: 0.00016161310486495495\n",
      "[step: 396] loss: 0.00016145819972734898\n",
      "[step: 397] loss: 0.00016130300355143845\n",
      "[step: 398] loss: 0.00016114806931000203\n",
      "[step: 399] loss: 0.00016099309141281992\n",
      "[step: 400] loss: 0.0001608377933735028\n",
      "[step: 401] loss: 0.0001606835867278278\n",
      "[step: 402] loss: 0.00016052935097832233\n",
      "[step: 403] loss: 0.0001603748823981732\n",
      "[step: 404] loss: 0.00016022090858314186\n",
      "[step: 405] loss: 0.00016006625082809478\n",
      "[step: 406] loss: 0.00015991184045560658\n",
      "[step: 407] loss: 0.00015975772112142295\n",
      "[step: 408] loss: 0.00015960336895659566\n",
      "[step: 409] loss: 0.0001594497443875298\n",
      "[step: 410] loss: 0.00015929572691675276\n",
      "[step: 411] loss: 0.00015914227697066963\n",
      "[step: 412] loss: 0.00015898817218840122\n",
      "[step: 413] loss: 0.00015883434389252216\n",
      "[step: 414] loss: 0.00015868066111579537\n",
      "[step: 415] loss: 0.00015852726937737316\n",
      "[step: 416] loss: 0.00015837412502150983\n",
      "[step: 417] loss: 0.00015822125715203583\n",
      "[step: 418] loss: 0.00015806773444637656\n",
      "[step: 419] loss: 0.0001579149829922244\n",
      "[step: 420] loss: 0.00015776141663081944\n",
      "[step: 421] loss: 0.00015760828682687134\n",
      "[step: 422] loss: 0.0001574555062688887\n",
      "[step: 423] loss: 0.0001573027257109061\n",
      "[step: 424] loss: 0.00015715105109848082\n",
      "[step: 425] loss: 0.00015699825598858297\n",
      "[step: 426] loss: 0.00015684567915741354\n",
      "[step: 427] loss: 0.000156693859025836\n",
      "[step: 428] loss: 0.0001565412530908361\n",
      "[step: 429] loss: 0.00015638908371329308\n",
      "[step: 430] loss: 0.00015623687068000436\n",
      "[step: 431] loss: 0.00015608487592544407\n",
      "[step: 432] loss: 0.000155932895722799\n",
      "[step: 433] loss: 0.00015578095917589962\n",
      "[step: 434] loss: 0.00015562980843242258\n",
      "[step: 435] loss: 0.0001554783812025562\n",
      "[step: 436] loss: 0.00015532708493992686\n",
      "[step: 437] loss: 0.0001551754685351625\n",
      "[step: 438] loss: 0.00015502452151849866\n",
      "[step: 439] loss: 0.00015487312339246273\n",
      "[step: 440] loss: 0.00015472240920644253\n",
      "[step: 441] loss: 0.0001545713603263721\n",
      "[step: 442] loss: 0.00015442051517311484\n",
      "[step: 443] loss: 0.00015426943718921393\n",
      "[step: 444] loss: 0.00015411961067002267\n",
      "[step: 445] loss: 0.00015396930393762887\n",
      "[step: 446] loss: 0.00015381877892650664\n",
      "[step: 447] loss: 0.00015366873412858695\n",
      "[step: 448] loss: 0.00015351812180597335\n",
      "[step: 449] loss: 0.00015336828073486686\n",
      "[step: 450] loss: 0.0001532188180135563\n",
      "[step: 451] loss: 0.00015306910790968686\n",
      "[step: 452] loss: 0.00015291837917175144\n",
      "[step: 453] loss: 0.00015276952763088048\n",
      "[step: 454] loss: 0.00015262018132489175\n",
      "[step: 455] loss: 0.00015247029659803957\n",
      "[step: 456] loss: 0.00015232169243972749\n",
      "[step: 457] loss: 0.00015217163308989257\n",
      "[step: 458] loss: 0.0001520230434834957\n",
      "[step: 459] loss: 0.00015187407552730292\n",
      "[step: 460] loss: 0.00015172587882261723\n",
      "[step: 461] loss: 0.00015157686721067876\n",
      "[step: 462] loss: 0.00015142856864258647\n",
      "[step: 463] loss: 0.00015128057566471398\n",
      "[step: 464] loss: 0.0001511320297140628\n",
      "[step: 465] loss: 0.0001509838766651228\n",
      "[step: 466] loss: 0.00015083522885106504\n",
      "[step: 467] loss: 0.00015068768698256463\n",
      "[step: 468] loss: 0.00015053975221235305\n",
      "[step: 469] loss: 0.00015039200661703944\n",
      "[step: 470] loss: 0.0001502447121310979\n",
      "[step: 471] loss: 0.0001500970683991909\n",
      "[step: 472] loss: 0.0001499499921919778\n",
      "[step: 473] loss: 0.00014980242121964693\n",
      "[step: 474] loss: 0.00014965560694690794\n",
      "[step: 475] loss: 0.0001495087635703385\n",
      "[step: 476] loss: 0.00014936196384951472\n",
      "[step: 477] loss: 0.00014921506226528436\n",
      "[step: 478] loss: 0.00014906880096532404\n",
      "[step: 479] loss: 0.00014892248145770282\n",
      "[step: 480] loss: 0.00014877553621772677\n",
      "[step: 481] loss: 0.00014862953685224056\n",
      "[step: 482] loss: 0.00014848331920802593\n",
      "[step: 483] loss: 0.00014833723253104836\n",
      "[step: 484] loss: 0.00014819188800174743\n",
      "[step: 485] loss: 0.00014804620877839625\n",
      "[step: 486] loss: 0.00014790063141845167\n",
      "[step: 487] loss: 0.00014775508316233754\n",
      "[step: 488] loss: 0.00014761013153474778\n",
      "[step: 489] loss: 0.00014746484521310776\n",
      "[step: 490] loss: 0.0001473199372412637\n",
      "[step: 491] loss: 0.0001471742580179125\n",
      "[step: 492] loss: 0.0001470297429477796\n",
      "[step: 493] loss: 0.0001468856498831883\n",
      "[step: 494] loss: 0.00014674110570922494\n",
      "[step: 495] loss: 0.00014659670705441386\n",
      "[step: 496] loss: 0.00014645217743236572\n",
      "[step: 497] loss: 0.00014630885561928153\n",
      "[step: 498] loss: 0.00014616482076235116\n",
      "[step: 499] loss: 0.000146020800457336\n",
      "[step: 500] loss: 0.00014587688201572746\n",
      "[step: 501] loss: 0.00014573354565072805\n",
      "[step: 502] loss: 0.00014558996190316975\n",
      "[step: 503] loss: 0.0001454469602322206\n",
      "[step: 504] loss: 0.000145302910823375\n",
      "[step: 505] loss: 0.00014516094233840704\n",
      "[step: 506] loss: 0.00014501782425213605\n",
      "[step: 507] loss: 0.00014487499720416963\n",
      "[step: 508] loss: 0.00014473273768089712\n",
      "[step: 509] loss: 0.00014458963414654136\n",
      "[step: 510] loss: 0.00014444747648667544\n",
      "[step: 511] loss: 0.00014430528972297907\n",
      "[step: 512] loss: 0.00014416285557672381\n",
      "[step: 513] loss: 0.00014402135275304317\n",
      "[step: 514] loss: 0.00014387913688551635\n",
      "[step: 515] loss: 0.00014373748854268342\n",
      "[step: 516] loss: 0.00014359570923261344\n",
      "[step: 517] loss: 0.00014345521049108356\n",
      "[step: 518] loss: 0.00014331357670016587\n",
      "[step: 519] loss: 0.00014317300519905984\n",
      "[step: 520] loss: 0.00014303161879070103\n",
      "[step: 521] loss: 0.0001428907416993752\n",
      "[step: 522] loss: 0.00014275060675572604\n",
      "[step: 523] loss: 0.0001426101807737723\n",
      "[step: 524] loss: 0.00014246979844756424\n",
      "[step: 525] loss: 0.00014232945977710187\n",
      "[step: 526] loss: 0.00014218893193174154\n",
      "[step: 527] loss: 0.00014204929175321013\n",
      "[step: 528] loss: 0.00014190956426318735\n",
      "[step: 529] loss: 0.00014176988042891026\n",
      "[step: 530] loss: 0.00014163079322315753\n",
      "[step: 531] loss: 0.00014149132766760886\n",
      "[step: 532] loss: 0.00014135146921034902\n",
      "[step: 533] loss: 0.0001412133133271709\n",
      "[step: 534] loss: 0.00014107434253674\n",
      "[step: 535] loss: 0.00014093518257141113\n",
      "[step: 536] loss: 0.00014079651737120003\n",
      "[step: 537] loss: 0.00014065833238419145\n",
      "[step: 538] loss: 0.00014052014739718288\n",
      "[step: 539] loss: 0.0001403815986122936\n",
      "[step: 540] loss: 0.00014024363190401345\n",
      "[step: 541] loss: 0.00014010553422849625\n",
      "[step: 542] loss: 0.0001399685861542821\n",
      "[step: 543] loss: 0.00013983066310174763\n",
      "[step: 544] loss: 0.00013969348219688982\n",
      "[step: 545] loss: 0.0001395558356307447\n",
      "[step: 546] loss: 0.00013941884390078485\n",
      "[step: 547] loss: 0.00013928192493040115\n",
      "[step: 548] loss: 0.00013914512237533927\n",
      "[step: 549] loss: 0.000139008421683684\n",
      "[step: 550] loss: 0.00013887163368053734\n",
      "[step: 551] loss: 0.00013873512216378003\n",
      "[step: 552] loss: 0.00013859904720447958\n",
      "[step: 553] loss: 0.0001384636852890253\n",
      "[step: 554] loss: 0.00013832701370120049\n",
      "[step: 555] loss: 0.0001381912879878655\n",
      "[step: 556] loss: 0.00013805566413793713\n",
      "[step: 557] loss: 0.00013792028767056763\n",
      "[step: 558] loss: 0.00013778491120319813\n",
      "[step: 559] loss: 0.00013764940376859158\n",
      "[step: 560] loss: 0.0001375152205582708\n",
      "[step: 561] loss: 0.0001373797276755795\n",
      "[step: 562] loss: 0.00013724510790780187\n",
      "[step: 563] loss: 0.00013711091014556587\n",
      "[step: 564] loss: 0.0001369757083011791\n",
      "[step: 565] loss: 0.00013684219447895885\n",
      "[step: 566] loss: 0.0001367079676128924\n",
      "[step: 567] loss: 0.00013657404633704573\n",
      "[step: 568] loss: 0.00013644008140545338\n",
      "[step: 569] loss: 0.00013630627654492855\n",
      "[step: 570] loss: 0.00013617293734569103\n",
      "[step: 571] loss: 0.0001360398600809276\n",
      "[step: 572] loss: 0.00013590634625870734\n",
      "[step: 573] loss: 0.00013577316713053733\n",
      "[step: 574] loss: 0.00013564026448875666\n",
      "[step: 575] loss: 0.00013550801668316126\n",
      "[step: 576] loss: 0.00013537546328734607\n",
      "[step: 577] loss: 0.00013524340465664864\n",
      "[step: 578] loss: 0.00013511086581274867\n",
      "[step: 579] loss: 0.00013497869076672941\n",
      "[step: 580] loss: 0.00013484664668794721\n",
      "[step: 581] loss: 0.00013471559213940054\n",
      "[step: 582] loss: 0.00013458332978188992\n",
      "[step: 583] loss: 0.00013445166405290365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 584] loss: 0.00013432049308903515\n",
      "[step: 585] loss: 0.00013418981689028442\n",
      "[step: 586] loss: 0.00013405820936895907\n",
      "[step: 587] loss: 0.00013392763503361493\n",
      "[step: 588] loss: 0.0001337971625616774\n",
      "[step: 589] loss: 0.0001336665591225028\n",
      "[step: 590] loss: 0.00013353602844290435\n",
      "[step: 591] loss: 0.00013340548321139067\n",
      "[step: 592] loss: 0.00013327618944458663\n",
      "[step: 593] loss: 0.0001331461826339364\n",
      "[step: 594] loss: 0.00013301624858286232\n",
      "[step: 595] loss: 0.0001328869111603126\n",
      "[step: 596] loss: 0.0001327569189015776\n",
      "[step: 597] loss: 0.0001326285710092634\n",
      "[step: 598] loss: 0.00013249872426968068\n",
      "[step: 599] loss: 0.0001323705946560949\n",
      "[step: 600] loss: 0.00013224119902588427\n",
      "[step: 601] loss: 0.00013211283658165485\n",
      "[step: 602] loss: 0.00013198450324125588\n",
      "[step: 603] loss: 0.00013185593707021326\n",
      "[step: 604] loss: 0.00013172783656045794\n",
      "[step: 605] loss: 0.00013159993977751583\n",
      "[step: 606] loss: 0.00013147205754648894\n",
      "[step: 607] loss: 0.0001313448155997321\n",
      "[step: 608] loss: 0.00013121694792062044\n",
      "[step: 609] loss: 0.00013108932762406766\n",
      "[step: 610] loss: 0.0001309628423769027\n",
      "[step: 611] loss: 0.00013083549856673926\n",
      "[step: 612] loss: 0.00013070904242340475\n",
      "[step: 613] loss: 0.00013058233889751136\n",
      "[step: 614] loss: 0.00013045519881416112\n",
      "[step: 615] loss: 0.00013032926653977484\n",
      "[step: 616] loss: 0.0001302028977079317\n",
      "[step: 617] loss: 0.00013007705274503678\n",
      "[step: 618] loss: 0.0001299506111536175\n",
      "[step: 619] loss: 0.00012982547923456877\n",
      "[step: 620] loss: 0.00012969967792741954\n",
      "[step: 621] loss: 0.00012957457511220127\n",
      "[step: 622] loss: 0.00012944845366291702\n",
      "[step: 623] loss: 0.00012932426761835814\n",
      "[step: 624] loss: 0.0001291990338359028\n",
      "[step: 625] loss: 0.0001290739164687693\n",
      "[step: 626] loss: 0.0001289499778067693\n",
      "[step: 627] loss: 0.00012882545706816018\n",
      "[step: 628] loss: 0.00012870068894699216\n",
      "[step: 629] loss: 0.00012857725960202515\n",
      "[step: 630] loss: 0.00012845291348639876\n",
      "[step: 631] loss: 0.00012832901848014444\n",
      "[step: 632] loss: 0.00012820538540836424\n",
      "[step: 633] loss: 0.00012808195606339723\n",
      "[step: 634] loss: 0.00012795886141248047\n",
      "[step: 635] loss: 0.0001278357085539028\n",
      "[step: 636] loss: 0.00012771219189744443\n",
      "[step: 637] loss: 0.0001275895192520693\n",
      "[step: 638] loss: 0.0001274670648854226\n",
      "[step: 639] loss: 0.0001273447705898434\n",
      "[step: 640] loss: 0.000127222272567451\n",
      "[step: 641] loss: 0.00012709986185654998\n",
      "[step: 642] loss: 0.00012697830970864743\n",
      "[step: 643] loss: 0.00012685665569733828\n",
      "[step: 644] loss: 0.0001267348852707073\n",
      "[step: 645] loss: 0.00012661292566917837\n",
      "[step: 646] loss: 0.00012649148993659765\n",
      "[step: 647] loss: 0.00012637108738999814\n",
      "[step: 648] loss: 0.00012624968076124787\n",
      "[step: 649] loss: 0.00012612865248229355\n",
      "[step: 650] loss: 0.00012600862828548998\n",
      "[step: 651] loss: 0.00012588741083163768\n",
      "[step: 652] loss: 0.00012576684821397066\n",
      "[step: 653] loss: 0.00012564768258016557\n",
      "[step: 654] loss: 0.00012552711996249855\n",
      "[step: 655] loss: 0.0001254071103176102\n",
      "[step: 656] loss: 0.00012528724619187415\n",
      "[step: 657] loss: 0.00012516856077127159\n",
      "[step: 658] loss: 0.00012504863843787462\n",
      "[step: 659] loss: 0.0001249294582521543\n",
      "[step: 660] loss: 0.00012481081648729742\n",
      "[step: 661] loss: 0.0001246912288479507\n",
      "[step: 662] loss: 0.00012457341654226184\n",
      "[step: 663] loss: 0.00012445385800674558\n",
      "[step: 664] loss: 0.00012433565279934555\n",
      "[step: 665] loss: 0.0001242176949745044\n",
      "[step: 666] loss: 0.00012409969349391758\n",
      "[step: 667] loss: 0.0001239818229805678\n",
      "[step: 668] loss: 0.00012386363232508302\n",
      "[step: 669] loss: 0.00012374621292110533\n",
      "[step: 670] loss: 0.00012362886627670377\n",
      "[step: 671] loss: 0.00012351167970336974\n",
      "[step: 672] loss: 0.0001233950024470687\n",
      "[step: 673] loss: 0.00012327765580266714\n",
      "[step: 674] loss: 0.00012316129868850112\n",
      "[step: 675] loss: 0.00012304478150326759\n",
      "[step: 676] loss: 0.00012292765313759446\n",
      "[step: 677] loss: 0.00012281157250981778\n",
      "[step: 678] loss: 0.00012269546277821064\n",
      "[step: 679] loss: 0.00012258028436917812\n",
      "[step: 680] loss: 0.00012246354890521616\n",
      "[step: 681] loss: 0.00012234802125021815\n",
      "[step: 682] loss: 0.00012223310477565974\n",
      "[step: 683] loss: 0.00012211728608235717\n",
      "[step: 684] loss: 0.00012200258061056957\n",
      "[step: 685] loss: 0.0001218873294419609\n",
      "[step: 686] loss: 0.0001217728276969865\n",
      "[step: 687] loss: 0.00012165784573880956\n",
      "[step: 688] loss: 0.00012154343130532652\n",
      "[step: 689] loss: 0.00012142941704951227\n",
      "[step: 690] loss: 0.00012131449329899624\n",
      "[step: 691] loss: 0.00012120085739297792\n",
      "[step: 692] loss: 0.00012108702503610402\n",
      "[step: 693] loss: 0.00012097389117116109\n",
      "[step: 694] loss: 0.00012086005153832957\n",
      "[step: 695] loss: 0.00012074675032636151\n",
      "[step: 696] loss: 0.00012063361646141857\n",
      "[step: 697] loss: 0.00012052052625222132\n",
      "[step: 698] loss: 0.00012040731962770224\n",
      "[step: 699] loss: 0.0001202954153995961\n",
      "[step: 700] loss: 0.00012018228881061077\n",
      "[step: 701] loss: 0.0001200703190988861\n",
      "[step: 702] loss: 0.00011995790555374697\n",
      "[step: 703] loss: 0.00011984648153884336\n",
      "[step: 704] loss: 0.000119734977488406\n",
      "[step: 705] loss: 0.0001196223747683689\n",
      "[step: 706] loss: 0.00011951079068239778\n",
      "[step: 707] loss: 0.00011939951946260408\n",
      "[step: 708] loss: 0.00011928877211175859\n",
      "[step: 709] loss: 0.00011917727533727884\n",
      "[step: 710] loss: 0.00011906672443728894\n",
      "[step: 711] loss: 0.00011895554780494422\n",
      "[step: 712] loss: 0.00011884506238857284\n",
      "[step: 713] loss: 0.00011873529001604766\n",
      "[step: 714] loss: 0.00011862564861075953\n",
      "[step: 715] loss: 0.0001185148794320412\n",
      "[step: 716] loss: 0.00011840501974802464\n",
      "[step: 717] loss: 0.0001182957857963629\n",
      "[step: 718] loss: 0.0001181856423499994\n",
      "[step: 719] loss: 0.00011807644477812573\n",
      "[step: 720] loss: 0.00011796712351497263\n",
      "[step: 721] loss: 0.00011785778042394668\n",
      "[step: 722] loss: 0.00011774902668548748\n",
      "[step: 723] loss: 0.00011764026567107067\n",
      "[step: 724] loss: 0.00011753213766496629\n",
      "[step: 725] loss: 0.0001174231874756515\n",
      "[step: 726] loss: 0.00011731515405699611\n",
      "[step: 727] loss: 0.00011720684415195137\n",
      "[step: 728] loss: 0.00011709869431797415\n",
      "[step: 729] loss: 0.00011699095193762332\n",
      "[step: 730] loss: 0.00011688312224578112\n",
      "[step: 731] loss: 0.00011677610746119171\n",
      "[step: 732] loss: 0.00011666861246339977\n",
      "[step: 733] loss: 0.00011656121932901442\n",
      "[step: 734] loss: 0.00011645475751720369\n",
      "[step: 735] loss: 0.00011634773545665666\n",
      "[step: 736] loss: 0.00011624132457654923\n",
      "[step: 737] loss: 0.00011613431706791744\n",
      "[step: 738] loss: 0.00011602805170696229\n",
      "[step: 739] loss: 0.00011592228838708252\n",
      "[step: 740] loss: 0.00011581623402889818\n",
      "[step: 741] loss: 0.00011571033246582374\n",
      "[step: 742] loss: 0.0001156047874246724\n",
      "[step: 743] loss: 0.00011549867485882714\n",
      "[step: 744] loss: 0.00011539371917024255\n",
      "[step: 745] loss: 0.0001152885306510143\n",
      "[step: 746] loss: 0.00011518391693243757\n",
      "[step: 747] loss: 0.00011507824820000678\n",
      "[step: 748] loss: 0.00011497381638037041\n",
      "[step: 749] loss: 0.00011486854054965079\n",
      "[step: 750] loss: 0.0001147649236372672\n",
      "[step: 751] loss: 0.00011466041178209707\n",
      "[step: 752] loss: 0.00011455611820565537\n",
      "[step: 753] loss: 0.0001144524067058228\n",
      "[step: 754] loss: 0.00011434917541919276\n",
      "[step: 755] loss: 0.00011424496915424243\n",
      "[step: 756] loss: 0.00011414177424740046\n",
      "[step: 757] loss: 0.00011403843382140622\n",
      "[step: 758] loss: 0.00011393544264137745\n",
      "[step: 759] loss: 0.0001138330262620002\n",
      "[step: 760] loss: 0.00011372962035238743\n",
      "[step: 761] loss: 0.0001136268547270447\n",
      "[step: 762] loss: 0.00011352511501172557\n",
      "[step: 763] loss: 0.00011342151265125722\n",
      "[step: 764] loss: 0.00011332007125020027\n",
      "[step: 765] loss: 0.00011321816418785602\n",
      "[step: 766] loss: 0.0001131163808167912\n",
      "[step: 767] loss: 0.00011301463382551447\n",
      "[step: 768] loss: 0.00011291337432339787\n",
      "[step: 769] loss: 0.00011281191109446809\n",
      "[step: 770] loss: 0.00011271056428086013\n",
      "[step: 771] loss: 0.00011260926839895546\n",
      "[step: 772] loss: 0.00011250865645706654\n",
      "[step: 773] loss: 0.00011240765161346644\n",
      "[step: 774] loss: 0.00011230697418795899\n",
      "[step: 775] loss: 0.00011220660235267133\n",
      "[step: 776] loss: 0.00011210687080165371\n",
      "[step: 777] loss: 0.00011200622975593433\n",
      "[step: 778] loss: 0.00011190641816938296\n",
      "[step: 779] loss: 0.00011180663568666205\n",
      "[step: 780] loss: 0.00011170736979693174\n",
      "[step: 781] loss: 0.0001116076746257022\n",
      "[step: 782] loss: 0.00011150841601192951\n",
      "[step: 783] loss: 0.00011140968126710504\n",
      "[step: 784] loss: 0.00011131008068332449\n",
      "[step: 785] loss: 0.00011121162242488936\n",
      "[step: 786] loss: 0.00011111228377558291\n",
      "[step: 787] loss: 0.00011101368727395311\n",
      "[step: 788] loss: 0.00011091558553744107\n",
      "[step: 789] loss: 0.00011081742559326813\n",
      "[step: 790] loss: 0.00011071944027207792\n",
      "[step: 791] loss: 0.00011062123667215928\n",
      "[step: 792] loss: 0.00011052354238927364\n",
      "[step: 793] loss: 0.00011042603000532836\n",
      "[step: 794] loss: 0.00011032856127712876\n",
      "[step: 795] loss: 0.00011023077240679413\n",
      "[step: 796] loss: 0.0001101342641049996\n",
      "[step: 797] loss: 0.00011003729741787538\n",
      "[step: 798] loss: 0.00010994073090841994\n",
      "[step: 799] loss: 0.00010984382970491424\n",
      "[step: 800] loss: 0.00010974734323099256\n",
      "[step: 801] loss: 0.00010965086403302848\n",
      "[step: 802] loss: 0.00010955496691167355\n",
      "[step: 803] loss: 0.00010945874237222597\n",
      "[step: 804] loss: 0.00010936288890661672\n",
      "[step: 805] loss: 0.00010926734830718488\n",
      "[step: 806] loss: 0.00010917167674051598\n",
      "[step: 807] loss: 0.00010907647083513439\n",
      "[step: 808] loss: 0.00010898101754719391\n",
      "[step: 809] loss: 0.000108885949885007\n",
      "[step: 810] loss: 0.00010879081673920155\n",
      "[step: 811] loss: 0.00010869627294596285\n",
      "[step: 812] loss: 0.00010860163456527516\n",
      "[step: 813] loss: 0.00010850725084310398\n",
      "[step: 814] loss: 0.00010841304174391553\n",
      "[step: 815] loss: 0.0001083184833987616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 816] loss: 0.00010822401236509904\n",
      "[step: 817] loss: 0.00010813007247634232\n",
      "[step: 818] loss: 0.00010803703480632976\n",
      "[step: 819] loss: 0.00010794330592034385\n",
      "[step: 820] loss: 0.00010784928599605337\n",
      "[step: 821] loss: 0.00010775651753647253\n",
      "[step: 822] loss: 0.00010766338527901098\n",
      "[step: 823] loss: 0.00010757015115814283\n",
      "[step: 824] loss: 0.00010747773922048509\n",
      "[step: 825] loss: 0.00010738488344941288\n",
      "[step: 826] loss: 0.00010729251516750082\n",
      "[step: 827] loss: 0.0001072005252353847\n",
      "[step: 828] loss: 0.00010710782953538\n",
      "[step: 829] loss: 0.00010701594146667048\n",
      "[step: 830] loss: 0.00010692432260839269\n",
      "[step: 831] loss: 0.00010683196160243824\n",
      "[step: 832] loss: 0.00010674078657757491\n",
      "[step: 833] loss: 0.0001066492623067461\n",
      "[step: 834] loss: 0.00010655757068889216\n",
      "[step: 835] loss: 0.00010646650480339304\n",
      "[step: 836] loss: 0.00010637578088790178\n",
      "[step: 837] loss: 0.00010628460586303845\n",
      "[step: 838] loss: 0.00010619407839840278\n",
      "[step: 839] loss: 0.00010610358731355518\n",
      "[step: 840] loss: 0.00010601333633530885\n",
      "[step: 841] loss: 0.00010592257604002953\n",
      "[step: 842] loss: 0.00010583251423668116\n",
      "[step: 843] loss: 0.00010574253246886656\n",
      "[step: 844] loss: 0.00010565295815467834\n",
      "[step: 845] loss: 0.00010556390770943835\n",
      "[step: 846] loss: 0.00010547335841692984\n",
      "[step: 847] loss: 0.00010538424976402894\n",
      "[step: 848] loss: 0.00010529480641707778\n",
      "[step: 849] loss: 0.00010520619252929464\n",
      "[step: 850] loss: 0.00010511725122341886\n",
      "[step: 851] loss: 0.00010502837540116161\n",
      "[step: 852] loss: 0.00010493952868273482\n",
      "[step: 853] loss: 0.00010485146776773036\n",
      "[step: 854] loss: 0.00010476312309037894\n",
      "[step: 855] loss: 0.00010467495303601027\n",
      "[step: 856] loss: 0.00010458696488058195\n",
      "[step: 857] loss: 0.00010449867113493383\n",
      "[step: 858] loss: 0.0001044110904331319\n",
      "[step: 859] loss: 0.00010432386625325307\n",
      "[step: 860] loss: 0.00010423617641208693\n",
      "[step: 861] loss: 0.00010414846474304795\n",
      "[step: 862] loss: 0.000104061487945728\n",
      "[step: 863] loss: 0.0001039747876347974\n",
      "[step: 864] loss: 0.00010388808004790917\n",
      "[step: 865] loss: 0.00010380066669313237\n",
      "[step: 866] loss: 0.00010371448297519237\n",
      "[step: 867] loss: 0.00010362860484747216\n",
      "[step: 868] loss: 0.00010354197729611769\n",
      "[step: 869] loss: 0.00010345578630222008\n",
      "[step: 870] loss: 0.00010336958803236485\n",
      "[step: 871] loss: 0.00010328374628443271\n",
      "[step: 872] loss: 0.00010319785360479727\n",
      "[step: 873] loss: 0.00010311228834325448\n",
      "[step: 874] loss: 0.0001030269922921434\n",
      "[step: 875] loss: 0.00010294189269188792\n",
      "[step: 876] loss: 0.00010285677126375958\n",
      "[step: 877] loss: 0.0001027719335979782\n",
      "[step: 878] loss: 0.00010268726327922195\n",
      "[step: 879] loss: 0.00010260263661621138\n",
      "[step: 880] loss: 0.00010251751518808305\n",
      "[step: 881] loss: 0.00010243332508252934\n",
      "[step: 882] loss: 0.00010234928777208552\n",
      "[step: 883] loss: 0.00010226509039057419\n",
      "[step: 884] loss: 0.00010218108945991844\n",
      "[step: 885] loss: 0.00010209729225607589\n",
      "[step: 886] loss: 0.00010201396071352065\n",
      "[step: 887] loss: 0.00010193009075010195\n",
      "[step: 888] loss: 0.0001018465482047759\n",
      "[step: 889] loss: 0.0001017633912852034\n",
      "[step: 890] loss: 0.00010168027802137658\n",
      "[step: 891] loss: 0.00010159716475754976\n",
      "[step: 892] loss: 0.00010151417518500239\n",
      "[step: 893] loss: 0.00010143221152247861\n",
      "[step: 894] loss: 0.00010134936746908352\n",
      "[step: 895] loss: 0.00010126673441845924\n",
      "[step: 896] loss: 0.00010118455975316465\n",
      "[step: 897] loss: 0.00010110234870808199\n",
      "[step: 898] loss: 0.00010102021042257547\n",
      "[step: 899] loss: 0.00010093870514538139\n",
      "[step: 900] loss: 0.00010085684334626421\n",
      "[step: 901] loss: 0.00010077502520289272\n",
      "[step: 902] loss: 0.00010069384006783366\n",
      "[step: 903] loss: 0.00010061242210213095\n",
      "[step: 904] loss: 0.00010053126607090235\n",
      "[step: 905] loss: 0.0001004501391435042\n",
      "[step: 906] loss: 0.00010036939784185961\n",
      "[step: 907] loss: 0.00010028877295553684\n",
      "[step: 908] loss: 0.00010020799527410418\n",
      "[step: 909] loss: 0.00010012701386585832\n",
      "[step: 910] loss: 0.00010004739306168631\n",
      "[step: 911] loss: 9.996724111260846e-05\n",
      "[step: 912] loss: 9.988702367991209e-05\n",
      "[step: 913] loss: 9.980706090573221e-05\n",
      "[step: 914] loss: 9.972693078452721e-05\n",
      "[step: 915] loss: 9.964755736291409e-05\n",
      "[step: 916] loss: 9.95683076325804e-05\n",
      "[step: 917] loss: 9.948872320819646e-05\n",
      "[step: 918] loss: 9.940910240402445e-05\n",
      "[step: 919] loss: 9.933035471476614e-05\n",
      "[step: 920] loss: 9.925119229592383e-05\n",
      "[step: 921] loss: 9.917268471326679e-05\n",
      "[step: 922] loss: 9.909407526720315e-05\n",
      "[step: 923] loss: 9.9015305750072e-05\n",
      "[step: 924] loss: 9.893706010188907e-05\n",
      "[step: 925] loss: 9.885886538540944e-05\n",
      "[step: 926] loss: 9.878055425360799e-05\n",
      "[step: 927] loss: 9.870235953712836e-05\n",
      "[step: 928] loss: 9.862496517598629e-05\n",
      "[step: 929] loss: 9.854695963440463e-05\n",
      "[step: 930] loss: 9.846960165305063e-05\n",
      "[step: 931] loss: 9.839214908424765e-05\n",
      "[step: 932] loss: 9.831449278863147e-05\n",
      "[step: 933] loss: 9.823768050409853e-05\n",
      "[step: 934] loss: 9.816046804189682e-05\n",
      "[step: 935] loss: 9.808329195948318e-05\n",
      "[step: 936] loss: 9.800714906305075e-05\n",
      "[step: 937] loss: 9.793043864192441e-05\n",
      "[step: 938] loss: 9.785407019080594e-05\n",
      "[step: 939] loss: 9.77773088379763e-05\n",
      "[step: 940] loss: 9.770118776941672e-05\n",
      "[step: 941] loss: 9.762445552041754e-05\n",
      "[step: 942] loss: 9.754889470059425e-05\n",
      "[step: 943] loss: 9.74734066403471e-05\n",
      "[step: 944] loss: 9.739726374391466e-05\n",
      "[step: 945] loss: 9.732216858537868e-05\n",
      "[step: 946] loss: 9.724659321364015e-05\n",
      "[step: 947] loss: 9.71711560850963e-05\n",
      "[step: 948] loss: 9.709589357953519e-05\n",
      "[step: 949] loss: 9.702083480078727e-05\n",
      "[step: 950] loss: 9.694603795651346e-05\n",
      "[step: 951] loss: 9.687118290457875e-05\n",
      "[step: 952] loss: 9.679665527073666e-05\n",
      "[step: 953] loss: 9.672179294284433e-05\n",
      "[step: 954] loss: 9.664741810411215e-05\n",
      "[step: 955] loss: 9.657309419708326e-05\n",
      "[step: 956] loss: 9.649920684751123e-05\n",
      "[step: 957] loss: 9.64245482464321e-05\n",
      "[step: 958] loss: 9.635071910452098e-05\n",
      "[step: 959] loss: 9.627740655560046e-05\n",
      "[step: 960] loss: 9.620359196560457e-05\n",
      "[step: 961] loss: 9.613036672817543e-05\n",
      "[step: 962] loss: 9.605680679669604e-05\n",
      "[step: 963] loss: 9.598374890629202e-05\n",
      "[step: 964] loss: 9.591031994204968e-05\n",
      "[step: 965] loss: 9.583767678122967e-05\n",
      "[step: 966] loss: 9.576450247550383e-05\n",
      "[step: 967] loss: 9.569240501150489e-05\n",
      "[step: 968] loss: 9.561938350088894e-05\n",
      "[step: 969] loss: 9.554697317071259e-05\n",
      "[step: 970] loss: 9.547427907818928e-05\n",
      "[step: 971] loss: 9.540211613057181e-05\n",
      "[step: 972] loss: 9.533012052997947e-05\n",
      "[step: 973] loss: 9.525803034193814e-05\n",
      "[step: 974] loss: 9.518599108560011e-05\n",
      "[step: 975] loss: 9.5114424766507e-05\n",
      "[step: 976] loss: 9.50435278355144e-05\n",
      "[step: 977] loss: 9.49717141338624e-05\n",
      "[step: 978] loss: 9.489995136391371e-05\n",
      "[step: 979] loss: 9.482901077717543e-05\n",
      "[step: 980] loss: 9.475793194724247e-05\n",
      "[step: 981] loss: 9.468709322391078e-05\n",
      "[step: 982] loss: 9.461597801418975e-05\n",
      "[step: 983] loss: 9.454558312427253e-05\n",
      "[step: 984] loss: 9.447497723158449e-05\n",
      "[step: 985] loss: 9.440414578421041e-05\n",
      "[step: 986] loss: 9.433413652004674e-05\n",
      "[step: 987] loss: 9.426377800991759e-05\n",
      "[step: 988] loss: 9.419397974852473e-05\n",
      "[step: 989] loss: 9.412413783138618e-05\n",
      "[step: 990] loss: 9.40540776355192e-05\n",
      "[step: 991] loss: 9.398441761732101e-05\n",
      "[step: 992] loss: 9.391454659635201e-05\n",
      "[step: 993] loss: 9.384562144987285e-05\n",
      "[step: 994] loss: 9.377631067764014e-05\n",
      "[step: 995] loss: 9.370704356115311e-05\n",
      "[step: 996] loss: 9.363753633806482e-05\n",
      "[step: 997] loss: 9.356902592116967e-05\n",
      "[step: 998] loss: 9.35001444304362e-05\n",
      "[step: 999] loss: 9.343126293970272e-05\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,7,9,'month') #7은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2178.676251601592,\n",
       " 1461.6115387298612,\n",
       " 2651.7794099982207,\n",
       " 3007.6696959836563,\n",
       " 5874.4991051425168,\n",
       " 3859.3472809028963,\n",
       " 3301.8477645126191,\n",
       " 4105.2784993045379,\n",
       " 11852.520793829079]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
