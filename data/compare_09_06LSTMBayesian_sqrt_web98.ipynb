{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list(np.sqrt(rawArrayDatas[1]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list(np.sqrt(rawArrayDatas[1][:-forecastDay] ))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-(mockForcastDay+forecastDay)] & np.log\n",
    "    ds = rawArrayDatas[0][:-(mockForcastDay+forecastDay)]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-(mockForcastDay+forecastDay)]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of (mockForcastDay+forecastDay)  rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "    testY= rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출       \n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'day')\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "    \n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    print('LSTM realforecast :',realForecastDictionary['LSTM'])\n",
    "    print('Bayseian realforecast :',realForecastDictionary['Bayseian'] ) \n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "#         listedLogPredict=test_predict[-1].tolist()\n",
    "#     return [np.exp(y) for y in listedLogPredict]\n",
    "    return np.square(test_predict[-1]).tolist()\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM testforecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian testforecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 19\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('webMonth.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.2)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2007-12-01',\n",
       "  '2008-01-01',\n",
       "  '2008-02-01',\n",
       "  '2008-03-01',\n",
       "  '2008-04-01',\n",
       "  '2008-05-01',\n",
       "  '2008-06-01',\n",
       "  '2008-07-01',\n",
       "  '2008-08-01',\n",
       "  '2008-09-01',\n",
       "  '2008-10-01',\n",
       "  '2008-11-01',\n",
       "  '2008-12-01',\n",
       "  '2009-01-01',\n",
       "  '2009-02-01',\n",
       "  '2009-03-01',\n",
       "  '2009-04-01',\n",
       "  '2009-05-01',\n",
       "  '2009-06-01',\n",
       "  '2009-07-01',\n",
       "  '2009-08-01',\n",
       "  '2009-09-01',\n",
       "  '2009-10-01',\n",
       "  '2009-11-01',\n",
       "  '2009-12-01',\n",
       "  '2010-01-01',\n",
       "  '2010-02-01',\n",
       "  '2010-03-01',\n",
       "  '2010-04-01',\n",
       "  '2010-05-01',\n",
       "  '2010-06-01',\n",
       "  '2010-07-01',\n",
       "  '2010-08-01',\n",
       "  '2010-09-01',\n",
       "  '2010-10-01',\n",
       "  '2010-11-01',\n",
       "  '2010-12-01',\n",
       "  '2011-01-01',\n",
       "  '2011-02-01',\n",
       "  '2011-03-01',\n",
       "  '2011-04-01',\n",
       "  '2011-05-01',\n",
       "  '2011-06-01',\n",
       "  '2011-07-01',\n",
       "  '2011-08-01',\n",
       "  '2011-09-01',\n",
       "  '2011-10-01',\n",
       "  '2011-11-01',\n",
       "  '2011-12-01',\n",
       "  '2012-01-01',\n",
       "  '2012-02-01',\n",
       "  '2012-03-01',\n",
       "  '2012-04-01',\n",
       "  '2012-05-01',\n",
       "  '2012-06-01',\n",
       "  '2012-07-01',\n",
       "  '2012-08-01',\n",
       "  '2012-09-01',\n",
       "  '2012-10-01',\n",
       "  '2012-11-01',\n",
       "  '2012-12-01',\n",
       "  '2013-01-01',\n",
       "  '2013-02-01',\n",
       "  '2013-03-01',\n",
       "  '2013-04-01',\n",
       "  '2013-05-01',\n",
       "  '2013-06-01',\n",
       "  '2013-07-01',\n",
       "  '2013-08-01',\n",
       "  '2013-09-01',\n",
       "  '2013-10-01',\n",
       "  '2013-11-01',\n",
       "  '2013-12-01',\n",
       "  '2014-01-01',\n",
       "  '2014-02-01',\n",
       "  '2014-03-01',\n",
       "  '2014-04-01',\n",
       "  '2014-05-01',\n",
       "  '2014-06-01',\n",
       "  '2014-07-01',\n",
       "  '2014-08-01',\n",
       "  '2014-09-01',\n",
       "  '2014-10-01',\n",
       "  '2014-11-01',\n",
       "  '2014-12-01',\n",
       "  '2015-01-01',\n",
       "  '2015-02-01',\n",
       "  '2015-03-01',\n",
       "  '2015-04-01',\n",
       "  '2015-05-01',\n",
       "  '2015-06-01',\n",
       "  '2015-07-01',\n",
       "  '2015-08-01',\n",
       "  '2015-09-01',\n",
       "  '2015-10-01',\n",
       "  '2015-11-01',\n",
       "  '2015-12-01',\n",
       "  '2016-01-01'],\n",
       " [4958.0,\n",
       "  8359.2999999999993,\n",
       "  12178.14286,\n",
       "  1735.5357140000001,\n",
       "  2178.5666670000001,\n",
       "  1424.419355,\n",
       "  1237.7142859999999,\n",
       "  1343.4545449999998,\n",
       "  2100.580645,\n",
       "  3135.5999999999999,\n",
       "  2644.517241,\n",
       "  2929.0,\n",
       "  2971.4838709999999,\n",
       "  4590.8709680000002,\n",
       "  2453.4642859999999,\n",
       "  1603.2903229999999,\n",
       "  1760.5,\n",
       "  1483.4838710000001,\n",
       "  1376.5333330000001,\n",
       "  1725.9354840000001,\n",
       "  2844.6451609999999,\n",
       "  5786.4615380000005,\n",
       "  6445.6428569999998,\n",
       "  6952.2142860000004,\n",
       "  7556.0322580000002,\n",
       "  13760.172409999999,\n",
       "  12469.23077,\n",
       "  2277.3225809999999,\n",
       "  2503.9666670000001,\n",
       "  1698.451613,\n",
       "  1425.5357140000001,\n",
       "  1848.6153850000001,\n",
       "  3838.3225810000004,\n",
       "  9605.4666670000006,\n",
       "  6985.6774189999996,\n",
       "  8732.5,\n",
       "  7899.0645159999995,\n",
       "  9540.9032260000004,\n",
       "  3907.1071430000002,\n",
       "  2085.8709680000002,\n",
       "  3015.5666670000001,\n",
       "  1886.193548,\n",
       "  1695.9000000000001,\n",
       "  3470.6451609999999,\n",
       "  4622.2258060000004,\n",
       "  9086.6896550000001,\n",
       "  6864.8666670000002,\n",
       "  7461.9666670000006,\n",
       "  7556.3103449999999,\n",
       "  19610.45161,\n",
       "  25008.793099999999,\n",
       "  24771.03226,\n",
       "  3943.3103450000003,\n",
       "  3136.3870969999998,\n",
       "  2406.8666670000002,\n",
       "  3127.6451609999999,\n",
       "  5017.7741939999996,\n",
       "  13273.733329999999,\n",
       "  9823.8064519999989,\n",
       "  7519.6666670000004,\n",
       "  9169.16129,\n",
       "  13726.903230000002,\n",
       "  4432.5,\n",
       "  2776.5161290000001,\n",
       "  2286.6333329999998,\n",
       "  1913.451613,\n",
       "  1650.9333329999999,\n",
       "  2040.0333329999999,\n",
       "  2734.0645159999999,\n",
       "  13740.266669999999,\n",
       "  10564.35484,\n",
       "  7608.3999999999996,\n",
       "  8706.1935480000011,\n",
       "  26366.56667,\n",
       "  24504.89286,\n",
       "  2169.0322579999997,\n",
       "  1879.0,\n",
       "  2449.5483870000003,\n",
       "  1487.9000000000001,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArrayDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 4.056002616882324\n",
      "[step: 1] loss: 2.239607810974121\n",
      "[step: 2] loss: 1.0944609642028809\n",
      "[step: 3] loss: 0.4757412075996399\n",
      "[step: 4] loss: 0.25414401292800903\n",
      "[step: 5] loss: 0.2867094874382019\n",
      "[step: 6] loss: 0.42198461294174194\n",
      "[step: 7] loss: 0.5423859357833862\n",
      "[step: 8] loss: 0.5898133516311646\n",
      "[step: 9] loss: 0.5613851547241211\n",
      "[step: 10] loss: 0.4848596453666687\n",
      "[step: 11] loss: 0.39340585470199585\n",
      "[step: 12] loss: 0.31138330698013306\n",
      "[step: 13] loss: 0.2506399154663086\n",
      "[step: 14] loss: 0.21273574233055115\n",
      "[step: 15] loss: 0.19317218661308289\n",
      "[step: 16] loss: 0.18540246784687042\n",
      "[step: 17] loss: 0.1834794580936432\n",
      "[step: 18] loss: 0.18329213559627533\n",
      "[step: 19] loss: 0.18278414011001587\n",
      "[step: 20] loss: 0.18151724338531494\n",
      "[step: 21] loss: 0.17991620302200317\n",
      "[step: 22] loss: 0.1785127818584442\n",
      "[step: 23] loss: 0.17743057012557983\n",
      "[step: 24] loss: 0.17625868320465088\n",
      "[step: 25] loss: 0.17428365349769592\n",
      "[step: 26] loss: 0.1708613932132721\n",
      "[step: 27] loss: 0.1657021939754486\n",
      "[step: 28] loss: 0.15897271037101746\n",
      "[step: 29] loss: 0.15123845636844635\n",
      "[step: 30] loss: 0.14331188797950745\n",
      "[step: 31] loss: 0.1360662281513214\n",
      "[step: 32] loss: 0.13026116788387299\n",
      "[step: 33] loss: 0.12641018629074097\n",
      "[step: 34] loss: 0.12470407038927078\n",
      "[step: 35] loss: 0.12498960644006729\n",
      "[step: 36] loss: 0.12680098414421082\n",
      "[step: 37] loss: 0.12944309413433075\n",
      "[step: 38] loss: 0.13212357461452484\n",
      "[step: 39] loss: 0.134113147854805\n",
      "[step: 40] loss: 0.13489742577075958\n",
      "[step: 41] loss: 0.13427838683128357\n",
      "[step: 42] loss: 0.13239502906799316\n",
      "[step: 43] loss: 0.129659503698349\n",
      "[step: 44] loss: 0.1266312599182129\n",
      "[step: 45] loss: 0.12386879324913025\n",
      "[step: 46] loss: 0.12179919332265854\n",
      "[step: 47] loss: 0.12063578516244888\n",
      "[step: 48] loss: 0.1203591600060463\n",
      "[step: 49] loss: 0.12076117098331451\n",
      "[step: 50] loss: 0.12153277546167374\n",
      "[step: 51] loss: 0.12236262857913971\n",
      "[step: 52] loss: 0.12301205843687057\n",
      "[step: 53] loss: 0.12334666401147842\n",
      "[step: 54] loss: 0.12333141267299652\n",
      "[step: 55] loss: 0.12300735712051392\n",
      "[step: 56] loss: 0.12246771901845932\n",
      "[step: 57] loss: 0.12183476239442825\n",
      "[step: 58] loss: 0.12123241275548935\n",
      "[step: 59] loss: 0.12075584381818771\n",
      "[step: 60] loss: 0.1204473152756691\n",
      "[step: 61] loss: 0.12029317766427994\n",
      "[step: 62] loss: 0.12024228274822235\n",
      "[step: 63] loss: 0.12023576349020004\n",
      "[step: 64] loss: 0.12023156881332397\n",
      "[step: 65] loss: 0.12021344155073166\n",
      "[step: 66] loss: 0.12018533051013947\n",
      "[step: 67] loss: 0.12015833705663681\n",
      "[step: 68] loss: 0.12013860046863556\n",
      "[step: 69] loss: 0.12012146413326263\n",
      "[step: 70] loss: 0.12009349465370178\n",
      "[step: 71] loss: 0.12004037946462631\n",
      "[step: 72] loss: 0.1199551522731781\n",
      "[step: 73] loss: 0.11984230577945709\n",
      "[step: 74] loss: 0.11971563845872879\n",
      "[step: 75] loss: 0.11959274113178253\n",
      "[step: 76] loss: 0.11948913335800171\n",
      "[step: 77] loss: 0.11941489577293396\n",
      "[step: 78] loss: 0.11937282979488373\n",
      "[step: 79] loss: 0.11935912072658539\n",
      "[step: 80] loss: 0.11936422437429428\n",
      "[step: 81] loss: 0.11937522888183594\n",
      "[step: 82] loss: 0.11937864869832993\n",
      "[step: 83] loss: 0.11936396360397339\n",
      "[step: 84] loss: 0.11932632327079773\n",
      "[step: 85] loss: 0.11926791816949844\n",
      "[step: 86] loss: 0.1191970631480217\n",
      "[step: 87] loss: 0.11912499368190765\n",
      "[step: 88] loss: 0.11906173825263977\n",
      "[step: 89] loss: 0.11901277303695679\n",
      "[step: 90] loss: 0.11897848546504974\n",
      "[step: 91] loss: 0.11895496398210526\n",
      "[step: 92] loss: 0.11893641203641891\n",
      "[step: 93] loss: 0.11891724169254303\n",
      "[step: 94] loss: 0.11889377236366272\n",
      "[step: 95] loss: 0.11886447668075562\n",
      "[step: 96] loss: 0.11882973462343216\n",
      "[step: 97] loss: 0.11879092454910278\n",
      "[step: 98] loss: 0.11875009536743164\n",
      "[step: 99] loss: 0.11870917677879333\n",
      "[step: 100] loss: 0.11866997182369232\n",
      "[step: 101] loss: 0.11863330751657486\n",
      "[step: 102] loss: 0.11859908699989319\n",
      "[step: 103] loss: 0.11856631934642792\n",
      "[step: 104] loss: 0.11853395402431488\n",
      "[step: 105] loss: 0.11850143224000931\n",
      "[step: 106] loss: 0.11846861243247986\n",
      "[step: 107] loss: 0.11843561381101608\n",
      "[step: 108] loss: 0.11840235441923141\n",
      "[step: 109] loss: 0.11836858093738556\n",
      "[step: 110] loss: 0.11833405494689941\n",
      "[step: 111] loss: 0.11829876154661179\n",
      "[step: 112] loss: 0.11826278269290924\n",
      "[step: 113] loss: 0.11822637915611267\n",
      "[step: 114] loss: 0.11818993091583252\n",
      "[step: 115] loss: 0.11815376579761505\n",
      "[step: 116] loss: 0.11811813712120056\n",
      "[step: 117] loss: 0.11808302253484726\n",
      "[step: 118] loss: 0.1180482879281044\n",
      "[step: 119] loss: 0.1180136501789093\n",
      "[step: 120] loss: 0.11797871440649033\n",
      "[step: 121] loss: 0.117943175137043\n",
      "[step: 122] loss: 0.11790688335895538\n",
      "[step: 123] loss: 0.11786992847919464\n",
      "[step: 124] loss: 0.1178324967622757\n",
      "[step: 125] loss: 0.11779503524303436\n",
      "[step: 126] loss: 0.11775768548250198\n",
      "[step: 127] loss: 0.11772052198648453\n",
      "[step: 128] loss: 0.1176835224032402\n",
      "[step: 129] loss: 0.1176464632153511\n",
      "[step: 130] loss: 0.11760920286178589\n",
      "[step: 131] loss: 0.11757167428731918\n",
      "[step: 132] loss: 0.11753380298614502\n",
      "[step: 133] loss: 0.1174955815076828\n",
      "[step: 134] loss: 0.11745712906122208\n",
      "[step: 135] loss: 0.11741840094327927\n",
      "[step: 136] loss: 0.1173795759677887\n",
      "[step: 137] loss: 0.1173405647277832\n",
      "[step: 138] loss: 0.11730141937732697\n",
      "[step: 139] loss: 0.11726213991641998\n",
      "[step: 140] loss: 0.11722268164157867\n",
      "[step: 141] loss: 0.11718298494815826\n",
      "[step: 142] loss: 0.11714312434196472\n",
      "[step: 143] loss: 0.11710307002067566\n",
      "[step: 144] loss: 0.11706274747848511\n",
      "[step: 145] loss: 0.11702223867177963\n",
      "[step: 146] loss: 0.11698150634765625\n",
      "[step: 147] loss: 0.11694050580263138\n",
      "[step: 148] loss: 0.1168992817401886\n",
      "[step: 149] loss: 0.11685784161090851\n",
      "[step: 150] loss: 0.11681617051362991\n",
      "[step: 151] loss: 0.11677432805299759\n",
      "[step: 152] loss: 0.11673229187726974\n",
      "[step: 153] loss: 0.11668999493122101\n",
      "[step: 154] loss: 0.11664749681949615\n",
      "[step: 155] loss: 0.11660479009151459\n",
      "[step: 156] loss: 0.11656174063682556\n",
      "[step: 157] loss: 0.11651849001646042\n",
      "[step: 158] loss: 0.116474948823452\n",
      "[step: 159] loss: 0.11643119901418686\n",
      "[step: 160] loss: 0.11638715863227844\n",
      "[step: 161] loss: 0.1163429319858551\n",
      "[step: 162] loss: 0.11629839986562729\n",
      "[step: 163] loss: 0.11625361442565918\n",
      "[step: 164] loss: 0.11620859801769257\n",
      "[step: 165] loss: 0.11616328358650208\n",
      "[step: 166] loss: 0.11611772328615189\n",
      "[step: 167] loss: 0.11607185006141663\n",
      "[step: 168] loss: 0.11602569371461868\n",
      "[step: 169] loss: 0.11597920209169388\n",
      "[step: 170] loss: 0.11593250185251236\n",
      "[step: 171] loss: 0.11588545143604279\n",
      "[step: 172] loss: 0.11583812534809113\n",
      "[step: 173] loss: 0.1157904714345932\n",
      "[step: 174] loss: 0.11574253439903259\n",
      "[step: 175] loss: 0.11569425463676453\n",
      "[step: 176] loss: 0.11564566195011139\n",
      "[step: 177] loss: 0.115596704185009\n",
      "[step: 178] loss: 0.11554747074842453\n",
      "[step: 179] loss: 0.115497887134552\n",
      "[step: 180] loss: 0.11544794589281082\n",
      "[step: 181] loss: 0.1153976321220398\n",
      "[step: 182] loss: 0.11534695327281952\n",
      "[step: 183] loss: 0.11529594659805298\n",
      "[step: 184] loss: 0.1152445524930954\n",
      "[step: 185] loss: 0.11519274860620499\n",
      "[step: 186] loss: 0.11514055728912354\n",
      "[step: 187] loss: 0.11508800834417343\n",
      "[step: 188] loss: 0.11503501981496811\n",
      "[step: 189] loss: 0.11498162150382996\n",
      "[step: 190] loss: 0.11492781341075897\n",
      "[step: 191] loss: 0.11487355828285217\n",
      "[step: 192] loss: 0.11481887102127075\n",
      "[step: 193] loss: 0.11476373672485352\n",
      "[step: 194] loss: 0.11470809578895569\n",
      "[step: 195] loss: 0.11465204507112503\n",
      "[step: 196] loss: 0.11459547281265259\n",
      "[step: 197] loss: 0.11453840881586075\n",
      "[step: 198] loss: 0.11448083072900772\n",
      "[step: 199] loss: 0.1144227385520935\n",
      "[step: 200] loss: 0.1143641471862793\n",
      "[step: 201] loss: 0.11430494487285614\n",
      "[step: 202] loss: 0.11424523591995239\n",
      "[step: 203] loss: 0.11418497562408447\n",
      "[step: 204] loss: 0.11412408947944641\n",
      "[step: 205] loss: 0.1140626072883606\n",
      "[step: 206] loss: 0.11400049924850464\n",
      "[step: 207] loss: 0.11393777281045914\n",
      "[step: 208] loss: 0.1138743907213211\n",
      "[step: 209] loss: 0.11381030827760696\n",
      "[step: 210] loss: 0.11374557018280029\n",
      "[step: 211] loss: 0.11368013918399811\n",
      "[step: 212] loss: 0.11361394077539444\n",
      "[step: 213] loss: 0.11354696750640869\n",
      "[step: 214] loss: 0.11347930133342743\n",
      "[step: 215] loss: 0.11341080069541931\n",
      "[step: 216] loss: 0.11334147304296494\n",
      "[step: 217] loss: 0.11327134817838669\n",
      "[step: 218] loss: 0.11320029199123383\n",
      "[step: 219] loss: 0.11312837153673172\n",
      "[step: 220] loss: 0.1130555272102356\n",
      "[step: 221] loss: 0.11298172175884247\n",
      "[step: 222] loss: 0.11290694773197174\n",
      "[step: 223] loss: 0.11283114552497864\n",
      "[step: 224] loss: 0.11275430023670197\n",
      "[step: 225] loss: 0.11267637461423874\n",
      "[step: 226] loss: 0.11259730160236359\n",
      "[step: 227] loss: 0.11251712590456009\n",
      "[step: 228] loss: 0.1124357134103775\n",
      "[step: 229] loss: 0.11235307902097702\n",
      "[step: 230] loss: 0.11226913332939148\n",
      "[step: 231] loss: 0.11218386888504028\n",
      "[step: 232] loss: 0.11209730058908463\n",
      "[step: 233] loss: 0.1120092123746872\n",
      "[step: 234] loss: 0.11191967874765396\n",
      "[step: 235] loss: 0.11182859539985657\n",
      "[step: 236] loss: 0.11173596233129501\n",
      "[step: 237] loss: 0.11164165288209915\n",
      "[step: 238] loss: 0.1115456372499466\n",
      "[step: 239] loss: 0.11144786328077316\n",
      "[step: 240] loss: 0.1113482341170311\n",
      "[step: 241] loss: 0.11124670505523682\n",
      "[step: 242] loss: 0.11114316433668137\n",
      "[step: 243] loss: 0.11103756725788116\n",
      "[step: 244] loss: 0.11092982441186905\n",
      "[step: 245] loss: 0.11081983149051666\n",
      "[step: 246] loss: 0.11070752888917923\n",
      "[step: 247] loss: 0.11059277504682541\n",
      "[step: 248] loss: 0.11047554761171341\n",
      "[step: 249] loss: 0.11035571247339249\n",
      "[step: 250] loss: 0.1102331280708313\n",
      "[step: 251] loss: 0.11010771989822388\n",
      "[step: 252] loss: 0.10997933149337769\n",
      "[step: 253] loss: 0.10984788835048676\n",
      "[step: 254] loss: 0.10971326380968094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 255] loss: 0.10957527905702591\n",
      "[step: 256] loss: 0.10943381488323212\n",
      "[step: 257] loss: 0.10928872972726822\n",
      "[step: 258] loss: 0.10913988947868347\n",
      "[step: 259] loss: 0.10898715257644653\n",
      "[step: 260] loss: 0.10883030295372009\n",
      "[step: 261] loss: 0.1086692065000534\n",
      "[step: 262] loss: 0.10850364714860916\n",
      "[step: 263] loss: 0.10833350569009781\n",
      "[step: 264] loss: 0.10815852880477905\n",
      "[step: 265] loss: 0.10797859728336334\n",
      "[step: 266] loss: 0.107793428003788\n",
      "[step: 267] loss: 0.10760285705327988\n",
      "[step: 268] loss: 0.10740667581558228\n",
      "[step: 269] loss: 0.1072046309709549\n",
      "[step: 270] loss: 0.10699652880430222\n",
      "[step: 271] loss: 0.10678210109472275\n",
      "[step: 272] loss: 0.10656111687421799\n",
      "[step: 273] loss: 0.10633335262537003\n",
      "[step: 274] loss: 0.10609851032495499\n",
      "[step: 275] loss: 0.10585635900497437\n",
      "[step: 276] loss: 0.10560666024684906\n",
      "[step: 277] loss: 0.10534914582967758\n",
      "[step: 278] loss: 0.10508349537849426\n",
      "[step: 279] loss: 0.10480953752994537\n",
      "[step: 280] loss: 0.10452690720558167\n",
      "[step: 281] loss: 0.1042354479432106\n",
      "[step: 282] loss: 0.10393474251031876\n",
      "[step: 283] loss: 0.10362468659877777\n",
      "[step: 284] loss: 0.10330493003129959\n",
      "[step: 285] loss: 0.10297524183988571\n",
      "[step: 286] loss: 0.10263536125421524\n",
      "[step: 287] loss: 0.10228507220745087\n",
      "[step: 288] loss: 0.10192416608333588\n",
      "[step: 289] loss: 0.10155239701271057\n",
      "[step: 290] loss: 0.10116957128047943\n",
      "[step: 291] loss: 0.10077552497386932\n",
      "[step: 292] loss: 0.10037007927894592\n",
      "[step: 293] loss: 0.0999530628323555\n",
      "[step: 294] loss: 0.09952437877655029\n",
      "[step: 295] loss: 0.09908393025398254\n",
      "[step: 296] loss: 0.0986318364739418\n",
      "[step: 297] loss: 0.09817111492156982\n",
      "[step: 298] loss: 0.09775519371032715\n",
      "[step: 299] loss: 0.09824085235595703\n",
      "[step: 300] loss: 0.10331606864929199\n",
      "[step: 301] loss: 0.1011490523815155\n",
      "[step: 302] loss: 0.09656496345996857\n",
      "[step: 303] loss: 0.1011829525232315\n",
      "[step: 304] loss: 0.09520179033279419\n",
      "[step: 305] loss: 0.0990542322397232\n",
      "[step: 306] loss: 0.09507261216640472\n",
      "[step: 307] loss: 0.09662695229053497\n",
      "[step: 308] loss: 0.09491726756095886\n",
      "[step: 309] loss: 0.09452693909406662\n",
      "[step: 310] loss: 0.09451987594366074\n",
      "[step: 311] loss: 0.09281308948993683\n",
      "[step: 312] loss: 0.09384128451347351\n",
      "[step: 313] loss: 0.09147166460752487\n",
      "[step: 314] loss: 0.09284651279449463\n",
      "[step: 315] loss: 0.09050323814153671\n",
      "[step: 316] loss: 0.0915052592754364\n",
      "[step: 317] loss: 0.08987264335155487\n",
      "[step: 318] loss: 0.0898725688457489\n",
      "[step: 319] loss: 0.08941502869129181\n",
      "[step: 320] loss: 0.0882517546415329\n",
      "[step: 321] loss: 0.08869078755378723\n",
      "[step: 322] loss: 0.08716289699077606\n",
      "[step: 323] loss: 0.0872718095779419\n",
      "[step: 324] loss: 0.08668957650661469\n",
      "[step: 325] loss: 0.08563414961099625\n",
      "[step: 326] loss: 0.08582551777362823\n",
      "[step: 327] loss: 0.08489636331796646\n",
      "[step: 328] loss: 0.08415201306343079\n",
      "[step: 329] loss: 0.08419296145439148\n",
      "[step: 330] loss: 0.08340324461460114\n",
      "[step: 331] loss: 0.08256909251213074\n",
      "[step: 332] loss: 0.08245435357093811\n",
      "[step: 333] loss: 0.08209257572889328\n",
      "[step: 334] loss: 0.08122996985912323\n",
      "[step: 335] loss: 0.08060381561517715\n",
      "[step: 336] loss: 0.08039337396621704\n",
      "[step: 337] loss: 0.08014050126075745\n",
      "[step: 338] loss: 0.07957986742258072\n",
      "[step: 339] loss: 0.07886575162410736\n",
      "[step: 340] loss: 0.07824437320232391\n",
      "[step: 341] loss: 0.07780801504850388\n",
      "[step: 342] loss: 0.07751350104808807\n",
      "[step: 343] loss: 0.07730228453874588\n",
      "[step: 344] loss: 0.07716397941112518\n",
      "[step: 345] loss: 0.07712546736001968\n",
      "[step: 346] loss: 0.07723657786846161\n",
      "[step: 347] loss: 0.07745054364204407\n",
      "[step: 348] loss: 0.07758660614490509\n",
      "[step: 349] loss: 0.07715001702308655\n",
      "[step: 350] loss: 0.07591751217842102\n",
      "[step: 351] loss: 0.07432392984628677\n",
      "[step: 352] loss: 0.07338029891252518\n",
      "[step: 353] loss: 0.07342829555273056\n",
      "[step: 354] loss: 0.07380455732345581\n",
      "[step: 355] loss: 0.07364107668399811\n",
      "[step: 356] loss: 0.07275263220071793\n",
      "[step: 357] loss: 0.07182963192462921\n",
      "[step: 358] loss: 0.07150197774171829\n",
      "[step: 359] loss: 0.07162795215845108\n",
      "[step: 360] loss: 0.07161848992109299\n",
      "[step: 361] loss: 0.07115137577056885\n",
      "[step: 362] loss: 0.07045512646436691\n",
      "[step: 363] loss: 0.06994090229272842\n",
      "[step: 364] loss: 0.06976436823606491\n",
      "[step: 365] loss: 0.06975996494293213\n",
      "[step: 366] loss: 0.06968449056148529\n",
      "[step: 367] loss: 0.06942219287157059\n",
      "[step: 368] loss: 0.0690029039978981\n",
      "[step: 369] loss: 0.06854698807001114\n",
      "[step: 370] loss: 0.06815815716981888\n",
      "[step: 371] loss: 0.06787954270839691\n",
      "[step: 372] loss: 0.06769946217536926\n",
      "[step: 373] loss: 0.0675860121846199\n",
      "[step: 374] loss: 0.06751371920108795\n",
      "[step: 375] loss: 0.06747151911258698\n",
      "[step: 376] loss: 0.06746234744787216\n",
      "[step: 377] loss: 0.06748148053884506\n",
      "[step: 378] loss: 0.06752335280179977\n",
      "[step: 379] loss: 0.06752822548151016\n",
      "[step: 380] loss: 0.06742766499519348\n",
      "[step: 381] loss: 0.0671083927154541\n",
      "[step: 382] loss: 0.06657138466835022\n",
      "[step: 383] loss: 0.06593155860900879\n",
      "[step: 384] loss: 0.06542392820119858\n",
      "[step: 385] loss: 0.0652017891407013\n",
      "[step: 386] loss: 0.06522494554519653\n",
      "[step: 387] loss: 0.06530261039733887\n",
      "[step: 388] loss: 0.065242700278759\n",
      "[step: 389] loss: 0.06498981267213821\n",
      "[step: 390] loss: 0.06463642418384552\n",
      "[step: 391] loss: 0.06433874368667603\n",
      "[step: 392] loss: 0.06418377161026001\n",
      "[step: 393] loss: 0.06414676457643509\n",
      "[step: 394] loss: 0.06414072215557098\n",
      "[step: 395] loss: 0.06408888101577759\n",
      "[step: 396] loss: 0.06396397203207016\n",
      "[step: 397] loss: 0.06378019601106644\n",
      "[step: 398] loss: 0.06357435882091522\n",
      "[step: 399] loss: 0.06337807327508926\n",
      "[step: 400] loss: 0.0632089376449585\n",
      "[step: 401] loss: 0.06306980550289154\n",
      "[step: 402] loss: 0.06295570731163025\n",
      "[step: 403] loss: 0.06286093592643738\n",
      "[step: 404] loss: 0.06278367340564728\n",
      "[step: 405] loss: 0.06272894144058228\n",
      "[step: 406] loss: 0.06271322071552277\n",
      "[step: 407] loss: 0.06277439743280411\n",
      "[step: 408] loss: 0.06299136579036713\n",
      "[step: 409] loss: 0.06352709233760834\n",
      "[step: 410] loss: 0.06465187668800354\n",
      "[step: 411] loss: 0.06668436527252197\n",
      "[step: 412] loss: 0.06924192607402802\n",
      "[step: 413] loss: 0.07024101912975311\n",
      "[step: 414] loss: 0.06696215271949768\n",
      "[step: 415] loss: 0.06239267811179161\n",
      "[step: 416] loss: 0.06262335181236267\n",
      "[step: 417] loss: 0.0653611496090889\n",
      "[step: 418] loss: 0.06405695527791977\n",
      "[step: 419] loss: 0.06164950504899025\n",
      "[step: 420] loss: 0.06312785297632217\n",
      "[step: 421] loss: 0.06373538076877594\n",
      "[step: 422] loss: 0.0617222860455513\n",
      "[step: 423] loss: 0.06206689774990082\n",
      "[step: 424] loss: 0.06303885579109192\n",
      "[step: 425] loss: 0.06169614940881729\n",
      "[step: 426] loss: 0.06140120327472687\n",
      "[step: 427] loss: 0.062330156564712524\n",
      "[step: 428] loss: 0.061659540981054306\n",
      "[step: 429] loss: 0.060928601771593094\n",
      "[step: 430] loss: 0.06154844909906387\n",
      "[step: 431] loss: 0.061617013067007065\n",
      "[step: 432] loss: 0.060819078236818314\n",
      "[step: 433] loss: 0.06074019894003868\n",
      "[step: 434] loss: 0.061215899884700775\n",
      "[step: 435] loss: 0.06106734648346901\n",
      "[step: 436] loss: 0.06050575524568558\n",
      "[step: 437] loss: 0.06041496619582176\n",
      "[step: 438] loss: 0.060728348791599274\n",
      "[step: 439] loss: 0.060785312205553055\n",
      "[step: 440] loss: 0.060445141047239304\n",
      "[step: 441] loss: 0.060137417167425156\n",
      "[step: 442] loss: 0.06015360355377197\n",
      "[step: 443] loss: 0.06032751873135567\n",
      "[step: 444] loss: 0.060351260006427765\n",
      "[step: 445] loss: 0.06015044450759888\n",
      "[step: 446] loss: 0.059904709458351135\n",
      "[step: 447] loss: 0.05980238318443298\n",
      "[step: 448] loss: 0.0598500594496727\n",
      "[step: 449] loss: 0.05990931764245033\n",
      "[step: 450] loss: 0.05986107140779495\n",
      "[step: 451] loss: 0.0597117617726326\n",
      "[step: 452] loss: 0.05956050381064415\n",
      "[step: 453] loss: 0.05948977917432785\n",
      "[step: 454] loss: 0.059495169669389725\n",
      "[step: 455] loss: 0.05950884521007538\n",
      "[step: 456] loss: 0.059471823275089264\n",
      "[step: 457] loss: 0.059380315244197845\n",
      "[step: 458] loss: 0.05927550792694092\n",
      "[step: 459] loss: 0.05920010060071945\n",
      "[step: 460] loss: 0.05916594713926315\n",
      "[step: 461] loss: 0.05915418267250061\n",
      "[step: 462] loss: 0.059137117117643356\n",
      "[step: 463] loss: 0.05909830331802368\n",
      "[step: 464] loss: 0.059038206934928894\n",
      "[step: 465] loss: 0.05896816402673721\n",
      "[step: 466] loss: 0.058900490403175354\n",
      "[step: 467] loss: 0.05884268507361412\n",
      "[step: 468] loss: 0.058796316385269165\n",
      "[step: 469] loss: 0.05875934287905693\n",
      "[step: 470] loss: 0.05872888118028641\n",
      "[step: 471] loss: 0.05870310217142105\n",
      "[step: 472] loss: 0.05868209898471832\n",
      "[step: 473] loss: 0.05866833031177521\n",
      "[step: 474] loss: 0.058667492121458054\n",
      "[step: 475] loss: 0.058690257370471954\n",
      "[step: 476] loss: 0.05875670909881592\n",
      "[step: 477] loss: 0.05890388786792755\n",
      "[step: 478] loss: 0.05920091271400452\n",
      "[step: 479] loss: 0.05976680666208267\n",
      "[step: 480] loss: 0.06078513711690903\n",
      "[step: 481] loss: 0.062421075999736786\n",
      "[step: 482] loss: 0.06452029943466187\n",
      "[step: 483] loss: 0.06590606272220612\n",
      "[step: 484] loss: 0.06465281546115875\n",
      "[step: 485] loss: 0.06072676181793213\n",
      "[step: 486] loss: 0.058232471346855164\n",
      "[step: 487] loss: 0.059724532067775726\n",
      "[step: 488] loss: 0.06143965572118759\n",
      "[step: 489] loss: 0.059867288917303085\n",
      "[step: 490] loss: 0.05822043493390083\n",
      "[step: 491] loss: 0.05942319706082344\n",
      "[step: 492] loss: 0.06011446937918663\n",
      "[step: 493] loss: 0.05863779038190842\n",
      "[step: 494] loss: 0.058273714035749435\n",
      "[step: 495] loss: 0.05933142453432083\n",
      "[step: 496] loss: 0.059053659439086914\n",
      "[step: 497] loss: 0.05802803859114647\n",
      "[step: 498] loss: 0.05827891454100609\n",
      "[step: 499] loss: 0.058890312910079956\n",
      "[step: 500] loss: 0.0584491491317749\n",
      "[step: 501] loss: 0.057793207466602325\n",
      "[step: 502] loss: 0.057979192584753036\n",
      "[step: 503] loss: 0.05843633413314819\n",
      "[step: 504] loss: 0.05830550193786621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 505] loss: 0.05779936909675598\n",
      "[step: 506] loss: 0.057600103318691254\n",
      "[step: 507] loss: 0.057839058339595795\n",
      "[step: 508] loss: 0.05810234695672989\n",
      "[step: 509] loss: 0.05805432051420212\n",
      "[step: 510] loss: 0.05775976553559303\n",
      "[step: 511] loss: 0.05749909207224846\n",
      "[step: 512] loss: 0.05745594948530197\n",
      "[step: 513] loss: 0.05758638307452202\n",
      "[step: 514] loss: 0.05771888419985771\n",
      "[step: 515] loss: 0.05772031843662262\n",
      "[step: 516] loss: 0.057580962777137756\n",
      "[step: 517] loss: 0.05739665776491165\n",
      "[step: 518] loss: 0.05727636441588402\n",
      "[step: 519] loss: 0.05726403743028641\n",
      "[step: 520] loss: 0.057319968938827515\n",
      "[step: 521] loss: 0.05736570432782173\n",
      "[step: 522] loss: 0.057346656918525696\n",
      "[step: 523] loss: 0.05726398527622223\n",
      "[step: 524] loss: 0.05716385319828987\n",
      "[step: 525] loss: 0.05709563195705414\n",
      "[step: 526] loss: 0.05707792937755585\n",
      "[step: 527] loss: 0.05709299445152283\n",
      "[step: 528] loss: 0.05710697919130325\n",
      "[step: 529] loss: 0.057095255702733994\n",
      "[step: 530] loss: 0.0570540726184845\n",
      "[step: 531] loss: 0.0569971427321434\n",
      "[step: 532] loss: 0.056942641735076904\n",
      "[step: 533] loss: 0.056902818381786346\n",
      "[step: 534] loss: 0.056880202144384384\n",
      "[step: 535] loss: 0.05687014013528824\n",
      "[step: 536] loss: 0.056865643709897995\n",
      "[step: 537] loss: 0.05686112493276596\n",
      "[step: 538] loss: 0.05685386806726456\n",
      "[step: 539] loss: 0.05684361606836319\n",
      "[step: 540] loss: 0.05683201923966408\n",
      "[step: 541] loss: 0.05682135745882988\n",
      "[step: 542] loss: 0.056814733892679214\n",
      "[step: 543] loss: 0.056815750896930695\n",
      "[step: 544] loss: 0.056830115616321564\n",
      "[step: 545] loss: 0.05686585232615471\n",
      "[step: 546] loss: 0.05693720281124115\n",
      "[step: 547] loss: 0.057065777480602264\n",
      "[step: 548] loss: 0.05728892236948013\n",
      "[step: 549] loss: 0.05765809863805771\n",
      "[step: 550] loss: 0.05824749916791916\n",
      "[step: 551] loss: 0.05910439044237137\n",
      "[step: 552] loss: 0.06019892171025276\n",
      "[step: 553] loss: 0.06117812171578407\n",
      "[step: 554] loss: 0.06139695271849632\n",
      "[step: 555] loss: 0.060152504593133926\n",
      "[step: 556] loss: 0.05796054005622864\n",
      "[step: 557] loss: 0.056531235575675964\n",
      "[step: 558] loss: 0.056980445981025696\n",
      "[step: 559] loss: 0.058166492730379105\n",
      "[step: 560] loss: 0.058180373162031174\n",
      "[step: 561] loss: 0.057039715349674225\n",
      "[step: 562] loss: 0.05644422769546509\n",
      "[step: 563] loss: 0.05702909827232361\n",
      "[step: 564] loss: 0.05755922198295593\n",
      "[step: 565] loss: 0.05712244287133217\n",
      "[step: 566] loss: 0.05642852187156677\n",
      "[step: 567] loss: 0.05641868710517883\n",
      "[step: 568] loss: 0.05688706785440445\n",
      "[step: 569] loss: 0.057046424597501755\n",
      "[step: 570] loss: 0.056682877242565155\n",
      "[step: 571] loss: 0.05626324564218521\n",
      "[step: 572] loss: 0.05620444193482399\n",
      "[step: 573] loss: 0.05645293742418289\n",
      "[step: 574] loss: 0.05668959021568298\n",
      "[step: 575] loss: 0.05669533833861351\n",
      "[step: 576] loss: 0.056491971015930176\n",
      "[step: 577] loss: 0.05623621121048927\n",
      "[step: 578] loss: 0.056075770407915115\n",
      "[step: 579] loss: 0.05606098845601082\n",
      "[step: 580] loss: 0.056153349578380585\n",
      "[step: 581] loss: 0.056277066469192505\n",
      "[step: 582] loss: 0.05636442452669144\n",
      "[step: 583] loss: 0.05638013407588005\n",
      "[step: 584] loss: 0.056319452822208405\n",
      "[step: 585] loss: 0.056206196546554565\n",
      "[step: 586] loss: 0.056075021624565125\n",
      "[step: 587] loss: 0.055962637066841125\n",
      "[step: 588] loss: 0.05589279532432556\n",
      "[step: 589] loss: 0.05587124079465866\n",
      "[step: 590] loss: 0.055886369198560715\n",
      "[step: 591] loss: 0.05591670051217079\n",
      "[step: 592] loss: 0.0559406578540802\n",
      "[step: 593] loss: 0.05594353750348091\n",
      "[step: 594] loss: 0.055921636521816254\n",
      "[step: 595] loss: 0.05587977170944214\n",
      "[step: 596] loss: 0.055828504264354706\n",
      "[step: 597] loss: 0.05577830225229263\n",
      "[step: 598] loss: 0.055736787617206573\n",
      "[step: 599] loss: 0.055707093328237534\n",
      "[step: 600] loss: 0.055688660591840744\n",
      "[step: 601] loss: 0.05567888915538788\n",
      "[step: 602] loss: 0.055674903094768524\n",
      "[step: 603] loss: 0.05567469075322151\n",
      "[step: 604] loss: 0.05567758530378342\n",
      "[step: 605] loss: 0.05568461865186691\n",
      "[step: 606] loss: 0.05569838732481003\n",
      "[step: 607] loss: 0.0557241290807724\n",
      "[step: 608] loss: 0.055769965052604675\n",
      "[step: 609] loss: 0.05585038289427757\n",
      "[step: 610] loss: 0.05598803609609604\n",
      "[step: 611] loss: 0.05622265487909317\n",
      "[step: 612] loss: 0.05661095678806305\n",
      "[step: 613] loss: 0.05724039673805237\n",
      "[step: 614] loss: 0.0581812709569931\n",
      "[step: 615] loss: 0.05944467708468437\n",
      "[step: 616] loss: 0.060692254453897476\n",
      "[step: 617] loss: 0.061213213950395584\n",
      "[step: 618] loss: 0.06003277376294136\n",
      "[step: 619] loss: 0.057498857378959656\n",
      "[step: 620] loss: 0.05559922009706497\n",
      "[step: 621] loss: 0.055982284247875214\n",
      "[step: 622] loss: 0.057417191565036774\n",
      "[step: 623] loss: 0.0574374794960022\n",
      "[step: 624] loss: 0.05606527626514435\n",
      "[step: 625] loss: 0.05550298094749451\n",
      "[step: 626] loss: 0.05632447451353073\n",
      "[step: 627] loss: 0.05676545947790146\n",
      "[step: 628] loss: 0.056018929928541183\n",
      "[step: 629] loss: 0.055383265018463135\n",
      "[step: 630] loss: 0.05572595074772835\n",
      "[step: 631] loss: 0.056222833693027496\n",
      "[step: 632] loss: 0.05600861832499504\n",
      "[step: 633] loss: 0.05542847514152527\n",
      "[step: 634] loss: 0.05525999516248703\n",
      "[step: 635] loss: 0.05557650327682495\n",
      "[step: 636] loss: 0.05586253106594086\n",
      "[step: 637] loss: 0.05577402561903\n",
      "[step: 638] loss: 0.05543454736471176\n",
      "[step: 639] loss: 0.055177319794893265\n",
      "[step: 640] loss: 0.0551765151321888\n",
      "[step: 641] loss: 0.05535731464624405\n",
      "[step: 642] loss: 0.055534400045871735\n",
      "[step: 643] loss: 0.05557258427143097\n",
      "[step: 644] loss: 0.05545864626765251\n",
      "[step: 645] loss: 0.055268269032239914\n",
      "[step: 646] loss: 0.055105648934841156\n",
      "[step: 647] loss: 0.05503649264574051\n",
      "[step: 648] loss: 0.05506296455860138\n",
      "[step: 649] loss: 0.05513649433851242\n",
      "[step: 650] loss: 0.05519355833530426\n",
      "[step: 651] loss: 0.055192623287439346\n",
      "[step: 652] loss: 0.05513010546565056\n",
      "[step: 653] loss: 0.05503775179386139\n",
      "[step: 654] loss: 0.05495777353644371\n",
      "[step: 655] loss: 0.054919060319662094\n",
      "[step: 656] loss: 0.054922983050346375\n",
      "[step: 657] loss: 0.0549483597278595\n",
      "[step: 658] loss: 0.054967641830444336\n",
      "[step: 659] loss: 0.0549626424908638\n",
      "[step: 660] loss: 0.05493197962641716\n",
      "[step: 661] loss: 0.0548868328332901\n",
      "[step: 662] loss: 0.05484269931912422\n",
      "[step: 663] loss: 0.05481070280075073\n",
      "[step: 664] loss: 0.054794248193502426\n",
      "[step: 665] loss: 0.05479005351662636\n",
      "[step: 666] loss: 0.05479186400771141\n",
      "[step: 667] loss: 0.05479384958744049\n",
      "[step: 668] loss: 0.05479231849312782\n",
      "[step: 669] loss: 0.054786089807748795\n",
      "[step: 670] loss: 0.05477570742368698\n",
      "[step: 671] loss: 0.0547625869512558\n",
      "[step: 672] loss: 0.05474820360541344\n",
      "[step: 673] loss: 0.054734066128730774\n",
      "[step: 674] loss: 0.05472125485539436\n",
      "[step: 675] loss: 0.05471084639430046\n",
      "[step: 676] loss: 0.054703861474990845\n",
      "[step: 677] loss: 0.0547017976641655\n",
      "[step: 678] loss: 0.0547066330909729\n",
      "[step: 679] loss: 0.05472181364893913\n",
      "[step: 680] loss: 0.054752103984355927\n",
      "[step: 681] loss: 0.0548056922852993\n",
      "[step: 682] loss: 0.05489398539066315\n",
      "[step: 683] loss: 0.055035725235939026\n",
      "[step: 684] loss: 0.055254172533750534\n",
      "[step: 685] loss: 0.05558409541845322\n",
      "[step: 686] loss: 0.05605129152536392\n",
      "[step: 687] loss: 0.05667351186275482\n",
      "[step: 688] loss: 0.05737198889255524\n",
      "[step: 689] loss: 0.057972297072410583\n",
      "[step: 690] loss: 0.05808689072728157\n",
      "[step: 691] loss: 0.05743761360645294\n",
      "[step: 692] loss: 0.05609646812081337\n",
      "[step: 693] loss: 0.0548369362950325\n",
      "[step: 694] loss: 0.054475512355566025\n",
      "[step: 695] loss: 0.05503865331411362\n",
      "[step: 696] loss: 0.05568251758813858\n",
      "[step: 697] loss: 0.05561203882098198\n",
      "[step: 698] loss: 0.054942674934864044\n",
      "[step: 699] loss: 0.054432038217782974\n",
      "[step: 700] loss: 0.05453956127166748\n",
      "[step: 701] loss: 0.05496255308389664\n",
      "[step: 702] loss: 0.05513259768486023\n",
      "[step: 703] loss: 0.054875198751688004\n",
      "[step: 704] loss: 0.05447345972061157\n",
      "[step: 705] loss: 0.054284851998090744\n",
      "[step: 706] loss: 0.054393839091062546\n",
      "[step: 707] loss: 0.054628342390060425\n",
      "[step: 708] loss: 0.05477745831012726\n",
      "[step: 709] loss: 0.05474650859832764\n",
      "[step: 710] loss: 0.054578378796577454\n",
      "[step: 711] loss: 0.05437428504228592\n",
      "[step: 712] loss: 0.0542256161570549\n",
      "[step: 713] loss: 0.05417151004076004\n",
      "[step: 714] loss: 0.054202258586883545\n",
      "[step: 715] loss: 0.05428141728043556\n",
      "[step: 716] loss: 0.05436839908361435\n",
      "[step: 717] loss: 0.05443285405635834\n",
      "[step: 718] loss: 0.054456677287817\n",
      "[step: 719] loss: 0.05443662405014038\n",
      "[step: 720] loss: 0.054377228021621704\n",
      "[step: 721] loss: 0.054292697459459305\n",
      "[step: 722] loss: 0.05419924482703209\n",
      "[step: 723] loss: 0.05411446839570999\n",
      "[step: 724] loss: 0.0540509894490242\n",
      "[step: 725] loss: 0.05401487648487091\n",
      "[step: 726] loss: 0.054004356265068054\n",
      "[step: 727] loss: 0.054011911153793335\n",
      "[step: 728] loss: 0.054027438163757324\n",
      "[step: 729] loss: 0.05404163524508476\n",
      "[step: 730] loss: 0.054048292338848114\n",
      "[step: 731] loss: 0.05404461547732353\n",
      "[step: 732] loss: 0.05403132736682892\n",
      "[step: 733] loss: 0.05401064082980156\n",
      "[step: 734] loss: 0.0539858341217041\n",
      "[step: 735] loss: 0.05395958572626114\n",
      "[step: 736] loss: 0.05393413454294205\n",
      "[step: 737] loss: 0.053910620510578156\n",
      "[step: 738] loss: 0.0538896769285202\n",
      "[step: 739] loss: 0.05387144535779953\n",
      "[step: 740] loss: 0.053856011480093\n",
      "[step: 741] loss: 0.05384339392185211\n",
      "[step: 742] loss: 0.053834084421396255\n",
      "[step: 743] loss: 0.05382905900478363\n",
      "[step: 744] loss: 0.053830262273550034\n",
      "[step: 745] loss: 0.0538412481546402\n",
      "[step: 746] loss: 0.053868379443883896\n",
      "[step: 747] loss: 0.05392249673604965\n",
      "[step: 748] loss: 0.0540231317281723\n",
      "[step: 749] loss: 0.05420251935720444\n",
      "[step: 750] loss: 0.05451697111129761\n",
      "[step: 751] loss: 0.05504946783185005\n",
      "[step: 752] loss: 0.05592314526438713\n",
      "[step: 753] loss: 0.05722849816083908\n",
      "[step: 754] loss: 0.05893121287226677\n",
      "[step: 755] loss: 0.06044168025255203\n",
      "[step: 756] loss: 0.060650911182165146\n",
      "[step: 757] loss: 0.058467842638492584\n",
      "[step: 758] loss: 0.05513659864664078\n",
      "[step: 759] loss: 0.05371123552322388\n",
      "[step: 760] loss: 0.05516066402196884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 761] loss: 0.05648218095302582\n",
      "[step: 762] loss: 0.05531848594546318\n",
      "[step: 763] loss: 0.05380221828818321\n",
      "[step: 764] loss: 0.05435928702354431\n",
      "[step: 765] loss: 0.05536745488643646\n",
      "[step: 766] loss: 0.05472143366932869\n",
      "[step: 767] loss: 0.0537002831697464\n",
      "[step: 768] loss: 0.054037488996982574\n",
      "[step: 769] loss: 0.05472887307405472\n",
      "[step: 770] loss: 0.054368432611227036\n",
      "[step: 771] loss: 0.053621985018253326\n",
      "[step: 772] loss: 0.053646381944417953\n",
      "[step: 773] loss: 0.05416981875896454\n",
      "[step: 774] loss: 0.05426378548145294\n",
      "[step: 775] loss: 0.05380944162607193\n",
      "[step: 776] loss: 0.05344036966562271\n",
      "[step: 777] loss: 0.05353865772485733\n",
      "[step: 778] loss: 0.0538615956902504\n",
      "[step: 779] loss: 0.05398765206336975\n",
      "[step: 780] loss: 0.05379386991262436\n",
      "[step: 781] loss: 0.053494229912757874\n",
      "[step: 782] loss: 0.05334855616092682\n",
      "[step: 783] loss: 0.05342436581850052\n",
      "[step: 784] loss: 0.05358676612377167\n",
      "[step: 785] loss: 0.05365859717130661\n",
      "[step: 786] loss: 0.05357322841882706\n",
      "[step: 787] loss: 0.05340225622057915\n",
      "[step: 788] loss: 0.05327456444501877\n",
      "[step: 789] loss: 0.05326124280691147\n",
      "[step: 790] loss: 0.053329527378082275\n",
      "[step: 791] loss: 0.05338829755783081\n",
      "[step: 792] loss: 0.053372107446193695\n",
      "[step: 793] loss: 0.05329037085175514\n",
      "[step: 794] loss: 0.05320468544960022\n",
      "[step: 795] loss: 0.05316932499408722\n",
      "[step: 796] loss: 0.05318762734532356\n",
      "[step: 797] loss: 0.05322009325027466\n",
      "[step: 798] loss: 0.05322478339076042\n",
      "[step: 799] loss: 0.053190261125564575\n",
      "[step: 800] loss: 0.053137294948101044\n",
      "[step: 801] loss: 0.05309556797146797\n",
      "[step: 802] loss: 0.05308043956756592\n",
      "[step: 803] loss: 0.05308607220649719\n",
      "[step: 804] loss: 0.053095076233148575\n",
      "[step: 805] loss: 0.0530925914645195\n",
      "[step: 806] loss: 0.05307410657405853\n",
      "[step: 807] loss: 0.05304504930973053\n",
      "[step: 808] loss: 0.05301472172141075\n",
      "[step: 809] loss: 0.0529906190931797\n",
      "[step: 810] loss: 0.052975647151470184\n",
      "[step: 811] loss: 0.05296824872493744\n",
      "[step: 812] loss: 0.05296467989683151\n",
      "[step: 813] loss: 0.052960995584726334\n",
      "[step: 814] loss: 0.052954480051994324\n",
      "[step: 815] loss: 0.052944011986255646\n",
      "[step: 816] loss: 0.05292985960841179\n",
      "[step: 817] loss: 0.05291302502155304\n",
      "[step: 818] loss: 0.052894871681928635\n",
      "[step: 819] loss: 0.052876509726047516\n",
      "[step: 820] loss: 0.0528588704764843\n",
      "[step: 821] loss: 0.0528424009680748\n",
      "[step: 822] loss: 0.05282728374004364\n",
      "[step: 823] loss: 0.05281341075897217\n",
      "[step: 824] loss: 0.05280061066150665\n",
      "[step: 825] loss: 0.052788615226745605\n",
      "[step: 826] loss: 0.05277718976140022\n",
      "[step: 827] loss: 0.0527661070227623\n",
      "[step: 828] loss: 0.05275529623031616\n",
      "[step: 829] loss: 0.052744634449481964\n",
      "[step: 830] loss: 0.05273424834012985\n",
      "[step: 831] loss: 0.052724115550518036\n",
      "[step: 832] loss: 0.052714500576257706\n",
      "[step: 833] loss: 0.052705634385347366\n",
      "[step: 834] loss: 0.052697956562042236\n",
      "[step: 835] loss: 0.052692122757434845\n",
      "[step: 836] loss: 0.052689120173454285\n",
      "[step: 837] loss: 0.05269046127796173\n",
      "[step: 838] loss: 0.052698589861392975\n",
      "[step: 839] loss: 0.05271748825907707\n",
      "[step: 840] loss: 0.05275335907936096\n",
      "[step: 841] loss: 0.05281662568449974\n",
      "[step: 842] loss: 0.052923351526260376\n",
      "[step: 843] loss: 0.053100213408470154\n",
      "[step: 844] loss: 0.0533854216337204\n",
      "[step: 845] loss: 0.053836483508348465\n",
      "[step: 846] loss: 0.05451243370771408\n",
      "[step: 847] loss: 0.055460840463638306\n",
      "[step: 848] loss: 0.05658997595310211\n",
      "[step: 849] loss: 0.05759415403008461\n",
      "[step: 850] loss: 0.05777961388230324\n",
      "[step: 851] loss: 0.05660343915224075\n",
      "[step: 852] loss: 0.05437135323882103\n",
      "[step: 853] loss: 0.05269457772374153\n",
      "[step: 854] loss: 0.05283364653587341\n",
      "[step: 855] loss: 0.05404471978545189\n",
      "[step: 856] loss: 0.054431501775979996\n",
      "[step: 857] loss: 0.05344783514738083\n",
      "[step: 858] loss: 0.052543774247169495\n",
      "[step: 859] loss: 0.05283258110284805\n",
      "[step: 860] loss: 0.05354033783078194\n",
      "[step: 861] loss: 0.05345798283815384\n",
      "[step: 862] loss: 0.05273621529340744\n",
      "[step: 863] loss: 0.05240148305892944\n",
      "[step: 864] loss: 0.052746180444955826\n",
      "[step: 865] loss: 0.053119659423828125\n",
      "[step: 866] loss: 0.05299118533730507\n",
      "[step: 867] loss: 0.052549149841070175\n",
      "[step: 868] loss: 0.052298784255981445\n",
      "[step: 869] loss: 0.052423786371946335\n",
      "[step: 870] loss: 0.05269358307123184\n",
      "[step: 871] loss: 0.05280448496341705\n",
      "[step: 872] loss: 0.05266543850302696\n",
      "[step: 873] loss: 0.05241302400827408\n",
      "[step: 874] loss: 0.05223695933818817\n",
      "[step: 875] loss: 0.05222705006599426\n",
      "[step: 876] loss: 0.05233646184206009\n",
      "[step: 877] loss: 0.05245019495487213\n",
      "[step: 878] loss: 0.052476830780506134\n",
      "[step: 879] loss: 0.05239812284708023\n",
      "[step: 880] loss: 0.05226511508226395\n",
      "[step: 881] loss: 0.05215200036764145\n",
      "[step: 882] loss: 0.05210776627063751\n",
      "[step: 883] loss: 0.05213111639022827\n",
      "[step: 884] loss: 0.05218125134706497\n",
      "[step: 885] loss: 0.05221014842391014\n",
      "[step: 886] loss: 0.05219238996505737\n",
      "[step: 887] loss: 0.05213582143187523\n",
      "[step: 888] loss: 0.052069585770368576\n",
      "[step: 889] loss: 0.052022941410541534\n",
      "[step: 890] loss: 0.05200817808508873\n",
      "[step: 891] loss: 0.05201764404773712\n",
      "[step: 892] loss: 0.0520329512655735\n",
      "[step: 893] loss: 0.05203719437122345\n",
      "[step: 894] loss: 0.05202322453260422\n",
      "[step: 895] loss: 0.05199410766363144\n",
      "[step: 896] loss: 0.0519588440656662\n",
      "[step: 897] loss: 0.05192667990922928\n",
      "[step: 898] loss: 0.05190319940447807\n",
      "[step: 899] loss: 0.051889486610889435\n",
      "[step: 900] loss: 0.05188325047492981\n",
      "[step: 901] loss: 0.05188083276152611\n",
      "[step: 902] loss: 0.051878757774829865\n",
      "[step: 903] loss: 0.05187474191188812\n",
      "[step: 904] loss: 0.05186770483851433\n",
      "[step: 905] loss: 0.05185758322477341\n",
      "[step: 906] loss: 0.05184495821595192\n",
      "[step: 907] loss: 0.051830582320690155\n",
      "[step: 908] loss: 0.0518152192234993\n",
      "[step: 909] loss: 0.0517994649708271\n",
      "[step: 910] loss: 0.05178382247686386\n",
      "[step: 911] loss: 0.05176854878664017\n",
      "[step: 912] loss: 0.05175388976931572\n",
      "[step: 913] loss: 0.05173996463418007\n",
      "[step: 914] loss: 0.05172682926058769\n",
      "[step: 915] loss: 0.051714587956666946\n",
      "[step: 916] loss: 0.05170341581106186\n",
      "[step: 917] loss: 0.05169351398944855\n",
      "[step: 918] loss: 0.05168525129556656\n",
      "[step: 919] loss: 0.051679227501153946\n",
      "[step: 920] loss: 0.05167624354362488\n",
      "[step: 921] loss: 0.05167773365974426\n",
      "[step: 922] loss: 0.05168566480278969\n",
      "[step: 923] loss: 0.05170339345932007\n",
      "[step: 924] loss: 0.0517357774078846\n",
      "[step: 925] loss: 0.05179080739617348\n",
      "[step: 926] loss: 0.051880285143852234\n",
      "[step: 927] loss: 0.05202292650938034\n",
      "[step: 928] loss: 0.05224482715129852\n",
      "[step: 929] loss: 0.052583955228328705\n",
      "[step: 930] loss: 0.053080640733242035\n",
      "[step: 931] loss: 0.05377090722322464\n",
      "[step: 932] loss: 0.05462269484996796\n",
      "[step: 933] loss: 0.05548665672540665\n",
      "[step: 934] loss: 0.05597200617194176\n",
      "[step: 935] loss: 0.05561494827270508\n",
      "[step: 936] loss: 0.0542156919836998\n",
      "[step: 937] loss: 0.052445799112319946\n",
      "[step: 938] loss: 0.051488980650901794\n",
      "[step: 939] loss: 0.05188297480344772\n",
      "[step: 940] loss: 0.052805591374635696\n",
      "[step: 941] loss: 0.05299512296915054\n",
      "[step: 942] loss: 0.05223884433507919\n",
      "[step: 943] loss: 0.051496028900146484\n",
      "[step: 944] loss: 0.05156783387064934\n",
      "[step: 945] loss: 0.0521167516708374\n",
      "[step: 946] loss: 0.05230056494474411\n",
      "[step: 947] loss: 0.05189477279782295\n",
      "[step: 948] loss: 0.05141150951385498\n",
      "[step: 949] loss: 0.05134788900613785\n",
      "[step: 950] loss: 0.05164014920592308\n",
      "[step: 951] loss: 0.05187995731830597\n",
      "[step: 952] loss: 0.051812849938869476\n",
      "[step: 953] loss: 0.05152573809027672\n",
      "[step: 954] loss: 0.05127600207924843\n",
      "[step: 955] loss: 0.0512281209230423\n",
      "[step: 956] loss: 0.051354262977838516\n",
      "[step: 957] loss: 0.051511891186237335\n",
      "[step: 958] loss: 0.05157382786273956\n",
      "[step: 959] loss: 0.05150379240512848\n",
      "[step: 960] loss: 0.0513513907790184\n",
      "[step: 961] loss: 0.05120246484875679\n",
      "[step: 962] loss: 0.05112237483263016\n",
      "[step: 963] loss: 0.05112580209970474\n",
      "[step: 964] loss: 0.05118104815483093\n",
      "[step: 965] loss: 0.05123695731163025\n",
      "[step: 966] loss: 0.0512532964348793\n",
      "[step: 967] loss: 0.051218077540397644\n",
      "[step: 968] loss: 0.0511477068066597\n",
      "[step: 969] loss: 0.05107272416353226\n",
      "[step: 970] loss: 0.05102028325200081\n",
      "[step: 971] loss: 0.05100148543715477\n",
      "[step: 972] loss: 0.05100981891155243\n",
      "[step: 973] loss: 0.05102832242846489\n",
      "[step: 974] loss: 0.051039788872003555\n",
      "[step: 975] loss: 0.05103432387113571\n",
      "[step: 976] loss: 0.05101129785180092\n",
      "[step: 977] loss: 0.050976965576410294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 978] loss: 0.050939977169036865\n",
      "[step: 979] loss: 0.05090762674808502\n",
      "[step: 980] loss: 0.050883810967206955\n",
      "[step: 981] loss: 0.050868891179561615\n",
      "[step: 982] loss: 0.05086095258593559\n",
      "[step: 983] loss: 0.05085720866918564\n",
      "[step: 984] loss: 0.05085498094558716\n",
      "[step: 985] loss: 0.050852470099925995\n",
      "[step: 986] loss: 0.05084867775440216\n",
      "[step: 987] loss: 0.05084335058927536\n",
      "[step: 988] loss: 0.050836820155382156\n",
      "[step: 989] loss: 0.05082953721284866\n",
      "[step: 990] loss: 0.050822190940380096\n",
      "[step: 991] loss: 0.05081547796726227\n",
      "[step: 992] loss: 0.05081011727452278\n",
      "[step: 993] loss: 0.05080687254667282\n",
      "[step: 994] loss: 0.050806738436222076\n",
      "[step: 995] loss: 0.05081078037619591\n",
      "[step: 996] loss: 0.05082052946090698\n",
      "[step: 997] loss: 0.05083779990673065\n",
      "[step: 998] loss: 0.0508652925491333\n",
      "[step: 999] loss: 0.050906188786029816\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "testY is:  [9169.16129, 13726.903230000002, 4432.5, 2776.5161290000001, 2286.6333329999998, 1913.451613, 1650.9333329999999, 2040.0333329999999, 2734.0645159999999, 13740.266669999999, 10564.35484, 7608.3999999999996, 8706.1935480000011, 26366.56667, 24504.89286, 2169.0322579999997, 1879.0, 2449.5483870000003, 1487.9000000000001]\n",
      "\n",
      "\n",
      "LSTM testforecast : [9004.275390625, 10185.4482421875, 7377.861328125, 5150.2919921875, 2749.010498046875, 1346.928955078125, 719.3789672851562, 753.03076171875, 1952.0594482421875, 5418.96337890625, 8963.607421875, 13124.470703125, 15050.8955078125, 14982.7822265625, 17686.19921875, 15694.197265625, 10897.8837890625, 6386.5732421875, 3230.5341796875] \n",
      "@@@@@LSTM rmse:  5767.10581645\n",
      "Bayseian testforecast : [10779.439455906822, 24407.494116357997, 39468.275116213939, 1450.6909320945927, 6148.6248637620456, 3986.0758871295261, 3467.5480521732943, 6775.363282302862, 8218.0736341408538, 14645.889595148958, 8956.5419389051822, 11343.677741923408, 13245.482280307642, 34051.27146402275, 73451.564333287213, 1240.0368248255031, 8277.0371940050172, 5192.6954384799137, 4533.8132120377759] \n",
      "@@@@@Bayseian rmse:  14459.2608485\n",
      "\n",
      "\n",
      "LSTM WON!!!!!!\n",
      "[step: 0] loss: 6.6305718421936035\n",
      "[step: 1] loss: 3.698164224624634\n",
      "[step: 2] loss: 1.8414429426193237\n",
      "[step: 3] loss: 0.8254557251930237\n",
      "[step: 4] loss: 0.45276251435279846\n",
      "[step: 5] loss: 0.4981619417667389\n",
      "[step: 6] loss: 0.7187191843986511\n",
      "[step: 7] loss: 0.9199478626251221\n",
      "[step: 8] loss: 1.0015696287155151\n",
      "[step: 9] loss: 0.9561218023300171\n",
      "[step: 10] loss: 0.8300191760063171\n",
      "[step: 11] loss: 0.6791892051696777\n",
      "[step: 12] loss: 0.5444709658622742\n",
      "[step: 13] loss: 0.44543004035949707\n",
      "[step: 14] loss: 0.38447147607803345\n",
      "[step: 15] loss: 0.35399433970451355\n",
      "[step: 16] loss: 0.34280699491500854\n",
      "[step: 17] loss: 0.34068194031715393\n",
      "[step: 18] loss: 0.34076258540153503\n",
      "[step: 19] loss: 0.3399158716201782\n",
      "[step: 20] loss: 0.33768734335899353\n",
      "[step: 21] loss: 0.33480578660964966\n",
      "[step: 22] loss: 0.331990510225296\n",
      "[step: 23] loss: 0.32937052845954895\n",
      "[step: 24] loss: 0.32645875215530396\n",
      "[step: 25] loss: 0.3224394917488098\n",
      "[step: 26] loss: 0.31652510166168213\n",
      "[step: 27] loss: 0.3082680404186249\n",
      "[step: 28] loss: 0.2977818250656128\n",
      "[step: 29] loss: 0.2857947051525116\n",
      "[step: 30] loss: 0.27349159121513367\n",
      "[step: 31] loss: 0.2622087895870209\n",
      "[step: 32] loss: 0.2531147301197052\n",
      "[step: 33] loss: 0.2469855695962906\n",
      "[step: 34] loss: 0.24410414695739746\n",
      "[step: 35] loss: 0.24425069987773895\n",
      "[step: 36] loss: 0.24675707519054413\n",
      "[step: 37] loss: 0.2506193518638611\n",
      "[step: 38] loss: 0.2546742260456085\n",
      "[step: 39] loss: 0.2578171193599701\n",
      "[step: 40] loss: 0.25921809673309326\n",
      "[step: 41] loss: 0.2584807574748993\n",
      "[step: 42] loss: 0.25571009516716003\n",
      "[step: 43] loss: 0.25146523118019104\n",
      "[step: 44] loss: 0.24660331010818481\n",
      "[step: 45] loss: 0.24204811453819275\n",
      "[step: 46] loss: 0.23855236172676086\n",
      "[step: 47] loss: 0.23652678728103638\n",
      "[step: 48] loss: 0.2359858751296997\n",
      "[step: 49] loss: 0.23661206662654877\n",
      "[step: 50] loss: 0.23789918422698975\n",
      "[step: 51] loss: 0.2393181174993515\n",
      "[step: 52] loss: 0.24044814705848694\n",
      "[step: 53] loss: 0.24104377627372742\n",
      "[step: 54] loss: 0.24103747308254242\n",
      "[step: 55] loss: 0.24050092697143555\n",
      "[step: 56] loss: 0.23959605395793915\n",
      "[step: 57] loss: 0.23852884769439697\n",
      "[step: 58] loss: 0.237505242228508\n",
      "[step: 59] loss: 0.23668676614761353\n",
      "[step: 60] loss: 0.23615334928035736\n",
      "[step: 61] loss: 0.23589421808719635\n",
      "[step: 62] loss: 0.23583120107650757\n",
      "[step: 63] loss: 0.23586317896842957\n",
      "[step: 64] loss: 0.23590657114982605\n",
      "[step: 65] loss: 0.23591595888137817\n",
      "[step: 66] loss: 0.23588283360004425\n",
      "[step: 67] loss: 0.23582129180431366\n",
      "[step: 68] loss: 0.23574979603290558\n",
      "[step: 69] loss: 0.23567737638950348\n",
      "[step: 70] loss: 0.23559913039207458\n",
      "[step: 71] loss: 0.2355012148618698\n",
      "[step: 72] loss: 0.2353709191083908\n",
      "[step: 73] loss: 0.23520605266094208\n",
      "[step: 74] loss: 0.23501765727996826\n",
      "[step: 75] loss: 0.23482735455036163\n",
      "[step: 76] loss: 0.2346598207950592\n",
      "[step: 77] loss: 0.23453454673290253\n",
      "[step: 78] loss: 0.23445948958396912\n",
      "[step: 79] loss: 0.23442912101745605\n",
      "[step: 80] loss: 0.23442809283733368\n",
      "[step: 81] loss: 0.2344367653131485\n",
      "[step: 82] loss: 0.23443551361560822\n",
      "[step: 83] loss: 0.23440907895565033\n",
      "[step: 84] loss: 0.23434917628765106\n",
      "[step: 85] loss: 0.2342570722103119\n",
      "[step: 86] loss: 0.23414306342601776\n",
      "[step: 87] loss: 0.2340235859155655\n",
      "[step: 88] loss: 0.23391522467136383\n",
      "[step: 89] loss: 0.2338290512561798\n",
      "[step: 90] loss: 0.23376762866973877\n",
      "[step: 91] loss: 0.2337256222963333\n",
      "[step: 92] loss: 0.23369328677654266\n",
      "[step: 93] loss: 0.23366057872772217\n",
      "[step: 94] loss: 0.23362086713314056\n",
      "[step: 95] loss: 0.23357102274894714\n",
      "[step: 96] loss: 0.2335113286972046\n",
      "[step: 97] loss: 0.23344433307647705\n",
      "[step: 98] loss: 0.2333735227584839\n",
      "[step: 99] loss: 0.23330263793468475\n",
      "[step: 100] loss: 0.23323477804660797\n",
      "[step: 101] loss: 0.23317138850688934\n",
      "[step: 102] loss: 0.23311257362365723\n",
      "[step: 103] loss: 0.2330566644668579\n",
      "[step: 104] loss: 0.23300182819366455\n",
      "[step: 105] loss: 0.2329464703798294\n",
      "[step: 106] loss: 0.23288993537425995\n",
      "[step: 107] loss: 0.23283231258392334\n",
      "[step: 108] loss: 0.2327738106250763\n",
      "[step: 109] loss: 0.2327144593000412\n",
      "[step: 110] loss: 0.2326543927192688\n",
      "[step: 111] loss: 0.2325933575630188\n",
      "[step: 112] loss: 0.23253154754638672\n",
      "[step: 113] loss: 0.2324691265821457\n",
      "[step: 114] loss: 0.2324066162109375\n",
      "[step: 115] loss: 0.23234425485134125\n",
      "[step: 116] loss: 0.23228266835212708\n",
      "[step: 117] loss: 0.23222175240516663\n",
      "[step: 118] loss: 0.232161283493042\n",
      "[step: 119] loss: 0.23210088908672333\n",
      "[step: 120] loss: 0.23204001784324646\n",
      "[step: 121] loss: 0.2319783866405487\n",
      "[step: 122] loss: 0.2319156676530838\n",
      "[step: 123] loss: 0.23185177147388458\n",
      "[step: 124] loss: 0.23178717494010925\n",
      "[step: 125] loss: 0.2317221760749817\n",
      "[step: 126] loss: 0.23165728151798248\n",
      "[step: 127] loss: 0.23159268498420715\n",
      "[step: 128] loss: 0.2315283715724945\n",
      "[step: 129] loss: 0.23146402835845947\n",
      "[step: 130] loss: 0.23139940202236176\n",
      "[step: 131] loss: 0.23133428394794464\n",
      "[step: 132] loss: 0.2312684953212738\n",
      "[step: 133] loss: 0.23120206594467163\n",
      "[step: 134] loss: 0.23113521933555603\n",
      "[step: 135] loss: 0.23106789588928223\n",
      "[step: 136] loss: 0.23100027441978455\n",
      "[step: 137] loss: 0.23093248903751373\n",
      "[step: 138] loss: 0.23086437582969666\n",
      "[step: 139] loss: 0.23079605400562286\n",
      "[step: 140] loss: 0.23072737455368042\n",
      "[step: 141] loss: 0.23065833747386932\n",
      "[step: 142] loss: 0.2305889129638672\n",
      "[step: 143] loss: 0.23051907122135162\n",
      "[step: 144] loss: 0.23044881224632263\n",
      "[step: 145] loss: 0.2303781658411026\n",
      "[step: 146] loss: 0.23030713200569153\n",
      "[step: 147] loss: 0.23023562133312225\n",
      "[step: 148] loss: 0.23016376793384552\n",
      "[step: 149] loss: 0.2300913780927658\n",
      "[step: 150] loss: 0.23001864552497864\n",
      "[step: 151] loss: 0.22994555532932281\n",
      "[step: 152] loss: 0.22987210750579834\n",
      "[step: 153] loss: 0.22979822754859924\n",
      "[step: 154] loss: 0.2297239452600479\n",
      "[step: 155] loss: 0.2296491414308548\n",
      "[step: 156] loss: 0.22957389056682587\n",
      "[step: 157] loss: 0.22949817776679993\n",
      "[step: 158] loss: 0.22942198812961578\n",
      "[step: 159] loss: 0.22934526205062866\n",
      "[step: 160] loss: 0.2292681187391281\n",
      "[step: 161] loss: 0.22919048368930817\n",
      "[step: 162] loss: 0.2291123867034912\n",
      "[step: 163] loss: 0.22903378307819366\n",
      "[step: 164] loss: 0.2289547175168991\n",
      "[step: 165] loss: 0.22887514531612396\n",
      "[step: 166] loss: 0.22879497706890106\n",
      "[step: 167] loss: 0.22871433198451996\n",
      "[step: 168] loss: 0.22863313555717468\n",
      "[step: 169] loss: 0.22855137288570404\n",
      "[step: 170] loss: 0.22846905887126923\n",
      "[step: 171] loss: 0.22838617861270905\n",
      "[step: 172] loss: 0.2283027470111847\n",
      "[step: 173] loss: 0.22821871936321259\n",
      "[step: 174] loss: 0.22813405096530914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 175] loss: 0.22804880142211914\n",
      "[step: 176] loss: 0.22796298563480377\n",
      "[step: 177] loss: 0.2278764545917511\n",
      "[step: 178] loss: 0.2277892827987671\n",
      "[step: 179] loss: 0.22770142555236816\n",
      "[step: 180] loss: 0.22761297225952148\n",
      "[step: 181] loss: 0.22752374410629272\n",
      "[step: 182] loss: 0.22743387520313263\n",
      "[step: 183] loss: 0.22734323143959045\n",
      "[step: 184] loss: 0.22725197672843933\n",
      "[step: 185] loss: 0.22715990245342255\n",
      "[step: 186] loss: 0.2270670235157013\n",
      "[step: 187] loss: 0.22697344422340393\n",
      "[step: 188] loss: 0.22687901556491852\n",
      "[step: 189] loss: 0.2267838567495346\n",
      "[step: 190] loss: 0.22668775916099548\n",
      "[step: 191] loss: 0.2265908271074295\n",
      "[step: 192] loss: 0.22649309039115906\n",
      "[step: 193] loss: 0.22639445960521698\n",
      "[step: 194] loss: 0.22629477083683014\n",
      "[step: 195] loss: 0.2261943221092224\n",
      "[step: 196] loss: 0.2260928452014923\n",
      "[step: 197] loss: 0.22599036991596222\n",
      "[step: 198] loss: 0.22588686645030975\n",
      "[step: 199] loss: 0.2257823497056961\n",
      "[step: 200] loss: 0.2256767898797989\n",
      "[step: 201] loss: 0.22557012736797333\n",
      "[step: 202] loss: 0.2254623919725418\n",
      "[step: 203] loss: 0.22535347938537598\n",
      "[step: 204] loss: 0.22524338960647583\n",
      "[step: 205] loss: 0.22513209283351898\n",
      "[step: 206] loss: 0.22501951456069946\n",
      "[step: 207] loss: 0.22490575909614563\n",
      "[step: 208] loss: 0.22479060292243958\n",
      "[step: 209] loss: 0.22467418015003204\n",
      "[step: 210] loss: 0.22455622255802155\n",
      "[step: 211] loss: 0.22443687915802002\n",
      "[step: 212] loss: 0.22431612014770508\n",
      "[step: 213] loss: 0.2241937816143036\n",
      "[step: 214] loss: 0.2240699976682663\n",
      "[step: 215] loss: 0.2239445447921753\n",
      "[step: 216] loss: 0.223817378282547\n",
      "[step: 217] loss: 0.2236884981393814\n",
      "[step: 218] loss: 0.22355793416500092\n",
      "[step: 219] loss: 0.22342543303966522\n",
      "[step: 220] loss: 0.22329111397266388\n",
      "[step: 221] loss: 0.22315485775470734\n",
      "[step: 222] loss: 0.22301651537418365\n",
      "[step: 223] loss: 0.2228761613368988\n",
      "[step: 224] loss: 0.22273369133472443\n",
      "[step: 225] loss: 0.2225889265537262\n",
      "[step: 226] loss: 0.22244183719158173\n",
      "[step: 227] loss: 0.22229236364364624\n",
      "[step: 228] loss: 0.22214047610759735\n",
      "[step: 229] loss: 0.22198602557182312\n",
      "[step: 230] loss: 0.22182899713516235\n",
      "[step: 231] loss: 0.22166916728019714\n",
      "[step: 232] loss: 0.22150644659996033\n",
      "[step: 233] loss: 0.2213408350944519\n",
      "[step: 234] loss: 0.22117218375205994\n",
      "[step: 235] loss: 0.22100034356117249\n",
      "[step: 236] loss: 0.22082513570785522\n",
      "[step: 237] loss: 0.22064654529094696\n",
      "[step: 238] loss: 0.2204645574092865\n",
      "[step: 239] loss: 0.22027873992919922\n",
      "[step: 240] loss: 0.2200891226530075\n",
      "[step: 241] loss: 0.21989542245864868\n",
      "[step: 242] loss: 0.21969768404960632\n",
      "[step: 243] loss: 0.21949562430381775\n",
      "[step: 244] loss: 0.21928907930850983\n",
      "[step: 245] loss: 0.21907782554626465\n",
      "[step: 246] loss: 0.21886174380779266\n",
      "[step: 247] loss: 0.21864061057567596\n",
      "[step: 248] loss: 0.21841415762901306\n",
      "[step: 249] loss: 0.2181822657585144\n",
      "[step: 250] loss: 0.2179446816444397\n",
      "[step: 251] loss: 0.2177010476589203\n",
      "[step: 252] loss: 0.21745125949382782\n",
      "[step: 253] loss: 0.21719494462013245\n",
      "[step: 254] loss: 0.2169318050146103\n",
      "[step: 255] loss: 0.216661736369133\n",
      "[step: 256] loss: 0.21638423204421997\n",
      "[step: 257] loss: 0.2160990685224533\n",
      "[step: 258] loss: 0.21580582857131958\n",
      "[step: 259] loss: 0.21550427377223969\n",
      "[step: 260] loss: 0.2151939421892166\n",
      "[step: 261] loss: 0.21487446129322052\n",
      "[step: 262] loss: 0.21454554796218872\n",
      "[step: 263] loss: 0.2142065316438675\n",
      "[step: 264] loss: 0.21385721862316132\n",
      "[step: 265] loss: 0.21349692344665527\n",
      "[step: 266] loss: 0.2131252884864807\n",
      "[step: 267] loss: 0.21274186670780182\n",
      "[step: 268] loss: 0.21234597265720367\n",
      "[step: 269] loss: 0.21193715929985046\n",
      "[step: 270] loss: 0.21151478588581085\n",
      "[step: 271] loss: 0.2110782116651535\n",
      "[step: 272] loss: 0.21062691509723663\n",
      "[step: 273] loss: 0.21016019582748413\n",
      "[step: 274] loss: 0.20967736840248108\n",
      "[step: 275] loss: 0.2091776579618454\n",
      "[step: 276] loss: 0.20866045355796814\n",
      "[step: 277] loss: 0.2081247866153717\n",
      "[step: 278] loss: 0.20757010579109192\n",
      "[step: 279] loss: 0.20699544250965118\n",
      "[step: 280] loss: 0.20639991760253906\n",
      "[step: 281] loss: 0.20578284561634064\n",
      "[step: 282] loss: 0.20514312386512756\n",
      "[step: 283] loss: 0.20447993278503418\n",
      "[step: 284] loss: 0.20379236340522766\n",
      "[step: 285] loss: 0.2030794322490692\n",
      "[step: 286] loss: 0.2023402452468872\n",
      "[step: 287] loss: 0.20157374441623688\n",
      "[step: 288] loss: 0.20077908039093018\n",
      "[step: 289] loss: 0.19995522499084473\n",
      "[step: 290] loss: 0.19910134375095367\n",
      "[step: 291] loss: 0.1982164829969406\n",
      "[step: 292] loss: 0.19729982316493988\n",
      "[step: 293] loss: 0.19635048508644104\n",
      "[step: 294] loss: 0.1953679472208023\n",
      "[step: 295] loss: 0.19435225427150726\n",
      "[step: 296] loss: 0.19331255555152893\n",
      "[step: 297] loss: 0.1923833191394806\n",
      "[step: 298] loss: 0.19331414997577667\n",
      "[step: 299] loss: 0.19797424972057343\n",
      "[step: 300] loss: 0.1920885294675827\n",
      "[step: 301] loss: 0.190021350979805\n",
      "[step: 302] loss: 0.19249430298805237\n",
      "[step: 303] loss: 0.18607096374034882\n",
      "[step: 304] loss: 0.1898660510778427\n",
      "[step: 305] loss: 0.18482425808906555\n",
      "[step: 306] loss: 0.185532808303833\n",
      "[step: 307] loss: 0.18432816863059998\n",
      "[step: 308] loss: 0.18132320046424866\n",
      "[step: 309] loss: 0.18295714259147644\n",
      "[step: 310] loss: 0.17870751023292542\n",
      "[step: 311] loss: 0.17972056567668915\n",
      "[step: 312] loss: 0.17780956625938416\n",
      "[step: 313] loss: 0.17556709051132202\n",
      "[step: 314] loss: 0.17642781138420105\n",
      "[step: 315] loss: 0.1733584702014923\n",
      "[step: 316] loss: 0.17256301641464233\n",
      "[step: 317] loss: 0.17256540060043335\n",
      "[step: 318] loss: 0.1697695553302765\n",
      "[step: 319] loss: 0.16891615092754364\n",
      "[step: 320] loss: 0.1689593642950058\n",
      "[step: 321] loss: 0.16686800122261047\n",
      "[step: 322] loss: 0.16503380239009857\n",
      "[step: 323] loss: 0.16490338742733002\n",
      "[step: 324] loss: 0.1645480990409851\n",
      "[step: 325] loss: 0.162919282913208\n",
      "[step: 326] loss: 0.1610441356897354\n",
      "[step: 327] loss: 0.15993233025074005\n",
      "[step: 328] loss: 0.15959210693836212\n",
      "[step: 329] loss: 0.15971329808235168\n",
      "[step: 330] loss: 0.16024617850780487\n",
      "[step: 331] loss: 0.16142475605010986\n",
      "[step: 332] loss: 0.16294939815998077\n",
      "[step: 333] loss: 0.16375212371349335\n",
      "[step: 334] loss: 0.16124998033046722\n",
      "[step: 335] loss: 0.1562875658273697\n",
      "[step: 336] loss: 0.15290780365467072\n",
      "[step: 337] loss: 0.15361443161964417\n",
      "[step: 338] loss: 0.15587279200553894\n",
      "[step: 339] loss: 0.15568706393241882\n",
      "[step: 340] loss: 0.15264688432216644\n",
      "[step: 341] loss: 0.1500348597764969\n",
      "[step: 342] loss: 0.15028205513954163\n",
      "[step: 343] loss: 0.15164080262184143\n",
      "[step: 344] loss: 0.15124648809432983\n",
      "[step: 345] loss: 0.14908473193645477\n",
      "[step: 346] loss: 0.14739713072776794\n",
      "[step: 347] loss: 0.1474793404340744\n",
      "[step: 348] loss: 0.1482248157262802\n",
      "[step: 349] loss: 0.1480109542608261\n",
      "[step: 350] loss: 0.14667412638664246\n",
      "[step: 351] loss: 0.1452445238828659\n",
      "[step: 352] loss: 0.1446698009967804\n",
      "[step: 353] loss: 0.1448708325624466\n",
      "[step: 354] loss: 0.1451663374900818\n",
      "[step: 355] loss: 0.14503809809684753\n",
      "[step: 356] loss: 0.14435653388500214\n",
      "[step: 357] loss: 0.1434113085269928\n",
      "[step: 358] loss: 0.14252980053424835\n",
      "[step: 359] loss: 0.1419319212436676\n",
      "[step: 360] loss: 0.14163760840892792\n",
      "[step: 361] loss: 0.14156675338745117\n",
      "[step: 362] loss: 0.14164584875106812\n",
      "[step: 363] loss: 0.1418481469154358\n",
      "[step: 364] loss: 0.14222612977027893\n",
      "[step: 365] loss: 0.14280998706817627\n",
      "[step: 366] loss: 0.1436864584684372\n",
      "[step: 367] loss: 0.14460140466690063\n",
      "[step: 368] loss: 0.14520947635173798\n",
      "[step: 369] loss: 0.14452111721038818\n",
      "[step: 370] loss: 0.14241689443588257\n",
      "[step: 371] loss: 0.1397264301776886\n",
      "[step: 372] loss: 0.1382884979248047\n",
      "[step: 373] loss: 0.1387321799993515\n",
      "[step: 374] loss: 0.13988004624843597\n",
      "[step: 375] loss: 0.14009042084217072\n",
      "[step: 376] loss: 0.13888733088970184\n",
      "[step: 377] loss: 0.13749738037586212\n",
      "[step: 378] loss: 0.1371605545282364\n",
      "[step: 379] loss: 0.13773949444293976\n",
      "[step: 380] loss: 0.1381334513425827\n",
      "[step: 381] loss: 0.13765670359134674\n",
      "[step: 382] loss: 0.13672684133052826\n",
      "[step: 383] loss: 0.1361398845911026\n",
      "[step: 384] loss: 0.13619084656238556\n",
      "[step: 385] loss: 0.1365126669406891\n",
      "[step: 386] loss: 0.13660037517547607\n",
      "[step: 387] loss: 0.13628649711608887\n",
      "[step: 388] loss: 0.13573315739631653\n",
      "[step: 389] loss: 0.13523869216442108\n",
      "[step: 390] loss: 0.1349738985300064\n",
      "[step: 391] loss: 0.13493703305721283\n",
      "[step: 392] loss: 0.1350308656692505\n",
      "[step: 393] loss: 0.13515618443489075\n",
      "[step: 394] loss: 0.13526463508605957\n",
      "[step: 395] loss: 0.13533532619476318\n",
      "[step: 396] loss: 0.13539379835128784\n",
      "[step: 397] loss: 0.13544000685214996\n",
      "[step: 398] loss: 0.13550962507724762\n",
      "[step: 399] loss: 0.13557668030261993\n",
      "[step: 400] loss: 0.1356557458639145\n",
      "[step: 401] loss: 0.13566789031028748\n",
      "[step: 402] loss: 0.13559280335903168\n",
      "[step: 403] loss: 0.13531939685344696\n",
      "[step: 404] loss: 0.1348600834608078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 405] loss: 0.13422566652297974\n",
      "[step: 406] loss: 0.133572518825531\n",
      "[step: 407] loss: 0.13305196166038513\n",
      "[step: 408] loss: 0.13277874886989594\n",
      "[step: 409] loss: 0.13275229930877686\n",
      "[step: 410] loss: 0.13287527859210968\n",
      "[step: 411] loss: 0.13301485776901245\n",
      "[step: 412] loss: 0.13305790722370148\n",
      "[step: 413] loss: 0.13296383619308472\n",
      "[step: 414] loss: 0.1327427625656128\n",
      "[step: 415] loss: 0.13246284425258636\n",
      "[step: 416] loss: 0.13219018280506134\n",
      "[step: 417] loss: 0.13197726011276245\n",
      "[step: 418] loss: 0.1318414956331253\n",
      "[step: 419] loss: 0.1317743957042694\n",
      "[step: 420] loss: 0.1317553073167801\n",
      "[step: 421] loss: 0.13176418840885162\n",
      "[step: 422] loss: 0.13179093599319458\n",
      "[step: 423] loss: 0.13183307647705078\n",
      "[step: 424] loss: 0.13190260529518127\n",
      "[step: 425] loss: 0.13201238214969635\n",
      "[step: 426] loss: 0.13219690322875977\n",
      "[step: 427] loss: 0.1324787586927414\n",
      "[step: 428] loss: 0.13291724026203156\n",
      "[step: 429] loss: 0.13350847363471985\n",
      "[step: 430] loss: 0.13427603244781494\n",
      "[step: 431] loss: 0.13499198853969574\n",
      "[step: 432] loss: 0.13542744517326355\n",
      "[step: 433] loss: 0.13500599563121796\n",
      "[step: 434] loss: 0.133690744638443\n",
      "[step: 435] loss: 0.1318787932395935\n",
      "[step: 436] loss: 0.13064812123775482\n",
      "[step: 437] loss: 0.13062456250190735\n",
      "[step: 438] loss: 0.13141842186450958\n",
      "[step: 439] loss: 0.13200384378433228\n",
      "[step: 440] loss: 0.13168352842330933\n",
      "[step: 441] loss: 0.13079485297203064\n",
      "[step: 442] loss: 0.13018755614757538\n",
      "[step: 443] loss: 0.1303037852048874\n",
      "[step: 444] loss: 0.13076546788215637\n",
      "[step: 445] loss: 0.13092227280139923\n",
      "[step: 446] loss: 0.13057701289653778\n",
      "[step: 447] loss: 0.13005344569683075\n",
      "[step: 448] loss: 0.12979111075401306\n",
      "[step: 449] loss: 0.12989135086536407\n",
      "[step: 450] loss: 0.13012318313121796\n",
      "[step: 451] loss: 0.1302138864994049\n",
      "[step: 452] loss: 0.13006159663200378\n",
      "[step: 453] loss: 0.12977269291877747\n",
      "[step: 454] loss: 0.12950968742370605\n",
      "[step: 455] loss: 0.12938068807125092\n",
      "[step: 456] loss: 0.1293889284133911\n",
      "[step: 457] loss: 0.12947076559066772\n",
      "[step: 458] loss: 0.12955309450626373\n",
      "[step: 459] loss: 0.12958472967147827\n",
      "[step: 460] loss: 0.12955360114574432\n",
      "[step: 461] loss: 0.12946413457393646\n",
      "[step: 462] loss: 0.1293412148952484\n",
      "[step: 463] loss: 0.12920351326465607\n",
      "[step: 464] loss: 0.12907125055789948\n",
      "[step: 465] loss: 0.12895415723323822\n",
      "[step: 466] loss: 0.12885773181915283\n",
      "[step: 467] loss: 0.12878172099590302\n",
      "[step: 468] loss: 0.1287233978509903\n",
      "[step: 469] loss: 0.12867894768714905\n",
      "[step: 470] loss: 0.128644660115242\n",
      "[step: 471] loss: 0.12861761450767517\n",
      "[step: 472] loss: 0.12859578430652618\n",
      "[step: 473] loss: 0.12857873737812042\n",
      "[step: 474] loss: 0.12856660783290863\n",
      "[step: 475] loss: 0.12856131792068481\n",
      "[step: 476] loss: 0.12856458127498627\n",
      "[step: 477] loss: 0.1285816878080368\n",
      "[step: 478] loss: 0.12861596047878265\n",
      "[step: 479] loss: 0.12867772579193115\n",
      "[step: 480] loss: 0.12877203524112701\n",
      "[step: 481] loss: 0.12891729176044464\n",
      "[step: 482] loss: 0.12911616265773773\n",
      "[step: 483] loss: 0.12939408421516418\n",
      "[step: 484] loss: 0.12972787022590637\n",
      "[step: 485] loss: 0.13012643158435822\n",
      "[step: 486] loss: 0.13047485053539276\n",
      "[step: 487] loss: 0.13071578741073608\n",
      "[step: 488] loss: 0.13061951100826263\n",
      "[step: 489] loss: 0.13016414642333984\n",
      "[step: 490] loss: 0.1293344497680664\n",
      "[step: 491] loss: 0.1284477561712265\n",
      "[step: 492] loss: 0.12783077359199524\n",
      "[step: 493] loss: 0.12769590318202972\n",
      "[step: 494] loss: 0.1279604434967041\n",
      "[step: 495] loss: 0.1283293515443802\n",
      "[step: 496] loss: 0.1285138726234436\n",
      "[step: 497] loss: 0.12837453186511993\n",
      "[step: 498] loss: 0.12801574170589447\n",
      "[step: 499] loss: 0.127634197473526\n",
      "[step: 500] loss: 0.12741181254386902\n",
      "[step: 501] loss: 0.1273975968360901\n",
      "[step: 502] loss: 0.12752464413642883\n",
      "[step: 503] loss: 0.12768308818340302\n",
      "[step: 504] loss: 0.12778151035308838\n",
      "[step: 505] loss: 0.12778940796852112\n",
      "[step: 506] loss: 0.1277064085006714\n",
      "[step: 507] loss: 0.12757183611392975\n",
      "[step: 508] loss: 0.1274144947528839\n",
      "[step: 509] loss: 0.12726625800132751\n",
      "[step: 510] loss: 0.1271395981311798\n",
      "[step: 511] loss: 0.12704052031040192\n",
      "[step: 512] loss: 0.1269662231206894\n",
      "[step: 513] loss: 0.12691177427768707\n",
      "[step: 514] loss: 0.126871719956398\n",
      "[step: 515] loss: 0.12684208154678345\n",
      "[step: 516] loss: 0.1268208771944046\n",
      "[step: 517] loss: 0.12680816650390625\n",
      "[step: 518] loss: 0.12680698931217194\n",
      "[step: 519] loss: 0.12682312726974487\n",
      "[step: 520] loss: 0.1268686205148697\n",
      "[step: 521] loss: 0.12696164846420288\n",
      "[step: 522] loss: 0.12713837623596191\n",
      "[step: 523] loss: 0.1274474561214447\n",
      "[step: 524] loss: 0.12798741459846497\n",
      "[step: 525] loss: 0.12885114550590515\n",
      "[step: 526] loss: 0.13021960854530334\n",
      "[step: 527] loss: 0.1319919377565384\n",
      "[step: 528] loss: 0.13400326669216156\n",
      "[step: 529] loss: 0.1348559558391571\n",
      "[step: 530] loss: 0.13365648686885834\n",
      "[step: 531] loss: 0.1299983710050583\n",
      "[step: 532] loss: 0.12682734429836273\n",
      "[step: 533] loss: 0.12676294147968292\n",
      "[step: 534] loss: 0.12883654236793518\n",
      "[step: 535] loss: 0.12956854701042175\n",
      "[step: 536] loss: 0.12777605652809143\n",
      "[step: 537] loss: 0.12633492052555084\n",
      "[step: 538] loss: 0.1271076649427414\n",
      "[step: 539] loss: 0.12818774580955505\n",
      "[step: 540] loss: 0.12758296728134155\n",
      "[step: 541] loss: 0.1263391673564911\n",
      "[step: 542] loss: 0.12636825442314148\n",
      "[step: 543] loss: 0.1272096484899521\n",
      "[step: 544] loss: 0.1272606998682022\n",
      "[step: 545] loss: 0.12645480036735535\n",
      "[step: 546] loss: 0.12594343721866608\n",
      "[step: 547] loss: 0.12624739110469818\n",
      "[step: 548] loss: 0.12673094868659973\n",
      "[step: 549] loss: 0.126691535115242\n",
      "[step: 550] loss: 0.1262083202600479\n",
      "[step: 551] loss: 0.12579497694969177\n",
      "[step: 552] loss: 0.1257811337709427\n",
      "[step: 553] loss: 0.1260520964860916\n",
      "[step: 554] loss: 0.12629055976867676\n",
      "[step: 555] loss: 0.12629951536655426\n",
      "[step: 556] loss: 0.12607957422733307\n",
      "[step: 557] loss: 0.12579092383384705\n",
      "[step: 558] loss: 0.1255832016468048\n",
      "[step: 559] loss: 0.1255275011062622\n",
      "[step: 560] loss: 0.12559640407562256\n",
      "[step: 561] loss: 0.12570883333683014\n",
      "[step: 562] loss: 0.1257859617471695\n",
      "[step: 563] loss: 0.12577781081199646\n",
      "[step: 564] loss: 0.1256883591413498\n",
      "[step: 565] loss: 0.12554705142974854\n",
      "[step: 566] loss: 0.12540550529956818\n",
      "[step: 567] loss: 0.12530258297920227\n",
      "[step: 568] loss: 0.12525594234466553\n",
      "[step: 569] loss: 0.12525734305381775\n",
      "[step: 570] loss: 0.12528184056282043\n",
      "[step: 571] loss: 0.1253015249967575\n",
      "[step: 572] loss: 0.12529608607292175\n",
      "[step: 573] loss: 0.12526091933250427\n",
      "[step: 574] loss: 0.12520159780979156\n",
      "[step: 575] loss: 0.1251329481601715\n",
      "[step: 576] loss: 0.12506814301013947\n",
      "[step: 577] loss: 0.12501664459705353\n",
      "[step: 578] loss: 0.12498126924037933\n",
      "[step: 579] loss: 0.12495975196361542\n",
      "[step: 580] loss: 0.12494724988937378\n",
      "[step: 581] loss: 0.12493870407342911\n",
      "[step: 582] loss: 0.12493060529232025\n",
      "[step: 583] loss: 0.12492090463638306\n",
      "[step: 584] loss: 0.12490996718406677\n",
      "[step: 585] loss: 0.12489831447601318\n",
      "[step: 586] loss: 0.12488873302936554\n",
      "[step: 587] loss: 0.1248830109834671\n",
      "[step: 588] loss: 0.12488596886396408\n",
      "[step: 589] loss: 0.12490135431289673\n",
      "[step: 590] loss: 0.12493892014026642\n",
      "[step: 591] loss: 0.1250077784061432\n",
      "[step: 592] loss: 0.12513121962547302\n",
      "[step: 593] loss: 0.12533099949359894\n",
      "[step: 594] loss: 0.12566335499286652\n",
      "[step: 595] loss: 0.12616655230522156\n",
      "[step: 596] loss: 0.1269514262676239\n",
      "[step: 597] loss: 0.12799258530139923\n",
      "[step: 598] loss: 0.1293555349111557\n",
      "[step: 599] loss: 0.1305052489042282\n",
      "[step: 600] loss: 0.13107897341251373\n",
      "[step: 601] loss: 0.12994951009750366\n",
      "[step: 602] loss: 0.12755422294139862\n",
      "[step: 603] loss: 0.12510837614536285\n",
      "[step: 604] loss: 0.12445657700300217\n",
      "[step: 605] loss: 0.12561561167240143\n",
      "[step: 606] loss: 0.1267307847738266\n",
      "[step: 607] loss: 0.12634976208209991\n",
      "[step: 608] loss: 0.1249632015824318\n",
      "[step: 609] loss: 0.12433861196041107\n",
      "[step: 610] loss: 0.12495554238557816\n",
      "[step: 611] loss: 0.12564806640148163\n",
      "[step: 612] loss: 0.12542681396007538\n",
      "[step: 613] loss: 0.12459387630224228\n",
      "[step: 614] loss: 0.12416034191846848\n",
      "[step: 615] loss: 0.12444522231817245\n",
      "[step: 616] loss: 0.12492051720619202\n",
      "[step: 617] loss: 0.12501326203346252\n",
      "[step: 618] loss: 0.12463953346014023\n",
      "[step: 619] loss: 0.12417812645435333\n",
      "[step: 620] loss: 0.12396614253520966\n",
      "[step: 621] loss: 0.12407124042510986\n",
      "[step: 622] loss: 0.12433166056871414\n",
      "[step: 623] loss: 0.12453966587781906\n",
      "[step: 624] loss: 0.12459145486354828\n",
      "[step: 625] loss: 0.12446992844343185\n",
      "[step: 626] loss: 0.12425682693719864\n",
      "[step: 627] loss: 0.12402670085430145\n",
      "[step: 628] loss: 0.12385031580924988\n",
      "[step: 629] loss: 0.12375558912754059\n",
      "[step: 630] loss: 0.1237402856349945\n",
      "[step: 631] loss: 0.12378092110157013\n",
      "[step: 632] loss: 0.12384632974863052\n",
      "[step: 633] loss: 0.1239096000790596\n",
      "[step: 634] loss: 0.12394759804010391\n",
      "[step: 635] loss: 0.12395363301038742\n",
      "[step: 636] loss: 0.12392054498195648\n",
      "[step: 637] loss: 0.12385914474725723\n",
      "[step: 638] loss: 0.12377472221851349\n",
      "[step: 639] loss: 0.1236836239695549\n",
      "[step: 640] loss: 0.12359520047903061\n",
      "[step: 641] loss: 0.1235199123620987\n",
      "[step: 642] loss: 0.1234620213508606\n",
      "[step: 643] loss: 0.12342222034931183\n",
      "[step: 644] loss: 0.12339811027050018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 645] loss: 0.12338551878929138\n",
      "[step: 646] loss: 0.12338059395551682\n",
      "[step: 647] loss: 0.12338020652532578\n",
      "[step: 648] loss: 0.12338327616453171\n",
      "[step: 649] loss: 0.12338974326848984\n",
      "[step: 650] loss: 0.12340271472930908\n",
      "[step: 651] loss: 0.12342531979084015\n",
      "[step: 652] loss: 0.12346664071083069\n",
      "[step: 653] loss: 0.12353537976741791\n",
      "[step: 654] loss: 0.12365471571683884\n",
      "[step: 655] loss: 0.12384700030088425\n",
      "[step: 656] loss: 0.12417268007993698\n",
      "[step: 657] loss: 0.12468039989471436\n",
      "[step: 658] loss: 0.12551188468933105\n",
      "[step: 659] loss: 0.12668950855731964\n",
      "[step: 660] loss: 0.12838876247406006\n",
      "[step: 661] loss: 0.13008779287338257\n",
      "[step: 662] loss: 0.13143512606620789\n",
      "[step: 663] loss: 0.13064612448215485\n",
      "[step: 664] loss: 0.12787401676177979\n",
      "[step: 665] loss: 0.1243295818567276\n",
      "[step: 666] loss: 0.12305742502212524\n",
      "[step: 667] loss: 0.12456370145082474\n",
      "[step: 668] loss: 0.12609688937664032\n",
      "[step: 669] loss: 0.12539632618427277\n",
      "[step: 670] loss: 0.123502716422081\n",
      "[step: 671] loss: 0.12315379828214645\n",
      "[step: 672] loss: 0.1243373304605484\n",
      "[step: 673] loss: 0.12479325383901596\n",
      "[step: 674] loss: 0.1238303929567337\n",
      "[step: 675] loss: 0.12290772050619125\n",
      "[step: 676] loss: 0.12318095564842224\n",
      "[step: 677] loss: 0.12392716854810715\n",
      "[step: 678] loss: 0.1239616796374321\n",
      "[step: 679] loss: 0.12329515814781189\n",
      "[step: 680] loss: 0.12272584438323975\n",
      "[step: 681] loss: 0.12278100103139877\n",
      "[step: 682] loss: 0.12322551757097244\n",
      "[step: 683] loss: 0.1235269233584404\n",
      "[step: 684] loss: 0.12344220280647278\n",
      "[step: 685] loss: 0.12306226044893265\n",
      "[step: 686] loss: 0.12268850207328796\n",
      "[step: 687] loss: 0.122519850730896\n",
      "[step: 688] loss: 0.12258515506982803\n",
      "[step: 689] loss: 0.12278011441230774\n",
      "[step: 690] loss: 0.1229623332619667\n",
      "[step: 691] loss: 0.1230420172214508\n",
      "[step: 692] loss: 0.12297768890857697\n",
      "[step: 693] loss: 0.12281360477209091\n",
      "[step: 694] loss: 0.12260529398918152\n",
      "[step: 695] loss: 0.1224282905459404\n",
      "[step: 696] loss: 0.12232613563537598\n",
      "[step: 697] loss: 0.12230838090181351\n",
      "[step: 698] loss: 0.12235018610954285\n",
      "[step: 699] loss: 0.12240934371948242\n",
      "[step: 700] loss: 0.12244729697704315\n",
      "[step: 701] loss: 0.12243931740522385\n",
      "[step: 702] loss: 0.12238781154155731\n",
      "[step: 703] loss: 0.12230675667524338\n",
      "[step: 704] loss: 0.12222203612327576\n",
      "[step: 705] loss: 0.12215294688940048\n",
      "[step: 706] loss: 0.12211005389690399\n",
      "[step: 707] loss: 0.12209277600049973\n",
      "[step: 708] loss: 0.12209326028823853\n",
      "[step: 709] loss: 0.1221015676856041\n",
      "[step: 710] loss: 0.122109055519104\n",
      "[step: 711] loss: 0.12211162596940994\n",
      "[step: 712] loss: 0.12210699915885925\n",
      "[step: 713] loss: 0.12209756672382355\n",
      "[step: 714] loss: 0.12208431214094162\n",
      "[step: 715] loss: 0.12207173556089401\n",
      "[step: 716] loss: 0.12206123769283295\n",
      "[step: 717] loss: 0.1220577210187912\n",
      "[step: 718] loss: 0.12206307053565979\n",
      "[step: 719] loss: 0.12208490073680878\n",
      "[step: 720] loss: 0.12212779372930527\n",
      "[step: 721] loss: 0.12220807373523712\n",
      "[step: 722] loss: 0.12233749032020569\n",
      "[step: 723] loss: 0.12255441397428513\n",
      "[step: 724] loss: 0.12288188934326172\n",
      "[step: 725] loss: 0.1234041228890419\n",
      "[step: 726] loss: 0.12412828952074051\n",
      "[step: 727] loss: 0.12518154084682465\n",
      "[step: 728] loss: 0.12633846700191498\n",
      "[step: 729] loss: 0.1275666207075119\n",
      "[step: 730] loss: 0.1279102861881256\n",
      "[step: 731] loss: 0.12717580795288086\n",
      "[step: 732] loss: 0.12490589916706085\n",
      "[step: 733] loss: 0.12257206439971924\n",
      "[step: 734] loss: 0.12164869159460068\n",
      "[step: 735] loss: 0.12249552458524704\n",
      "[step: 736] loss: 0.12372613698244095\n",
      "[step: 737] loss: 0.1237124428153038\n",
      "[step: 738] loss: 0.12257687002420425\n",
      "[step: 739] loss: 0.12163830548524857\n",
      "[step: 740] loss: 0.1218276172876358\n",
      "[step: 741] loss: 0.1225941851735115\n",
      "[step: 742] loss: 0.12284571677446365\n",
      "[step: 743] loss: 0.1223381981253624\n",
      "[step: 744] loss: 0.12163348495960236\n",
      "[step: 745] loss: 0.12139887362718582\n",
      "[step: 746] loss: 0.12169234454631805\n",
      "[step: 747] loss: 0.12211137264966965\n",
      "[step: 748] loss: 0.12228871881961823\n",
      "[step: 749] loss: 0.12210576981306076\n",
      "[step: 750] loss: 0.12174377590417862\n",
      "[step: 751] loss: 0.12140420079231262\n",
      "[step: 752] loss: 0.12123434245586395\n",
      "[step: 753] loss: 0.12125218659639359\n",
      "[step: 754] loss: 0.12139556556940079\n",
      "[step: 755] loss: 0.1215810626745224\n",
      "[step: 756] loss: 0.12173312157392502\n",
      "[step: 757] loss: 0.12182223051786423\n",
      "[step: 758] loss: 0.12181775271892548\n",
      "[step: 759] loss: 0.12174377590417862\n",
      "[step: 760] loss: 0.12160096317529678\n",
      "[step: 761] loss: 0.12143205851316452\n",
      "[step: 762] loss: 0.12125789374113083\n",
      "[step: 763] loss: 0.12111295759677887\n",
      "[step: 764] loss: 0.12101373821496964\n",
      "[step: 765] loss: 0.12096680700778961\n",
      "[step: 766] loss: 0.12096453458070755\n",
      "[step: 767] loss: 0.12099038809537888\n",
      "[step: 768] loss: 0.12102555483579636\n",
      "[step: 769] loss: 0.12105298042297363\n",
      "[step: 770] loss: 0.12106449902057648\n",
      "[step: 771] loss: 0.12105491757392883\n",
      "[step: 772] loss: 0.12102943658828735\n",
      "[step: 773] loss: 0.12099047005176544\n",
      "[step: 774] loss: 0.1209467276930809\n",
      "[step: 775] loss: 0.12090099602937698\n",
      "[step: 776] loss: 0.12085851281881332\n",
      "[step: 777] loss: 0.12081989645957947\n",
      "[step: 778] loss: 0.12078680098056793\n",
      "[step: 779] loss: 0.12075860053300858\n",
      "[step: 780] loss: 0.12073598802089691\n",
      "[step: 781] loss: 0.12071866542100906\n",
      "[step: 782] loss: 0.1207084059715271\n",
      "[step: 783] loss: 0.12070722132921219\n",
      "[step: 784] loss: 0.12072089314460754\n",
      "[step: 785] loss: 0.12075748294591904\n",
      "[step: 786] loss: 0.1208355501294136\n",
      "[step: 787] loss: 0.12098094820976257\n",
      "[step: 788] loss: 0.12125371396541595\n",
      "[step: 789] loss: 0.12172891199588776\n",
      "[step: 790] loss: 0.12259085476398468\n",
      "[step: 791] loss: 0.12398450076580048\n",
      "[step: 792] loss: 0.1263173222541809\n",
      "[step: 793] loss: 0.1292489618062973\n",
      "[step: 794] loss: 0.1326398253440857\n",
      "[step: 795] loss: 0.1331418752670288\n",
      "[step: 796] loss: 0.1299038678407669\n",
      "[step: 797] loss: 0.12341304868459702\n",
      "[step: 798] loss: 0.12058525532484055\n",
      "[step: 799] loss: 0.12342903017997742\n",
      "[step: 800] loss: 0.12562689185142517\n",
      "[step: 801] loss: 0.1231672540307045\n",
      "[step: 802] loss: 0.12069714069366455\n",
      "[step: 803] loss: 0.12239190191030502\n",
      "[step: 804] loss: 0.12381265312433243\n",
      "[step: 805] loss: 0.12180066108703613\n",
      "[step: 806] loss: 0.12058155983686447\n",
      "[step: 807] loss: 0.12199267745018005\n",
      "[step: 808] loss: 0.12253358960151672\n",
      "[step: 809] loss: 0.12110503762960434\n",
      "[step: 810] loss: 0.12032181769609451\n",
      "[step: 811] loss: 0.12121034413576126\n",
      "[step: 812] loss: 0.12187802046537399\n",
      "[step: 813] loss: 0.12117335945367813\n",
      "[step: 814] loss: 0.12025726586580276\n",
      "[step: 815] loss: 0.12027543038129807\n",
      "[step: 816] loss: 0.12094702571630478\n",
      "[step: 817] loss: 0.12132308632135391\n",
      "[step: 818] loss: 0.12097541987895966\n",
      "[step: 819] loss: 0.12035563588142395\n",
      "[step: 820] loss: 0.1200307160615921\n",
      "[step: 821] loss: 0.12018447369337082\n",
      "[step: 822] loss: 0.12054283171892166\n",
      "[step: 823] loss: 0.12071172893047333\n",
      "[step: 824] loss: 0.120553158223629\n",
      "[step: 825] loss: 0.12019383907318115\n",
      "[step: 826] loss: 0.11992315202951431\n",
      "[step: 827] loss: 0.1198977530002594\n",
      "[step: 828] loss: 0.12005054205656052\n",
      "[step: 829] loss: 0.1201835498213768\n",
      "[step: 830] loss: 0.12014798820018768\n",
      "[step: 831] loss: 0.11997640877962112\n",
      "[step: 832] loss: 0.11980649828910828\n",
      "[step: 833] loss: 0.11975621432065964\n",
      "[step: 834] loss: 0.11981790512800217\n",
      "[step: 835] loss: 0.11989059299230576\n",
      "[step: 836] loss: 0.11988738924264908\n",
      "[step: 837] loss: 0.11980020999908447\n",
      "[step: 838] loss: 0.11969474703073502\n",
      "[step: 839] loss: 0.11963503807783127\n",
      "[step: 840] loss: 0.11963728070259094\n",
      "[step: 841] loss: 0.11966903507709503\n",
      "[step: 842] loss: 0.11968541145324707\n",
      "[step: 843] loss: 0.11966419965028763\n",
      "[step: 844] loss: 0.11961080133914948\n",
      "[step: 845] loss: 0.11954965442419052\n",
      "[step: 846] loss: 0.11950207501649857\n",
      "[step: 847] loss: 0.11947768181562424\n",
      "[step: 848] loss: 0.1194726899266243\n",
      "[step: 849] loss: 0.11947636306285858\n",
      "[step: 850] loss: 0.11947812139987946\n",
      "[step: 851] loss: 0.11947066336870193\n",
      "[step: 852] loss: 0.1194525808095932\n",
      "[step: 853] loss: 0.11942515522241592\n",
      "[step: 854] loss: 0.11939264088869095\n",
      "[step: 855] loss: 0.11935856193304062\n",
      "[step: 856] loss: 0.11932618170976639\n",
      "[step: 857] loss: 0.1192973181605339\n",
      "[step: 858] loss: 0.11927255988121033\n",
      "[step: 859] loss: 0.1192517876625061\n",
      "[step: 860] loss: 0.11923417448997498\n",
      "[step: 861] loss: 0.11921868473291397\n",
      "[step: 862] loss: 0.11920445412397385\n",
      "[step: 863] loss: 0.11919080466032028\n",
      "[step: 864] loss: 0.11917723715305328\n",
      "[step: 865] loss: 0.11916352808475494\n",
      "[step: 866] loss: 0.11914964765310287\n",
      "[step: 867] loss: 0.11913589388132095\n",
      "[step: 868] loss: 0.11912254989147186\n",
      "[step: 869] loss: 0.11911031603813171\n",
      "[step: 870] loss: 0.11909975856542587\n",
      "[step: 871] loss: 0.11909222602844238\n",
      "[step: 872] loss: 0.11908890306949615\n",
      "[step: 873] loss: 0.1190929040312767\n",
      "[step: 874] loss: 0.1191069632768631\n",
      "[step: 875] loss: 0.11913858354091644\n",
      "[step: 876] loss: 0.11919504404067993\n",
      "[step: 877] loss: 0.11929568648338318\n",
      "[step: 878] loss: 0.11945891380310059\n",
      "[step: 879] loss: 0.11973652243614197\n",
      "[step: 880] loss: 0.12016671150922775\n",
      "[step: 881] loss: 0.12087863683700562\n",
      "[step: 882] loss: 0.1218935176730156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 883] loss: 0.12343788892030716\n",
      "[step: 884] loss: 0.12513284385204315\n",
      "[step: 885] loss: 0.1269608587026596\n",
      "[step: 886] loss: 0.12717869877815247\n",
      "[step: 887] loss: 0.12562884390354156\n",
      "[step: 888] loss: 0.12196696549654007\n",
      "[step: 889] loss: 0.11917870491743088\n",
      "[step: 890] loss: 0.1193016916513443\n",
      "[step: 891] loss: 0.12124939262866974\n",
      "[step: 892] loss: 0.12199696153402328\n",
      "[step: 893] loss: 0.12040000408887863\n",
      "[step: 894] loss: 0.11890602856874466\n",
      "[step: 895] loss: 0.11932443082332611\n",
      "[step: 896] loss: 0.12048456817865372\n",
      "[step: 897] loss: 0.12048039585351944\n",
      "[step: 898] loss: 0.11932242661714554\n",
      "[step: 899] loss: 0.11864006519317627\n",
      "[step: 900] loss: 0.11906571686267853\n",
      "[step: 901] loss: 0.11976911872625351\n",
      "[step: 902] loss: 0.11985558271408081\n",
      "[step: 903] loss: 0.1192525252699852\n",
      "[step: 904] loss: 0.11863572895526886\n",
      "[step: 905] loss: 0.1184849739074707\n",
      "[step: 906] loss: 0.11878585815429688\n",
      "[step: 907] loss: 0.11919897794723511\n",
      "[step: 908] loss: 0.11938706040382385\n",
      "[step: 909] loss: 0.11927737295627594\n",
      "[step: 910] loss: 0.11893631517887115\n",
      "[step: 911] loss: 0.11858002841472626\n",
      "[step: 912] loss: 0.11835777759552002\n",
      "[step: 913] loss: 0.11833031475543976\n",
      "[step: 914] loss: 0.11844727396965027\n",
      "[step: 915] loss: 0.11859747767448425\n",
      "[step: 916] loss: 0.11868180334568024\n",
      "[step: 917] loss: 0.11864092200994492\n",
      "[step: 918] loss: 0.11850475519895554\n",
      "[step: 919] loss: 0.11833234876394272\n",
      "[step: 920] loss: 0.11820259690284729\n",
      "[step: 921] loss: 0.11815466731786728\n",
      "[step: 922] loss: 0.11818066239356995\n",
      "[step: 923] loss: 0.11823789775371552\n",
      "[step: 924] loss: 0.11827696114778519\n",
      "[step: 925] loss: 0.11827164888381958\n",
      "[step: 926] loss: 0.11821874976158142\n",
      "[step: 927] loss: 0.11814196407794952\n",
      "[step: 928] loss: 0.11806564778089523\n",
      "[step: 929] loss: 0.11800981312990189\n",
      "[step: 930] loss: 0.11798086017370224\n",
      "[step: 931] loss: 0.11797460913658142\n",
      "[step: 932] loss: 0.11798164993524551\n",
      "[step: 933] loss: 0.11799248307943344\n",
      "[step: 934] loss: 0.11800090223550797\n",
      "[step: 935] loss: 0.1180029809474945\n",
      "[step: 936] loss: 0.11799991130828857\n",
      "[step: 937] loss: 0.11799126863479614\n",
      "[step: 938] loss: 0.11798127740621567\n",
      "[step: 939] loss: 0.1179698184132576\n",
      "[step: 940] loss: 0.1179613247513771\n",
      "[step: 941] loss: 0.11795520782470703\n",
      "[step: 942] loss: 0.11795629560947418\n",
      "[step: 943] loss: 0.11796356737613678\n",
      "[step: 944] loss: 0.11798365414142609\n",
      "[step: 945] loss: 0.11801528185606003\n",
      "[step: 946] loss: 0.11806921660900116\n",
      "[step: 947] loss: 0.11814306676387787\n",
      "[step: 948] loss: 0.11825574934482574\n",
      "[step: 949] loss: 0.11839989572763443\n",
      "[step: 950] loss: 0.1186080351471901\n",
      "[step: 951] loss: 0.11885460466146469\n",
      "[step: 952] loss: 0.11918957531452179\n",
      "[step: 953] loss: 0.11953188478946686\n",
      "[step: 954] loss: 0.11994021385908127\n",
      "[step: 955] loss: 0.12021312117576599\n",
      "[step: 956] loss: 0.12040270864963531\n",
      "[step: 957] loss: 0.12020956724882126\n",
      "[step: 958] loss: 0.11976108700037003\n",
      "[step: 959] loss: 0.11897370964288712\n",
      "[step: 960] loss: 0.11817993968725204\n",
      "[step: 961] loss: 0.1176043227314949\n",
      "[step: 962] loss: 0.11743339151144028\n",
      "[step: 963] loss: 0.11762488633394241\n",
      "[step: 964] loss: 0.1179722249507904\n",
      "[step: 965] loss: 0.11825896054506302\n",
      "[step: 966] loss: 0.11832224577665329\n",
      "[step: 967] loss: 0.11818940192461014\n",
      "[step: 968] loss: 0.11790729314088821\n",
      "[step: 969] loss: 0.11761344969272614\n",
      "[step: 970] loss: 0.11737899482250214\n",
      "[step: 971] loss: 0.11724851280450821\n",
      "[step: 972] loss: 0.11721803992986679\n",
      "[step: 973] loss: 0.11726395785808563\n",
      "[step: 974] loss: 0.11736094206571579\n",
      "[step: 975] loss: 0.1174900233745575\n",
      "[step: 976] loss: 0.1176527738571167\n",
      "[step: 977] loss: 0.11784454435110092\n",
      "[step: 978] loss: 0.1181013360619545\n",
      "[step: 979] loss: 0.11840953677892685\n",
      "[step: 980] loss: 0.1188383623957634\n",
      "[step: 981] loss: 0.11930779367685318\n",
      "[step: 982] loss: 0.11989717185497284\n",
      "[step: 983] loss: 0.12032902985811234\n",
      "[step: 984] loss: 0.12063863128423691\n",
      "[step: 985] loss: 0.12034409493207932\n",
      "[step: 986] loss: 0.11961442232131958\n",
      "[step: 987] loss: 0.11842603981494904\n",
      "[step: 988] loss: 0.11739814281463623\n",
      "[step: 989] loss: 0.11695818603038788\n",
      "[step: 990] loss: 0.11720317602157593\n",
      "[step: 991] loss: 0.11776103079319\n",
      "[step: 992] loss: 0.11809258162975311\n",
      "[step: 993] loss: 0.11797778308391571\n",
      "[step: 994] loss: 0.11749283224344254\n",
      "[step: 995] loss: 0.11701588332653046\n",
      "[step: 996] loss: 0.11680012196302414\n",
      "[step: 997] loss: 0.11688914895057678\n",
      "[step: 998] loss: 0.11714698374271393\n",
      "[step: 999] loss: 0.11739370226860046\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "LSTM realforecast : [4260.25, 7416.30810546875, 10204.5029296875, 12543.4189453125, 12502.0380859375, 10352.0888671875, 6890.15869140625, 4086.7216796875, 2252.292236328125, 1619.9642333984375, 1773.136474609375, 2929.975341796875, 4677.8828125, 7465.01171875, 12251.361328125, 15952.8505859375, 16215.6162109375, 13491.8291015625, 9029.412109375]\n",
      "Bayseian realforecast : [4734.3611822098783, 5144.5556652188916, 9031.3574396636741, 4938.5477472229886, 5971.5927043597967, 7771.7319245239014, 20554.171100061489, 68463.184419017038, 377.24924247804387, 3057.9638346778779, 2839.3791262178192, 1990.9398730447015, 5161.7069636415936, 5164.8972963772876, 8502.3199902633132, 4332.2750692906975, 5527.9554345500128, 7479.8788888506924, 21555.976045785443]\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,0,forecastDay,'month') #0은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4260.25,\n",
       " 7416.30810546875,\n",
       " 10204.5029296875,\n",
       " 12543.4189453125,\n",
       " 12502.0380859375,\n",
       " 10352.0888671875,\n",
       " 6890.15869140625,\n",
       " 4086.7216796875,\n",
       " 2252.292236328125,\n",
       " 1619.9642333984375,\n",
       " 1773.136474609375,\n",
       " 2929.975341796875,\n",
       " 4677.8828125,\n",
       " 7465.01171875,\n",
       " 12251.361328125,\n",
       " 15952.8505859375,\n",
       " 16215.6162109375,\n",
       " 13491.8291015625,\n",
       " 9029.412109375]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawArrayDatas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
