{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list(np.sqrt(rawArrayDatas[1]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list(np.sqrt(rawArrayDatas[1][:-forecastDay] ))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-(mockForcastDay+forecastDay)] & np.log\n",
    "    ds = rawArrayDatas[0][:-(mockForcastDay+forecastDay)]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-(mockForcastDay+forecastDay)]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of (mockForcastDay+forecastDay)  rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "    testY= rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출       \n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'day')\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "    \n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    print('LSTM realforecast :',realForecastDictionary['LSTM'])\n",
    "    print('Bayseian realforecast :',realForecastDictionary['Bayseian'] ) \n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "#         listedLogPredict=test_predict[-1].tolist()\n",
    "#     return [np.exp(y) for y in listedLogPredict]\n",
    "    return np.square(test_predict[-1]).tolist()\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM testforecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian testforecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 7\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('webMonth36.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.2)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2013-02-01',\n",
       "  '2013-03-01',\n",
       "  '2013-04-01',\n",
       "  '2013-05-01',\n",
       "  '2013-06-01',\n",
       "  '2013-07-01',\n",
       "  '2013-08-01',\n",
       "  '2013-09-01',\n",
       "  '2013-10-01',\n",
       "  '2013-11-01',\n",
       "  '2013-12-01',\n",
       "  '2014-01-01',\n",
       "  '2014-02-01',\n",
       "  '2014-03-01',\n",
       "  '2014-04-01',\n",
       "  '2014-05-01',\n",
       "  '2014-06-01',\n",
       "  '2014-07-01',\n",
       "  '2014-08-01',\n",
       "  '2014-09-01',\n",
       "  '2014-10-01',\n",
       "  '2014-11-01',\n",
       "  '2014-12-01',\n",
       "  '2015-01-01',\n",
       "  '2015-02-01',\n",
       "  '2015-03-01',\n",
       "  '2015-04-01',\n",
       "  '2015-05-01',\n",
       "  '2015-06-01',\n",
       "  '2015-07-01',\n",
       "  '2015-08-01',\n",
       "  '2015-09-01',\n",
       "  '2015-10-01',\n",
       "  '2015-11-01',\n",
       "  '2015-12-01',\n",
       "  '2016-01-01'],\n",
       " [4432.5,\n",
       "  2776.5161290000001,\n",
       "  2286.6333329999998,\n",
       "  1913.451613,\n",
       "  1650.9333329999999,\n",
       "  2040.0333329999999,\n",
       "  2734.0645159999999,\n",
       "  13740.266669999999,\n",
       "  10564.35484,\n",
       "  7608.3999999999996,\n",
       "  8706.1935480000011,\n",
       "  26366.56667,\n",
       "  24504.89286,\n",
       "  2169.0322579999997,\n",
       "  1879.0,\n",
       "  2449.5483870000003,\n",
       "  1487.9000000000001,\n",
       "  1903.580645,\n",
       "  3121.333333,\n",
       "  8773.6333329999998,\n",
       "  9429.2903230000011,\n",
       "  4370.5666670000001,\n",
       "  4122.7096770000007,\n",
       "  7644.7419349999991,\n",
       "  4147.0740740000001,\n",
       "  1799.7096770000001,\n",
       "  1405.5,\n",
       "  1648.193548,\n",
       "  1090.5666670000001,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArrayDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 0.19120542705059052\n",
      "[step: 1] loss: 0.05840928480029106\n",
      "[step: 2] loss: 0.016061391681432724\n",
      "[step: 3] loss: 0.02930806204676628\n",
      "[step: 4] loss: 0.0554005391895771\n",
      "[step: 5] loss: 0.06284425407648087\n",
      "[step: 6] loss: 0.05171339586377144\n",
      "[step: 7] loss: 0.03442814573645592\n",
      "[step: 8] loss: 0.020501837134361267\n",
      "[step: 9] loss: 0.013794281519949436\n",
      "[step: 10] loss: 0.013762188144028187\n",
      "[step: 11] loss: 0.017730500549077988\n",
      "[step: 12] loss: 0.022702645510435104\n",
      "[step: 13] loss: 0.026286548003554344\n",
      "[step: 14] loss: 0.02716263383626938\n",
      "[step: 15] loss: 0.025298263877630234\n",
      "[step: 16] loss: 0.02170492149889469\n",
      "[step: 17] loss: 0.017786119133234024\n",
      "[step: 18] loss: 0.014691222459077835\n",
      "[step: 19] loss: 0.013004560023546219\n",
      "[step: 20] loss: 0.012774770148098469\n",
      "[step: 21] loss: 0.013669103384017944\n",
      "[step: 22] loss: 0.015108455903828144\n",
      "[step: 23] loss: 0.016425661742687225\n",
      "[step: 24] loss: 0.01709100790321827\n",
      "[step: 25] loss: 0.016904251649975777\n",
      "[step: 26] loss: 0.016018815338611603\n",
      "[step: 27] loss: 0.014798935502767563\n",
      "[step: 28] loss: 0.01363155897706747\n",
      "[step: 29] loss: 0.012800183147192001\n",
      "[step: 30] loss: 0.012438721023499966\n",
      "[step: 31] loss: 0.012529919855296612\n",
      "[step: 32] loss: 0.012925995513796806\n",
      "[step: 33] loss: 0.013401678763329983\n",
      "[step: 34] loss: 0.013742348179221153\n",
      "[step: 35] loss: 0.013826644979417324\n",
      "[step: 36] loss: 0.01365066971629858\n",
      "[step: 37] loss: 0.013292187824845314\n",
      "[step: 38] loss: 0.012862947769463062\n",
      "[step: 39] loss: 0.012479996308684349\n",
      "[step: 40] loss: 0.012240038253366947\n",
      "[step: 41] loss: 0.012182858772575855\n",
      "[step: 42] loss: 0.012272298336029053\n",
      "[step: 43] loss: 0.012423349544405937\n",
      "[step: 44] loss: 0.012552416883409023\n",
      "[step: 45] loss: 0.012605948373675346\n",
      "[step: 46] loss: 0.012562201358377934\n",
      "[step: 47] loss: 0.012430944480001926\n",
      "[step: 48] loss: 0.012253820896148682\n",
      "[step: 49] loss: 0.012088146060705185\n",
      "[step: 50] loss: 0.01197805441915989\n",
      "[step: 51] loss: 0.01193670928478241\n",
      "[step: 52] loss: 0.011949649080634117\n",
      "[step: 53] loss: 0.011988110840320587\n",
      "[step: 54] loss: 0.012020815163850784\n",
      "[step: 55] loss: 0.012022950686514378\n",
      "[step: 56] loss: 0.011983862146735191\n",
      "[step: 57] loss: 0.011911023408174515\n",
      "[step: 58] loss: 0.011825216002762318\n",
      "[step: 59] loss: 0.011748498305678368\n",
      "[step: 60] loss: 0.011694228276610374\n",
      "[step: 61] loss: 0.011665375903248787\n",
      "[step: 62] loss: 0.01165629830211401\n",
      "[step: 63] loss: 0.01165409293025732\n",
      "[step: 64] loss: 0.011643785052001476\n",
      "[step: 65] loss: 0.011616840027272701\n",
      "[step: 66] loss: 0.011574093252420425\n",
      "[step: 67] loss: 0.011521917767822742\n",
      "[step: 68] loss: 0.011468565091490746\n",
      "[step: 69] loss: 0.011421959847211838\n",
      "[step: 70] loss: 0.01138640008866787\n",
      "[step: 71] loss: 0.011360242031514645\n",
      "[step: 72] loss: 0.011337676085531712\n",
      "[step: 73] loss: 0.011312487535178661\n",
      "[step: 74] loss: 0.011280915699899197\n",
      "[step: 75] loss: 0.011242224834859371\n",
      "[step: 76] loss: 0.011198270134627819\n",
      "[step: 77] loss: 0.011152880266308784\n",
      "[step: 78] loss: 0.011110068298876286\n",
      "[step: 79] loss: 0.011071479879319668\n",
      "[step: 80] loss: 0.01103613618761301\n",
      "[step: 81] loss: 0.011001814156770706\n",
      "[step: 82] loss: 0.010966001078486443\n",
      "[step: 83] loss: 0.010926824063062668\n",
      "[step: 84] loss: 0.010883874259889126\n",
      "[step: 85] loss: 0.010838363319635391\n",
      "[step: 86] loss: 0.010792218148708344\n",
      "[step: 87] loss: 0.010746787302196026\n",
      "[step: 88] loss: 0.010702445171773434\n",
      "[step: 89] loss: 0.010658755898475647\n",
      "[step: 90] loss: 0.010614510625600815\n",
      "[step: 91] loss: 0.010568482801318169\n",
      "[step: 92] loss: 0.010520100593566895\n",
      "[step: 93] loss: 0.01046953909099102\n",
      "[step: 94] loss: 0.010417439974844456\n",
      "[step: 95] loss: 0.010364489629864693\n",
      "[step: 96] loss: 0.01031122449785471\n",
      "[step: 97] loss: 0.010257557965815067\n",
      "[step: 98] loss: 0.010202953591942787\n",
      "[step: 99] loss: 0.010146829299628735\n",
      "[step: 100] loss: 0.010088780894875526\n",
      "[step: 101] loss: 0.010028695687651634\n",
      "[step: 102] loss: 0.009966742247343063\n",
      "[step: 103] loss: 0.009903298690915108\n",
      "[step: 104] loss: 0.009838588535785675\n",
      "[step: 105] loss: 0.009772555902600288\n",
      "[step: 106] loss: 0.009705008007586002\n",
      "[step: 107] loss: 0.009635630995035172\n",
      "[step: 108] loss: 0.009564166888594627\n",
      "[step: 109] loss: 0.009490490891039371\n",
      "[step: 110] loss: 0.009414678439497948\n",
      "[step: 111] loss: 0.009336859919130802\n",
      "[step: 112] loss: 0.009257111698389053\n",
      "[step: 113] loss: 0.009175436571240425\n",
      "[step: 114] loss: 0.009091708809137344\n",
      "[step: 115] loss: 0.009005771018564701\n",
      "[step: 116] loss: 0.008917509578168392\n",
      "[step: 117] loss: 0.008826903998851776\n",
      "[step: 118] loss: 0.008734001778066158\n",
      "[step: 119] loss: 0.008638917468488216\n",
      "[step: 120] loss: 0.008541730232536793\n",
      "[step: 121] loss: 0.008442500606179237\n",
      "[step: 122] loss: 0.008341245353221893\n",
      "[step: 123] loss: 0.008238023146986961\n",
      "[step: 124] loss: 0.008132904767990112\n",
      "[step: 125] loss: 0.008026069030165672\n",
      "[step: 126] loss: 0.007917772978544235\n",
      "[step: 127] loss: 0.007808786816895008\n",
      "[step: 128] loss: 0.007711258251219988\n",
      "[step: 129] loss: 0.007931430824100971\n",
      "[step: 130] loss: 0.010666626505553722\n",
      "[step: 131] loss: 0.010331815108656883\n",
      "[step: 132] loss: 0.007741098292171955\n",
      "[step: 133] loss: 0.010351162403821945\n",
      "[step: 134] loss: 0.007277338765561581\n",
      "[step: 135] loss: 0.009773976169526577\n",
      "[step: 136] loss: 0.007176124956458807\n",
      "[step: 137] loss: 0.009259958751499653\n",
      "[step: 138] loss: 0.007093707099556923\n",
      "[step: 139] loss: 0.00880019273608923\n",
      "[step: 140] loss: 0.007013028021901846\n",
      "[step: 141] loss: 0.00841541588306427\n",
      "[step: 142] loss: 0.006941267754882574\n",
      "[step: 143] loss: 0.008063359186053276\n",
      "[step: 144] loss: 0.006879145745187998\n",
      "[step: 145] loss: 0.007756954524666071\n",
      "[step: 146] loss: 0.0068106623366475105\n",
      "[step: 147] loss: 0.007486486807465553\n",
      "[step: 148] loss: 0.006734268274158239\n",
      "[step: 149] loss: 0.007259980775415897\n",
      "[step: 150] loss: 0.006639816798269749\n",
      "[step: 151] loss: 0.007069340907037258\n",
      "[step: 152] loss: 0.00653603533282876\n",
      "[step: 153] loss: 0.006908606272190809\n",
      "[step: 154] loss: 0.006425251252949238\n",
      "[step: 155] loss: 0.006762084551155567\n",
      "[step: 156] loss: 0.006323677022010088\n",
      "[step: 157] loss: 0.006616536993533373\n",
      "[step: 158] loss: 0.006245638243854046\n",
      "[step: 159] loss: 0.0064520686864852905\n",
      "[step: 160] loss: 0.006204233970493078\n",
      "[step: 161] loss: 0.006263252347707748\n",
      "[step: 162] loss: 0.006194380111992359\n",
      "[step: 163] loss: 0.00607516523450613\n",
      "[step: 164] loss: 0.006169070489704609\n",
      "[step: 165] loss: 0.005951059982180595\n",
      "[step: 166] loss: 0.006061298307031393\n",
      "[step: 167] loss: 0.005934782326221466\n",
      "[step: 168] loss: 0.00588403083384037\n",
      "[step: 169] loss: 0.005937917623668909\n",
      "[step: 170] loss: 0.0057854014448821545\n",
      "[step: 171] loss: 0.0058179511688649654\n",
      "[step: 172] loss: 0.0058072153478860855\n",
      "[step: 173] loss: 0.005691595375537872\n",
      "[step: 174] loss: 0.005726867821067572\n",
      "[step: 175] loss: 0.005712475161999464\n",
      "[step: 176] loss: 0.005617306102067232\n",
      "[step: 177] loss: 0.005626595579087734\n",
      "[step: 178] loss: 0.005639773793518543\n",
      "[step: 179] loss: 0.00556841678917408\n",
      "[step: 180] loss: 0.0055313194170594215\n",
      "[step: 181] loss: 0.00555373402312398\n",
      "[step: 182] loss: 0.005539506208151579\n",
      "[step: 183] loss: 0.005483111832290888\n",
      "[step: 184] loss: 0.005455451086163521\n",
      "[step: 185] loss: 0.005465521477162838\n",
      "[step: 186] loss: 0.005463024601340294\n",
      "[step: 187] loss: 0.005427963100373745\n",
      "[step: 188] loss: 0.005389837082475424\n",
      "[step: 189] loss: 0.005375276319682598\n",
      "[step: 190] loss: 0.005377092398703098\n",
      "[step: 191] loss: 0.005372251849621534\n",
      "[step: 192] loss: 0.005350240971893072\n",
      "[step: 193] loss: 0.005320052616298199\n",
      "[step: 194] loss: 0.005295970477163792\n",
      "[step: 195] loss: 0.005283804144710302\n",
      "[step: 196] loss: 0.005278375931084156\n",
      "[step: 197] loss: 0.00527062825858593\n",
      "[step: 198] loss: 0.005255506839603186\n",
      "[step: 199] loss: 0.005234268959611654\n",
      "[step: 200] loss: 0.005212300922721624\n",
      "[step: 201] loss: 0.005194367840886116\n",
      "[step: 202] loss: 0.005181732587516308\n",
      "[step: 203] loss: 0.0051721250638365746\n",
      "[step: 204] loss: 0.005161982495337725\n",
      "[step: 205] loss: 0.005148918367922306\n",
      "[step: 206] loss: 0.00513288052752614\n",
      "[step: 207] loss: 0.005115768872201443\n",
      "[step: 208] loss: 0.005099873524159193\n",
      "[step: 209] loss: 0.005086436867713928\n",
      "[step: 210] loss: 0.005075089167803526\n",
      "[step: 211] loss: 0.005064419470727444\n",
      "[step: 212] loss: 0.005052981432527304\n",
      "[step: 213] loss: 0.005040170159190893\n",
      "[step: 214] loss: 0.005026418250054121\n",
      "[step: 215] loss: 0.005012707319110632\n",
      "[step: 216] loss: 0.004999894183129072\n",
      "[step: 217] loss: 0.004988244269043207\n",
      "[step: 218] loss: 0.004977380391210318\n",
      "[step: 219] loss: 0.004966653883457184\n",
      "[step: 220] loss: 0.004955541808158159\n",
      "[step: 221] loss: 0.004943926818668842\n",
      "[step: 222] loss: 0.004932019393891096\n",
      "[step: 223] loss: 0.004920193459838629\n",
      "[step: 224] loss: 0.004908766597509384\n",
      "[step: 225] loss: 0.004897833336144686\n",
      "[step: 226] loss: 0.004887290298938751\n",
      "[step: 227] loss: 0.004876938182860613\n",
      "[step: 228] loss: 0.004866591654717922\n",
      "[step: 229] loss: 0.004856145475059748\n",
      "[step: 230] loss: 0.0048455907963216305\n",
      "[step: 231] loss: 0.004834980703890324\n",
      "[step: 232] loss: 0.004824410192668438\n",
      "[step: 233] loss: 0.004813952371478081\n",
      "[step: 234] loss: 0.004803644958883524\n",
      "[step: 235] loss: 0.00479349447414279\n",
      "[step: 236] loss: 0.0047834948636591434\n",
      "[step: 237] loss: 0.004773631691932678\n",
      "[step: 238] loss: 0.0047638751566410065\n",
      "[step: 239] loss: 0.004754210356622934\n",
      "[step: 240] loss: 0.0047446307726204395\n",
      "[step: 241] loss: 0.004735138267278671\n",
      "[step: 242] loss: 0.004725740756839514\n",
      "[step: 243] loss: 0.004716470371931791\n",
      "[step: 244] loss: 0.004707348998636007\n",
      "[step: 245] loss: 0.004698448348790407\n",
      "[step: 246] loss: 0.004689868073910475\n",
      "[step: 247] loss: 0.004681800492107868\n",
      "[step: 248] loss: 0.004674555733799934\n",
      "[step: 249] loss: 0.004668742418289185\n",
      "[step: 250] loss: 0.00466538080945611\n",
      "[step: 251] loss: 0.004666511435061693\n",
      "[step: 252] loss: 0.004675600677728653\n",
      "[step: 253] loss: 0.0046994611620903015\n",
      "[step: 254] loss: 0.0047487919218838215\n",
      "[step: 255] loss: 0.004842494614422321\n",
      "[step: 256] loss: 0.004996823612600565\n",
      "[step: 257] loss: 0.005218828096985817\n",
      "[step: 258] loss: 0.005420442670583725\n",
      "[step: 259] loss: 0.0054470691829919815\n",
      "[step: 260] loss: 0.005132187623530626\n",
      "[step: 261] loss: 0.004701197147369385\n",
      "[step: 262] loss: 0.004555919673293829\n",
      "[step: 263] loss: 0.0047606294974684715\n",
      "[step: 264] loss: 0.004925734829157591\n",
      "[step: 265] loss: 0.004765798337757587\n",
      "[step: 266] loss: 0.004538914188742638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 267] loss: 0.004575403407216072\n",
      "[step: 268] loss: 0.004722296725958586\n",
      "[step: 269] loss: 0.004670402500778437\n",
      "[step: 270] loss: 0.00451134005561471\n",
      "[step: 271] loss: 0.004517870023846626\n",
      "[step: 272] loss: 0.004615289159119129\n",
      "[step: 273] loss: 0.0045782420784235\n",
      "[step: 274] loss: 0.004471833351999521\n",
      "[step: 275] loss: 0.004476219415664673\n",
      "[step: 276] loss: 0.0045371465384960175\n",
      "[step: 277] loss: 0.004510143771767616\n",
      "[step: 278] loss: 0.004435969050973654\n",
      "[step: 279] loss: 0.004432471003383398\n",
      "[step: 280] loss: 0.004471724387258291\n",
      "[step: 281] loss: 0.004458783660084009\n",
      "[step: 282] loss: 0.0044059669598937035\n",
      "[step: 283] loss: 0.004388208035379648\n",
      "[step: 284] loss: 0.004410190507769585\n",
      "[step: 285] loss: 0.004414965398609638\n",
      "[step: 286] loss: 0.004383270163089037\n",
      "[step: 287] loss: 0.00435333838686347\n",
      "[step: 288] loss: 0.004352869000285864\n",
      "[step: 289] loss: 0.004364342428743839\n",
      "[step: 290] loss: 0.004359100945293903\n",
      "[step: 291] loss: 0.004335496108978987\n",
      "[step: 292] loss: 0.004314538091421127\n",
      "[step: 293] loss: 0.004309532232582569\n",
      "[step: 294] loss: 0.004313587676733732\n",
      "[step: 295] loss: 0.004312195349484682\n",
      "[step: 296] loss: 0.004299549385905266\n",
      "[step: 297] loss: 0.00428209500387311\n",
      "[step: 298] loss: 0.004269079305231571\n",
      "[step: 299] loss: 0.004263896495103836\n",
      "[step: 300] loss: 0.004262744914740324\n",
      "[step: 301] loss: 0.004259568173438311\n",
      "[step: 302] loss: 0.004251354373991489\n",
      "[step: 303] loss: 0.004239335190504789\n",
      "[step: 304] loss: 0.00422715675085783\n",
      "[step: 305] loss: 0.004217714536935091\n",
      "[step: 306] loss: 0.004211541265249252\n",
      "[step: 307] loss: 0.004207086283713579\n",
      "[step: 308] loss: 0.004202188923954964\n",
      "[step: 309] loss: 0.004195467568933964\n",
      "[step: 310] loss: 0.004186815582215786\n",
      "[step: 311] loss: 0.004177173599600792\n",
      "[step: 312] loss: 0.004167753271758556\n",
      "[step: 313] loss: 0.004159403033554554\n",
      "[step: 314] loss: 0.0041522858664393425\n",
      "[step: 315] loss: 0.004146004561334848\n",
      "[step: 316] loss: 0.004139928612858057\n",
      "[step: 317] loss: 0.0041334801353514194\n",
      "[step: 318] loss: 0.004126400221139193\n",
      "[step: 319] loss: 0.004118710290640593\n",
      "[step: 320] loss: 0.004110651556402445\n",
      "[step: 321] loss: 0.004102537874132395\n",
      "[step: 322] loss: 0.004094606265425682\n",
      "[step: 323] loss: 0.004086993634700775\n",
      "[step: 324] loss: 0.004079714883118868\n",
      "[step: 325] loss: 0.004072702024132013\n",
      "[step: 326] loss: 0.0040658325888216496\n",
      "[step: 327] loss: 0.004059027414768934\n",
      "[step: 328] loss: 0.00405220128595829\n",
      "[step: 329] loss: 0.004045328591018915\n",
      "[step: 330] loss: 0.004038398619741201\n",
      "[step: 331] loss: 0.004031429998576641\n",
      "[step: 332] loss: 0.004024450201541185\n",
      "[step: 333] loss: 0.004017503000795841\n",
      "[step: 334] loss: 0.004010638687759638\n",
      "[step: 335] loss: 0.004003926645964384\n",
      "[step: 336] loss: 0.003997443243861198\n",
      "[step: 337] loss: 0.00399135984480381\n",
      "[step: 338] loss: 0.003985888324677944\n",
      "[step: 339] loss: 0.003981487359851599\n",
      "[step: 340] loss: 0.003978838678449392\n",
      "[step: 341] loss: 0.003979363013058901\n",
      "[step: 342] loss: 0.003985302522778511\n",
      "[step: 343] loss: 0.004001353867352009\n",
      "[step: 344] loss: 0.004034657496958971\n",
      "[step: 345] loss: 0.004099917598068714\n",
      "[step: 346] loss: 0.004213384352624416\n",
      "[step: 347] loss: 0.004401919897645712\n",
      "[step: 348] loss: 0.004640365019440651\n",
      "[step: 349] loss: 0.004861980676651001\n",
      "[step: 350] loss: 0.004818284884095192\n",
      "[step: 351] loss: 0.004455179907381535\n",
      "[step: 352] loss: 0.004000374581664801\n",
      "[step: 353] loss: 0.003903547301888466\n",
      "[step: 354] loss: 0.0041520679369568825\n",
      "[step: 355] loss: 0.0042841206304728985\n",
      "[step: 356] loss: 0.004082600586116314\n",
      "[step: 357] loss: 0.0038691989611834288\n",
      "[step: 358] loss: 0.0039540184661746025\n",
      "[step: 359] loss: 0.004098543431609869\n",
      "[step: 360] loss: 0.004002455156296492\n",
      "[step: 361] loss: 0.0038495308253914118\n",
      "[step: 362] loss: 0.0038915821351110935\n",
      "[step: 363] loss: 0.0039862897247076035\n",
      "[step: 364] loss: 0.003926682285964489\n",
      "[step: 365] loss: 0.0038216854445636272\n",
      "[step: 366] loss: 0.0038443836383521557\n",
      "[step: 367] loss: 0.003908504731953144\n",
      "[step: 368] loss: 0.003872473491355777\n",
      "[step: 369] loss: 0.0037962317001074553\n",
      "[step: 370] loss: 0.003794377902522683\n",
      "[step: 371] loss: 0.003838525852188468\n",
      "[step: 372] loss: 0.003835186595097184\n",
      "[step: 373] loss: 0.0037821850273758173\n",
      "[step: 374] loss: 0.0037519442848861217\n",
      "[step: 375] loss: 0.0037690941244363785\n",
      "[step: 376] loss: 0.003790036542341113\n",
      "[step: 377] loss: 0.003777269273996353\n",
      "[step: 378] loss: 0.0037419882137328386\n",
      "[step: 379] loss: 0.003719381522387266\n",
      "[step: 380] loss: 0.0037225226406008005\n",
      "[step: 381] loss: 0.0037357984110713005\n",
      "[step: 382] loss: 0.0037385469768196344\n",
      "[step: 383] loss: 0.0037241624668240547\n",
      "[step: 384] loss: 0.003702198388054967\n",
      "[step: 385] loss: 0.0036850383039563894\n",
      "[step: 386] loss: 0.0036787367425858974\n",
      "[step: 387] loss: 0.0036807938013225794\n",
      "[step: 388] loss: 0.0036846324801445007\n",
      "[step: 389] loss: 0.003684891387820244\n",
      "[step: 390] loss: 0.0036791509483009577\n",
      "[step: 391] loss: 0.0036687219981104136\n",
      "[step: 392] loss: 0.0036559535656124353\n",
      "[step: 393] loss: 0.003643651260063052\n",
      "[step: 394] loss: 0.003633406013250351\n",
      "[step: 395] loss: 0.0036257822066545486\n",
      "[step: 396] loss: 0.0036204922944307327\n",
      "[step: 397] loss: 0.003616859670728445\n",
      "[step: 398] loss: 0.0036142272874712944\n",
      "[step: 399] loss: 0.0036121434532105923\n",
      "[step: 400] loss: 0.0036105450708419085\n",
      "[step: 401] loss: 0.0036094840615987778\n",
      "[step: 402] loss: 0.0036096004769206047\n",
      "[step: 403] loss: 0.003611394902691245\n",
      "[step: 404] loss: 0.0036165351048111916\n",
      "[step: 405] loss: 0.003626264166086912\n",
      "[step: 406] loss: 0.0036444386932998896\n",
      "[step: 407] loss: 0.003673199098557234\n",
      "[step: 408] loss: 0.0037200029473751783\n",
      "[step: 409] loss: 0.003782609710469842\n",
      "[step: 410] loss: 0.0038647023029625416\n",
      "[step: 411] loss: 0.0039318823255598545\n",
      "[step: 412] loss: 0.003961228299885988\n",
      "[step: 413] loss: 0.0038901525549590588\n",
      "[step: 414] loss: 0.0037448208313435316\n",
      "[step: 415] loss: 0.0035882804077118635\n",
      "[step: 416] loss: 0.0035188656765967607\n",
      "[step: 417] loss: 0.003557538613677025\n",
      "[step: 418] loss: 0.0036389136221259832\n",
      "[step: 419] loss: 0.003677284112200141\n",
      "[step: 420] loss: 0.0036311957519501448\n",
      "[step: 421] loss: 0.003547491505742073\n",
      "[step: 422] loss: 0.0034964492078870535\n",
      "[step: 423] loss: 0.003509012283757329\n",
      "[step: 424] loss: 0.0035537395160645247\n",
      "[step: 425] loss: 0.0035781278274953365\n",
      "[step: 426] loss: 0.00356058101169765\n",
      "[step: 427] loss: 0.003514919662848115\n",
      "[step: 428] loss: 0.0034761461429297924\n",
      "[step: 429] loss: 0.0034648196306079626\n",
      "[step: 430] loss: 0.0034783354494720697\n",
      "[step: 431] loss: 0.0035004850942641497\n",
      "[step: 432] loss: 0.003515393240377307\n",
      "[step: 433] loss: 0.0035173813812434673\n",
      "[step: 434] loss: 0.003506012726575136\n",
      "[step: 435] loss: 0.0034881578758358955\n",
      "[step: 436] loss: 0.003468130249530077\n",
      "[step: 437] loss: 0.003450599731877446\n",
      "[step: 438] loss: 0.0034366650506854057\n",
      "[step: 439] loss: 0.003426435636356473\n",
      "[step: 440] loss: 0.0034189478028565645\n",
      "[step: 441] loss: 0.0034132490400224924\n",
      "[step: 442] loss: 0.0034085416700690985\n",
      "[step: 443] loss: 0.0034043840132653713\n",
      "[step: 444] loss: 0.0034005509223788977\n",
      "[step: 445] loss: 0.0033970410004258156\n",
      "[step: 446] loss: 0.0033941178116947412\n",
      "[step: 447] loss: 0.0033925913739949465\n",
      "[step: 448] loss: 0.0033947417978197336\n",
      "[step: 449] loss: 0.003407020354643464\n",
      "[step: 450] loss: 0.0034488800447434187\n",
      "[step: 451] loss: 0.0035752919502556324\n",
      "[step: 452] loss: 0.003946428652852774\n",
      "[step: 453] loss: 0.004847201984375715\n",
      "[step: 454] loss: 0.006458512973040342\n",
      "[step: 455] loss: 0.006964422296732664\n",
      "[step: 456] loss: 0.00503262085840106\n",
      "[step: 457] loss: 0.0034529881086200476\n",
      "[step: 458] loss: 0.004808812402188778\n",
      "[step: 459] loss: 0.004804069176316261\n",
      "[step: 460] loss: 0.003507832298055291\n",
      "[step: 461] loss: 0.004404141567647457\n",
      "[step: 462] loss: 0.004107318352907896\n",
      "[step: 463] loss: 0.00364325731061399\n",
      "[step: 464] loss: 0.004211171064525843\n",
      "[step: 465] loss: 0.0036227458622306585\n",
      "[step: 466] loss: 0.003911108244210482\n",
      "[step: 467] loss: 0.003741796826943755\n",
      "[step: 468] loss: 0.003678896464407444\n",
      "[step: 469] loss: 0.0038011905271559954\n",
      "[step: 470] loss: 0.003522170940414071\n",
      "[step: 471] loss: 0.0038028722628951073\n",
      "[step: 472] loss: 0.003474744036793709\n",
      "[step: 473] loss: 0.00367263937368989\n",
      "[step: 474] loss: 0.0035576571244746447\n",
      "[step: 475] loss: 0.0034778162371367216\n",
      "[step: 476] loss: 0.0036298190243542194\n",
      "[step: 477] loss: 0.003387236502021551\n",
      "[step: 478] loss: 0.0035675091203302145\n",
      "[step: 479] loss: 0.0034315160010010004\n",
      "[step: 480] loss: 0.0034342664293944836\n",
      "[step: 481] loss: 0.003483276814222336\n",
      "[step: 482] loss: 0.003360849805176258\n",
      "[step: 483] loss: 0.0034583790693432093\n",
      "[step: 484] loss: 0.0033707062248140574\n",
      "[step: 485] loss: 0.003383750095963478\n",
      "[step: 486] loss: 0.003406928852200508\n",
      "[step: 487] loss: 0.003330020233988762\n",
      "[step: 488] loss: 0.003393565770238638\n",
      "[step: 489] loss: 0.003350591752678156\n",
      "[step: 490] loss: 0.003329365747049451\n",
      "[step: 491] loss: 0.0033754741307348013\n",
      "[step: 492] loss: 0.003320017596706748\n",
      "[step: 493] loss: 0.003330723149701953\n",
      "[step: 494] loss: 0.0033499456476420164\n",
      "[step: 495] loss: 0.0033071963116526604\n",
      "[step: 496] loss: 0.0033248437102884054\n",
      "[step: 497] loss: 0.003328066086396575\n",
      "[step: 498] loss: 0.0033000328112393618\n",
      "[step: 499] loss: 0.0033145176712423563\n",
      "[step: 500] loss: 0.0033111004158854485\n",
      "[step: 501] loss: 0.003292707959190011\n",
      "[step: 502] loss: 0.003302811412140727\n",
      "[step: 503] loss: 0.003297719405964017\n",
      "[step: 504] loss: 0.003284015692770481\n",
      "[step: 505] loss: 0.0032908637076616287\n",
      "[step: 506] loss: 0.0032867512200027704\n",
      "[step: 507] loss: 0.0032747439108788967\n",
      "[step: 508] loss: 0.0032788016833364964\n",
      "[step: 509] loss: 0.003277201671153307\n",
      "[step: 510] loss: 0.003266114741563797\n",
      "[step: 511] loss: 0.0032667950727045536\n",
      "[step: 512] loss: 0.003267901483923197\n",
      "[step: 513] loss: 0.0032588632311671972\n",
      "[step: 514] loss: 0.003255613846704364\n",
      "[step: 515] loss: 0.003257758915424347\n",
      "[step: 516] loss: 0.003252496477216482\n",
      "[step: 517] loss: 0.0032465115655213594\n",
      "[step: 518] loss: 0.003246747190132737\n",
      "[step: 519] loss: 0.003245225641876459\n",
      "[step: 520] loss: 0.003239792538806796\n",
      "[step: 521] loss: 0.003236710326746106\n",
      "[step: 522] loss: 0.003235799726098776\n",
      "[step: 523] loss: 0.0032332302071154118\n",
      "[step: 524] loss: 0.0032293652184307575\n",
      "[step: 525] loss: 0.003226228291168809\n",
      "[step: 526] loss: 0.0032243491150438786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 527] loss: 0.00322248344309628\n",
      "[step: 528] loss: 0.003219187492504716\n",
      "[step: 529] loss: 0.0032155094668269157\n",
      "[step: 530] loss: 0.0032133222557604313\n",
      "[step: 531] loss: 0.0032116922084242105\n",
      "[step: 532] loss: 0.0032088798470795155\n",
      "[step: 533] loss: 0.003205514047294855\n",
      "[step: 534] loss: 0.003202760824933648\n",
      "[step: 535] loss: 0.0032004450913518667\n",
      "[step: 536] loss: 0.0031981514766812325\n",
      "[step: 537] loss: 0.0031958038453012705\n",
      "[step: 538] loss: 0.003193062264472246\n",
      "[step: 539] loss: 0.0031900391913950443\n",
      "[step: 540] loss: 0.003187308320775628\n",
      "[step: 541] loss: 0.003184964880347252\n",
      "[step: 542] loss: 0.003182652872055769\n",
      "[step: 543] loss: 0.0031802731100469828\n",
      "[step: 544] loss: 0.003177881008014083\n",
      "[step: 545] loss: 0.003175352932885289\n",
      "[step: 546] loss: 0.0031726632732897997\n",
      "[step: 547] loss: 0.0031700138933956623\n",
      "[step: 548] loss: 0.0031674408819526434\n",
      "[step: 549] loss: 0.0031648457515984774\n",
      "[step: 550] loss: 0.0031622580718249083\n",
      "[step: 551] loss: 0.0031597481574863195\n",
      "[step: 552] loss: 0.0031572652515023947\n",
      "[step: 553] loss: 0.003154738573357463\n",
      "[step: 554] loss: 0.0031522498466074467\n",
      "[step: 555] loss: 0.0031498130410909653\n",
      "[step: 556] loss: 0.003147388808429241\n",
      "[step: 557] loss: 0.003145041409879923\n",
      "[step: 558] loss: 0.003142931964248419\n",
      "[step: 559] loss: 0.003141420893371105\n",
      "[step: 560] loss: 0.003141466062515974\n",
      "[step: 561] loss: 0.0031460027676075697\n",
      "[step: 562] loss: 0.0031638762447983027\n",
      "[step: 563] loss: 0.0032224312890321016\n",
      "[step: 564] loss: 0.0034088734537363052\n",
      "[step: 565] loss: 0.003986529540270567\n",
      "[step: 566] loss: 0.005643189884722233\n",
      "[step: 567] loss: 0.009007791988551617\n",
      "[step: 568] loss: 0.011312441900372505\n",
      "[step: 569] loss: 0.006390783470124006\n",
      "[step: 570] loss: 0.0033556753769516945\n",
      "[step: 571] loss: 0.0070539480075240135\n",
      "[step: 572] loss: 0.004562084097415209\n",
      "[step: 573] loss: 0.004006699658930302\n",
      "[step: 574] loss: 0.005515337456017733\n",
      "[step: 575] loss: 0.003204209264367819\n",
      "[step: 576] loss: 0.0050811003893613815\n",
      "[step: 577] loss: 0.0033172464463859797\n",
      "[step: 578] loss: 0.004481608048081398\n",
      "[step: 579] loss: 0.003443429246544838\n",
      "[step: 580] loss: 0.0040935343131423\n",
      "[step: 581] loss: 0.003465271322056651\n",
      "[step: 582] loss: 0.003877709386870265\n",
      "[step: 583] loss: 0.0034144085366278887\n",
      "[step: 584] loss: 0.0037606775294989347\n",
      "[step: 585] loss: 0.0033429162576794624\n",
      "[step: 586] loss: 0.003688754979521036\n",
      "[step: 587] loss: 0.0032757173758000135\n",
      "[step: 588] loss: 0.0036304760724306107\n",
      "[step: 589] loss: 0.003233246738091111\n",
      "[step: 590] loss: 0.0035656793043017387\n",
      "[step: 591] loss: 0.003217186313122511\n",
      "[step: 592] loss: 0.0034937746822834015\n",
      "[step: 593] loss: 0.0032228431664407253\n",
      "[step: 594] loss: 0.003418318461626768\n",
      "[step: 595] loss: 0.003238711040467024\n",
      "[step: 596] loss: 0.003349145408719778\n",
      "[step: 597] loss: 0.0032555731013417244\n",
      "[step: 598] loss: 0.0032907379791140556\n",
      "[step: 599] loss: 0.003268424654379487\n",
      "[step: 600] loss: 0.0032456116750836372\n",
      "[step: 601] loss: 0.003273990470916033\n",
      "[step: 602] loss: 0.00321492669172585\n",
      "[step: 603] loss: 0.0032706584315747023\n",
      "[step: 604] loss: 0.0031975151505321264\n",
      "[step: 605] loss: 0.003259720979258418\n",
      "[step: 606] loss: 0.003189672017470002\n",
      "[step: 607] loss: 0.0032449110876768827\n",
      "[step: 608] loss: 0.0031862563919276\n",
      "[step: 609] loss: 0.0032295335549861193\n",
      "[step: 610] loss: 0.0031848244834691286\n",
      "[step: 611] loss: 0.003214465221390128\n",
      "[step: 612] loss: 0.003184156259521842\n",
      "[step: 613] loss: 0.003200630657374859\n",
      "[step: 614] loss: 0.0031827674247324467\n",
      "[step: 615] loss: 0.0031890301033854485\n",
      "[step: 616] loss: 0.0031799208372831345\n",
      "[step: 617] loss: 0.0031795199029147625\n",
      "[step: 618] loss: 0.0031760353595018387\n",
      "[step: 619] loss: 0.003171335207298398\n",
      "[step: 620] loss: 0.0031715615186840296\n",
      "[step: 621] loss: 0.0031642871908843517\n",
      "[step: 622] loss: 0.003166408045217395\n",
      "[step: 623] loss: 0.0031583064701408148\n",
      "[step: 624] loss: 0.0031608648132532835\n",
      "[step: 625] loss: 0.003152881283313036\n",
      "[step: 626] loss: 0.0031553502194583416\n",
      "[step: 627] loss: 0.0031477471347898245\n",
      "[step: 628] loss: 0.0031498721800744534\n",
      "[step: 629] loss: 0.0031429242808371782\n",
      "[step: 630] loss: 0.0031444027554243803\n",
      "[step: 631] loss: 0.0031383181922137737\n",
      "[step: 632] loss: 0.0031390683725476265\n",
      "[step: 633] loss: 0.003133745864033699\n",
      "[step: 634] loss: 0.0031339037232100964\n",
      "[step: 635] loss: 0.0031292426865547895\n",
      "[step: 636] loss: 0.0031288049649447203\n",
      "[step: 637] loss: 0.003124831710010767\n",
      "[step: 638] loss: 0.003123789094388485\n",
      "[step: 639] loss: 0.003120436565950513\n",
      "[step: 640] loss: 0.003118888707831502\n",
      "[step: 641] loss: 0.003116039792075753\n",
      "[step: 642] loss: 0.0031140553764998913\n",
      "[step: 643] loss: 0.0031116611789911985\n",
      "[step: 644] loss: 0.0031092821154743433\n",
      "[step: 645] loss: 0.00310727721080184\n",
      "[step: 646] loss: 0.003104601288214326\n",
      "[step: 647] loss: 0.0031028517987579107\n",
      "[step: 648] loss: 0.003100006375461817\n",
      "[step: 649] loss: 0.0030984056647866964\n",
      "[step: 650] loss: 0.003095485968515277\n",
      "[step: 651] loss: 0.0030939257703721523\n",
      "[step: 652] loss: 0.003091047750785947\n",
      "[step: 653] loss: 0.003089409787207842\n",
      "[step: 654] loss: 0.003086687531322241\n",
      "[step: 655] loss: 0.003084869123995304\n",
      "[step: 656] loss: 0.0030823692213743925\n",
      "[step: 657] loss: 0.003080338705331087\n",
      "[step: 658] loss: 0.003078068606555462\n",
      "[step: 659] loss: 0.00307583250105381\n",
      "[step: 660] loss: 0.0030737598426640034\n",
      "[step: 661] loss: 0.0030713877640664577\n",
      "[step: 662] loss: 0.0030694087035954\n",
      "[step: 663] loss: 0.0030670168343931437\n",
      "[step: 664] loss: 0.00306502403691411\n",
      "[step: 665] loss: 0.003062694799154997\n",
      "[step: 666] loss: 0.003060620743781328\n",
      "[step: 667] loss: 0.003058401867747307\n",
      "[step: 668] loss: 0.0030562144238501787\n",
      "[step: 669] loss: 0.0030541005544364452\n",
      "[step: 670] loss: 0.0030518495477735996\n",
      "[step: 671] loss: 0.003049776190891862\n",
      "[step: 672] loss: 0.003047527512535453\n",
      "[step: 673] loss: 0.0030454271472990513\n",
      "[step: 674] loss: 0.003043237840756774\n",
      "[step: 675] loss: 0.0030410729814320803\n",
      "[step: 676] loss: 0.003038946073502302\n",
      "[step: 677] loss: 0.0030367420986294746\n",
      "[step: 678] loss: 0.003034629626199603\n",
      "[step: 679] loss: 0.0030324410181492567\n",
      "[step: 680] loss: 0.0030302994418889284\n",
      "[step: 681] loss: 0.0030281515792012215\n",
      "[step: 682] loss: 0.003025974379852414\n",
      "[step: 683] loss: 0.003023847471922636\n",
      "[step: 684] loss: 0.0030216705054044724\n",
      "[step: 685] loss: 0.00301953312009573\n",
      "[step: 686] loss: 0.0030173812992870808\n",
      "[step: 687] loss: 0.003015215275809169\n",
      "[step: 688] loss: 0.0030130825471132994\n",
      "[step: 689] loss: 0.0030109165236353874\n",
      "[step: 690] loss: 0.0030087665654718876\n",
      "[step: 691] loss: 0.0030066226609051228\n",
      "[step: 692] loss: 0.0030044601298868656\n",
      "[step: 693] loss: 0.003002320183441043\n",
      "[step: 694] loss: 0.003000155556946993\n",
      "[step: 695] loss: 0.0029980004765093327\n",
      "[step: 696] loss: 0.0029958540108054876\n",
      "[step: 697] loss: 0.0029936882201582193\n",
      "[step: 698] loss: 0.0029915396589785814\n",
      "[step: 699] loss: 0.002989383414387703\n",
      "[step: 700] loss: 0.002987221349030733\n",
      "[step: 701] loss: 0.0029850637074559927\n",
      "[step: 702] loss: 0.0029828990809619427\n",
      "[step: 703] loss: 0.0029807372484356165\n",
      "[step: 704] loss: 0.002978578442707658\n",
      "[step: 705] loss: 0.0029764100909233093\n",
      "[step: 706] loss: 0.0029742461629211903\n",
      "[step: 707] loss: 0.0029720766469836235\n",
      "[step: 708] loss: 0.0029699034057557583\n",
      "[step: 709] loss: 0.0029677385464310646\n",
      "[step: 710] loss: 0.0029655667021870613\n",
      "[step: 711] loss: 0.0029633869417011738\n",
      "[step: 712] loss: 0.0029612116049975157\n",
      "[step: 713] loss: 0.002959032543003559\n",
      "[step: 714] loss: 0.002956852549687028\n",
      "[step: 715] loss: 0.002954671625047922\n",
      "[step: 716] loss: 0.0029524839483201504\n",
      "[step: 717] loss: 0.002950292080640793\n",
      "[step: 718] loss: 0.00294810114428401\n",
      "[step: 719] loss: 0.00294591230340302\n",
      "[step: 720] loss: 0.0029437155462801456\n",
      "[step: 721] loss: 0.0029415148310363293\n",
      "[step: 722] loss: 0.0029393185395747423\n",
      "[step: 723] loss: 0.002937112469226122\n",
      "[step: 724] loss: 0.0029349091928452253\n",
      "[step: 725] loss: 0.002932698233053088\n",
      "[step: 726] loss: 0.002930485876277089\n",
      "[step: 727] loss: 0.002928265603259206\n",
      "[step: 728] loss: 0.002926047658547759\n",
      "[step: 729] loss: 0.002923827152699232\n",
      "[step: 730] loss: 0.0029216036200523376\n",
      "[step: 731] loss: 0.0029193731024861336\n",
      "[step: 732] loss: 0.002917138859629631\n",
      "[step: 733] loss: 0.0029149025212973356\n",
      "[step: 734] loss: 0.002912666881456971\n",
      "[step: 735] loss: 0.002910421695560217\n",
      "[step: 736] loss: 0.0029081739485263824\n",
      "[step: 737] loss: 0.0029059264343231916\n",
      "[step: 738] loss: 0.002903670771047473\n",
      "[step: 739] loss: 0.002901410451158881\n",
      "[step: 740] loss: 0.0028991487342864275\n",
      "[step: 741] loss: 0.00289688422344625\n",
      "[step: 742] loss: 0.0028946141246706247\n",
      "[step: 743] loss: 0.002892333548516035\n",
      "[step: 744] loss: 0.0028900564648211002\n",
      "[step: 745] loss: 0.002887771697714925\n",
      "[step: 746] loss: 0.0028854890260845423\n",
      "[step: 747] loss: 0.0028831956442445517\n",
      "[step: 748] loss: 0.0028808957431465387\n",
      "[step: 749] loss: 0.0028785946778953075\n",
      "[step: 750] loss: 0.0028762901201844215\n",
      "[step: 751] loss: 0.0028739788103848696\n",
      "[step: 752] loss: 0.002871665870770812\n",
      "[step: 753] loss: 0.0028693389613181353\n",
      "[step: 754] loss: 0.0028670139145106077\n",
      "[step: 755] loss: 0.002864682348445058\n",
      "[step: 756] loss: 0.0028623477555811405\n",
      "[step: 757] loss: 0.0028600094374269247\n",
      "[step: 758] loss: 0.00285766227170825\n",
      "[step: 759] loss: 0.002855310682207346\n",
      "[step: 760] loss: 0.002852953039109707\n",
      "[step: 761] loss: 0.002850588643923402\n",
      "[step: 762] loss: 0.0028482251800596714\n",
      "[step: 763] loss: 0.0028458537999540567\n",
      "[step: 764] loss: 0.002843472408130765\n",
      "[step: 765] loss: 0.0028410854283720255\n",
      "[step: 766] loss: 0.002838704502210021\n",
      "[step: 767] loss: 0.0028363033197820187\n",
      "[step: 768] loss: 0.0028339028358459473\n",
      "[step: 769] loss: 0.002831496065482497\n",
      "[step: 770] loss: 0.0028290890622884035\n",
      "[step: 771] loss: 0.0028266669251024723\n",
      "[step: 772] loss: 0.0028242478147149086\n",
      "[step: 773] loss: 0.002821821952238679\n",
      "[step: 774] loss: 0.0028194065671414137\n",
      "[step: 775] loss: 0.0028170181903988123\n",
      "[step: 776] loss: 0.0028147262055426836\n",
      "[step: 777] loss: 0.002812693128362298\n",
      "[step: 778] loss: 0.0028115068562328815\n",
      "[step: 779] loss: 0.0028129126876592636\n",
      "[step: 780] loss: 0.00282266060821712\n",
      "[step: 781] loss: 0.0028597977943718433\n",
      "[step: 782] loss: 0.002990063512697816\n",
      "[step: 783] loss: 0.003435898572206497\n",
      "[step: 784] loss: 0.004955526441335678\n",
      "[step: 785] loss: 0.009422863833606243\n",
      "[step: 786] loss: 0.018557004630565643\n",
      "[step: 787] loss: 0.018410993739962578\n",
      "[step: 788] loss: 0.004669408313930035\n",
      "[step: 789] loss: 0.0076005426235497\n",
      "[step: 790] loss: 0.00851370207965374\n",
      "[step: 791] loss: 0.0036482259165495634\n",
      "[step: 792] loss: 0.007629137486219406\n",
      "[step: 793] loss: 0.0033859447576105595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 794] loss: 0.006230633240193129\n",
      "[step: 795] loss: 0.0036982540041208267\n",
      "[step: 796] loss: 0.0048334551975131035\n",
      "[step: 797] loss: 0.004373893607407808\n",
      "[step: 798] loss: 0.003587396815419197\n",
      "[step: 799] loss: 0.004801964852958918\n",
      "[step: 800] loss: 0.0031438178848475218\n",
      "[step: 801] loss: 0.004352217074483633\n",
      "[step: 802] loss: 0.0036400111857801676\n",
      "[step: 803] loss: 0.0034301637206226587\n",
      "[step: 804] loss: 0.004079741425812244\n",
      "[step: 805] loss: 0.0031804032623767853\n",
      "[step: 806] loss: 0.003676473395898938\n",
      "[step: 807] loss: 0.00359707442112267\n",
      "[step: 808] loss: 0.0031696646474301815\n",
      "[step: 809] loss: 0.003627506084740162\n",
      "[step: 810] loss: 0.0033032565843313932\n",
      "[step: 811] loss: 0.00321705499663949\n",
      "[step: 812] loss: 0.003488450078293681\n",
      "[step: 813] loss: 0.0031481378246098757\n",
      "[step: 814] loss: 0.0032478433568030596\n",
      "[step: 815] loss: 0.003334619104862213\n",
      "[step: 816] loss: 0.003081402275711298\n",
      "[step: 817] loss: 0.0032477150671184063\n",
      "[step: 818] loss: 0.0031981882639229298\n",
      "[step: 819] loss: 0.0030729181598871946\n",
      "[step: 820] loss: 0.003209546208381653\n",
      "[step: 821] loss: 0.003109843237325549\n",
      "[step: 822] loss: 0.00307735288515687\n",
      "[step: 823] loss: 0.003159674583002925\n",
      "[step: 824] loss: 0.003058905713260174\n",
      "[step: 825] loss: 0.003079548943787813\n",
      "[step: 826] loss: 0.0031117869075387716\n",
      "[step: 827] loss: 0.0030307823326438665\n",
      "[step: 828] loss: 0.003079197835177183\n",
      "[step: 829] loss: 0.0030660617630928755\n",
      "[step: 830] loss: 0.0030225971713662148\n",
      "[step: 831] loss: 0.003068810561671853\n",
      "[step: 832] loss: 0.003030072897672653\n",
      "[step: 833] loss: 0.0030243806540966034\n",
      "[step: 834] loss: 0.003047583159059286\n",
      "[step: 835] loss: 0.003008331637829542\n",
      "[step: 836] loss: 0.003024858422577381\n",
      "[step: 837] loss: 0.0030220593325793743\n",
      "[step: 838] loss: 0.002999036107212305\n",
      "[step: 839] loss: 0.0030177903827279806\n",
      "[step: 840] loss: 0.0029997609090059996\n",
      "[step: 841] loss: 0.0029956214129924774\n",
      "[step: 842] loss: 0.003003458958119154\n",
      "[step: 843] loss: 0.0029848115518689156\n",
      "[step: 844] loss: 0.002991265617311001\n",
      "[step: 845] loss: 0.0029863808304071426\n",
      "[step: 846] loss: 0.0029764026403427124\n",
      "[step: 847] loss: 0.0029825824312865734\n",
      "[step: 848] loss: 0.00297147361561656\n",
      "[step: 849] loss: 0.00297069875523448\n",
      "[step: 850] loss: 0.0029701392631977797\n",
      "[step: 851] loss: 0.002961081685498357\n",
      "[step: 852] loss: 0.0029636214021593332\n",
      "[step: 853] loss: 0.002957260701805353\n",
      "[step: 854] loss: 0.0029539454262703657\n",
      "[step: 855] loss: 0.002953718416392803\n",
      "[step: 856] loss: 0.0029468233697116375\n",
      "[step: 857] loss: 0.002946924651041627\n",
      "[step: 858] loss: 0.0029426098335534334\n",
      "[step: 859] loss: 0.0029390216805040836\n",
      "[step: 860] loss: 0.002938034012913704\n",
      "[step: 861] loss: 0.0029327815864235163\n",
      "[step: 862] loss: 0.0029316635336726904\n",
      "[step: 863] loss: 0.0029280197340995073\n",
      "[step: 864] loss: 0.0029248062055557966\n",
      "[step: 865] loss: 0.002923032268881798\n",
      "[step: 866] loss: 0.002918819896876812\n",
      "[step: 867] loss: 0.0029171216301620007\n",
      "[step: 868] loss: 0.002913645701482892\n",
      "[step: 869] loss: 0.0029108652379363775\n",
      "[step: 870] loss: 0.0029084940906614065\n",
      "[step: 871] loss: 0.0029049627482891083\n",
      "[step: 872] loss: 0.0029029136057943106\n",
      "[step: 873] loss: 0.0028995382599532604\n",
      "[step: 874] loss: 0.002897056518122554\n",
      "[step: 875] loss: 0.0028942846693098545\n",
      "[step: 876] loss: 0.002891264855861664\n",
      "[step: 877] loss: 0.0028889102395623922\n",
      "[step: 878] loss: 0.002885728143155575\n",
      "[step: 879] loss: 0.002883349312469363\n",
      "[step: 880] loss: 0.0028803914319723845\n",
      "[step: 881] loss: 0.002877729944884777\n",
      "[step: 882] loss: 0.0028750936035066843\n",
      "[step: 883] loss: 0.0028721995186060667\n",
      "[step: 884] loss: 0.0028697336092591286\n",
      "[step: 885] loss: 0.002866809954866767\n",
      "[step: 886] loss: 0.0028642991092056036\n",
      "[step: 887] loss: 0.0028615084011107683\n",
      "[step: 888] loss: 0.0028588708955794573\n",
      "[step: 889] loss: 0.0028562291990965605\n",
      "[step: 890] loss: 0.0028534987941384315\n",
      "[step: 891] loss: 0.0028509313706308603\n",
      "[step: 892] loss: 0.0028481849003583193\n",
      "[step: 893] loss: 0.002845625625923276\n",
      "[step: 894] loss: 0.0028429185040295124\n",
      "[step: 895] loss: 0.0028403352480381727\n",
      "[step: 896] loss: 0.0028376681730151176\n",
      "[step: 897] loss: 0.0028350558131933212\n",
      "[step: 898] loss: 0.0028324369341135025\n",
      "[step: 899] loss: 0.0028298008255660534\n",
      "[step: 900] loss: 0.0028272164054214954\n",
      "[step: 901] loss: 0.002824573777616024\n",
      "[step: 902] loss: 0.0028220030944794416\n",
      "[step: 903] loss: 0.0028193644247949123\n",
      "[step: 904] loss: 0.002816799795255065\n",
      "[step: 905] loss: 0.002814178355038166\n",
      "[step: 906] loss: 0.0028116032481193542\n",
      "[step: 907] loss: 0.002808999503031373\n",
      "[step: 908] loss: 0.002806428587064147\n",
      "[step: 909] loss: 0.0028038392774760723\n",
      "[step: 910] loss: 0.00280125648714602\n",
      "[step: 911] loss: 0.0027986878994852304\n",
      "[step: 912] loss: 0.00279610906727612\n",
      "[step: 913] loss: 0.002793538151308894\n",
      "[step: 914] loss: 0.0027909670025110245\n",
      "[step: 915] loss: 0.0027884035371243954\n",
      "[step: 916] loss: 0.0027858358807861805\n",
      "[step: 917] loss: 0.002783281495794654\n",
      "[step: 918] loss: 0.0027807129081338644\n",
      "[step: 919] loss: 0.002778158988803625\n",
      "[step: 920] loss: 0.0027756004128605127\n",
      "[step: 921] loss: 0.002773051615804434\n",
      "[step: 922] loss: 0.0027704930398613214\n",
      "[step: 923] loss: 0.0027679428458213806\n",
      "[step: 924] loss: 0.0027653900906443596\n",
      "[step: 925] loss: 0.0027628461830317974\n",
      "[step: 926] loss: 0.002760295756161213\n",
      "[step: 927] loss: 0.002757749054580927\n",
      "[step: 928] loss: 0.0027552025858312845\n",
      "[step: 929] loss: 0.0027526558842509985\n",
      "[step: 930] loss: 0.002750114072114229\n",
      "[step: 931] loss: 0.002747569466009736\n",
      "[step: 932] loss: 0.002745023462921381\n",
      "[step: 933] loss: 0.002742483513429761\n",
      "[step: 934] loss: 0.0027399384416639805\n",
      "[step: 935] loss: 0.0027374012861400843\n",
      "[step: 936] loss: 0.0027348571456968784\n",
      "[step: 937] loss: 0.0027323164977133274\n",
      "[step: 938] loss: 0.00272977352142334\n",
      "[step: 939] loss: 0.002727231942117214\n",
      "[step: 940] loss: 0.0027246882673352957\n",
      "[step: 941] loss: 0.002722145291045308\n",
      "[step: 942] loss: 0.0027196018490940332\n",
      "[step: 943] loss: 0.0027170563116669655\n",
      "[step: 944] loss: 0.002714514033868909\n",
      "[step: 945] loss: 0.0027119710575789213\n",
      "[step: 946] loss: 0.002709425985813141\n",
      "[step: 947] loss: 0.0027068748604506254\n",
      "[step: 948] loss: 0.0027043260633945465\n",
      "[step: 949] loss: 0.002701779128983617\n",
      "[step: 950] loss: 0.0026992210187017918\n",
      "[step: 951] loss: 0.002696674084290862\n",
      "[step: 952] loss: 0.002694114111363888\n",
      "[step: 953] loss: 0.0026915608905255795\n",
      "[step: 954] loss: 0.0026890086010098457\n",
      "[step: 955] loss: 0.002686447463929653\n",
      "[step: 956] loss: 0.0026838884223252535\n",
      "[step: 957] loss: 0.002681322395801544\n",
      "[step: 958] loss: 0.0026787591632455587\n",
      "[step: 959] loss: 0.0026761889457702637\n",
      "[step: 960] loss: 0.0026736219879239798\n",
      "[step: 961] loss: 0.0026710540987551212\n",
      "[step: 962] loss: 0.0026684757322072983\n",
      "[step: 963] loss: 0.002665902953594923\n",
      "[step: 964] loss: 0.0026633245870471\n",
      "[step: 965] loss: 0.0026607385370880365\n",
      "[step: 966] loss: 0.0026581604033708572\n",
      "[step: 967] loss: 0.0026555703952908516\n",
      "[step: 968] loss: 0.0026529855094850063\n",
      "[step: 969] loss: 0.0026503896806389093\n",
      "[step: 970] loss: 0.0026477922219783068\n",
      "[step: 971] loss: 0.002645194996148348\n",
      "[step: 972] loss: 0.002642593113705516\n",
      "[step: 973] loss: 0.0026399886701256037\n",
      "[step: 974] loss: 0.0026373835280537605\n",
      "[step: 975] loss: 0.0026347620878368616\n",
      "[step: 976] loss: 0.0026321494951844215\n",
      "[step: 977] loss: 0.00262952852062881\n",
      "[step: 978] loss: 0.0026269019581377506\n",
      "[step: 979] loss: 0.00262428168207407\n",
      "[step: 980] loss: 0.002621652092784643\n",
      "[step: 981] loss: 0.0026190089993178844\n",
      "[step: 982] loss: 0.0026163689326494932\n",
      "[step: 983] loss: 0.002613731427118182\n",
      "[step: 984] loss: 0.002611083211377263\n",
      "[step: 985] loss: 0.002608430339023471\n",
      "[step: 986] loss: 0.0026057735085487366\n",
      "[step: 987] loss: 0.002603112021461129\n",
      "[step: 988] loss: 0.0026004447136074305\n",
      "[step: 989] loss: 0.002597776474431157\n",
      "[step: 990] loss: 0.002595102647319436\n",
      "[step: 991] loss: 0.0025924211367964745\n",
      "[step: 992] loss: 0.0025897400919348\n",
      "[step: 993] loss: 0.002587044145911932\n",
      "[step: 994] loss: 0.0025843523908406496\n",
      "[step: 995] loss: 0.0025816536508500576\n",
      "[step: 996] loss: 0.002578947227448225\n",
      "[step: 997] loss: 0.0025762361474335194\n",
      "[step: 998] loss: 0.002573520177975297\n",
      "[step: 999] loss: 0.0025707983877509832\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testY is:  [4122.7096770000007, 7644.7419349999991, 4147.0740740000001, 1799.7096770000001, 1405.5, 1648.193548, 1090.5666670000001]\n",
      "\n",
      "\n",
      "LSTM testforecast : [11501.126953125, 35231.7890625, 23929.568359375, 3238.9677734375, 3742.501953125, 2987.68798828125, 256.23822021484375] \n",
      "@@@@@LSTM rmse:  13184.7134547\n",
      "Bayseian testforecast : [734.03919080076446, 1894.5132882945629, 13344240.389051739, 65.965064237998888, 156.69537789750424, 2934.0792310475513, 467.71113202047519] \n",
      "@@@@@Bayseian rmse:  5042082.06438\n",
      "\n",
      "\n",
      "LSTM WON!!!!!!\n",
      "[step: 0] loss: 0.4193812906742096\n",
      "[step: 1] loss: 0.12354608625173569\n",
      "[step: 2] loss: 0.026315681636333466\n",
      "[step: 3] loss: 0.05264170840382576\n",
      "[step: 4] loss: 0.11139579117298126\n",
      "[step: 5] loss: 0.130569189786911\n",
      "[step: 6] loss: 0.10772176831960678\n",
      "[step: 7] loss: 0.06949283927679062\n",
      "[step: 8] loss: 0.037501294165849686\n",
      "[step: 9] loss: 0.021277157589793205\n",
      "[step: 10] loss: 0.02042325586080551\n",
      "[step: 11] loss: 0.02906809188425541\n",
      "[step: 12] loss: 0.040254343301057816\n",
      "[step: 13] loss: 0.04847980663180351\n",
      "[step: 14] loss: 0.050818704068660736\n",
      "[step: 15] loss: 0.047123998403549194\n",
      "[step: 16] loss: 0.039393167942762375\n",
      "[step: 17] loss: 0.03059941902756691\n",
      "[step: 18] loss: 0.02343164198100567\n",
      "[step: 19] loss: 0.019434059038758278\n",
      "[step: 20] loss: 0.018812470138072968\n",
      "[step: 21] loss: 0.020752068608999252\n",
      "[step: 22] loss: 0.023902803659439087\n",
      "[step: 23] loss: 0.02684088610112667\n",
      "[step: 24] loss: 0.028456231579184532\n",
      "[step: 25] loss: 0.028246944770216942\n",
      "[step: 26] loss: 0.026431292295455933\n",
      "[step: 27] loss: 0.023761386051774025\n",
      "[step: 28] loss: 0.021135414019227028\n",
      "[step: 29] loss: 0.019244903698563576\n",
      "[step: 30] loss: 0.018406366929411888\n",
      "[step: 31] loss: 0.01857202686369419\n",
      "[step: 32] loss: 0.019425248727202415\n",
      "[step: 33] loss: 0.02049875631928444\n",
      "[step: 34] loss: 0.02132641337811947\n",
      "[step: 35] loss: 0.021605588495731354\n",
      "[step: 36] loss: 0.021284880116581917\n",
      "[step: 37] loss: 0.02053036540746689\n",
      "[step: 38] loss: 0.019611181691288948\n",
      "[step: 39] loss: 0.01878945529460907\n",
      "[step: 40] loss: 0.01825900375843048\n",
      "[step: 41] loss: 0.018108241260051727\n",
      "[step: 42] loss: 0.018293779343366623\n",
      "[step: 43] loss: 0.01865679956972599\n",
      "[step: 44] loss: 0.018997667357325554\n",
      "[step: 45] loss: 0.01916741020977497\n",
      "[step: 46] loss: 0.019114159047603607\n",
      "[step: 47] loss: 0.01887103170156479\n",
      "[step: 48] loss: 0.018524430692195892\n",
      "[step: 49] loss: 0.01818489283323288\n",
      "[step: 50] loss: 0.017951183021068573\n",
      "[step: 51] loss: 0.017870834097266197\n",
      "[step: 52] loss: 0.017923105508089066\n",
      "[step: 53] loss: 0.018040616065263748\n",
      "[step: 54] loss: 0.01814926043152809\n",
      "[step: 55] loss: 0.01819543167948723\n",
      "[step: 56] loss: 0.018156306818127632\n",
      "[step: 57] loss: 0.018042074516415596\n",
      "[step: 58] loss: 0.017890626564621925\n",
      "[step: 59] loss: 0.017750581726431847\n",
      "[step: 60] loss: 0.017658347263932228\n",
      "[step: 61] loss: 0.017624959349632263\n",
      "[step: 62] loss: 0.01763758808374405\n",
      "[step: 63] loss: 0.01766904443502426\n",
      "[step: 64] loss: 0.01768934167921543\n",
      "[step: 65] loss: 0.01767730712890625\n",
      "[step: 66] loss: 0.01762881502509117\n",
      "[step: 67] loss: 0.017556337639689445\n",
      "[step: 68] loss: 0.017479930073022842\n",
      "[step: 69] loss: 0.017417140305042267\n",
      "[step: 70] loss: 0.017377091571688652\n",
      "[step: 71] loss: 0.0173589326441288\n",
      "[step: 72] loss: 0.017353016883134842\n",
      "[step: 73] loss: 0.017345555126667023\n",
      "[step: 74] loss: 0.017325764521956444\n",
      "[step: 75] loss: 0.017290662974119186\n",
      "[step: 76] loss: 0.017244501039385796\n",
      "[step: 77] loss: 0.017194803804159164\n",
      "[step: 78] loss: 0.01714915968477726\n",
      "[step: 79] loss: 0.01711280643939972\n",
      "[step: 80] loss: 0.01708616502583027\n",
      "[step: 81] loss: 0.017064642161130905\n",
      "[step: 82] loss: 0.017041876912117004\n",
      "[step: 83] loss: 0.017013732343912125\n",
      "[step: 84] loss: 0.016979198902845383\n",
      "[step: 85] loss: 0.016939805820584297\n",
      "[step: 86] loss: 0.016898874193429947\n",
      "[step: 87] loss: 0.0168601181358099\n",
      "[step: 88] loss: 0.016825463622808456\n",
      "[step: 89] loss: 0.016794243827462196\n",
      "[step: 90] loss: 0.016764240339398384\n",
      "[step: 91] loss: 0.016733091324567795\n",
      "[step: 92] loss: 0.016699161380529404\n",
      "[step: 93] loss: 0.016662130132317543\n",
      "[step: 94] loss: 0.01662316545844078\n",
      "[step: 95] loss: 0.016584021970629692\n",
      "[step: 96] loss: 0.016545984894037247\n",
      "[step: 97] loss: 0.016509387642145157\n",
      "[step: 98] loss: 0.016473665833473206\n",
      "[step: 99] loss: 0.016437722370028496\n",
      "[step: 100] loss: 0.016400493681430817\n",
      "[step: 101] loss: 0.01636153645813465\n",
      "[step: 102] loss: 0.016321197152137756\n",
      "[step: 103] loss: 0.016280129551887512\n",
      "[step: 104] loss: 0.016238983720541\n",
      "[step: 105] loss: 0.016198135912418365\n",
      "[step: 106] loss: 0.016157500445842743\n",
      "[step: 107] loss: 0.016116542741656303\n",
      "[step: 108] loss: 0.016074730083346367\n",
      "[step: 109] loss: 0.01603180356323719\n",
      "[step: 110] loss: 0.015987815335392952\n",
      "[step: 111] loss: 0.015942992642521858\n",
      "[step: 112] loss: 0.015897655859589577\n",
      "[step: 113] loss: 0.015852034091949463\n",
      "[step: 114] loss: 0.015806062147021294\n",
      "[step: 115] loss: 0.015759507194161415\n",
      "[step: 116] loss: 0.015712138265371323\n",
      "[step: 117] loss: 0.015663783997297287\n",
      "[step: 118] loss: 0.015614429488778114\n",
      "[step: 119] loss: 0.01556415855884552\n",
      "[step: 120] loss: 0.015513142570853233\n",
      "[step: 121] loss: 0.01546142902225256\n",
      "[step: 122] loss: 0.015408981591463089\n",
      "[step: 123] loss: 0.015355696901679039\n",
      "[step: 124] loss: 0.015301425009965897\n",
      "[step: 125] loss: 0.015246073715388775\n",
      "[step: 126] loss: 0.01518962811678648\n",
      "[step: 127] loss: 0.015132117085158825\n",
      "[step: 128] loss: 0.015073593705892563\n",
      "[step: 129] loss: 0.015014049597084522\n",
      "[step: 130] loss: 0.01495345588773489\n",
      "[step: 131] loss: 0.01489173248410225\n",
      "[step: 132] loss: 0.01482879277318716\n",
      "[step: 133] loss: 0.014764580875635147\n",
      "[step: 134] loss: 0.014699078164994717\n",
      "[step: 135] loss: 0.014632291160523891\n",
      "[step: 136] loss: 0.014564219862222672\n",
      "[step: 137] loss: 0.014494843780994415\n",
      "[step: 138] loss: 0.014424126595258713\n",
      "[step: 139] loss: 0.014352013356983662\n",
      "[step: 140] loss: 0.014278465881943703\n",
      "[step: 141] loss: 0.014203441329300404\n",
      "[step: 142] loss: 0.014126951806247234\n",
      "[step: 143] loss: 0.014048991724848747\n",
      "[step: 144] loss: 0.013969573192298412\n",
      "[step: 145] loss: 0.01388868410140276\n",
      "[step: 146] loss: 0.013806314207613468\n",
      "[step: 147] loss: 0.013722480274736881\n",
      "[step: 148] loss: 0.013637174852192402\n",
      "[step: 149] loss: 0.013550452888011932\n",
      "[step: 150] loss: 0.01346233393996954\n",
      "[step: 151] loss: 0.013372914865612984\n",
      "[step: 152] loss: 0.013282245956361294\n",
      "[step: 153] loss: 0.013190418481826782\n",
      "[step: 154] loss: 0.013097528368234634\n",
      "[step: 155] loss: 0.013003700412809849\n",
      "[step: 156] loss: 0.012909077107906342\n",
      "[step: 157] loss: 0.012813824228942394\n",
      "[step: 158] loss: 0.012718172743916512\n",
      "[step: 159] loss: 0.012622573412954807\n",
      "[step: 160] loss: 0.01252962090075016\n",
      "[step: 161] loss: 0.012466303072869778\n",
      "[step: 162] loss: 0.01274734828621149\n",
      "[step: 163] loss: 0.014998179860413074\n",
      "[step: 164] loss: 0.015641117468476295\n",
      "[step: 165] loss: 0.012132233940064907\n",
      "[step: 166] loss: 0.014983133412897587\n",
      "[step: 167] loss: 0.012493309564888477\n",
      "[step: 168] loss: 0.013709808699786663\n",
      "[step: 169] loss: 0.012763302773237228\n",
      "[step: 170] loss: 0.012855816632509232\n",
      "[step: 171] loss: 0.01279375795274973\n",
      "[step: 172] loss: 0.012345119379460812\n",
      "[step: 173] loss: 0.012677416205406189\n",
      "[step: 174] loss: 0.012015638872981071\n",
      "[step: 175] loss: 0.012507298029959202\n",
      "[step: 176] loss: 0.011794166639447212\n",
      "[step: 177] loss: 0.012324249371886253\n",
      "[step: 178] loss: 0.011619909666478634\n",
      "[step: 179] loss: 0.012142237275838852\n",
      "[step: 180] loss: 0.011480817571282387\n",
      "[step: 181] loss: 0.011968673206865788\n",
      "[step: 182] loss: 0.011363023892045021\n",
      "[step: 183] loss: 0.011797468177974224\n",
      "[step: 184] loss: 0.011265748180449009\n",
      "[step: 185] loss: 0.01162993535399437\n",
      "[step: 186] loss: 0.011187820695340633\n",
      "[step: 187] loss: 0.011458327062427998\n",
      "[step: 188] loss: 0.011129549704492092\n",
      "[step: 189] loss: 0.011284158565104008\n",
      "[step: 190] loss: 0.011088473722338676\n",
      "[step: 191] loss: 0.011111495085060596\n",
      "[step: 192] loss: 0.011053020134568214\n",
      "[step: 193] loss: 0.010953433811664581\n",
      "[step: 194] loss: 0.011007141321897507\n",
      "[step: 195] loss: 0.010829229839146137\n",
      "[step: 196] loss: 0.010931391268968582\n",
      "[step: 197] loss: 0.010752836242318153\n",
      "[step: 198] loss: 0.01081815268844366\n",
      "[step: 199] loss: 0.010718238539993763\n",
      "[step: 200] loss: 0.010687953792512417\n",
      "[step: 201] loss: 0.010690804570913315\n",
      "[step: 202] loss: 0.01058405451476574\n",
      "[step: 203] loss: 0.010627900250256062\n",
      "[step: 204] loss: 0.010534173808991909\n",
      "[step: 205] loss: 0.010526272468268871\n",
      "[step: 206] loss: 0.010510589927434921\n",
      "[step: 207] loss: 0.010436794720590115\n",
      "[step: 208] loss: 0.01045618299394846\n",
      "[step: 209] loss: 0.010397698730230331\n",
      "[step: 210] loss: 0.010369611904025078\n",
      "[step: 211] loss: 0.01036950945854187\n",
      "[step: 212] loss: 0.010310531593859196\n",
      "[step: 213] loss: 0.010304244235157967\n",
      "[step: 214] loss: 0.010285408236086369\n",
      "[step: 215] loss: 0.010239623486995697\n",
      "[step: 216] loss: 0.010236969217658043\n",
      "[step: 217] loss: 0.010210924781858921\n",
      "[step: 218] loss: 0.010175898671150208\n",
      "[step: 219] loss: 0.0101710744202137\n",
      "[step: 220] loss: 0.010145055130124092\n",
      "[step: 221] loss: 0.010116016492247581\n",
      "[step: 222] loss: 0.010108521208167076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 223] loss: 0.010085422545671463\n",
      "[step: 224] loss: 0.010059383697807789\n",
      "[step: 225] loss: 0.01004967000335455\n",
      "[step: 226] loss: 0.010030255652964115\n",
      "[step: 227] loss: 0.010006063617765903\n",
      "[step: 228] loss: 0.00999431498348713\n",
      "[step: 229] loss: 0.009978371672332287\n",
      "[step: 230] loss: 0.009956086054444313\n",
      "[step: 231] loss: 0.009942302480340004\n",
      "[step: 232] loss: 0.009928961284458637\n",
      "[step: 233] loss: 0.00990918930619955\n",
      "[step: 234] loss: 0.009893665090203285\n",
      "[step: 235] loss: 0.009881586767733097\n",
      "[step: 236] loss: 0.009864822961390018\n",
      "[step: 237] loss: 0.009848405607044697\n",
      "[step: 238] loss: 0.009836221113801003\n",
      "[step: 239] loss: 0.009822247549891472\n",
      "[step: 240] loss: 0.009806251153349876\n",
      "[step: 241] loss: 0.00979317631572485\n",
      "[step: 242] loss: 0.009780971333384514\n",
      "[step: 243] loss: 0.009766547940671444\n",
      "[step: 244] loss: 0.009752750396728516\n",
      "[step: 245] loss: 0.009740972891449928\n",
      "[step: 246] loss: 0.009728393517434597\n",
      "[step: 247] loss: 0.009714864194393158\n",
      "[step: 248] loss: 0.009702671319246292\n",
      "[step: 249] loss: 0.009691237471997738\n",
      "[step: 250] loss: 0.009678872302174568\n",
      "[step: 251] loss: 0.009666485711932182\n",
      "[step: 252] loss: 0.009655175730586052\n",
      "[step: 253] loss: 0.009643985889852047\n",
      "[step: 254] loss: 0.009632205590605736\n",
      "[step: 255] loss: 0.009620724245905876\n",
      "[step: 256] loss: 0.009609952569007874\n",
      "[step: 257] loss: 0.00959913618862629\n",
      "[step: 258] loss: 0.00958801619708538\n",
      "[step: 259] loss: 0.009577195160090923\n",
      "[step: 260] loss: 0.009566869586706161\n",
      "[step: 261] loss: 0.009556509554386139\n",
      "[step: 262] loss: 0.009545989334583282\n",
      "[step: 263] loss: 0.009535707533359528\n",
      "[step: 264] loss: 0.009525779634714127\n",
      "[step: 265] loss: 0.009515897370874882\n",
      "[step: 266] loss: 0.009505928494036198\n",
      "[step: 267] loss: 0.009496106766164303\n",
      "[step: 268] loss: 0.009486551396548748\n",
      "[step: 269] loss: 0.009477104991674423\n",
      "[step: 270] loss: 0.009467643685638905\n",
      "[step: 271] loss: 0.00945824570953846\n",
      "[step: 272] loss: 0.009449046105146408\n",
      "[step: 273] loss: 0.009439990855753422\n",
      "[step: 274] loss: 0.009430979378521442\n",
      "[step: 275] loss: 0.009422003291547298\n",
      "[step: 276] loss: 0.009413131512701511\n",
      "[step: 277] loss: 0.009404413402080536\n",
      "[step: 278] loss: 0.009395789355039597\n",
      "[step: 279] loss: 0.00938720628619194\n",
      "[step: 280] loss: 0.009378685615956783\n",
      "[step: 281] loss: 0.009370259009301662\n",
      "[step: 282] loss: 0.009361942298710346\n",
      "[step: 283] loss: 0.009353707544505596\n",
      "[step: 284] loss: 0.009345526807010174\n",
      "[step: 285] loss: 0.00933739822357893\n",
      "[step: 286] loss: 0.009329354390501976\n",
      "[step: 287] loss: 0.009321398101747036\n",
      "[step: 288] loss: 0.00931351538747549\n",
      "[step: 289] loss: 0.009305691346526146\n",
      "[step: 290] loss: 0.009297921322286129\n",
      "[step: 291] loss: 0.009290209040045738\n",
      "[step: 292] loss: 0.00928256195038557\n",
      "[step: 293] loss: 0.009274986572563648\n",
      "[step: 294] loss: 0.009267476387321949\n",
      "[step: 295] loss: 0.009260013699531555\n",
      "[step: 296] loss: 0.00925260130316019\n",
      "[step: 297] loss: 0.009245235472917557\n",
      "[step: 298] loss: 0.009237928315997124\n",
      "[step: 299] loss: 0.009230677969753742\n",
      "[step: 300] loss: 0.009223478846251965\n",
      "[step: 301] loss: 0.009216323494911194\n",
      "[step: 302] loss: 0.009209224954247475\n",
      "[step: 303] loss: 0.009202159941196442\n",
      "[step: 304] loss: 0.009195146150887012\n",
      "[step: 305] loss: 0.00918816588819027\n",
      "[step: 306] loss: 0.009181232191622257\n",
      "[step: 307] loss: 0.009174336679279804\n",
      "[step: 308] loss: 0.00916749145835638\n",
      "[step: 309] loss: 0.009160680696368217\n",
      "[step: 310] loss: 0.009153910912573338\n",
      "[step: 311] loss: 0.00914717372506857\n",
      "[step: 312] loss: 0.009140469133853912\n",
      "[step: 313] loss: 0.009133799932897091\n",
      "[step: 314] loss: 0.009127163328230381\n",
      "[step: 315] loss: 0.009120573289692402\n",
      "[step: 316] loss: 0.00911400280892849\n",
      "[step: 317] loss: 0.00910746119916439\n",
      "[step: 318] loss: 0.009100954048335552\n",
      "[step: 319] loss: 0.009094473905861378\n",
      "[step: 320] loss: 0.00908802729099989\n",
      "[step: 321] loss: 0.009081599302589893\n",
      "[step: 322] loss: 0.009075210429728031\n",
      "[step: 323] loss: 0.009068834595382214\n",
      "[step: 324] loss: 0.009062492288649082\n",
      "[step: 325] loss: 0.009056167677044868\n",
      "[step: 326] loss: 0.009049871936440468\n",
      "[step: 327] loss: 0.009043598547577858\n",
      "[step: 328] loss: 0.009037352167069912\n",
      "[step: 329] loss: 0.009031123481690884\n",
      "[step: 330] loss: 0.009024914354085922\n",
      "[step: 331] loss: 0.0090187294408679\n",
      "[step: 332] loss: 0.009012588299810886\n",
      "[step: 333] loss: 0.009006481617689133\n",
      "[step: 334] loss: 0.009000446647405624\n",
      "[step: 335] loss: 0.008994538336992264\n",
      "[step: 336] loss: 0.008988944813609123\n",
      "[step: 337] loss: 0.00898406095802784\n",
      "[step: 338] loss: 0.008980931714177132\n",
      "[step: 339] loss: 0.008982295170426369\n",
      "[step: 340] loss: 0.008995306678116322\n",
      "[step: 341] loss: 0.009039240889251232\n",
      "[step: 342] loss: 0.009164511226117611\n",
      "[step: 343] loss: 0.009497429244220257\n",
      "[step: 344] loss: 0.010279580950737\n",
      "[step: 345] loss: 0.011675986461341381\n",
      "[step: 346] loss: 0.012632432393729687\n",
      "[step: 347] loss: 0.011237655766308308\n",
      "[step: 348] loss: 0.009045079350471497\n",
      "[step: 349] loss: 0.009778864681720734\n",
      "[step: 350] loss: 0.010880209505558014\n",
      "[step: 351] loss: 0.00943197961896658\n",
      "[step: 352] loss: 0.009187440387904644\n",
      "[step: 353] loss: 0.01021304726600647\n",
      "[step: 354] loss: 0.009290015324950218\n",
      "[step: 355] loss: 0.009155714884400368\n",
      "[step: 356] loss: 0.0098232701420784\n",
      "[step: 357] loss: 0.009058550000190735\n",
      "[step: 358] loss: 0.009237110614776611\n",
      "[step: 359] loss: 0.00950741395354271\n",
      "[step: 360] loss: 0.008930640295147896\n",
      "[step: 361] loss: 0.00929823238402605\n",
      "[step: 362] loss: 0.009217490442097187\n",
      "[step: 363] loss: 0.00892737414687872\n",
      "[step: 364] loss: 0.00926557369530201\n",
      "[step: 365] loss: 0.009003877639770508\n",
      "[step: 366] loss: 0.008975083939731121\n",
      "[step: 367] loss: 0.00915384478867054\n",
      "[step: 368] loss: 0.008892304264008999\n",
      "[step: 369] loss: 0.00899885967373848\n",
      "[step: 370] loss: 0.009028084576129913\n",
      "[step: 371] loss: 0.008850214071571827\n",
      "[step: 372] loss: 0.008981126360595226\n",
      "[step: 373] loss: 0.00893094576895237\n",
      "[step: 374] loss: 0.008833548985421658\n",
      "[step: 375] loss: 0.008939793333411217\n",
      "[step: 376] loss: 0.008868816308677197\n",
      "[step: 377] loss: 0.008818045258522034\n",
      "[step: 378] loss: 0.008894658647477627\n",
      "[step: 379] loss: 0.008831360377371311\n",
      "[step: 380] loss: 0.008798583410680294\n",
      "[step: 381] loss: 0.008854208514094353\n",
      "[step: 382] loss: 0.008808120153844357\n",
      "[step: 383] loss: 0.00877762958407402\n",
      "[step: 384] loss: 0.00881870649755001\n",
      "[step: 385] loss: 0.008792182430624962\n",
      "[step: 386] loss: 0.008758761920034885\n",
      "[step: 387] loss: 0.00878614280372858\n",
      "[step: 388] loss: 0.008778363466262817\n",
      "[step: 389] loss: 0.008744600228965282\n",
      "[step: 390] loss: 0.008755844086408615\n",
      "[step: 391] loss: 0.008762230165302753\n",
      "[step: 392] loss: 0.00873516220599413\n",
      "[step: 393] loss: 0.008729907684028149\n",
      "[step: 394] loss: 0.00874115526676178\n",
      "[step: 395] loss: 0.008727089501917362\n",
      "[step: 396] loss: 0.008711143396794796\n",
      "[step: 397] loss: 0.008716629818081856\n",
      "[step: 398] loss: 0.008715229108929634\n",
      "[step: 399] loss: 0.008699147030711174\n",
      "[step: 400] loss: 0.008693952113389969\n",
      "[step: 401] loss: 0.008697093464434147\n",
      "[step: 402] loss: 0.008688938803970814\n",
      "[step: 403] loss: 0.00867742020636797\n",
      "[step: 404] loss: 0.008676006458699703\n",
      "[step: 405] loss: 0.008674896322190762\n",
      "[step: 406] loss: 0.008665632456541061\n",
      "[step: 407] loss: 0.008657894097268581\n",
      "[step: 408] loss: 0.008656522259116173\n",
      "[step: 409] loss: 0.00865292176604271\n",
      "[step: 410] loss: 0.00864457804709673\n",
      "[step: 411] loss: 0.008638684637844563\n",
      "[step: 412] loss: 0.008636409416794777\n",
      "[step: 413] loss: 0.008631903678178787\n",
      "[step: 414] loss: 0.008624667301774025\n",
      "[step: 415] loss: 0.008619364351034164\n",
      "[step: 416] loss: 0.008616259321570396\n",
      "[step: 417] loss: 0.008611655794084072\n",
      "[step: 418] loss: 0.008605255745351315\n",
      "[step: 419] loss: 0.008599952794611454\n",
      "[step: 420] loss: 0.008596207946538925\n",
      "[step: 421] loss: 0.008591773919761181\n",
      "[step: 422] loss: 0.008586007170379162\n",
      "[step: 423] loss: 0.008580554276704788\n",
      "[step: 424] loss: 0.00857624877244234\n",
      "[step: 425] loss: 0.008571954444050789\n",
      "[step: 426] loss: 0.008566761389374733\n",
      "[step: 427] loss: 0.008561283349990845\n",
      "[step: 428] loss: 0.00855642557144165\n",
      "[step: 429] loss: 0.008552026934921741\n",
      "[step: 430] loss: 0.008547318167984486\n",
      "[step: 431] loss: 0.008542110212147236\n",
      "[step: 432] loss: 0.008536910638213158\n",
      "[step: 433] loss: 0.008532097563147545\n",
      "[step: 434] loss: 0.008527502417564392\n",
      "[step: 435] loss: 0.00852272193878889\n",
      "[step: 436] loss: 0.008517660200595856\n",
      "[step: 437] loss: 0.008512548170983791\n",
      "[step: 438] loss: 0.008507615886628628\n",
      "[step: 439] loss: 0.008502866141498089\n",
      "[step: 440] loss: 0.008498105220496655\n",
      "[step: 441] loss: 0.00849319901317358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 442] loss: 0.008488179184496403\n",
      "[step: 443] loss: 0.008483151905238628\n",
      "[step: 444] loss: 0.008478214032948017\n",
      "[step: 445] loss: 0.008473352529108524\n",
      "[step: 446] loss: 0.008468512445688248\n",
      "[step: 447] loss: 0.00846363790333271\n",
      "[step: 448] loss: 0.0084586963057518\n",
      "[step: 449] loss: 0.008453711867332458\n",
      "[step: 450] loss: 0.008448710665106773\n",
      "[step: 451] loss: 0.008443720638751984\n",
      "[step: 452] loss: 0.008438758552074432\n",
      "[step: 453] loss: 0.008433817885816097\n",
      "[step: 454] loss: 0.00842889305204153\n",
      "[step: 455] loss: 0.008423960767686367\n",
      "[step: 456] loss: 0.008419019170105457\n",
      "[step: 457] loss: 0.00841408222913742\n",
      "[step: 458] loss: 0.008409127593040466\n",
      "[step: 459] loss: 0.008404169231653214\n",
      "[step: 460] loss: 0.00839921459555626\n",
      "[step: 461] loss: 0.00839428324252367\n",
      "[step: 462] loss: 0.008389382623136044\n",
      "[step: 463] loss: 0.008384553715586662\n",
      "[step: 464] loss: 0.008379857055842876\n",
      "[step: 465] loss: 0.008375419303774834\n",
      "[step: 466] loss: 0.008371519856154919\n",
      "[step: 467] loss: 0.008368720300495625\n",
      "[step: 468] loss: 0.008368286304175854\n",
      "[step: 469] loss: 0.008373061195015907\n",
      "[step: 470] loss: 0.008389773778617382\n",
      "[step: 471] loss: 0.008433920331299305\n",
      "[step: 472] loss: 0.008543147705495358\n",
      "[step: 473] loss: 0.008797460235655308\n",
      "[step: 474] loss: 0.009364532306790352\n",
      "[step: 475] loss: 0.010415244847536087\n",
      "[step: 476] loss: 0.011914223432540894\n",
      "[step: 477] loss: 0.012512234039604664\n",
      "[step: 478] loss: 0.010948682203888893\n",
      "[step: 479] loss: 0.008586471900343895\n",
      "[step: 480] loss: 0.008874931372702122\n",
      "[step: 481] loss: 0.010336138308048248\n",
      "[step: 482] loss: 0.009458585642278194\n",
      "[step: 483] loss: 0.008337840437889099\n",
      "[step: 484] loss: 0.009281833656132221\n",
      "[step: 485] loss: 0.009350068867206573\n",
      "[step: 486] loss: 0.008375676348805428\n",
      "[step: 487] loss: 0.008880075067281723\n",
      "[step: 488] loss: 0.009071661159396172\n",
      "[step: 489] loss: 0.008373884484171867\n",
      "[step: 490] loss: 0.008730703964829445\n",
      "[step: 491] loss: 0.008837518282234669\n",
      "[step: 492] loss: 0.008348657749593258\n",
      "[step: 493] loss: 0.008647071197628975\n",
      "[step: 494] loss: 0.008659509010612965\n",
      "[step: 495] loss: 0.008325645700097084\n",
      "[step: 496] loss: 0.008570990525186062\n",
      "[step: 497] loss: 0.008532242849469185\n",
      "[step: 498] loss: 0.008304357528686523\n",
      "[step: 499] loss: 0.00849277712404728\n",
      "[step: 500] loss: 0.008447508327662945\n",
      "[step: 501] loss: 0.008280858397483826\n",
      "[step: 502] loss: 0.008416260592639446\n",
      "[step: 503] loss: 0.00839337520301342\n",
      "[step: 504] loss: 0.008258496411144733\n",
      "[step: 505] loss: 0.008344021625816822\n",
      "[step: 506] loss: 0.008356931619346142\n",
      "[step: 507] loss: 0.008244927041232586\n",
      "[step: 508] loss: 0.008278146386146545\n",
      "[step: 509] loss: 0.008323516696691513\n",
      "[step: 510] loss: 0.008245638571679592\n",
      "[step: 511] loss: 0.00822630524635315\n",
      "[step: 512] loss: 0.008279357105493546\n",
      "[step: 513] loss: 0.008253505453467369\n",
      "[step: 514] loss: 0.008202130906283855\n",
      "[step: 515] loss: 0.008224993012845516\n",
      "[step: 516] loss: 0.008245343342423439\n",
      "[step: 517] loss: 0.008206875063478947\n",
      "[step: 518] loss: 0.0081851901486516\n",
      "[step: 519] loss: 0.008207723498344421\n",
      "[step: 520] loss: 0.008210165426135063\n",
      "[step: 521] loss: 0.00817989930510521\n",
      "[step: 522] loss: 0.008168543688952923\n",
      "[step: 523] loss: 0.008182812482118607\n",
      "[step: 524] loss: 0.008181712590157986\n",
      "[step: 525] loss: 0.008159850724041462\n",
      "[step: 526] loss: 0.008149427361786366\n",
      "[step: 527] loss: 0.008156605996191502\n",
      "[step: 528] loss: 0.00815661158412695\n",
      "[step: 529] loss: 0.008141661062836647\n",
      "[step: 530] loss: 0.008130056783556938\n",
      "[step: 531] loss: 0.008131172508001328\n",
      "[step: 532] loss: 0.008132416754961014\n",
      "[step: 533] loss: 0.008123566396534443\n",
      "[step: 534] loss: 0.008112030103802681\n",
      "[step: 535] loss: 0.00810782890766859\n",
      "[step: 536] loss: 0.008108148351311684\n",
      "[step: 537] loss: 0.008104246109724045\n",
      "[step: 538] loss: 0.008095217868685722\n",
      "[step: 539] loss: 0.008087598718702793\n",
      "[step: 540] loss: 0.008084689266979694\n",
      "[step: 541] loss: 0.008082796819508076\n",
      "[step: 542] loss: 0.008077690377831459\n",
      "[step: 543] loss: 0.00807018019258976\n",
      "[step: 544] loss: 0.008063942193984985\n",
      "[step: 545] loss: 0.008060336112976074\n",
      "[step: 546] loss: 0.008057307451963425\n",
      "[step: 547] loss: 0.00805259682238102\n",
      "[step: 548] loss: 0.008046300150454044\n",
      "[step: 549] loss: 0.0080402297899127\n",
      "[step: 550] loss: 0.008035548962652683\n",
      "[step: 551] loss: 0.008031760342419147\n",
      "[step: 552] loss: 0.008027644827961922\n",
      "[step: 553] loss: 0.00802252534776926\n",
      "[step: 554] loss: 0.0080167967826128\n",
      "[step: 555] loss: 0.008011248894035816\n",
      "[step: 556] loss: 0.008006333373486996\n",
      "[step: 557] loss: 0.008001917973160744\n",
      "[step: 558] loss: 0.007997552864253521\n",
      "[step: 559] loss: 0.007992873899638653\n",
      "[step: 560] loss: 0.00798780843615532\n",
      "[step: 561] loss: 0.007982492446899414\n",
      "[step: 562] loss: 0.007977173663675785\n",
      "[step: 563] loss: 0.007971993647515774\n",
      "[step: 564] loss: 0.007967031560838223\n",
      "[step: 565] loss: 0.007962202653288841\n",
      "[step: 566] loss: 0.007957475259900093\n",
      "[step: 567] loss: 0.00795274879783392\n",
      "[step: 568] loss: 0.007948019541800022\n",
      "[step: 569] loss: 0.007943257689476013\n",
      "[step: 570] loss: 0.007938508875668049\n",
      "[step: 571] loss: 0.007933808490633965\n",
      "[step: 572] loss: 0.007929239422082901\n",
      "[step: 573] loss: 0.007924901321530342\n",
      "[step: 574] loss: 0.007921014912426472\n",
      "[step: 575] loss: 0.00791797786951065\n",
      "[step: 576] loss: 0.007916566915810108\n",
      "[step: 577] loss: 0.007918364368379116\n",
      "[step: 578] loss: 0.007926820777356625\n",
      "[step: 579] loss: 0.007949091494083405\n",
      "[step: 580] loss: 0.008001896552741528\n",
      "[step: 581] loss: 0.00812034122645855\n",
      "[step: 582] loss: 0.008388331159949303\n",
      "[step: 583] loss: 0.008957026526331902\n",
      "[step: 584] loss: 0.010125309228897095\n",
      "[step: 585] loss: 0.011911483481526375\n",
      "[step: 586] loss: 0.013697877526283264\n",
      "[step: 587] loss: 0.012660358101129532\n",
      "[step: 588] loss: 0.009277231991291046\n",
      "[step: 589] loss: 0.007985343225300312\n",
      "[step: 590] loss: 0.010036674328148365\n",
      "[step: 591] loss: 0.010133896954357624\n",
      "[step: 592] loss: 0.008042887784540653\n",
      "[step: 593] loss: 0.008808722719550133\n",
      "[step: 594] loss: 0.00955469161272049\n",
      "[step: 595] loss: 0.008072330616414547\n",
      "[step: 596] loss: 0.008589742705225945\n",
      "[step: 597] loss: 0.009022973477840424\n",
      "[step: 598] loss: 0.007976184599101543\n",
      "[step: 599] loss: 0.008595885708928108\n",
      "[step: 600] loss: 0.008578942157328129\n",
      "[step: 601] loss: 0.007958612404763699\n",
      "[step: 602] loss: 0.008563783951103687\n",
      "[step: 603] loss: 0.008227000944316387\n",
      "[step: 604] loss: 0.00801702681928873\n",
      "[step: 605] loss: 0.008429612964391708\n",
      "[step: 606] loss: 0.008007898926734924\n",
      "[step: 607] loss: 0.008064799942076206\n",
      "[step: 608] loss: 0.008251669816672802\n",
      "[step: 609] loss: 0.007902422919869423\n",
      "[step: 610] loss: 0.008058120496571064\n",
      "[step: 611] loss: 0.008099542930722237\n",
      "[step: 612] loss: 0.00785430334508419\n",
      "[step: 613] loss: 0.008010545745491982\n",
      "[step: 614] loss: 0.007996437139809132\n",
      "[step: 615] loss: 0.007825098931789398\n",
      "[step: 616] loss: 0.0079470444470644\n",
      "[step: 617] loss: 0.007934966124594212\n",
      "[step: 618] loss: 0.007803218439221382\n",
      "[step: 619] loss: 0.007881641387939453\n",
      "[step: 620] loss: 0.00789825152605772\n",
      "[step: 621] loss: 0.0077915433794260025\n",
      "[step: 622] loss: 0.007820872589945793\n",
      "[step: 623] loss: 0.00786683801561594\n",
      "[step: 624] loss: 0.007793930359184742\n",
      "[step: 625] loss: 0.007773611228913069\n",
      "[step: 626] loss: 0.007824914529919624\n",
      "[step: 627] loss: 0.007801794912666082\n",
      "[step: 628] loss: 0.007752269506454468\n",
      "[step: 629] loss: 0.007773572579026222\n",
      "[step: 630] loss: 0.00779294315725565\n",
      "[step: 631] loss: 0.007755739614367485\n",
      "[step: 632] loss: 0.007735664490610361\n",
      "[step: 633] loss: 0.007757324259728193\n",
      "[step: 634] loss: 0.0077569084241986275\n",
      "[step: 635] loss: 0.007727191783487797\n",
      "[step: 636] loss: 0.007719142362475395\n",
      "[step: 637] loss: 0.007732533384114504\n",
      "[step: 638] loss: 0.007726933807134628\n",
      "[step: 639] loss: 0.00770507613196969\n",
      "[step: 640] loss: 0.0076995170675218105\n",
      "[step: 641] loss: 0.007706796284765005\n",
      "[step: 642] loss: 0.007701123598963022\n",
      "[step: 643] loss: 0.0076850103214383125\n",
      "[step: 644] loss: 0.00767878582701087\n",
      "[step: 645] loss: 0.007681871298700571\n",
      "[step: 646] loss: 0.007677956484258175\n",
      "[step: 647] loss: 0.00766584649682045\n",
      "[step: 648] loss: 0.007658370770514011\n",
      "[step: 649] loss: 0.007658220827579498\n",
      "[step: 650] loss: 0.007655826862901449\n",
      "[step: 651] loss: 0.007647176738828421\n",
      "[step: 652] loss: 0.007638835813850164\n",
      "[step: 653] loss: 0.007635699585080147\n",
      "[step: 654] loss: 0.007633787114173174\n",
      "[step: 655] loss: 0.0076281605288386345\n",
      "[step: 656] loss: 0.007620316930115223\n",
      "[step: 657] loss: 0.00761455949395895\n",
      "[step: 658] loss: 0.0076114642433822155\n",
      "[step: 659] loss: 0.007607868406921625\n",
      "[step: 660] loss: 0.007601939141750336\n",
      "[step: 661] loss: 0.007595231756567955\n",
      "[step: 662] loss: 0.007589899003505707\n",
      "[step: 663] loss: 0.007585990242660046\n",
      "[step: 664] loss: 0.0075819240882992744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 665] loss: 0.007576595991849899\n",
      "[step: 666] loss: 0.007570535410195589\n",
      "[step: 667] loss: 0.007564891595393419\n",
      "[step: 668] loss: 0.007560122292488813\n",
      "[step: 669] loss: 0.007555777672678232\n",
      "[step: 670] loss: 0.007551131770014763\n",
      "[step: 671] loss: 0.007545880042016506\n",
      "[step: 672] loss: 0.007540284190326929\n",
      "[step: 673] loss: 0.007534767035394907\n",
      "[step: 674] loss: 0.007529598195105791\n",
      "[step: 675] loss: 0.007524752523750067\n",
      "[step: 676] loss: 0.007520013488829136\n",
      "[step: 677] loss: 0.007515172008424997\n",
      "[step: 678] loss: 0.0075101349502801895\n",
      "[step: 679] loss: 0.007504911627620459\n",
      "[step: 680] loss: 0.007499587256461382\n",
      "[step: 681] loss: 0.007494247518479824\n",
      "[step: 682] loss: 0.007488942705094814\n",
      "[step: 683] loss: 0.00748369051143527\n",
      "[step: 684] loss: 0.007478495594114065\n",
      "[step: 685] loss: 0.007473332807421684\n",
      "[step: 686] loss: 0.00746821379289031\n",
      "[step: 687] loss: 0.007463097106665373\n",
      "[step: 688] loss: 0.007458019070327282\n",
      "[step: 689] loss: 0.007452984806150198\n",
      "[step: 690] loss: 0.007448046933859587\n",
      "[step: 691] loss: 0.0074433013796806335\n",
      "[step: 692] loss: 0.007438936270773411\n",
      "[step: 693] loss: 0.0074353646486997604\n",
      "[step: 694] loss: 0.007433474529534578\n",
      "[step: 695] loss: 0.007435304578393698\n",
      "[step: 696] loss: 0.00744546577334404\n",
      "[step: 697] loss: 0.0074753728695213795\n",
      "[step: 698] loss: 0.007551819086074829\n",
      "[step: 699] loss: 0.007745098322629929\n",
      "[step: 700] loss: 0.008214792236685753\n",
      "[step: 701] loss: 0.009371122345328331\n",
      "[step: 702] loss: 0.011780557222664356\n",
      "[step: 703] loss: 0.016020571812987328\n",
      "[step: 704] loss: 0.01808566227555275\n",
      "[step: 705] loss: 0.013973742723464966\n",
      "[step: 706] loss: 0.007636859081685543\n",
      "[step: 707] loss: 0.010455053299665451\n",
      "[step: 708] loss: 0.012330385856330395\n",
      "[step: 709] loss: 0.007736369501799345\n",
      "[step: 710] loss: 0.009983460418879986\n",
      "[step: 711] loss: 0.010004336945712566\n",
      "[step: 712] loss: 0.007757620420306921\n",
      "[step: 713] loss: 0.010184976272284985\n",
      "[step: 714] loss: 0.007947198115289211\n",
      "[step: 715] loss: 0.009012917056679726\n",
      "[step: 716] loss: 0.008627574890851974\n",
      "[step: 717] loss: 0.008096578530967236\n",
      "[step: 718] loss: 0.008911391720175743\n",
      "[step: 719] loss: 0.007740461267530918\n",
      "[step: 720] loss: 0.008790708146989346\n",
      "[step: 721] loss: 0.007739979773759842\n",
      "[step: 722] loss: 0.008435760624706745\n",
      "[step: 723] loss: 0.007881680503487587\n",
      "[step: 724] loss: 0.008053847588598728\n",
      "[step: 725] loss: 0.00802191998809576\n",
      "[step: 726] loss: 0.007754727732390165\n",
      "[step: 727] loss: 0.00806057546287775\n",
      "[step: 728] loss: 0.007596842013299465\n",
      "[step: 729] loss: 0.007959927432239056\n",
      "[step: 730] loss: 0.007586883381009102\n",
      "[step: 731] loss: 0.007764670066535473\n",
      "[step: 732] loss: 0.0076544987969100475\n",
      "[step: 733] loss: 0.007567691616714001\n",
      "[step: 734] loss: 0.007683314848691225\n",
      "[step: 735] loss: 0.007477440405637026\n",
      "[step: 736] loss: 0.0076009128242731094\n",
      "[step: 737] loss: 0.007503059227019548\n",
      "[step: 738] loss: 0.007471850141882896\n",
      "[step: 739] loss: 0.0075271097011864185\n",
      "[step: 740] loss: 0.007423466071486473\n",
      "[step: 741] loss: 0.007459688000380993\n",
      "[step: 742] loss: 0.007459593005478382\n",
      "[step: 743] loss: 0.007383621297776699\n",
      "[step: 744] loss: 0.007445086725056171\n",
      "[step: 745] loss: 0.007403658702969551\n",
      "[step: 746] loss: 0.007366166450083256\n",
      "[step: 747] loss: 0.007426414638757706\n",
      "[step: 748] loss: 0.007356904912739992\n",
      "[step: 749] loss: 0.007366278674453497\n",
      "[step: 750] loss: 0.007391383405774832\n",
      "[step: 751] loss: 0.007333717308938503\n",
      "[step: 752] loss: 0.007359088398516178\n",
      "[step: 753] loss: 0.007354919333010912\n",
      "[step: 754] loss: 0.007325360551476479\n",
      "[step: 755] loss: 0.007337078917771578\n",
      "[step: 756] loss: 0.007330512162297964\n",
      "[step: 757] loss: 0.007314268499612808\n",
      "[step: 758] loss: 0.0073118750005960464\n",
      "[step: 759] loss: 0.007314004935324192\n",
      "[step: 760] loss: 0.007296414580196142\n",
      "[step: 761] loss: 0.0072918846271932125\n",
      "[step: 762] loss: 0.007297074887901545\n",
      "[step: 763] loss: 0.00727741327136755\n",
      "[step: 764] loss: 0.007275951094925404\n",
      "[step: 765] loss: 0.007277774158865213\n",
      "[step: 766] loss: 0.007261265069246292\n",
      "[step: 767] loss: 0.0072603728622198105\n",
      "[step: 768] loss: 0.007258546072989702\n",
      "[step: 769] loss: 0.0072474004700779915\n",
      "[step: 770] loss: 0.007243972737342119\n",
      "[step: 771] loss: 0.007241136394441128\n",
      "[step: 772] loss: 0.007233867887407541\n",
      "[step: 773] loss: 0.007227587979286909\n",
      "[step: 774] loss: 0.007225302048027515\n",
      "[step: 775] loss: 0.007219684775918722\n",
      "[step: 776] loss: 0.007212038151919842\n",
      "[step: 777] loss: 0.007209986448287964\n",
      "[step: 778] loss: 0.007204933557659388\n",
      "[step: 779] loss: 0.007197378668934107\n",
      "[step: 780] loss: 0.007194593083113432\n",
      "[step: 781] loss: 0.007189916912466288\n",
      "[step: 782] loss: 0.00718323141336441\n",
      "[step: 783] loss: 0.00717913219705224\n",
      "[step: 784] loss: 0.007174748927354813\n",
      "[step: 785] loss: 0.007169163785874844\n",
      "[step: 786] loss: 0.007163886446505785\n",
      "[step: 787] loss: 0.007159470580518246\n",
      "[step: 788] loss: 0.007154835853725672\n",
      "[step: 789] loss: 0.007149062119424343\n",
      "[step: 790] loss: 0.007144219242036343\n",
      "[step: 791] loss: 0.007140042260289192\n",
      "[step: 792] loss: 0.007134551182389259\n",
      "[step: 793] loss: 0.007129271514713764\n",
      "[step: 794] loss: 0.00712485471740365\n",
      "[step: 795] loss: 0.007119940128177404\n",
      "[step: 796] loss: 0.007114707957953215\n",
      "[step: 797] loss: 0.007109703961759806\n",
      "[step: 798] loss: 0.007104944903403521\n",
      "[step: 799] loss: 0.007100127171725035\n",
      "[step: 800] loss: 0.007094928063452244\n",
      "[step: 801] loss: 0.007089847233146429\n",
      "[step: 802] loss: 0.007085161283612251\n",
      "[step: 803] loss: 0.007080228067934513\n",
      "[step: 804] loss: 0.007075064815580845\n",
      "[step: 805] loss: 0.007070081774145365\n",
      "[step: 806] loss: 0.007065183017402887\n",
      "[step: 807] loss: 0.007060255855321884\n",
      "[step: 808] loss: 0.00705524580553174\n",
      "[step: 809] loss: 0.0070501393638551235\n",
      "[step: 810] loss: 0.007045148406177759\n",
      "[step: 811] loss: 0.007040256168693304\n",
      "[step: 812] loss: 0.007035249378532171\n",
      "[step: 813] loss: 0.00703016621991992\n",
      "[step: 814] loss: 0.0070251296274363995\n",
      "[step: 815] loss: 0.007020118180662394\n",
      "[step: 816] loss: 0.007015135604888201\n",
      "[step: 817] loss: 0.007010130677372217\n",
      "[step: 818] loss: 0.007005054969340563\n",
      "[step: 819] loss: 0.006999978795647621\n",
      "[step: 820] loss: 0.006994945928454399\n",
      "[step: 821] loss: 0.006989920046180487\n",
      "[step: 822] loss: 0.006984885316342115\n",
      "[step: 823] loss: 0.006979831028729677\n",
      "[step: 824] loss: 0.006974744610488415\n",
      "[step: 825] loss: 0.006969654932618141\n",
      "[step: 826] loss: 0.006964587606489658\n",
      "[step: 827] loss: 0.006959519814699888\n",
      "[step: 828] loss: 0.00695444829761982\n",
      "[step: 829] loss: 0.006949376780539751\n",
      "[step: 830] loss: 0.006944288499653339\n",
      "[step: 831] loss: 0.00693918252363801\n",
      "[step: 832] loss: 0.006934071891009808\n",
      "[step: 833] loss: 0.006928964518010616\n",
      "[step: 834] loss: 0.006923847831785679\n",
      "[step: 835] loss: 0.006918732542544603\n",
      "[step: 836] loss: 0.006913617718964815\n",
      "[step: 837] loss: 0.006908498238772154\n",
      "[step: 838] loss: 0.006903378292918205\n",
      "[step: 839] loss: 0.006898259744048119\n",
      "[step: 840] loss: 0.0068931253626942635\n",
      "[step: 841] loss: 0.0068879942409694195\n",
      "[step: 842] loss: 0.006882871966809034\n",
      "[step: 843] loss: 0.006877748761326075\n",
      "[step: 844] loss: 0.00687264371663332\n",
      "[step: 845] loss: 0.006867568474262953\n",
      "[step: 846] loss: 0.006862577050924301\n",
      "[step: 847] loss: 0.006857746746391058\n",
      "[step: 848] loss: 0.006853266153484583\n",
      "[step: 849] loss: 0.0068495990708470345\n",
      "[step: 850] loss: 0.006847793702036142\n",
      "[step: 851] loss: 0.006850527599453926\n",
      "[step: 852] loss: 0.006864415016025305\n",
      "[step: 853] loss: 0.006907321512699127\n",
      "[step: 854] loss: 0.007025339640676975\n",
      "[step: 855] loss: 0.007350888103246689\n",
      "[step: 856] loss: 0.008210845291614532\n",
      "[step: 857] loss: 0.010528353042900562\n",
      "[step: 858] loss: 0.015477905049920082\n",
      "[step: 859] loss: 0.023848749697208405\n",
      "[step: 860] loss: 0.02299250289797783\n",
      "[step: 861] loss: 0.01122573297470808\n",
      "[step: 862] loss: 0.008378335274755955\n",
      "[step: 863] loss: 0.015225637704133987\n",
      "[step: 864] loss: 0.00860748440027237\n",
      "[step: 865] loss: 0.010096038691699505\n",
      "[step: 866] loss: 0.010712653398513794\n",
      "[step: 867] loss: 0.007834465242922306\n",
      "[step: 868] loss: 0.010702799074351788\n",
      "[step: 869] loss: 0.0074704429134726524\n",
      "[step: 870] loss: 0.010191154666244984\n",
      "[step: 871] loss: 0.0074707441963255405\n",
      "[step: 872] loss: 0.009618743322789669\n",
      "[step: 873] loss: 0.0075053321197628975\n",
      "[step: 874] loss: 0.009198997169733047\n",
      "[step: 875] loss: 0.00750760780647397\n",
      "[step: 876] loss: 0.008826770819723606\n",
      "[step: 877] loss: 0.00748512614518404\n",
      "[step: 878] loss: 0.008524688892066479\n",
      "[step: 879] loss: 0.007441292051225901\n",
      "[step: 880] loss: 0.008247055113315582\n",
      "[step: 881] loss: 0.007385424338281155\n",
      "[step: 882] loss: 0.008015694096684456\n",
      "[step: 883] loss: 0.007312726695090532\n",
      "[step: 884] loss: 0.007810138165950775\n",
      "[step: 885] loss: 0.007234388031065464\n",
      "[step: 886] loss: 0.0076430062763392925\n",
      "[step: 887] loss: 0.007152804639190435\n",
      "[step: 888] loss: 0.007496370933949947\n",
      "[step: 889] loss: 0.0070791724137961864\n",
      "[step: 890] loss: 0.007375204935669899\n",
      "[step: 891] loss: 0.007021342404186726\n",
      "[step: 892] loss: 0.0072622355073690414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 893] loss: 0.006985949352383614\n",
      "[step: 894] loss: 0.007158966735005379\n",
      "[step: 895] loss: 0.006976468022912741\n",
      "[step: 896] loss: 0.007059192750602961\n",
      "[step: 897] loss: 0.006985069252550602\n",
      "[step: 898] loss: 0.006972259841859341\n",
      "[step: 899] loss: 0.006997063290327787\n",
      "[step: 900] loss: 0.006910810247063637\n",
      "[step: 901] loss: 0.006994585040956736\n",
      "[step: 902] loss: 0.0068842563778162\n",
      "[step: 903] loss: 0.006966775748878717\n",
      "[step: 904] loss: 0.0068894000723958015\n",
      "[step: 905] loss: 0.006922387983649969\n",
      "[step: 906] loss: 0.006907110568135977\n",
      "[step: 907] loss: 0.006880789063870907\n",
      "[step: 908] loss: 0.0069136787205934525\n",
      "[step: 909] loss: 0.006859442684799433\n",
      "[step: 910] loss: 0.006896985229104757\n",
      "[step: 911] loss: 0.0068593923933804035\n",
      "[step: 912] loss: 0.006865113507956266\n",
      "[step: 913] loss: 0.006865036208182573\n",
      "[step: 914] loss: 0.006837137974798679\n",
      "[step: 915] loss: 0.006858511827886105\n",
      "[step: 916] loss: 0.006825403776019812\n",
      "[step: 917] loss: 0.006836588494479656\n",
      "[step: 918] loss: 0.0068244109861552715\n",
      "[step: 919] loss: 0.006812111008912325\n",
      "[step: 920] loss: 0.006819434463977814\n",
      "[step: 921] loss: 0.006797731388360262\n",
      "[step: 922] loss: 0.006804198492318392\n",
      "[step: 923] loss: 0.006792454980313778\n",
      "[step: 924] loss: 0.006785720586776733\n",
      "[step: 925] loss: 0.006786541547626257\n",
      "[step: 926] loss: 0.0067726727575063705\n",
      "[step: 927] loss: 0.006774887442588806\n",
      "[step: 928] loss: 0.0067653837613761425\n",
      "[step: 929] loss: 0.006760713178664446\n",
      "[step: 930] loss: 0.006758545059710741\n",
      "[step: 931] loss: 0.006748946383595467\n",
      "[step: 932] loss: 0.00674867071211338\n",
      "[step: 933] loss: 0.006740622688084841\n",
      "[step: 934] loss: 0.006736664101481438\n",
      "[step: 935] loss: 0.006733132526278496\n",
      "[step: 936] loss: 0.0067256442271173\n",
      "[step: 937] loss: 0.006723632570356131\n",
      "[step: 938] loss: 0.006717079784721136\n",
      "[step: 939] loss: 0.006712404545396566\n",
      "[step: 940] loss: 0.006708982866257429\n",
      "[step: 941] loss: 0.006702179089188576\n",
      "[step: 942] loss: 0.0066991038620471954\n",
      "[step: 943] loss: 0.0066936565563082695\n",
      "[step: 944] loss: 0.006688479799777269\n",
      "[step: 945] loss: 0.006684782449156046\n",
      "[step: 946] loss: 0.006678983569145203\n",
      "[step: 947] loss: 0.0066747646778821945\n",
      "[step: 948] loss: 0.006670154165476561\n",
      "[step: 949] loss: 0.006664817687124014\n",
      "[step: 950] loss: 0.006660780403763056\n",
      "[step: 951] loss: 0.006655589677393436\n",
      "[step: 952] loss: 0.006650921422988176\n",
      "[step: 953] loss: 0.00664649810642004\n",
      "[step: 954] loss: 0.006641325540840626\n",
      "[step: 955] loss: 0.0066369143314659595\n",
      "[step: 956] loss: 0.006632212083786726\n",
      "[step: 957] loss: 0.006627190858125687\n",
      "[step: 958] loss: 0.0066228341311216354\n",
      "[step: 959] loss: 0.006617946084588766\n",
      "[step: 960] loss: 0.006613133940845728\n",
      "[step: 961] loss: 0.006608661264181137\n",
      "[step: 962] loss: 0.006603756453841925\n",
      "[step: 963] loss: 0.006599039305001497\n",
      "[step: 964] loss: 0.006594473961740732\n",
      "[step: 965] loss: 0.006589592434465885\n",
      "[step: 966] loss: 0.006584935821592808\n",
      "[step: 967] loss: 0.0065802778117358685\n",
      "[step: 968] loss: 0.006575456354767084\n",
      "[step: 969] loss: 0.006570796482264996\n",
      "[step: 970] loss: 0.006566107738763094\n",
      "[step: 971] loss: 0.0065613011829555035\n",
      "[step: 972] loss: 0.006556642707437277\n",
      "[step: 973] loss: 0.006551939528435469\n",
      "[step: 974] loss: 0.006547140423208475\n",
      "[step: 975] loss: 0.006542472168803215\n",
      "[step: 976] loss: 0.0065377578139305115\n",
      "[step: 977] loss: 0.006532978266477585\n",
      "[step: 978] loss: 0.006528281606733799\n",
      "[step: 979] loss: 0.006523569580167532\n",
      "[step: 980] loss: 0.006518807262182236\n",
      "[step: 981] loss: 0.006514097563922405\n",
      "[step: 982] loss: 0.006509377621114254\n",
      "[step: 983] loss: 0.0065046148374676704\n",
      "[step: 984] loss: 0.006499891635030508\n",
      "[step: 985] loss: 0.0064951698295772076\n",
      "[step: 986] loss: 0.00649041123688221\n",
      "[step: 987] loss: 0.006485675927251577\n",
      "[step: 988] loss: 0.0064809489995241165\n",
      "[step: 989] loss: 0.006476197857409716\n",
      "[step: 990] loss: 0.006471447180956602\n",
      "[step: 991] loss: 0.006466717924922705\n",
      "[step: 992] loss: 0.006461974699050188\n",
      "[step: 993] loss: 0.006457221228629351\n",
      "[step: 994] loss: 0.006452478468418121\n",
      "[step: 995] loss: 0.006447731517255306\n",
      "[step: 996] loss: 0.006442983169108629\n",
      "[step: 997] loss: 0.006438234820961952\n",
      "[step: 998] loss: 0.006433491129428148\n",
      "[step: 999] loss: 0.006428729742765427\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "LSTM realforecast : [11564.00390625, 19378.8515625, 12206.912109375, 3621.99755859375, 6344.431640625, 4044.051025390625, 1171.41552734375]\n",
      "Bayseian realforecast : [934.67590488609881, 1660.8676990102022, 4777.8656352160351, 5393.0442755984013, 2664.1705383767676, 2745.4086336296377, 5034.9435872385957]\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,0,forecastDay,'month') #0은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11564.00390625,\n",
       " 19378.8515625,\n",
       " 12206.912109375,\n",
       " 3621.99755859375,\n",
       " 6344.431640625,\n",
       " 4044.051025390625,\n",
       " 1171.41552734375]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawArrayDatas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
