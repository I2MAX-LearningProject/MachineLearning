{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=2*forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list(np.log(rawArrayDatas[1]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list(np.log(rawArrayDatas[1][:-forecastDay] ))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-3*forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-3*forecastDay]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-3*forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of 2*forecastDay:  rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "    testY= rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출       \n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'day')\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "    \n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    print('LSTM realforecast :',realForecastDictionary['LSTM'])\n",
    "    print('Bayseian realforecast :',realForecastDictionary['Bayseian'] ) \n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "        listedLogPredict=test_predict[-1].tolist()\n",
    "    return [np.exp(y) for y in listedLogPredict]\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM testforecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian testforecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 3\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('walMonth.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.1)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2010-02-01',\n",
       "  '2010-03-01',\n",
       "  '2010-04-01',\n",
       "  '2010-05-01',\n",
       "  '2010-06-01',\n",
       "  '2010-07-01',\n",
       "  '2010-08-01',\n",
       "  '2010-09-01',\n",
       "  '2010-10-01',\n",
       "  '2010-11-01',\n",
       "  '2010-12-01',\n",
       "  '2011-01-01',\n",
       "  '2011-02-01',\n",
       "  '2011-03-01',\n",
       "  '2011-04-01',\n",
       "  '2011-05-01',\n",
       "  '2011-06-01',\n",
       "  '2011-07-01',\n",
       "  '2011-08-01',\n",
       "  '2011-09-01',\n",
       "  '2011-10-01',\n",
       "  '2011-11-01',\n",
       "  '2011-12-01',\n",
       "  '2012-01-01',\n",
       "  '2012-02-01',\n",
       "  '2012-03-01',\n",
       "  '2012-04-01',\n",
       "  '2012-05-01',\n",
       "  '2012-06-01',\n",
       "  '2012-07-01',\n",
       "  '2012-08-01',\n",
       "  '2012-09-01',\n",
       "  '2012-10-01'],\n",
       " [32990.769999999997,\n",
       "  22809.285,\n",
       "  30103.352000000003,\n",
       "  16673.537499999999,\n",
       "  16685.174999999999,\n",
       "  16383.002,\n",
       "  16144.702499999999,\n",
       "  17978.317500000001,\n",
       "  26928.906000000003,\n",
       "  23040.349999999999,\n",
       "  34796.775999999998,\n",
       "  17286.647499999999,\n",
       "  31440.657500000001,\n",
       "  20705.834999999999,\n",
       "  33011.389999999999,\n",
       "  17062.93,\n",
       "  15744.6425,\n",
       "  15771.246000000001,\n",
       "  14765.487499999999,\n",
       "  17551.281999999999,\n",
       "  24701.7075,\n",
       "  24634.377499999999,\n",
       "  34902.413999999997,\n",
       "  17551.337500000001,\n",
       "  33670.824999999997,\n",
       "  22936.107999999997,\n",
       "  31400.029999999999,\n",
       "  17698.244999999999,\n",
       "  16729.308000000001,\n",
       "  16617.197499999998,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArrayDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 0.4497160017490387\n",
      "[step: 1] loss: 0.15955907106399536\n",
      "[step: 2] loss: 0.030341386795043945\n",
      "[step: 3] loss: 0.012750663794577122\n",
      "[step: 4] loss: 0.04733975604176521\n",
      "[step: 5] loss: 0.08296124637126923\n",
      "[step: 6] loss: 0.09543914347887039\n",
      "[step: 7] loss: 0.08449675887823105\n",
      "[step: 8] loss: 0.06093190237879753\n",
      "[step: 9] loss: 0.03567023575305939\n",
      "[step: 10] loss: 0.015806075185537338\n",
      "[step: 11] loss: 0.004462819546461105\n",
      "[step: 12] loss: 0.0016636924119666219\n",
      "[step: 13] loss: 0.005317428149282932\n",
      "[step: 14] loss: 0.012218638323247433\n",
      "[step: 15] loss: 0.01907535269856453\n",
      "[step: 16] loss: 0.023439636453986168\n",
      "[step: 17] loss: 0.024225562810897827\n",
      "[step: 18] loss: 0.021643398329615593\n",
      "[step: 19] loss: 0.016770483925938606\n",
      "[step: 20] loss: 0.011046843603253365\n",
      "[step: 21] loss: 0.0058437371626496315\n",
      "[step: 22] loss: 0.002152955625206232\n",
      "[step: 23] loss: 0.0004130933084525168\n",
      "[step: 24] loss: 0.0004892863798886538\n",
      "[step: 25] loss: 0.0018091389210894704\n",
      "[step: 26] loss: 0.0036132964305579662\n",
      "[step: 27] loss: 0.005220765247941017\n",
      "[step: 28] loss: 0.00619751401245594\n",
      "[step: 29] loss: 0.006386174354702234\n",
      "[step: 30] loss: 0.005841101985424757\n",
      "[step: 31] loss: 0.004744276870042086\n",
      "[step: 32] loss: 0.0033484564628452063\n",
      "[step: 33] loss: 0.0019429782405495644\n",
      "[step: 34] loss: 0.0008099529077298939\n",
      "[step: 35] loss: 0.00015657450421713293\n",
      "[step: 36] loss: 4.885891030426137e-05\n",
      "[step: 37] loss: 0.00038941390812397003\n",
      "[step: 38] loss: 0.0009611340356059372\n",
      "[step: 39] loss: 0.0015174641739577055\n",
      "[step: 40] loss: 0.001871208194643259\n",
      "[step: 41] loss: 0.001940583111718297\n",
      "[step: 42] loss: 0.001744376728311181\n",
      "[step: 43] loss: 0.001366780954413116\n",
      "[step: 44] loss: 0.0009179324842989445\n",
      "[step: 45] loss: 0.0005035954527556896\n",
      "[step: 46] loss: 0.000204171403311193\n",
      "[step: 47] loss: 6.0441321693360806e-05\n",
      "[step: 48] loss: 6.808100442867726e-05\n",
      "[step: 49] loss: 0.00018460529099684209\n",
      "[step: 50] loss: 0.00034733748179860413\n",
      "[step: 51] loss: 0.0004944012616761029\n",
      "[step: 52] loss: 0.0005801841034553945\n",
      "[step: 53] loss: 0.0005823863320983946\n",
      "[step: 54] loss: 0.0005031803739257157\n",
      "[step: 55] loss: 0.00036692715366370976\n",
      "[step: 56] loss: 0.00021362323604989797\n",
      "[step: 57] loss: 8.655288547743112e-05\n",
      "[step: 58] loss: 1.6771426089690067e-05\n",
      "[step: 59] loss: 1.17416257126024e-05\n",
      "[step: 60] loss: 5.492090349434875e-05\n",
      "[step: 61] loss: 0.00011652743705781177\n",
      "[step: 62] loss: 0.00016835008864291012\n",
      "[step: 63] loss: 0.00019380469166208059\n",
      "[step: 64] loss: 0.00018941107555292547\n",
      "[step: 65] loss: 0.00016056434833444655\n",
      "[step: 66] loss: 0.00011690158862620592\n",
      "[step: 67] loss: 6.98575604474172e-05\n",
      "[step: 68] loss: 3.1089231924852356e-05\n",
      "[step: 69] loss: 9.724272786115762e-06\n",
      "[step: 70] loss: 8.891826837498229e-06\n",
      "[step: 71] loss: 2.4215656594606116e-05\n",
      "[step: 72] loss: 4.601043474394828e-05\n",
      "[step: 73] loss: 6.395360105670989e-05\n",
      "[step: 74] loss: 7.12265755282715e-05\n",
      "[step: 75] loss: 6.6177781263832e-05\n",
      "[step: 76] loss: 5.167376730241813e-05\n",
      "[step: 77] loss: 3.3302541851298884e-05\n",
      "[step: 78] loss: 1.7167085388791747e-05\n",
      "[step: 79] loss: 7.673608706681989e-06\n",
      "[step: 80] loss: 6.0557549659279175e-06\n",
      "[step: 81] loss: 1.0439273864903953e-05\n",
      "[step: 82] loss: 1.735560545057524e-05\n",
      "[step: 83] loss: 2.3533531930297613e-05\n",
      "[step: 84] loss: 2.685122672119178e-05\n",
      "[step: 85] loss: 2.6405961762066e-05\n",
      "[step: 86] loss: 2.2393231120076962e-05\n",
      "[step: 87] loss: 1.6127323760883883e-05\n",
      "[step: 88] loss: 9.788241186470259e-06\n",
      "[step: 89] loss: 5.5604054978175554e-06\n",
      "[step: 90] loss: 4.544727744359989e-06\n",
      "[step: 91] loss: 6.281083187786862e-06\n",
      "[step: 92] loss: 9.229331226379145e-06\n",
      "[step: 93] loss: 1.1743786672013812e-05\n",
      "[step: 94] loss: 1.2809976396965794e-05\n",
      "[step: 95] loss: 1.2223701560287736e-05\n",
      "[step: 96] loss: 1.039455491991248e-05\n",
      "[step: 97] loss: 8.056903425313067e-06\n",
      "[step: 98] loss: 5.997851530992193e-06\n",
      "[step: 99] loss: 4.799922407983104e-06\n",
      "[step: 100] loss: 4.653946234611794e-06\n",
      "[step: 101] loss: 5.337794846127508e-06\n",
      "[step: 102] loss: 6.369104994519148e-06\n",
      "[step: 103] loss: 7.226808520499617e-06\n",
      "[step: 104] loss: 7.533180450991495e-06\n",
      "[step: 105] loss: 7.16852400728385e-06\n",
      "[step: 106] loss: 6.303398549789563e-06\n",
      "[step: 107] loss: 5.322488505044021e-06\n",
      "[step: 108] loss: 4.624206667358521e-06\n",
      "[step: 109] loss: 4.408392214827472e-06\n",
      "[step: 110] loss: 4.6046802708588075e-06\n",
      "[step: 111] loss: 4.984065981261665e-06\n",
      "[step: 112] loss: 5.3247204050421715e-06\n",
      "[step: 113] loss: 5.495974619407207e-06\n",
      "[step: 114] loss: 5.449917352962075e-06\n",
      "[step: 115] loss: 5.204383342061192e-06\n",
      "[step: 116] loss: 4.8483357204531785e-06\n",
      "[step: 117] loss: 4.523022198554827e-06\n",
      "[step: 118] loss: 4.351623829279561e-06\n",
      "[step: 119] loss: 4.36832669947762e-06\n",
      "[step: 120] loss: 4.5111960389476735e-06\n",
      "[step: 121] loss: 4.675079708249541e-06\n",
      "[step: 122] loss: 4.771502062794752e-06\n",
      "[step: 123] loss: 4.760539923154283e-06\n",
      "[step: 124] loss: 4.655651082430268e-06\n",
      "[step: 125] loss: 4.508861366048222e-06\n",
      "[step: 126] loss: 4.382242877909448e-06\n",
      "[step: 127] loss: 4.3161649045941886e-06\n",
      "[step: 128] loss: 4.3161171561223455e-06\n",
      "[step: 129] loss: 4.3601794459391385e-06\n",
      "[step: 130] loss: 4.416639058035798e-06\n",
      "[step: 131] loss: 4.455620000953786e-06\n",
      "[step: 132] loss: 4.456414444575785e-06\n",
      "[step: 133] loss: 4.415912826516433e-06\n",
      "[step: 134] loss: 4.351755706011318e-06\n",
      "[step: 135] loss: 4.294043264962966e-06\n",
      "[step: 136] loss: 4.2651536205084994e-06\n",
      "[step: 137] loss: 4.268003522156505e-06\n",
      "[step: 138] loss: 4.2883639252977446e-06\n",
      "[step: 139] loss: 4.309039013605798e-06\n",
      "[step: 140] loss: 4.317882940085838e-06\n",
      "[step: 141] loss: 4.311089014663594e-06\n",
      "[step: 142] loss: 4.290301149012521e-06\n",
      "[step: 143] loss: 4.263605205778731e-06\n",
      "[step: 144] loss: 4.2406968532304745e-06\n",
      "[step: 145] loss: 4.22860830440186e-06\n",
      "[step: 146] loss: 4.228214038448641e-06\n",
      "[step: 147] loss: 4.234594143781578e-06\n",
      "[step: 148] loss: 4.241018814354902e-06\n",
      "[step: 149] loss: 4.241521310177632e-06\n",
      "[step: 150] loss: 4.234004791214829e-06\n",
      "[step: 151] loss: 4.220661594445119e-06\n",
      "[step: 152] loss: 4.2068527363881e-06\n",
      "[step: 153] loss: 4.197067482891725e-06\n",
      "[step: 154] loss: 4.192393134871963e-06\n",
      "[step: 155] loss: 4.191269908915274e-06\n",
      "[step: 156] loss: 4.19133994000731e-06\n",
      "[step: 157] loss: 4.190415893390309e-06\n",
      "[step: 158] loss: 4.186931164440466e-06\n",
      "[step: 159] loss: 4.1804810280154925e-06\n",
      "[step: 160] loss: 4.172336957708467e-06\n",
      "[step: 161] loss: 4.16453076468315e-06\n",
      "[step: 162] loss: 4.158811861998402e-06\n",
      "[step: 163] loss: 4.15543036069721e-06\n",
      "[step: 164] loss: 4.153201189183164e-06\n",
      "[step: 165] loss: 4.1507978494337294e-06\n",
      "[step: 166] loss: 4.147373601881554e-06\n",
      "[step: 167] loss: 4.1425664676353335e-06\n",
      "[step: 168] loss: 4.1368712118128315e-06\n",
      "[step: 169] loss: 4.131130936002592e-06\n",
      "[step: 170] loss: 4.1261005208070856e-06\n",
      "[step: 171] loss: 4.121873189433245e-06\n",
      "[step: 172] loss: 4.118148353882134e-06\n",
      "[step: 173] loss: 4.114935563848121e-06\n",
      "[step: 174] loss: 4.111278030904941e-06\n",
      "[step: 175] loss: 4.1070115912589245e-06\n",
      "[step: 176] loss: 4.1022690311365295e-06\n",
      "[step: 177] loss: 4.0972186070575844e-06\n",
      "[step: 178] loss: 4.092673862032825e-06\n",
      "[step: 179] loss: 4.088261903234525e-06\n",
      "[step: 180] loss: 4.084435204276815e-06\n",
      "[step: 181] loss: 4.080533017258858e-06\n",
      "[step: 182] loss: 4.076656296092551e-06\n",
      "[step: 183] loss: 4.072403953614412e-06\n",
      "[step: 184] loss: 4.067987902089953e-06\n",
      "[step: 185] loss: 4.0634854485688265e-06\n",
      "[step: 186] loss: 4.058995273226174e-06\n",
      "[step: 187] loss: 4.054705641465262e-06\n",
      "[step: 188] loss: 4.050497409480158e-06\n",
      "[step: 189] loss: 4.04642514695297e-06\n",
      "[step: 190] loss: 4.042370619572466e-06\n",
      "[step: 191] loss: 4.038098268210888e-06\n",
      "[step: 192] loss: 4.033836830785731e-06\n",
      "[step: 193] loss: 4.029338469990762e-06\n",
      "[step: 194] loss: 4.025039288535481e-06\n",
      "[step: 195] loss: 4.020783762825886e-06\n",
      "[step: 196] loss: 4.016489583591465e-06\n",
      "[step: 197] loss: 4.012272711406695e-06\n",
      "[step: 198] loss: 4.008084943052381e-06\n",
      "[step: 199] loss: 4.0038944462139625e-06\n",
      "[step: 200] loss: 3.999545697297435e-06\n",
      "[step: 201] loss: 3.9952069528226275e-06\n",
      "[step: 202] loss: 3.990867298853118e-06\n",
      "[step: 203] loss: 3.986597221228294e-06\n",
      "[step: 204] loss: 3.98221436626045e-06\n",
      "[step: 205] loss: 3.978039330831962e-06\n",
      "[step: 206] loss: 3.9736742110108025e-06\n",
      "[step: 207] loss: 3.969483714172384e-06\n",
      "[step: 208] loss: 3.965080850321101e-06\n",
      "[step: 209] loss: 3.960850790463155e-06\n",
      "[step: 210] loss: 3.956458385800943e-06\n",
      "[step: 211] loss: 3.952178303734399e-06\n",
      "[step: 212] loss: 3.947853201680118e-06\n",
      "[step: 213] loss: 3.943607225664891e-06\n",
      "[step: 214] loss: 3.939311682188418e-06\n",
      "[step: 215] loss: 3.935016593459295e-06\n",
      "[step: 216] loss: 3.930620096070925e-06\n",
      "[step: 217] loss: 3.926298631995451e-06\n",
      "[step: 218] loss: 3.9219416976266075e-06\n",
      "[step: 219] loss: 3.91761341234087e-06\n",
      "[step: 220] loss: 3.9133501559263095e-06\n",
      "[step: 221] loss: 3.90904142477666e-06\n",
      "[step: 222] loss: 3.904689947376028e-06\n",
      "[step: 223] loss: 3.900290266756201e-06\n",
      "[step: 224] loss: 3.895911504514515e-06\n",
      "[step: 225] loss: 3.891614142048638e-06\n",
      "[step: 226] loss: 3.8873367884662e-06\n",
      "[step: 227] loss: 3.882927558152005e-06\n",
      "[step: 228] loss: 3.878639745380497e-06\n",
      "[step: 229] loss: 3.874290996463969e-06\n",
      "[step: 230] loss: 3.8698963180650026e-06\n",
      "[step: 231] loss: 3.865590315399459e-06\n",
      "[step: 232] loss: 3.8612247408309486e-06\n",
      "[step: 233] loss: 3.8568632589885965e-06\n",
      "[step: 234] loss: 3.852485860988963e-06\n",
      "[step: 235] loss: 3.8481639421661384e-06\n",
      "[step: 236] loss: 3.84386567020556e-06\n",
      "[step: 237] loss: 3.8394118746509776e-06\n",
      "[step: 238] loss: 3.835119059658609e-06\n",
      "[step: 239] loss: 3.830733930954011e-06\n",
      "[step: 240] loss: 3.8264306567725725e-06\n",
      "[step: 241] loss: 3.821990048891166e-06\n",
      "[step: 242] loss: 3.817634478764376e-06\n",
      "[step: 243] loss: 3.81326867682219e-06\n",
      "[step: 244] loss: 3.8089690406195587e-06\n",
      "[step: 245] loss: 3.8046055124141276e-06\n",
      "[step: 246] loss: 3.800122613029089e-06\n",
      "[step: 247] loss: 3.7958611756039318e-06\n",
      "[step: 248] loss: 3.791489007198834e-06\n",
      "[step: 249] loss: 3.7871673157496843e-06\n",
      "[step: 250] loss: 3.782813337238622e-06\n",
      "[step: 251] loss: 3.7784379856020678e-06\n",
      "[step: 252] loss: 3.7740010156994686e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 253] loss: 3.7696656818297924e-06\n",
      "[step: 254] loss: 3.7653160234185634e-06\n",
      "[step: 255] loss: 3.760921572393272e-06\n",
      "[step: 256] loss: 3.756556907319464e-06\n",
      "[step: 257] loss: 3.752242264454253e-06\n",
      "[step: 258] loss: 3.7477593650692143e-06\n",
      "[step: 259] loss: 3.743466095329495e-06\n",
      "[step: 260] loss: 3.7390566376416245e-06\n",
      "[step: 261] loss: 3.7347706438595196e-06\n",
      "[step: 262] loss: 3.7304405395843787e-06\n",
      "[step: 263] loss: 3.7259942473610863e-06\n",
      "[step: 264] loss: 3.721669600054156e-06\n",
      "[step: 265] loss: 3.717224444699241e-06\n",
      "[step: 266] loss: 3.712914349307539e-06\n",
      "[step: 267] loss: 3.7085885651322315e-06\n",
      "[step: 268] loss: 3.7041716041130712e-06\n",
      "[step: 269] loss: 3.699845365190413e-06\n",
      "[step: 270] loss: 3.695514806167921e-06\n",
      "[step: 271] loss: 3.691088750201743e-06\n",
      "[step: 272] loss: 3.6867311337118736e-06\n",
      "[step: 273] loss: 3.682408987515373e-06\n",
      "[step: 274] loss: 3.678039320220705e-06\n",
      "[step: 275] loss: 3.6735948469868163e-06\n",
      "[step: 276] loss: 3.6692540561489295e-06\n",
      "[step: 277] loss: 3.664931227831403e-06\n",
      "[step: 278] loss: 3.660600668808911e-06\n",
      "[step: 279] loss: 3.6562619243341032e-06\n",
      "[step: 280] loss: 3.651862471087952e-06\n",
      "[step: 281] loss: 3.6475196338869864e-06\n",
      "[step: 282] loss: 3.6432234082894865e-06\n",
      "[step: 283] loss: 3.638880571088521e-06\n",
      "[step: 284] loss: 3.6345004446047824e-06\n",
      "[step: 285] loss: 3.630178753155633e-06\n",
      "[step: 286] loss: 3.625749741331674e-06\n",
      "[step: 287] loss: 3.621453288360499e-06\n",
      "[step: 288] loss: 3.6170338262309087e-06\n",
      "[step: 289] loss: 3.612776708905585e-06\n",
      "[step: 290] loss: 3.6084302337258123e-06\n",
      "[step: 291] loss: 3.6041221846971894e-06\n",
      "[step: 292] loss: 3.5997691156808287e-06\n",
      "[step: 293] loss: 3.5954280974692665e-06\n",
      "[step: 294] loss: 3.591117092582863e-06\n",
      "[step: 295] loss: 3.5867569749825634e-06\n",
      "[step: 296] loss: 3.582396857382264e-06\n",
      "[step: 297] loss: 3.578087216737913e-06\n",
      "[step: 298] loss: 3.5737245980271837e-06\n",
      "[step: 299] loss: 3.5694145026354818e-06\n",
      "[step: 300] loss: 3.565141923900228e-06\n",
      "[step: 301] loss: 3.560888217180036e-06\n",
      "[step: 302] loss: 3.5565285543270875e-06\n",
      "[step: 303] loss: 3.5521359222911997e-06\n",
      "[step: 304] loss: 3.5478897189022973e-06\n",
      "[step: 305] loss: 3.5434791243460495e-06\n",
      "[step: 306] loss: 3.5392145036894362e-06\n",
      "[step: 307] loss: 3.5348891742614796e-06\n",
      "[step: 308] loss: 3.5306070458318572e-06\n",
      "[step: 309] loss: 3.5263944937469205e-06\n",
      "[step: 310] loss: 3.521960479702102e-06\n",
      "[step: 311] loss: 3.5176879009668482e-06\n",
      "[step: 312] loss: 3.513461706461385e-06\n",
      "[step: 313] loss: 3.5091379686491564e-06\n",
      "[step: 314] loss: 3.5048822155658854e-06\n",
      "[step: 315] loss: 3.50048048858298e-06\n",
      "[step: 316] loss: 3.496325234664255e-06\n",
      "[step: 317] loss: 3.4919676181743853e-06\n",
      "[step: 318] loss: 3.487700496407342e-06\n",
      "[step: 319] loss: 3.4833547033485956e-06\n",
      "[step: 320] loss: 3.479099177639e-06\n",
      "[step: 321] loss: 3.4747779409372015e-06\n",
      "[step: 322] loss: 3.4705233247223077e-06\n",
      "[step: 323] loss: 3.466275302344002e-06\n",
      "[step: 324] loss: 3.462020231381757e-06\n",
      "[step: 325] loss: 3.4577660699142143e-06\n",
      "[step: 326] loss: 3.453537829045672e-06\n",
      "[step: 327] loss: 3.449256837484427e-06\n",
      "[step: 328] loss: 3.4449665236024885e-06\n",
      "[step: 329] loss: 3.4406696158839623e-06\n",
      "[step: 330] loss: 3.4365098144917283e-06\n",
      "[step: 331] loss: 3.4322224564675707e-06\n",
      "[step: 332] loss: 3.427970341363107e-06\n",
      "[step: 333] loss: 3.423702310101362e-06\n",
      "[step: 334] loss: 3.419536824367242e-06\n",
      "[step: 335] loss: 3.4153163142036647e-06\n",
      "[step: 336] loss: 3.410973477002699e-06\n",
      "[step: 337] loss: 3.4068114018737106e-06\n",
      "[step: 338] loss: 3.4025372315227287e-06\n",
      "[step: 339] loss: 3.3983856155828107e-06\n",
      "[step: 340] loss: 3.394130999367917e-06\n",
      "[step: 341] loss: 3.3899134450621204e-06\n",
      "[step: 342] loss: 3.3855931178550236e-06\n",
      "[step: 343] loss: 3.381473561603343e-06\n",
      "[step: 344] loss: 3.3771800644899486e-06\n",
      "[step: 345] loss: 3.3730357245076448e-06\n",
      "[step: 346] loss: 3.3688577332213754e-06\n",
      "[step: 347] loss: 3.3646506381046493e-06\n",
      "[step: 348] loss: 3.3604355849092826e-06\n",
      "[step: 349] loss: 3.3562357657501707e-06\n",
      "[step: 350] loss: 3.352060275574331e-06\n",
      "[step: 351] loss: 3.347985057189362e-06\n",
      "[step: 352] loss: 3.3436986086599063e-06\n",
      "[step: 353] loss: 3.339542672620155e-06\n",
      "[step: 354] loss: 3.33533353114035e-06\n",
      "[step: 355] loss: 3.331176458232221e-06\n",
      "[step: 356] loss: 3.3270152925979346e-06\n",
      "[step: 357] loss: 3.322845941511332e-06\n",
      "[step: 358] loss: 3.318745029901038e-06\n",
      "[step: 359] loss: 3.3144665394502226e-06\n",
      "[step: 360] loss: 3.310392457933631e-06\n",
      "[step: 361] loss: 3.306198323116405e-06\n",
      "[step: 362] loss: 3.302164714114042e-06\n",
      "[step: 363] loss: 3.297897137599648e-06\n",
      "[step: 364] loss: 3.2938728509179782e-06\n",
      "[step: 365] loss: 3.289689402663498e-06\n",
      "[step: 366] loss: 3.285585307821748e-06\n",
      "[step: 367] loss: 3.2814200494613033e-06\n",
      "[step: 368] loss: 3.2772907161415787e-06\n",
      "[step: 369] loss: 3.273093625466572e-06\n",
      "[step: 370] loss: 3.2691170872567454e-06\n",
      "[step: 371] loss: 3.2649136301188264e-06\n",
      "[step: 372] loss: 3.2608909350528847e-06\n",
      "[step: 373] loss: 3.25676160173316e-06\n",
      "[step: 374] loss: 3.252658189012436e-06\n",
      "[step: 375] loss: 3.2486266263731522e-06\n",
      "[step: 376] loss: 3.244489334974787e-06\n",
      "[step: 377] loss: 3.240476644350565e-06\n",
      "[step: 378] loss: 3.236420980101684e-06\n",
      "[step: 379] loss: 3.23217636832851e-06\n",
      "[step: 380] loss: 3.228192554161069e-06\n",
      "[step: 381] loss: 3.224048214178765e-06\n",
      "[step: 382] loss: 3.2199741326621734e-06\n",
      "[step: 383] loss: 3.2159241527551785e-06\n",
      "[step: 384] loss: 3.211876446584938e-06\n",
      "[step: 385] loss: 3.2078066851681797e-06\n",
      "[step: 386] loss: 3.2037610253610183e-06\n",
      "[step: 387] loss: 3.199717411916936e-06\n",
      "[step: 388] loss: 3.1957440569385653e-06\n",
      "[step: 389] loss: 3.191667246937868e-06\n",
      "[step: 390] loss: 3.187657057424076e-06\n",
      "[step: 391] loss: 3.183611170243239e-06\n",
      "[step: 392] loss: 3.179586201440543e-06\n",
      "[step: 393] loss: 3.1755457712279167e-06\n",
      "[step: 394] loss: 3.171571279381169e-06\n",
      "[step: 395] loss: 3.1675726859248243e-06\n",
      "[step: 396] loss: 3.163503379255417e-06\n",
      "[step: 397] loss: 3.1594540814694483e-06\n",
      "[step: 398] loss: 3.155538252030965e-06\n",
      "[step: 399] loss: 3.151516011712374e-06\n",
      "[step: 400] loss: 3.1475644846068462e-06\n",
      "[step: 401] loss: 3.143546564388089e-06\n",
      "[step: 402] loss: 3.139612999802921e-06\n",
      "[step: 403] loss: 3.1355939427157864e-06\n",
      "[step: 404] loss: 3.1315587420976954e-06\n",
      "[step: 405] loss: 3.1276815661840374e-06\n",
      "[step: 406] loss: 3.1236788800015347e-06\n",
      "[step: 407] loss: 3.119787379546324e-06\n",
      "[step: 408] loss: 3.115796971542295e-06\n",
      "[step: 409] loss: 3.1118640890781535e-06\n",
      "[step: 410] loss: 3.107904603893985e-06\n",
      "[step: 411] loss: 3.103961262240773e-06\n",
      "[step: 412] loss: 3.0999733553471742e-06\n",
      "[step: 413] loss: 3.0961723496147897e-06\n",
      "[step: 414] loss: 3.092172164542717e-06\n",
      "[step: 415] loss: 3.088279072471778e-06\n",
      "[step: 416] loss: 3.084460331592709e-06\n",
      "[step: 417] loss: 3.080533588217804e-06\n",
      "[step: 418] loss: 3.076604571106145e-06\n",
      "[step: 419] loss: 3.072589379371493e-06\n",
      "[step: 420] loss: 3.0687708658660995e-06\n",
      "[step: 421] loss: 3.0648645861219848e-06\n",
      "[step: 422] loss: 3.0609667192038614e-06\n",
      "[step: 423] loss: 3.057107278436888e-06\n",
      "[step: 424] loss: 3.053218733839458e-06\n",
      "[step: 425] loss: 3.049422730327933e-06\n",
      "[step: 426] loss: 3.045468019990949e-06\n",
      "[step: 427] loss: 3.0416229037655285e-06\n",
      "[step: 428] loss: 3.0377657367353095e-06\n",
      "[step: 429] loss: 3.03390311273688e-06\n",
      "[step: 430] loss: 3.0301082460937323e-06\n",
      "[step: 431] loss: 3.02626472148404e-06\n",
      "[step: 432] loss: 3.0223366138670826e-06\n",
      "[step: 433] loss: 3.018558572875918e-06\n",
      "[step: 434] loss: 3.0147089091769885e-06\n",
      "[step: 435] loss: 3.010836053363164e-06\n",
      "[step: 436] loss: 3.0070962111494737e-06\n",
      "[step: 437] loss: 3.0032990707695717e-06\n",
      "[step: 438] loss: 2.9994607757544145e-06\n",
      "[step: 439] loss: 2.995619297507801e-06\n",
      "[step: 440] loss: 2.9918837753939442e-06\n",
      "[step: 441] loss: 2.988071628351463e-06\n",
      "[step: 442] loss: 2.9842412914149463e-06\n",
      "[step: 443] loss: 2.9805257781845285e-06\n",
      "[step: 444] loss: 2.976688847411424e-06\n",
      "[step: 445] loss: 2.972973788928357e-06\n",
      "[step: 446] loss: 2.96926123155572e-06\n",
      "[step: 447] loss: 2.9654233912879135e-06\n",
      "[step: 448] loss: 2.9616385290864855e-06\n",
      "[step: 449] loss: 2.95789095616783e-06\n",
      "[step: 450] loss: 2.9541515687014908e-06\n",
      "[step: 451] loss: 2.9504055873985635e-06\n",
      "[step: 452] loss: 2.9467448712239275e-06\n",
      "[step: 453] loss: 2.942865876320866e-06\n",
      "[step: 454] loss: 2.9392244869086426e-06\n",
      "[step: 455] loss: 2.9354987418628298e-06\n",
      "[step: 456] loss: 2.9317939151951578e-06\n",
      "[step: 457] loss: 2.928123649326153e-06\n",
      "[step: 458] loss: 2.9243581138871377e-06\n",
      "[step: 459] loss: 2.9206876206444576e-06\n",
      "[step: 460] loss: 2.9170028028602246e-06\n",
      "[step: 461] loss: 2.913250909841736e-06\n",
      "[step: 462] loss: 2.9095776881149504e-06\n",
      "[step: 463] loss: 2.9059299322398147e-06\n",
      "[step: 464] loss: 2.90222124021966e-06\n",
      "[step: 465] loss: 2.898594402722665e-06\n",
      "[step: 466] loss: 2.8949093575647566e-06\n",
      "[step: 467] loss: 2.8912486413901206e-06\n",
      "[step: 468] loss: 2.8876454507553717e-06\n",
      "[step: 469] loss: 2.883913111872971e-06\n",
      "[step: 470] loss: 2.880287865991704e-06\n",
      "[step: 471] loss: 2.8766278319380945e-06\n",
      "[step: 472] loss: 2.873086941690417e-06\n",
      "[step: 473] loss: 2.8693750664388062e-06\n",
      "[step: 474] loss: 2.865799388018786e-06\n",
      "[step: 475] loss: 2.862157771232887e-06\n",
      "[step: 476] loss: 2.8585266136360588e-06\n",
      "[step: 477] loss: 2.8548797672556248e-06\n",
      "[step: 478] loss: 2.8513118195405696e-06\n",
      "[step: 479] loss: 2.8477450086938916e-06\n",
      "[step: 480] loss: 2.84408656625601e-06\n",
      "[step: 481] loss: 2.840515662683174e-06\n",
      "[step: 482] loss: 2.8369445317366626e-06\n",
      "[step: 483] loss: 2.8334391117823543e-06\n",
      "[step: 484] loss: 2.8298286451899912e-06\n",
      "[step: 485] loss: 2.8262625164643396e-06\n",
      "[step: 486] loss: 2.822698434101767e-06\n",
      "[step: 487] loss: 2.819159362843493e-06\n",
      "[step: 488] loss: 2.8156082407804206e-06\n",
      "[step: 489] loss: 2.812042112054769e-06\n",
      "[step: 490] loss: 2.8085294161428465e-06\n",
      "[step: 491] loss: 2.8049703360011335e-06\n",
      "[step: 492] loss: 2.801399659801973e-06\n",
      "[step: 493] loss: 2.7978705929854186e-06\n",
      "[step: 494] loss: 2.7944304292759625e-06\n",
      "[step: 495] loss: 2.7909447908314178e-06\n",
      "[step: 496] loss: 2.787455514408066e-06\n",
      "[step: 497] loss: 2.783889158308739e-06\n",
      "[step: 498] loss: 2.7803987450170098e-06\n",
      "[step: 499] loss: 2.7769026473833947e-06\n",
      "[step: 500] loss: 2.7734722607419826e-06\n",
      "[step: 501] loss: 2.7699156817106996e-06\n",
      "[step: 502] loss: 2.766429588518804e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 503] loss: 2.762982376225409e-06\n",
      "[step: 504] loss: 2.759520157269435e-06\n",
      "[step: 505] loss: 2.7560365651879692e-06\n",
      "[step: 506] loss: 2.752615728240926e-06\n",
      "[step: 507] loss: 2.7491482796904165e-06\n",
      "[step: 508] loss: 2.745711981333443e-06\n",
      "[step: 509] loss: 2.742332981142681e-06\n",
      "[step: 510] loss: 2.7388671242079e-06\n",
      "[step: 511] loss: 2.735485622906708e-06\n",
      "[step: 512] loss: 2.73201112577226e-06\n",
      "[step: 513] loss: 2.7285650503472425e-06\n",
      "[step: 514] loss: 2.7251614937995328e-06\n",
      "[step: 515] loss: 2.721780447245692e-06\n",
      "[step: 516] loss: 2.718407586144167e-06\n",
      "[step: 517] loss: 2.714986521823448e-06\n",
      "[step: 518] loss: 2.7116548153571784e-06\n",
      "[step: 519] loss: 2.7081748612545198e-06\n",
      "[step: 520] loss: 2.7048263291362673e-06\n",
      "[step: 521] loss: 2.7015362320526037e-06\n",
      "[step: 522] loss: 2.698130629141815e-06\n",
      "[step: 523] loss: 2.6946918296744116e-06\n",
      "[step: 524] loss: 2.6913712645182386e-06\n",
      "[step: 525] loss: 2.6879711185756605e-06\n",
      "[step: 526] loss: 2.6845859792956617e-06\n",
      "[step: 527] loss: 2.6813156637217617e-06\n",
      "[step: 528] loss: 2.6780010102811502e-06\n",
      "[step: 529] loss: 2.6746708954306087e-06\n",
      "[step: 530] loss: 2.6713523766375147e-06\n",
      "[step: 531] loss: 2.6680033897719113e-06\n",
      "[step: 532] loss: 2.664733528945362e-06\n",
      "[step: 533] loss: 2.6613981845002854e-06\n",
      "[step: 534] loss: 2.658106723174569e-06\n",
      "[step: 535] loss: 2.654863919815398e-06\n",
      "[step: 536] loss: 2.651530166986049e-06\n",
      "[step: 537] loss: 2.648243253133842e-06\n",
      "[step: 538] loss: 2.6450379664311185e-06\n",
      "[step: 539] loss: 2.6416892069391906e-06\n",
      "[step: 540] loss: 2.6384209377283696e-06\n",
      "[step: 541] loss: 2.6352020086051198e-06\n",
      "[step: 542] loss: 2.6318709842598764e-06\n",
      "[step: 543] loss: 2.628696847750689e-06\n",
      "[step: 544] loss: 2.6253560463374015e-06\n",
      "[step: 545] loss: 2.622200327095925e-06\n",
      "[step: 546] loss: 2.618984353830456e-06\n",
      "[step: 547] loss: 2.615693574625766e-06\n",
      "[step: 548] loss: 2.6124862415599637e-06\n",
      "[step: 549] loss: 2.6092016014445107e-06\n",
      "[step: 550] loss: 2.6059512947540497e-06\n",
      "[step: 551] loss: 2.6027510102721862e-06\n",
      "[step: 552] loss: 2.5995161649916554e-06\n",
      "[step: 553] loss: 2.596379545138916e-06\n",
      "[step: 554] loss: 2.5931090021913406e-06\n",
      "[step: 555] loss: 2.5900551463564625e-06\n",
      "[step: 556] loss: 2.5867452677630354e-06\n",
      "[step: 557] loss: 2.583616833362612e-06\n",
      "[step: 558] loss: 2.580547061370453e-06\n",
      "[step: 559] loss: 2.5773185825528344e-06\n",
      "[step: 560] loss: 2.5741351237229537e-06\n",
      "[step: 561] loss: 2.57102965406375e-06\n",
      "[step: 562] loss: 2.567912133599748e-06\n",
      "[step: 563] loss: 2.5647486836533062e-06\n",
      "[step: 564] loss: 2.561496785347117e-06\n",
      "[step: 565] loss: 2.558429514465388e-06\n",
      "[step: 566] loss: 2.5552606075507356e-06\n",
      "[step: 567] loss: 2.5521801489958307e-06\n",
      "[step: 568] loss: 2.54909150498861e-06\n",
      "[step: 569] loss: 2.5459205517108785e-06\n",
      "[step: 570] loss: 2.542754145906656e-06\n",
      "[step: 571] loss: 2.5396975615876727e-06\n",
      "[step: 572] loss: 2.536644842621172e-06\n",
      "[step: 573] loss: 2.5335375539725646e-06\n",
      "[step: 574] loss: 2.5304020709882025e-06\n",
      "[step: 575] loss: 2.52744075623923e-06\n",
      "[step: 576] loss: 2.524317324059666e-06\n",
      "[step: 577] loss: 2.5211979846062604e-06\n",
      "[step: 578] loss: 2.5180938791891094e-06\n",
      "[step: 579] loss: 2.5151257432298735e-06\n",
      "[step: 580] loss: 2.511988668629783e-06\n",
      "[step: 581] loss: 2.5089705104619497e-06\n",
      "[step: 582] loss: 2.505895736248931e-06\n",
      "[step: 583] loss: 2.502858933439711e-06\n",
      "[step: 584] loss: 2.4998298613354564e-06\n",
      "[step: 585] loss: 2.496846036592615e-06\n",
      "[step: 586] loss: 2.493837882866501e-06\n",
      "[step: 587] loss: 2.4907403712859377e-06\n",
      "[step: 588] loss: 2.487757456037798e-06\n",
      "[step: 589] loss: 2.4847890927048866e-06\n",
      "[step: 590] loss: 2.481718638591701e-06\n",
      "[step: 591] loss: 2.4787202619336313e-06\n",
      "[step: 592] loss: 2.4757819119258784e-06\n",
      "[step: 593] loss: 2.4727783056732733e-06\n",
      "[step: 594] loss: 2.469812670824467e-06\n",
      "[step: 595] loss: 2.466852038196521e-06\n",
      "[step: 596] loss: 2.4638945888000308e-06\n",
      "[step: 597] loss: 2.460901896483847e-06\n",
      "[step: 598] loss: 2.457918299114681e-06\n",
      "[step: 599] loss: 2.4549565296183573e-06\n",
      "[step: 600] loss: 2.4520350052625872e-06\n",
      "[step: 601] loss: 2.449082558086957e-06\n",
      "[step: 602] loss: 2.4461351131321862e-06\n",
      "[step: 603] loss: 2.4432306418020744e-06\n",
      "[step: 604] loss: 2.4402172584814252e-06\n",
      "[step: 605] loss: 2.437381226627622e-06\n",
      "[step: 606] loss: 2.4344299163203686e-06\n",
      "[step: 607] loss: 2.431459279250703e-06\n",
      "[step: 608] loss: 2.42859277932439e-06\n",
      "[step: 609] loss: 2.4256837605207693e-06\n",
      "[step: 610] loss: 2.4227581434388412e-06\n",
      "[step: 611] loss: 2.4198993742174935e-06\n",
      "[step: 612] loss: 2.417026962575619e-06\n",
      "[step: 613] loss: 2.4141297672031214e-06\n",
      "[step: 614] loss: 2.4112180199153954e-06\n",
      "[step: 615] loss: 2.4084170036076102e-06\n",
      "[step: 616] loss: 2.4055011635937262e-06\n",
      "[step: 617] loss: 2.402662630629493e-06\n",
      "[step: 618] loss: 2.399758614046732e-06\n",
      "[step: 619] loss: 2.396937134108157e-06\n",
      "[step: 620] loss: 2.3940128812682815e-06\n",
      "[step: 621] loss: 2.3912371034384705e-06\n",
      "[step: 622] loss: 2.3883233097876655e-06\n",
      "[step: 623] loss: 2.385562083873083e-06\n",
      "[step: 624] loss: 2.382799721090123e-06\n",
      "[step: 625] loss: 2.3799170776328538e-06\n",
      "[step: 626] loss: 2.3770812731527258e-06\n",
      "[step: 627] loss: 2.37427730098716e-06\n",
      "[step: 628] loss: 2.371439450143953e-06\n",
      "[step: 629] loss: 2.3686720851401333e-06\n",
      "[step: 630] loss: 2.3658390091441106e-06\n",
      "[step: 631] loss: 2.3631657768419245e-06\n",
      "[step: 632] loss: 2.360307689741603e-06\n",
      "[step: 633] loss: 2.357531229790766e-06\n",
      "[step: 634] loss: 2.3547777345811483e-06\n",
      "[step: 635] loss: 2.351954208279494e-06\n",
      "[step: 636] loss: 2.349231181142386e-06\n",
      "[step: 637] loss: 2.346486553506111e-06\n",
      "[step: 638] loss: 2.3436787159880623e-06\n",
      "[step: 639] loss: 2.3409152163367253e-06\n",
      "[step: 640] loss: 2.3381683149636956e-06\n",
      "[step: 641] loss: 2.3354959921562113e-06\n",
      "[step: 642] loss: 2.332701114937663e-06\n",
      "[step: 643] loss: 2.329949438717449e-06\n",
      "[step: 644] loss: 2.3272539237950696e-06\n",
      "[step: 645] loss: 2.3245477223099442e-06\n",
      "[step: 646] loss: 2.321776491953642e-06\n",
      "[step: 647] loss: 2.3190846150100697e-06\n",
      "[step: 648] loss: 2.3163370315160137e-06\n",
      "[step: 649] loss: 2.3136217350838706e-06\n",
      "[step: 650] loss: 2.3109721496439306e-06\n",
      "[step: 651] loss: 2.3082609459379455e-06\n",
      "[step: 652] loss: 2.30559317060397e-06\n",
      "[step: 653] loss: 2.3029817839415045e-06\n",
      "[step: 654] loss: 2.3001673525868682e-06\n",
      "[step: 655] loss: 2.2975609681452624e-06\n",
      "[step: 656] loss: 2.2948961486690678e-06\n",
      "[step: 657] loss: 2.292246108481777e-06\n",
      "[step: 658] loss: 2.289602662131074e-06\n",
      "[step: 659] loss: 2.2869319309393177e-06\n",
      "[step: 660] loss: 2.2842375528853154e-06\n",
      "[step: 661] loss: 2.2816002456238493e-06\n",
      "[step: 662] loss: 2.2789768081565853e-06\n",
      "[step: 663] loss: 2.2763360902899876e-06\n",
      "[step: 664] loss: 2.2737567633157596e-06\n",
      "[step: 665] loss: 2.2711460587743204e-06\n",
      "[step: 666] loss: 2.2684348550683353e-06\n",
      "[step: 667] loss: 2.2658796297037043e-06\n",
      "[step: 668] loss: 2.2632875698036514e-06\n",
      "[step: 669] loss: 2.260649580421159e-06\n",
      "[step: 670] loss: 2.2580350105272373e-06\n",
      "[step: 671] loss: 2.2554959286935627e-06\n",
      "[step: 672] loss: 2.2528959107148694e-06\n",
      "[step: 673] loss: 2.2503131731355097e-06\n",
      "[step: 674] loss: 2.2476306185126305e-06\n",
      "[step: 675] loss: 2.2451697532233084e-06\n",
      "[step: 676] loss: 2.242566097265808e-06\n",
      "[step: 677] loss: 2.2399553927243687e-06\n",
      "[step: 678] loss: 2.2374099444277817e-06\n",
      "[step: 679] loss: 2.2348895072354935e-06\n",
      "[step: 680] loss: 2.2323285975289764e-06\n",
      "[step: 681] loss: 2.229779283879907e-06\n",
      "[step: 682] loss: 2.227227241746732e-06\n",
      "[step: 683] loss: 2.2246399566938635e-06\n",
      "[step: 684] loss: 2.2222320694709197e-06\n",
      "[step: 685] loss: 2.2196902591531398e-06\n",
      "[step: 686] loss: 2.2170584088598844e-06\n",
      "[step: 687] loss: 2.2145761704450706e-06\n",
      "[step: 688] loss: 2.2120307221484836e-06\n",
      "[step: 689] loss: 2.209577814937802e-06\n",
      "[step: 690] loss: 2.207012357757776e-06\n",
      "[step: 691] loss: 2.2045201149012428e-06\n",
      "[step: 692] loss: 2.202019459218718e-06\n",
      "[step: 693] loss: 2.199528807977913e-06\n",
      "[step: 694] loss: 2.1970067791698966e-06\n",
      "[step: 695] loss: 2.1945679691270925e-06\n",
      "[step: 696] loss: 2.192072997786454e-06\n",
      "[step: 697] loss: 2.1896028101764387e-06\n",
      "[step: 698] loss: 2.187147401855327e-06\n",
      "[step: 699] loss: 2.1847251900908304e-06\n",
      "[step: 700] loss: 2.1822161215823144e-06\n",
      "[step: 701] loss: 2.1798205125378445e-06\n",
      "[step: 702] loss: 2.1772921172669157e-06\n",
      "[step: 703] loss: 2.1749024199380074e-06\n",
      "[step: 704] loss: 2.1724761154473526e-06\n",
      "[step: 705] loss: 2.1699304397770902e-06\n",
      "[step: 706] loss: 2.167517322959611e-06\n",
      "[step: 707] loss: 2.165080104532535e-06\n",
      "[step: 708] loss: 2.1626610759994946e-06\n",
      "[step: 709] loss: 2.1602868400805164e-06\n",
      "[step: 710] loss: 2.157827793780598e-06\n",
      "[step: 711] loss: 2.1553585156652844e-06\n",
      "[step: 712] loss: 2.153085233658203e-06\n",
      "[step: 713] loss: 2.1505841232283274e-06\n",
      "[step: 714] loss: 2.1482476313394727e-06\n",
      "[step: 715] loss: 2.1458383798744762e-06\n",
      "[step: 716] loss: 2.1434650534501998e-06\n",
      "[step: 717] loss: 2.1410994577308884e-06\n",
      "[step: 718] loss: 2.1387029391917167e-06\n",
      "[step: 719] loss: 2.136267994501395e-06\n",
      "[step: 720] loss: 2.133910811608075e-06\n",
      "[step: 721] loss: 2.131554083462106e-06\n",
      "[step: 722] loss: 2.1292767087288667e-06\n",
      "[step: 723] loss: 2.126873596353107e-06\n",
      "[step: 724] loss: 2.124493221344892e-06\n",
      "[step: 725] loss: 2.1220871531113517e-06\n",
      "[step: 726] loss: 2.1197618025325937e-06\n",
      "[step: 727] loss: 2.1174446374061517e-06\n",
      "[step: 728] loss: 2.1151215605641482e-06\n",
      "[step: 729] loss: 2.1127593754499685e-06\n",
      "[step: 730] loss: 2.1105154246470192e-06\n",
      "[step: 731] loss: 2.1081020804558648e-06\n",
      "[step: 732] loss: 2.1058206129964674e-06\n",
      "[step: 733] loss: 2.10344933293527e-06\n",
      "[step: 734] loss: 2.1011683202232234e-06\n",
      "[step: 735] loss: 2.0989095901313704e-06\n",
      "[step: 736] loss: 2.0965273961337516e-06\n",
      "[step: 737] loss: 2.094320279866224e-06\n",
      "[step: 738] loss: 2.092011300192098e-06\n",
      "[step: 739] loss: 2.089743247779552e-06\n",
      "[step: 740] loss: 2.0874078927590745e-06\n",
      "[step: 741] loss: 2.0851382487308e-06\n",
      "[step: 742] loss: 2.082859509755508e-06\n",
      "[step: 743] loss: 2.0806312477361644e-06\n",
      "[step: 744] loss: 2.0783268155355472e-06\n",
      "[step: 745] loss: 2.076071041301475e-06\n",
      "[step: 746] loss: 2.0737952581839636e-06\n",
      "[step: 747] loss: 2.071523340418935e-06\n",
      "[step: 748] loss: 2.0692634734587045e-06\n",
      "[step: 749] loss: 2.0670720459747827e-06\n",
      "[step: 750] loss: 2.0648346890084213e-06\n",
      "[step: 751] loss: 2.0625341221602866e-06\n",
      "[step: 752] loss: 2.0603281427611364e-06\n",
      "[step: 753] loss: 2.0581003354891436e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 754] loss: 2.0558268261083867e-06\n",
      "[step: 755] loss: 2.053641537713702e-06\n",
      "[step: 756] loss: 2.051441470030113e-06\n",
      "[step: 757] loss: 2.049182285190909e-06\n",
      "[step: 758] loss: 2.047068164756638e-06\n",
      "[step: 759] loss: 2.0447921542654512e-06\n",
      "[step: 760] loss: 2.0426130049600033e-06\n",
      "[step: 761] loss: 2.040389063040493e-06\n",
      "[step: 762] loss: 2.0382049115141854e-06\n",
      "[step: 763] loss: 2.036031673924299e-06\n",
      "[step: 764] loss: 2.0338486592663685e-06\n",
      "[step: 765] loss: 2.0316488189564552e-06\n",
      "[step: 766] loss: 2.029441930062603e-06\n",
      "[step: 767] loss: 2.0272593701520236e-06\n",
      "[step: 768] loss: 2.0251006844773656e-06\n",
      "[step: 769] loss: 2.022934040724067e-06\n",
      "[step: 770] loss: 2.020685542447609e-06\n",
      "[step: 771] loss: 2.0186153051326983e-06\n",
      "[step: 772] loss: 2.0164568468317157e-06\n",
      "[step: 773] loss: 2.014274286921136e-06\n",
      "[step: 774] loss: 2.0121967736486113e-06\n",
      "[step: 775] loss: 2.009911213463056e-06\n",
      "[step: 776] loss: 2.0078857687622076e-06\n",
      "[step: 777] loss: 2.00573504116619e-06\n",
      "[step: 778] loss: 2.003624558710726e-06\n",
      "[step: 779] loss: 2.00150020646106e-06\n",
      "[step: 780] loss: 1.999353344217525e-06\n",
      "[step: 781] loss: 1.997253320951131e-06\n",
      "[step: 782] loss: 1.9951414742536144e-06\n",
      "[step: 783] loss: 1.9929752852476668e-06\n",
      "[step: 784] loss: 1.9909246020688443e-06\n",
      "[step: 785] loss: 1.9888343558704946e-06\n",
      "[step: 786] loss: 1.9867668470396893e-06\n",
      "[step: 787] loss: 1.984602477023145e-06\n",
      "[step: 788] loss: 1.982541789402603e-06\n",
      "[step: 789] loss: 1.9803956092800945e-06\n",
      "[step: 790] loss: 1.978368118216167e-06\n",
      "[step: 791] loss: 1.976332441699924e-06\n",
      "[step: 792] loss: 1.974230826817802e-06\n",
      "[step: 793] loss: 1.972114205273101e-06\n",
      "[step: 794] loss: 1.9700787561305333e-06\n",
      "[step: 795] loss: 1.9679437173181213e-06\n",
      "[step: 796] loss: 1.96590895029658e-06\n",
      "[step: 797] loss: 1.9639292077044956e-06\n",
      "[step: 798] loss: 1.961788257176522e-06\n",
      "[step: 799] loss: 1.9597609934862703e-06\n",
      "[step: 800] loss: 1.9577009879867546e-06\n",
      "[step: 801] loss: 1.9556316601665458e-06\n",
      "[step: 802] loss: 1.953658738784725e-06\n",
      "[step: 803] loss: 1.9515814528858755e-06\n",
      "[step: 804] loss: 1.949549186974764e-06\n",
      "[step: 805] loss: 1.9475176031846786e-06\n",
      "[step: 806] loss: 1.9454721496003913e-06\n",
      "[step: 807] loss: 1.9435167359915795e-06\n",
      "[step: 808] loss: 1.94145741261309e-06\n",
      "[step: 809] loss: 1.9394369701331016e-06\n",
      "[step: 810] loss: 1.9374399471416837e-06\n",
      "[step: 811] loss: 1.9354531559656607e-06\n",
      "[step: 812] loss: 1.9334124772285577e-06\n",
      "[step: 813] loss: 1.9314359178679297e-06\n",
      "[step: 814] loss: 1.9295030142529868e-06\n",
      "[step: 815] loss: 1.9274566511739977e-06\n",
      "[step: 816] loss: 1.925364358612569e-06\n",
      "[step: 817] loss: 1.9234910269005923e-06\n",
      "[step: 818] loss: 1.921490820677718e-06\n",
      "[step: 819] loss: 1.919490159707493e-06\n",
      "[step: 820] loss: 1.917575673360261e-06\n",
      "[step: 821] loss: 1.9156141206622124e-06\n",
      "[step: 822] loss: 1.9135500224365387e-06\n",
      "[step: 823] loss: 1.911640310936491e-06\n",
      "[step: 824] loss: 1.909708998937276e-06\n",
      "[step: 825] loss: 1.9077233446296304e-06\n",
      "[step: 826] loss: 1.9058454654441448e-06\n",
      "[step: 827] loss: 1.9038305936192046e-06\n",
      "[step: 828] loss: 1.9019099681827356e-06\n",
      "[step: 829] loss: 1.8999573967448669e-06\n",
      "[step: 830] loss: 1.8979854985445854e-06\n",
      "[step: 831] loss: 1.8960730585604324e-06\n",
      "[step: 832] loss: 1.8941885855383589e-06\n",
      "[step: 833] loss: 1.8922578419733327e-06\n",
      "[step: 834] loss: 1.8903036789197358e-06\n",
      "[step: 835] loss: 1.8883479242504109e-06\n",
      "[step: 836] loss: 1.8864781168304035e-06\n",
      "[step: 837] loss: 1.8845129261535476e-06\n",
      "[step: 838] loss: 1.8825694496626966e-06\n",
      "[step: 839] loss: 1.8807102151185973e-06\n",
      "[step: 840] loss: 1.878744797068066e-06\n",
      "[step: 841] loss: 1.8769121652439935e-06\n",
      "[step: 842] loss: 1.8749768742054584e-06\n",
      "[step: 843] loss: 1.873106270977587e-06\n",
      "[step: 844] loss: 1.8712806877374533e-06\n",
      "[step: 845] loss: 1.8693597212404711e-06\n",
      "[step: 846] loss: 1.8674978718991042e-06\n",
      "[step: 847] loss: 1.8656004385775304e-06\n",
      "[step: 848] loss: 1.8637105085872463e-06\n",
      "[step: 849] loss: 1.861805117187032e-06\n",
      "[step: 850] loss: 1.8599810118757887e-06\n",
      "[step: 851] loss: 1.858070504567877e-06\n",
      "[step: 852] loss: 1.8561977412900887e-06\n",
      "[step: 853] loss: 1.85436442734499e-06\n",
      "[step: 854] loss: 1.8524635834182845e-06\n",
      "[step: 855] loss: 1.8505952539271675e-06\n",
      "[step: 856] loss: 1.848777742452512e-06\n",
      "[step: 857] loss: 1.846979671427107e-06\n",
      "[step: 858] loss: 1.8450781453793752e-06\n",
      "[step: 859] loss: 1.843254381128645e-06\n",
      "[step: 860] loss: 1.841478365349758e-06\n",
      "[step: 861] loss: 1.8395919596514432e-06\n",
      "[step: 862] loss: 1.837812305893749e-06\n",
      "[step: 863] loss: 1.8359493196840049e-06\n",
      "[step: 864] loss: 1.834107479226077e-06\n",
      "[step: 865] loss: 1.8322912183066364e-06\n",
      "[step: 866] loss: 1.8304940567759331e-06\n",
      "[step: 867] loss: 1.8286312979398645e-06\n",
      "[step: 868] loss: 1.8268614212502143e-06\n",
      "[step: 869] loss: 1.8250079847348388e-06\n",
      "[step: 870] loss: 1.8232315142086009e-06\n",
      "[step: 871] loss: 1.8214146848549717e-06\n",
      "[step: 872] loss: 1.8196686824012431e-06\n",
      "[step: 873] loss: 1.8178801610702067e-06\n",
      "[step: 874] loss: 1.8160752688345383e-06\n",
      "[step: 875] loss: 1.8142500266549177e-06\n",
      "[step: 876] loss: 1.8124601410818286e-06\n",
      "[step: 877] loss: 1.8106314882970764e-06\n",
      "[step: 878] loss: 1.808920160328853e-06\n",
      "[step: 879] loss: 1.8071409613185097e-06\n",
      "[step: 880] loss: 1.8053804069495527e-06\n",
      "[step: 881] loss: 1.8035505036095856e-06\n",
      "[step: 882] loss: 1.8018201899394626e-06\n",
      "[step: 883] loss: 1.8000671389017953e-06\n",
      "[step: 884] loss: 1.79831647528772e-06\n",
      "[step: 885] loss: 1.796576384549553e-06\n",
      "[step: 886] loss: 1.7948125332623022e-06\n",
      "[step: 887] loss: 1.7930371996044414e-06\n",
      "[step: 888] loss: 1.7912905150296865e-06\n",
      "[step: 889] loss: 1.7895904420583975e-06\n",
      "[step: 890] loss: 1.7878361404655152e-06\n",
      "[step: 891] loss: 1.7860718344309134e-06\n",
      "[step: 892] loss: 1.7842734223449952e-06\n",
      "[step: 893] loss: 1.7825855138653424e-06\n",
      "[step: 894] loss: 1.7808681604947196e-06\n",
      "[step: 895] loss: 1.7791328446037369e-06\n",
      "[step: 896] loss: 1.7774422076399787e-06\n",
      "[step: 897] loss: 1.7756696024662233e-06\n",
      "[step: 898] loss: 1.7739611166689429e-06\n",
      "[step: 899] loss: 1.7722348957249778e-06\n",
      "[step: 900] loss: 1.7704933270579204e-06\n",
      "[step: 901] loss: 1.7687855233816663e-06\n",
      "[step: 902] loss: 1.7670446368356352e-06\n",
      "[step: 903] loss: 1.7653666191108641e-06\n",
      "[step: 904] loss: 1.7637083828958566e-06\n",
      "[step: 905] loss: 1.762009333106107e-06\n",
      "[step: 906] loss: 1.7602928892301861e-06\n",
      "[step: 907] loss: 1.7586309013495338e-06\n",
      "[step: 908] loss: 1.756867050062283e-06\n",
      "[step: 909] loss: 1.7552562212586054e-06\n",
      "[step: 910] loss: 1.7535890037834179e-06\n",
      "[step: 911] loss: 1.7518661934445845e-06\n",
      "[step: 912] loss: 1.7501429283584002e-06\n",
      "[step: 913] loss: 1.748509475874016e-06\n",
      "[step: 914] loss: 1.7468379382989951e-06\n",
      "[step: 915] loss: 1.7451602616347373e-06\n",
      "[step: 916] loss: 1.7434653045711457e-06\n",
      "[step: 917] loss: 1.7418370816812967e-06\n",
      "[step: 918] loss: 1.7401532659278018e-06\n",
      "[step: 919] loss: 1.738522087180172e-06\n",
      "[step: 920] loss: 1.7368585076837917e-06\n",
      "[step: 921] loss: 1.7351878796034725e-06\n",
      "[step: 922] loss: 1.7335854636257864e-06\n",
      "[step: 923] loss: 1.731916995595384e-06\n",
      "[step: 924] loss: 1.730248300191306e-06\n",
      "[step: 925] loss: 1.7286342881561723e-06\n",
      "[step: 926] loss: 1.7269644558837172e-06\n",
      "[step: 927] loss: 1.7253495343538816e-06\n",
      "[step: 928] loss: 1.7237252905033529e-06\n",
      "[step: 929] loss: 1.7220556856045732e-06\n",
      "[step: 930] loss: 1.7204707773998962e-06\n",
      "[step: 931] loss: 1.7187890080094803e-06\n",
      "[step: 932] loss: 1.7172363868667162e-06\n",
      "[step: 933] loss: 1.7155897467091563e-06\n",
      "[step: 934] loss: 1.7139863075499306e-06\n",
      "[step: 935] loss: 1.7123225006798748e-06\n",
      "[step: 936] loss: 1.710711785563035e-06\n",
      "[step: 937] loss: 1.7091294921556255e-06\n",
      "[step: 938] loss: 1.7074885363399517e-06\n",
      "[step: 939] loss: 1.705894078440906e-06\n",
      "[step: 940] loss: 1.704297687865619e-06\n",
      "[step: 941] loss: 1.702714826024021e-06\n",
      "[step: 942] loss: 1.7011369664032827e-06\n",
      "[step: 943] loss: 1.6995477380987722e-06\n",
      "[step: 944] loss: 1.6979332713162876e-06\n",
      "[step: 945] loss: 1.6963820144155761e-06\n",
      "[step: 946] loss: 1.6947990388871403e-06\n",
      "[step: 947] loss: 1.6932017388171516e-06\n",
      "[step: 948] loss: 1.6915714695642237e-06\n",
      "[step: 949] loss: 1.6900445416467846e-06\n",
      "[step: 950] loss: 1.6884351907719974e-06\n",
      "[step: 951] loss: 1.6869111050255015e-06\n",
      "[step: 952] loss: 1.685312099652947e-06\n",
      "[step: 953] loss: 1.683730374679726e-06\n",
      "[step: 954] loss: 1.6821778672237997e-06\n",
      "[step: 955] loss: 1.6805906852823682e-06\n",
      "[step: 956] loss: 1.6790550034784246e-06\n",
      "[step: 957] loss: 1.6775028370830114e-06\n",
      "[step: 958] loss: 1.6759032632762683e-06\n",
      "[step: 959] loss: 1.67443317877769e-06\n",
      "[step: 960] loss: 1.6728133687138325e-06\n",
      "[step: 961] loss: 1.671311110840179e-06\n",
      "[step: 962] loss: 1.6697791807018803e-06\n",
      "[step: 963] loss: 1.6682374734955374e-06\n",
      "[step: 964] loss: 1.6666756437189179e-06\n",
      "[step: 965] loss: 1.6651681562507292e-06\n",
      "[step: 966] loss: 1.6636136024317238e-06\n",
      "[step: 967] loss: 1.6620944052192499e-06\n",
      "[step: 968] loss: 1.6605315522610908e-06\n",
      "[step: 969] loss: 1.6590799987170612e-06\n",
      "[step: 970] loss: 1.657472694205353e-06\n",
      "[step: 971] loss: 1.6559924915782176e-06\n",
      "[step: 972] loss: 1.654489096836187e-06\n",
      "[step: 973] loss: 1.6529930917386082e-06\n",
      "[step: 974] loss: 1.6514336493855808e-06\n",
      "[step: 975] loss: 1.6499295725225238e-06\n",
      "[step: 976] loss: 1.6484211755596334e-06\n",
      "[step: 977] loss: 1.6469439287902787e-06\n",
      "[step: 978] loss: 1.6454147271360853e-06\n",
      "[step: 979] loss: 1.6438988268419052e-06\n",
      "[step: 980] loss: 1.6424003206338966e-06\n",
      "[step: 981] loss: 1.6408753253926989e-06\n",
      "[step: 982] loss: 1.6394303656852571e-06\n",
      "[step: 983] loss: 1.637901050344226e-06\n",
      "[step: 984] loss: 1.6364203929697396e-06\n",
      "[step: 985] loss: 1.6349773659385392e-06\n",
      "[step: 986] loss: 1.6334339534296305e-06\n",
      "[step: 987] loss: 1.6319820588250877e-06\n",
      "[step: 988] loss: 1.630491397008882e-06\n",
      "[step: 989] loss: 1.6290696294163354e-06\n",
      "[step: 990] loss: 1.627547703719756e-06\n",
      "[step: 991] loss: 1.626039193070028e-06\n",
      "[step: 992] loss: 1.6246067389147356e-06\n",
      "[step: 993] loss: 1.62313153850846e-06\n",
      "[step: 994] loss: 1.6216836229432374e-06\n",
      "[step: 995] loss: 1.6201877315324964e-06\n",
      "[step: 996] loss: 1.6187376559173572e-06\n",
      "[step: 997] loss: 1.6172484720300417e-06\n",
      "[step: 998] loss: 1.615751671124599e-06\n",
      "[step: 999] loss: 1.6143825405379175e-06\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testY is:  [33670.824999999997, 22936.107999999997, 31400.029999999999, 17698.244999999999, 16729.308000000001, 16617.197499999998]\n",
      "\n",
      "\n",
      "LSTM testforecast : [20659.65165046417, 21041.465883492256, 42424.21822808135, 25271.560698865589, 32790.586842107092, 16207.489377113679] \n",
      "@@@@@LSTM rmse:  10082.1680558\n",
      "Bayseian testforecast : [20953.691612653518, 18360.313809394112, 36190.737818519941, 17831.411781722676, 15210.407792773669, 15590.762041885999] \n",
      "@@@@@Bayseian rmse:  5901.87281646\n",
      "\n",
      "\n",
      "Bayseian WON!!!!!!\n",
      "[step: 0] loss: 0.1272420734167099\n",
      "[step: 1] loss: 0.008132096379995346\n",
      "[step: 2] loss: 0.04031354933977127\n",
      "[step: 3] loss: 0.06276451796293259\n",
      "[step: 4] loss: 0.03878691792488098\n",
      "[step: 5] loss: 0.009004450403153896\n",
      "[step: 6] loss: 0.0003044874465558678\n",
      "[step: 7] loss: 0.011315764859318733\n",
      "[step: 8] loss: 0.02378539741039276\n",
      "[step: 9] loss: 0.02541215717792511\n",
      "[step: 10] loss: 0.0175168476998806\n",
      "[step: 11] loss: 0.007338566705584526\n",
      "[step: 12] loss: 0.0010996346827596426\n",
      "[step: 13] loss: 0.0011889509623870254\n",
      "[step: 14] loss: 0.00581186031922698\n",
      "[step: 15] loss: 0.010528950951993465\n",
      "[step: 16] loss: 0.011527659371495247\n",
      "[step: 17] loss: 0.008401176892220974\n",
      "[step: 18] loss: 0.003782326355576515\n",
      "[step: 19] loss: 0.0006803475553169847\n",
      "[step: 20] loss: 0.0003494972479529679\n",
      "[step: 21] loss: 0.0020736772567033768\n",
      "[step: 22] loss: 0.004198990296572447\n",
      "[step: 23] loss: 0.005225056782364845\n",
      "[step: 24] loss: 0.004522437229752541\n",
      "[step: 25] loss: 0.0026005562394857407\n",
      "[step: 26] loss: 0.0007285700994543731\n",
      "[step: 27] loss: 9.18757632462075e-06\n",
      "[step: 28] loss: 0.0005851318710483611\n",
      "[step: 29] loss: 0.0016859944444149733\n",
      "[step: 30] loss: 0.0023945996072143316\n",
      "[step: 31] loss: 0.00228867051191628\n",
      "[step: 32] loss: 0.0015187282115221024\n",
      "[step: 33] loss: 0.0005856409552507102\n",
      "[step: 34] loss: 4.405579238664359e-05\n",
      "[step: 35] loss: 0.00015477403940167278\n",
      "[step: 36] loss: 0.0006896934355609119\n",
      "[step: 37] loss: 0.0011381623335182667\n",
      "[step: 38] loss: 0.00115393556188792\n",
      "[step: 39] loss: 0.0007848786772228777\n",
      "[step: 40] loss: 0.00032321669277735054\n",
      "[step: 41] loss: 4.577029176289216e-05\n",
      "[step: 42] loss: 7.284082676051185e-05\n",
      "[step: 43] loss: 0.000323084561387077\n",
      "[step: 44] loss: 0.0005601245211437345\n",
      "[step: 45] loss: 0.0005773062002845109\n",
      "[step: 46] loss: 0.0003760753315873444\n",
      "[step: 47] loss: 0.00013344743638299406\n",
      "[step: 48] loss: 1.5805690054548904e-05\n",
      "[step: 49] loss: 5.9768350183730945e-05\n",
      "[step: 50] loss: 0.00019076628086622804\n",
      "[step: 51] loss: 0.0002918528625741601\n",
      "[step: 52] loss: 0.0002768933482002467\n",
      "[step: 53] loss: 0.00015869768685661256\n",
      "[step: 54] loss: 3.9167414797702804e-05\n",
      "[step: 55] loss: 5.78758545088931e-06\n",
      "[step: 56] loss: 5.6161854445235804e-05\n",
      "[step: 57] loss: 0.00012569825048558414\n",
      "[step: 58] loss: 0.00015365447325166315\n",
      "[step: 59] loss: 0.00011974827793892473\n",
      "[step: 60] loss: 5.1786300900857896e-05\n",
      "[step: 61] loss: 5.310450433171354e-06\n",
      "[step: 62] loss: 1.2678344319283497e-05\n",
      "[step: 63] loss: 5.2394465456018224e-05\n",
      "[step: 64] loss: 7.983713294379413e-05\n",
      "[step: 65] loss: 7.286238542292267e-05\n",
      "[step: 66] loss: 4.094386531505734e-05\n",
      "[step: 67] loss: 1.0061356078949757e-05\n",
      "[step: 68] loss: 3.258672904848936e-06\n",
      "[step: 69] loss: 2.126997605955694e-05\n",
      "[step: 70] loss: 4.127630018047057e-05\n",
      "[step: 71] loss: 4.240286216372624e-05\n",
      "[step: 72] loss: 2.6240119041176513e-05\n",
      "[step: 73] loss: 8.600427463534288e-06\n",
      "[step: 74] loss: 2.500255959603237e-06\n",
      "[step: 75] loss: 1.0083878805744462e-05\n",
      "[step: 76] loss: 2.1659092453774065e-05\n",
      "[step: 77] loss: 2.4375698558287695e-05\n",
      "[step: 78] loss: 1.5917814380372874e-05\n",
      "[step: 79] loss: 5.62455716135446e-06\n",
      "[step: 80] loss: 2.216624807260814e-06\n",
      "[step: 81] loss: 6.2373119362746365e-06\n",
      "[step: 82] loss: 1.2303871699259616e-05\n",
      "[step: 83] loss: 1.39545627462212e-05\n",
      "[step: 84] loss: 9.388890248374082e-06\n",
      "[step: 85] loss: 3.536981239449233e-06\n",
      "[step: 86] loss: 1.931768565555103e-06\n",
      "[step: 87] loss: 4.615204034053022e-06\n",
      "[step: 88] loss: 7.760299922665581e-06\n",
      "[step: 89] loss: 8.166139195964206e-06\n",
      "[step: 90] loss: 5.469583811645862e-06\n",
      "[step: 91] loss: 2.3779871298756916e-06\n",
      "[step: 92] loss: 1.8831959778253804e-06\n",
      "[step: 93] loss: 3.724592033904628e-06\n",
      "[step: 94] loss: 5.291631168802269e-06\n",
      "[step: 95] loss: 4.986030035070144e-06\n",
      "[step: 96] loss: 3.292123892606469e-06\n",
      "[step: 97] loss: 1.8488923387849354e-06\n",
      "[step: 98] loss: 1.9617843918240396e-06\n",
      "[step: 99] loss: 3.1462184324482223e-06\n",
      "[step: 100] loss: 3.7816005260538077e-06\n",
      "[step: 101] loss: 3.229268259019591e-06\n",
      "[step: 102] loss: 2.212069375673309e-06\n",
      "[step: 103] loss: 1.6946997902778094e-06\n",
      "[step: 104] loss: 2.037121248577023e-06\n",
      "[step: 105] loss: 2.695486728043761e-06\n",
      "[step: 106] loss: 2.805533313221531e-06\n",
      "[step: 107] loss: 2.281394472447573e-06\n",
      "[step: 108] loss: 1.7659333479969064e-06\n",
      "[step: 109] loss: 1.7131525282820803e-06\n",
      "[step: 110] loss: 2.0394629700604128e-06\n",
      "[step: 111] loss: 2.3110028450901154e-06\n",
      "[step: 112] loss: 2.1827333966939477e-06\n",
      "[step: 113] loss: 1.8202493947683251e-06\n",
      "[step: 114] loss: 1.6446787185486755e-06\n",
      "[step: 115] loss: 1.7630181901040487e-06\n",
      "[step: 116] loss: 1.959746896318393e-06\n",
      "[step: 117] loss: 1.995107822949649e-06\n",
      "[step: 118] loss: 1.822230501602462e-06\n",
      "[step: 119] loss: 1.6442681953776628e-06\n",
      "[step: 120] loss: 1.6501004438396194e-06\n",
      "[step: 121] loss: 1.7707512824927107e-06\n",
      "[step: 122] loss: 1.8323206631976063e-06\n",
      "[step: 123] loss: 1.768979586813657e-06\n",
      "[step: 124] loss: 1.6515072047695867e-06\n",
      "[step: 125] loss: 1.6090519920908264e-06\n",
      "[step: 126] loss: 1.6687291690686834e-06\n",
      "[step: 127] loss: 1.7258430489164311e-06\n",
      "[step: 128] loss: 1.7061778407878592e-06\n",
      "[step: 129] loss: 1.6393602209063829e-06\n",
      "[step: 130] loss: 1.5962145880621392e-06\n",
      "[step: 131] loss: 1.615279529687541e-06\n",
      "[step: 132] loss: 1.6564221141379676e-06\n",
      "[step: 133] loss: 1.6562158862143406e-06\n",
      "[step: 134] loss: 1.6171733250303078e-06\n",
      "[step: 135] loss: 1.5856836625971482e-06\n",
      "[step: 136] loss: 1.5880841601756401e-06\n",
      "[step: 137] loss: 1.611216589481046e-06\n",
      "[step: 138] loss: 1.6175614518942893e-06\n",
      "[step: 139] loss: 1.5954148011587677e-06\n",
      "[step: 140] loss: 1.5726540141258738e-06\n",
      "[step: 141] loss: 1.5705272744526155e-06\n",
      "[step: 142] loss: 1.5820868384253117e-06\n",
      "[step: 143] loss: 1.5873688425926957e-06\n",
      "[step: 144] loss: 1.575412966303702e-06\n",
      "[step: 145] loss: 1.5592314639434335e-06\n",
      "[step: 146] loss: 1.5556997823296115e-06\n",
      "[step: 147] loss: 1.5614632502547465e-06\n",
      "[step: 148] loss: 1.5640226820323733e-06\n",
      "[step: 149] loss: 1.5569746665278217e-06\n",
      "[step: 150] loss: 1.545982513562194e-06\n",
      "[step: 151] loss: 1.5419964256579988e-06\n",
      "[step: 152] loss: 1.544736164760252e-06\n",
      "[step: 153] loss: 1.5452095567525248e-06\n",
      "[step: 154] loss: 1.5401401469716802e-06\n",
      "[step: 155] loss: 1.5328628251154441e-06\n",
      "[step: 156] loss: 1.529120481791324e-06\n",
      "[step: 157] loss: 1.5297099480449106e-06\n",
      "[step: 158] loss: 1.5291860790966894e-06\n",
      "[step: 159] loss: 1.52493373661855e-06\n",
      "[step: 160] loss: 1.5195760170172434e-06\n",
      "[step: 161] loss: 1.5164150681812316e-06\n",
      "[step: 162] loss: 1.5157795587583678e-06\n",
      "[step: 163] loss: 1.5144892131502274e-06\n",
      "[step: 164] loss: 1.5106819546417682e-06\n",
      "[step: 165] loss: 1.5065935485836235e-06\n",
      "[step: 166] loss: 1.5038468745842692e-06\n",
      "[step: 167] loss: 1.5024622825876577e-06\n",
      "[step: 168] loss: 1.500646476415568e-06\n",
      "[step: 169] loss: 1.4973187489886186e-06\n",
      "[step: 170] loss: 1.4938484582671663e-06\n",
      "[step: 171] loss: 1.4914122630216298e-06\n",
      "[step: 172] loss: 1.489466626480862e-06\n",
      "[step: 173] loss: 1.4872771316731814e-06\n",
      "[step: 174] loss: 1.4842604514342383e-06\n",
      "[step: 175] loss: 1.4812188737778342e-06\n",
      "[step: 176] loss: 1.4788628277528915e-06\n",
      "[step: 177] loss: 1.476865236327285e-06\n",
      "[step: 178] loss: 1.4743623069080058e-06\n",
      "[step: 179] loss: 1.47154037222208e-06\n",
      "[step: 180] loss: 1.4687418570247246e-06\n",
      "[step: 181] loss: 1.4664734635516652e-06\n",
      "[step: 182] loss: 1.4641879033661098e-06\n",
      "[step: 183] loss: 1.4617279475714895e-06\n",
      "[step: 184] loss: 1.4589636521122884e-06\n",
      "[step: 185] loss: 1.4563389640898095e-06\n",
      "[step: 186] loss: 1.4540460142598022e-06\n",
      "[step: 187] loss: 1.4517111139866756e-06\n",
      "[step: 188] loss: 1.449189653612848e-06\n",
      "[step: 189] loss: 1.4466736502072308e-06\n",
      "[step: 190] loss: 1.4441163784795208e-06\n",
      "[step: 191] loss: 1.4418147884498467e-06\n",
      "[step: 192] loss: 1.4393621086128405e-06\n",
      "[step: 193] loss: 1.4368592928803992e-06\n",
      "[step: 194] loss: 1.4343805787575548e-06\n",
      "[step: 195] loss: 1.4319176671051537e-06\n",
      "[step: 196] loss: 1.4295416121967719e-06\n",
      "[step: 197] loss: 1.4271064401327749e-06\n",
      "[step: 198] loss: 1.424615675205132e-06\n",
      "[step: 199] loss: 1.4221609490050469e-06\n",
      "[step: 200] loss: 1.4197731843523798e-06\n",
      "[step: 201] loss: 1.4174516991261044e-06\n",
      "[step: 202] loss: 1.414967641721887e-06\n",
      "[step: 203] loss: 1.412535311828833e-06\n",
      "[step: 204] loss: 1.4101533452048898e-06\n",
      "[step: 205] loss: 1.4077808145884774e-06\n",
      "[step: 206] loss: 1.4053885024623014e-06\n",
      "[step: 207] loss: 1.4029925523573183e-06\n",
      "[step: 208] loss: 1.400593419020879e-06\n",
      "[step: 209] loss: 1.3983147937324247e-06\n",
      "[step: 210] loss: 1.3958344879938522e-06\n",
      "[step: 211] loss: 1.3934439948570798e-06\n",
      "[step: 212] loss: 1.3910433835917502e-06\n",
      "[step: 213] loss: 1.388656869494298e-06\n",
      "[step: 214] loss: 1.3863611911801854e-06\n",
      "[step: 215] loss: 1.3839415942129563e-06\n",
      "[step: 216] loss: 1.3815997590427287e-06\n",
      "[step: 217] loss: 1.3792576964988257e-06\n",
      "[step: 218] loss: 1.3769807765129372e-06\n",
      "[step: 219] loss: 1.3746115428148187e-06\n",
      "[step: 220] loss: 1.3722649327974068e-06\n",
      "[step: 221] loss: 1.369911728943407e-06\n",
      "[step: 222] loss: 1.3676195749212638e-06\n",
      "[step: 223] loss: 1.3652677353093168e-06\n",
      "[step: 224] loss: 1.3629585282615153e-06\n",
      "[step: 225] loss: 1.360571218356199e-06\n",
      "[step: 226] loss: 1.3583318150267587e-06\n",
      "[step: 227] loss: 1.35603295348119e-06\n",
      "[step: 228] loss: 1.3536952110371203e-06\n",
      "[step: 229] loss: 1.3514095371647272e-06\n",
      "[step: 230] loss: 1.3491101071849698e-06\n",
      "[step: 231] loss: 1.3468519455273054e-06\n",
      "[step: 232] loss: 1.3445232980302535e-06\n",
      "[step: 233] loss: 1.342296172879287e-06\n",
      "[step: 234] loss: 1.3400130001173238e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 235] loss: 1.3377452887652908e-06\n",
      "[step: 236] loss: 1.335459046458709e-06\n",
      "[step: 237] loss: 1.3332106618690887e-06\n",
      "[step: 238] loss: 1.3309755786394817e-06\n",
      "[step: 239] loss: 1.3286958164826501e-06\n",
      "[step: 240] loss: 1.3265098459669389e-06\n",
      "[step: 241] loss: 1.3242063232610235e-06\n",
      "[step: 242] loss: 1.3220243317846325e-06\n",
      "[step: 243] loss: 1.319785837949894e-06\n",
      "[step: 244] loss: 1.3175683761801338e-06\n",
      "[step: 245] loss: 1.3153016880096402e-06\n",
      "[step: 246] loss: 1.31310218876024e-06\n",
      "[step: 247] loss: 1.3108874554745853e-06\n",
      "[step: 248] loss: 1.3087405932310503e-06\n",
      "[step: 249] loss: 1.3065465509498608e-06\n",
      "[step: 250] loss: 1.3043450053373817e-06\n",
      "[step: 251] loss: 1.3021060567552922e-06\n",
      "[step: 252] loss: 1.299901327911357e-06\n",
      "[step: 253] loss: 1.297789140153327e-06\n",
      "[step: 254] loss: 1.2956551245224546e-06\n",
      "[step: 255] loss: 1.29344437027612e-06\n",
      "[step: 256] loss: 1.29130773984798e-06\n",
      "[step: 257] loss: 1.2891648566437652e-06\n",
      "[step: 258] loss: 1.287046643483336e-06\n",
      "[step: 259] loss: 1.2848204278270714e-06\n",
      "[step: 260] loss: 1.282743596675573e-06\n",
      "[step: 261] loss: 1.2805328424292384e-06\n",
      "[step: 262] loss: 1.2784557839040644e-06\n",
      "[step: 263] loss: 1.276320972465328e-06\n",
      "[step: 264] loss: 1.2741942327920697e-06\n",
      "[step: 265] loss: 1.2720628319584648e-06\n",
      "[step: 266] loss: 1.2699479157163296e-06\n",
      "[step: 267] loss: 1.2678393659371068e-06\n",
      "[step: 268] loss: 1.2657789056902402e-06\n",
      "[step: 269] loss: 1.2636641031349427e-06\n",
      "[step: 270] loss: 1.2616259255082696e-06\n",
      "[step: 271] loss: 1.259561372535245e-06\n",
      "[step: 272] loss: 1.2574596439662855e-06\n",
      "[step: 273] loss: 1.2554374961837311e-06\n",
      "[step: 274] loss: 1.2532816526800161e-06\n",
      "[step: 275] loss: 1.2512248304119566e-06\n",
      "[step: 276] loss: 1.2491728966779192e-06\n",
      "[step: 277] loss: 1.2471493846533122e-06\n",
      "[step: 278] loss: 1.2451421298464993e-06\n",
      "[step: 279] loss: 1.2430936067175935e-06\n",
      "[step: 280] loss: 1.2410487215674948e-06\n",
      "[step: 281] loss: 1.2390714800858404e-06\n",
      "[step: 282] loss: 1.2370068134259782e-06\n",
      "[step: 283] loss: 1.2350265023997054e-06\n",
      "[step: 284] loss: 1.2330168601693003e-06\n",
      "[step: 285] loss: 1.2309972134971758e-06\n",
      "[step: 286] loss: 1.2290552149352152e-06\n",
      "[step: 287] loss: 1.2269845228729537e-06\n",
      "[step: 288] loss: 1.2249811334186234e-06\n",
      "[step: 289] loss: 1.2229824051246396e-06\n",
      "[step: 290] loss: 1.221048933075508e-06\n",
      "[step: 291] loss: 1.2191372888992191e-06\n",
      "[step: 292] loss: 1.217159592670214e-06\n",
      "[step: 293] loss: 1.2151574537710985e-06\n",
      "[step: 294] loss: 1.2132340998505242e-06\n",
      "[step: 295] loss: 1.211291078107024e-06\n",
      "[step: 296] loss: 1.2093192935935804e-06\n",
      "[step: 297] loss: 1.2074175401721732e-06\n",
      "[step: 298] loss: 1.2054609896949842e-06\n",
      "[step: 299] loss: 1.2035307008773088e-06\n",
      "[step: 300] loss: 1.2016444088658318e-06\n",
      "[step: 301] loss: 1.1997466344837449e-06\n",
      "[step: 302] loss: 1.1977886060776655e-06\n",
      "[step: 303] loss: 1.1959107268921798e-06\n",
      "[step: 304] loss: 1.1940326203330187e-06\n",
      "[step: 305] loss: 1.192131890093151e-06\n",
      "[step: 306] loss: 1.190225134450884e-06\n",
      "[step: 307] loss: 1.188336796076328e-06\n",
      "[step: 308] loss: 1.186532585961686e-06\n",
      "[step: 309] loss: 1.1846049119412783e-06\n",
      "[step: 310] loss: 1.1827399930552929e-06\n",
      "[step: 311] loss: 1.1808695035142591e-06\n",
      "[step: 312] loss: 1.179093487735372e-06\n",
      "[step: 313] loss: 1.177243916572479e-06\n",
      "[step: 314] loss: 1.1753770650102524e-06\n",
      "[step: 315] loss: 1.173530108644627e-06\n",
      "[step: 316] loss: 1.1717727375071263e-06\n",
      "[step: 317] loss: 1.1699250990204746e-06\n",
      "[step: 318] loss: 1.1681306659738766e-06\n",
      "[step: 319] loss: 1.1662961014735629e-06\n",
      "[step: 320] loss: 1.164496325145592e-06\n",
      "[step: 321] loss: 1.1626430023170542e-06\n",
      "[step: 322] loss: 1.1609220109676244e-06\n",
      "[step: 323] loss: 1.1591566817514831e-06\n",
      "[step: 324] loss: 1.1573175697776605e-06\n",
      "[step: 325] loss: 1.155500513050356e-06\n",
      "[step: 326] loss: 1.1537589443832985e-06\n",
      "[step: 327] loss: 1.1520131693032454e-06\n",
      "[step: 328] loss: 1.1502373808980337e-06\n",
      "[step: 329] loss: 1.1485002460176474e-06\n",
      "[step: 330] loss: 1.1467383274066378e-06\n",
      "[step: 331] loss: 1.1450002830315498e-06\n",
      "[step: 332] loss: 1.143264967140567e-06\n",
      "[step: 333] loss: 1.1415245353418868e-06\n",
      "[step: 334] loss: 1.1398384458516375e-06\n",
      "[step: 335] loss: 1.1381272315702518e-06\n",
      "[step: 336] loss: 1.136408286583901e-06\n",
      "[step: 337] loss: 1.1346733117534313e-06\n",
      "[step: 338] loss: 1.132988472818397e-06\n",
      "[step: 339] loss: 1.1312999959045555e-06\n",
      "[step: 340] loss: 1.1295652484477614e-06\n",
      "[step: 341] loss: 1.1279213367743068e-06\n",
      "[step: 342] loss: 1.1262544603596325e-06\n",
      "[step: 343] loss: 1.1245665518799797e-06\n",
      "[step: 344] loss: 1.122868752645445e-06\n",
      "[step: 345] loss: 1.1212447361685918e-06\n",
      "[step: 346] loss: 1.1195367051186622e-06\n",
      "[step: 347] loss: 1.1178764225405757e-06\n",
      "[step: 348] loss: 1.1162069313286338e-06\n",
      "[step: 349] loss: 1.1145677945023635e-06\n",
      "[step: 350] loss: 1.1129696986245108e-06\n",
      "[step: 351] loss: 1.11134954750014e-06\n",
      "[step: 352] loss: 1.1096786920461454e-06\n",
      "[step: 353] loss: 1.1081249340350041e-06\n",
      "[step: 354] loss: 1.1065202443205635e-06\n",
      "[step: 355] loss: 1.1048206260966253e-06\n",
      "[step: 356] loss: 1.1033373539248714e-06\n",
      "[step: 357] loss: 1.101672182812763e-06\n",
      "[step: 358] loss: 1.1000935273841606e-06\n",
      "[step: 359] loss: 1.0985032758981106e-06\n",
      "[step: 360] loss: 1.0969232562274556e-06\n",
      "[step: 361] loss: 1.0953010587400058e-06\n",
      "[step: 362] loss: 1.093807100005506e-06\n",
      "[step: 363] loss: 1.0921727380264201e-06\n",
      "[step: 364] loss: 1.0906223906204104e-06\n",
      "[step: 365] loss: 1.089097281692375e-06\n",
      "[step: 366] loss: 1.0875304496948957e-06\n",
      "[step: 367] loss: 1.0859909025384695e-06\n",
      "[step: 368] loss: 1.0844753433048027e-06\n",
      "[step: 369] loss: 1.0828454151123879e-06\n",
      "[step: 370] loss: 1.0813820381372352e-06\n",
      "[step: 371] loss: 1.079853177543555e-06\n",
      "[step: 372] loss: 1.078346599570068e-06\n",
      "[step: 373] loss: 1.0768479796752217e-06\n",
      "[step: 374] loss: 1.0753651622508187e-06\n",
      "[step: 375] loss: 1.0738433502410771e-06\n",
      "[step: 376] loss: 1.0722986871769535e-06\n",
      "[step: 377] loss: 1.0708496347433538e-06\n",
      "[step: 378] loss: 1.069321797331213e-06\n",
      "[step: 379] loss: 1.0678820672183065e-06\n",
      "[step: 380] loss: 1.0664019782780088e-06\n",
      "[step: 381] loss: 1.0649641808413435e-06\n",
      "[step: 382] loss: 1.0634597629177733e-06\n",
      "[step: 383] loss: 1.062063233803201e-06\n",
      "[step: 384] loss: 1.060547333509021e-06\n",
      "[step: 385] loss: 1.0590946430966142e-06\n",
      "[step: 386] loss: 1.0576336535450537e-06\n",
      "[step: 387] loss: 1.056235646501591e-06\n",
      "[step: 388] loss: 1.0548097861828865e-06\n",
      "[step: 389] loss: 1.0533954082347918e-06\n",
      "[step: 390] loss: 1.051944764185464e-06\n",
      "[step: 391] loss: 1.0505806358196423e-06\n",
      "[step: 392] loss: 1.0490911108718137e-06\n",
      "[step: 393] loss: 1.047709019985632e-06\n",
      "[step: 394] loss: 1.0462986210768577e-06\n",
      "[step: 395] loss: 1.0449020919622853e-06\n",
      "[step: 396] loss: 1.043480665430252e-06\n",
      "[step: 397] loss: 1.0420914122732938e-06\n",
      "[step: 398] loss: 1.040763663695543e-06\n",
      "[step: 399] loss: 1.03931677131186e-06\n",
      "[step: 400] loss: 1.0379894774814602e-06\n",
      "[step: 401] loss: 1.0365993148298003e-06\n",
      "[step: 402] loss: 1.0352921435696771e-06\n",
      "[step: 403] loss: 1.033842863762402e-06\n",
      "[step: 404] loss: 1.0325428547730553e-06\n",
      "[step: 405] loss: 1.0311823643860407e-06\n",
      "[step: 406] loss: 1.0298524557583733e-06\n",
      "[step: 407] loss: 1.028490601129306e-06\n",
      "[step: 408] loss: 1.0271587598253973e-06\n",
      "[step: 409] loss: 1.0258270322083263e-06\n",
      "[step: 410] loss: 1.0245385055895895e-06\n",
      "[step: 411] loss: 1.0231642590952106e-06\n",
      "[step: 412] loss: 1.0218731176792062e-06\n",
      "[step: 413] loss: 1.0205509397565038e-06\n",
      "[step: 414] loss: 1.0192650279350346e-06\n",
      "[step: 415] loss: 1.017990712171013e-06\n",
      "[step: 416] loss: 1.0166670563194202e-06\n",
      "[step: 417] loss: 1.0153822813663282e-06\n",
      "[step: 418] loss: 1.0140703352590208e-06\n",
      "[step: 419] loss: 1.0128338772119605e-06\n",
      "[step: 420] loss: 1.011508857118315e-06\n",
      "[step: 421] loss: 1.010285359370755e-06\n",
      "[step: 422] loss: 1.0090425348607823e-06\n",
      "[step: 423] loss: 1.0077220622406458e-06\n",
      "[step: 424] loss: 1.0064970865641953e-06\n",
      "[step: 425] loss: 1.005203102977248e-06\n",
      "[step: 426] loss: 1.003957095235819e-06\n",
      "[step: 427] loss: 1.0027547432400752e-06\n",
      "[step: 428] loss: 1.0014820190917817e-06\n",
      "[step: 429] loss: 1.0003516308643157e-06\n",
      "[step: 430] loss: 9.990266107706702e-07\n",
      "[step: 431] loss: 9.977977697417373e-07\n",
      "[step: 432] loss: 9.96630319605174e-07\n",
      "[step: 433] loss: 9.953751032298896e-07\n",
      "[step: 434] loss: 9.941838925442426e-07\n",
      "[step: 435] loss: 9.930030273608281e-07\n",
      "[step: 436] loss: 9.917805527948076e-07\n",
      "[step: 437] loss: 9.905644446916995e-07\n",
      "[step: 438] loss: 9.893328751786612e-07\n",
      "[step: 439] loss: 9.881623554974794e-07\n",
      "[step: 440] loss: 9.870282156043686e-07\n",
      "[step: 441] loss: 9.85824158306059e-07\n",
      "[step: 442] loss: 9.84655798674794e-07\n",
      "[step: 443] loss: 9.83461632131366e-07\n",
      "[step: 444] loss: 9.82271672000934e-07\n",
      "[step: 445] loss: 9.81155949375534e-07\n",
      "[step: 446] loss: 9.799881581784575e-07\n",
      "[step: 447] loss: 9.788253692022408e-07\n",
      "[step: 448] loss: 9.776856586540816e-07\n",
      "[step: 449] loss: 9.764960395841626e-07\n",
      "[step: 450] loss: 9.754049870025483e-07\n",
      "[step: 451] loss: 9.742593647388276e-07\n",
      "[step: 452] loss: 9.731606951390859e-07\n",
      "[step: 453] loss: 9.719819900055882e-07\n",
      "[step: 454] loss: 9.709142432257067e-07\n",
      "[step: 455] loss: 9.69786583482346e-07\n",
      "[step: 456] loss: 9.686434623290552e-07\n",
      "[step: 457] loss: 9.675102319306461e-07\n",
      "[step: 458] loss: 9.664446452006814e-07\n",
      "[step: 459] loss: 9.653603001424926e-07\n",
      "[step: 460] loss: 9.64220362220658e-07\n",
      "[step: 461] loss: 9.63150000643509e-07\n",
      "[step: 462] loss: 9.620623586670263e-07\n",
      "[step: 463] loss: 9.609050266590202e-07\n",
      "[step: 464] loss: 9.599308441465837e-07\n",
      "[step: 465] loss: 9.588279681338463e-07\n",
      "[step: 466] loss: 9.577260016158107e-07\n",
      "[step: 467] loss: 9.567140750732506e-07\n",
      "[step: 468] loss: 9.556417808198603e-07\n",
      "[step: 469] loss: 9.545677812639042e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 470] loss: 9.535054914522334e-07\n",
      "[step: 471] loss: 9.524329698251677e-07\n",
      "[step: 472] loss: 9.514025123280589e-07\n",
      "[step: 473] loss: 9.503562523605069e-07\n",
      "[step: 474] loss: 9.492865160609654e-07\n",
      "[step: 475] loss: 9.48300851177919e-07\n",
      "[step: 476] loss: 9.472057058701466e-07\n",
      "[step: 477] loss: 9.462151524530782e-07\n",
      "[step: 478] loss: 9.452022595723975e-07\n",
      "[step: 479] loss: 9.442182999919169e-07\n",
      "[step: 480] loss: 9.431236094314954e-07\n",
      "[step: 481] loss: 9.421057143299549e-07\n",
      "[step: 482] loss: 9.411145924786979e-07\n",
      "[step: 483] loss: 9.401549618814897e-07\n",
      "[step: 484] loss: 9.391503112965438e-07\n",
      "[step: 485] loss: 9.381976724398555e-07\n",
      "[step: 486] loss: 9.371467513119569e-07\n",
      "[step: 487] loss: 9.361448860545352e-07\n",
      "[step: 488] loss: 9.351564926873834e-07\n",
      "[step: 489] loss: 9.341767963633174e-07\n",
      "[step: 490] loss: 9.332076160717406e-07\n",
      "[step: 491] loss: 9.322579330728331e-07\n",
      "[step: 492] loss: 9.312944371231424e-07\n",
      "[step: 493] loss: 9.302763714913453e-07\n",
      "[step: 494] loss: 9.293485163652804e-07\n",
      "[step: 495] loss: 9.28345684769738e-07\n",
      "[step: 496] loss: 9.274282319893246e-07\n",
      "[step: 497] loss: 9.264599043490307e-07\n",
      "[step: 498] loss: 9.255143140762812e-07\n",
      "[step: 499] loss: 9.246064678336552e-07\n",
      "[step: 500] loss: 9.236584332938946e-07\n",
      "[step: 501] loss: 9.226958468389057e-07\n",
      "[step: 502] loss: 9.217543492923141e-07\n",
      "[step: 503] loss: 9.208853271047701e-07\n",
      "[step: 504] loss: 9.200198292091954e-07\n",
      "[step: 505] loss: 9.191590493173862e-07\n",
      "[step: 506] loss: 9.18262344384857e-07\n",
      "[step: 507] loss: 9.174913770948478e-07\n",
      "[step: 508] loss: 9.168936117021076e-07\n",
      "[step: 509] loss: 9.163799745692813e-07\n",
      "[step: 510] loss: 9.163043728221965e-07\n",
      "[step: 511] loss: 9.169239660877793e-07\n",
      "[step: 512] loss: 9.188017315864272e-07\n",
      "[step: 513] loss: 9.230046771335765e-07\n",
      "[step: 514] loss: 9.319505238636339e-07\n",
      "[step: 515] loss: 9.499039492766315e-07\n",
      "[step: 516] loss: 9.858171097221202e-07\n",
      "[step: 517] loss: 1.0578979754427564e-06\n",
      "[step: 518] loss: 1.2029395293211564e-06\n",
      "[step: 519] loss: 1.4976703823776916e-06\n",
      "[step: 520] loss: 2.1026719423389295e-06\n",
      "[step: 521] loss: 3.3541487027832773e-06\n",
      "[step: 522] loss: 5.967807737761177e-06\n",
      "[step: 523] loss: 1.142833298217738e-05\n",
      "[step: 524] loss: 2.2830085072200745e-05\n",
      "[step: 525] loss: 4.5961507566971704e-05\n",
      "[step: 526] loss: 9.070288069779053e-05\n",
      "[step: 527] loss: 0.0001659337867749855\n",
      "[step: 528] loss: 0.00026518345111981034\n",
      "[step: 529] loss: 0.0003289266605861485\n",
      "[step: 530] loss: 0.0002789893187582493\n",
      "[step: 531] loss: 0.00012014418462058529\n",
      "[step: 532] loss: 6.215400389919523e-06\n",
      "[step: 533] loss: 3.957573062507436e-05\n",
      "[step: 534] loss: 0.0001382213522447273\n",
      "[step: 535] loss: 0.0001537409843876958\n",
      "[step: 536] loss: 6.441974255722016e-05\n",
      "[step: 537] loss: 1.3357855550566455e-06\n",
      "[step: 538] loss: 3.924817428924143e-05\n",
      "[step: 539] loss: 9.380109258927405e-05\n",
      "[step: 540] loss: 7.15781789040193e-05\n",
      "[step: 541] loss: 1.2246161531948019e-05\n",
      "[step: 542] loss: 8.507136953994632e-06\n",
      "[step: 543] loss: 4.9333022616337985e-05\n",
      "[step: 544] loss: 5.5237585911527276e-05\n",
      "[step: 545] loss: 1.802496080927085e-05\n",
      "[step: 546] loss: 1.5638661352568306e-06\n",
      "[step: 547] loss: 2.474434586474672e-05\n",
      "[step: 548] loss: 3.817007745965384e-05\n",
      "[step: 549] loss: 1.793856790754944e-05\n",
      "[step: 550] loss: 9.521586434857454e-07\n",
      "[step: 551] loss: 1.2087532923032995e-05\n",
      "[step: 552] loss: 2.495217449904885e-05\n",
      "[step: 553] loss: 1.5538058505626395e-05\n",
      "[step: 554] loss: 1.7109912278101547e-06\n",
      "[step: 555] loss: 5.6163698900491e-06\n",
      "[step: 556] loss: 1.5768795492476784e-05\n",
      "[step: 557] loss: 1.258820884686429e-05\n",
      "[step: 558] loss: 2.5781962449400453e-06\n",
      "[step: 559] loss: 2.45893875217007e-06\n",
      "[step: 560] loss: 9.50081812334247e-06\n",
      "[step: 561] loss: 9.780631444300525e-06\n",
      "[step: 562] loss: 3.243720811951789e-06\n",
      "[step: 563] loss: 1.1463450846349588e-06\n",
      "[step: 564] loss: 5.387097189668566e-06\n",
      "[step: 565] loss: 7.240171271405416e-06\n",
      "[step: 566] loss: 3.597694330892409e-06\n",
      "[step: 567] loss: 8.723665700927086e-07\n",
      "[step: 568] loss: 2.8247307000128785e-06\n",
      "[step: 569] loss: 5.047394552093465e-06\n",
      "[step: 570] loss: 3.5877549180440838e-06\n",
      "[step: 571] loss: 1.1272979918430792e-06\n",
      "[step: 572] loss: 1.4346735497383634e-06\n",
      "[step: 573] loss: 3.243049377488205e-06\n",
      "[step: 574] loss: 3.2199773158936296e-06\n",
      "[step: 575] loss: 1.533653062324447e-06\n",
      "[step: 576] loss: 9.027172609421541e-07\n",
      "[step: 577] loss: 1.921060857057455e-06\n",
      "[step: 578] loss: 2.5761341930774506e-06\n",
      "[step: 579] loss: 1.8110895325662568e-06\n",
      "[step: 580] loss: 9.167808343590877e-07\n",
      "[step: 581] loss: 1.1387784297767212e-06\n",
      "[step: 582] loss: 1.8380947039986495e-06\n",
      "[step: 583] loss: 1.8073338878821232e-06\n",
      "[step: 584] loss: 1.149247736975667e-06\n",
      "[step: 585] loss: 8.627568490737758e-07\n",
      "[step: 586] loss: 1.2243525588928605e-06\n",
      "[step: 587] loss: 1.5376282362922211e-06\n",
      "[step: 588] loss: 1.3153382951713866e-06\n",
      "[step: 589] loss: 9.24318214856612e-07\n",
      "[step: 590] loss: 8.990887749860121e-07\n",
      "[step: 591] loss: 1.1672789241856663e-06\n",
      "[step: 592] loss: 1.270921757168253e-06\n",
      "[step: 593] loss: 1.0692242540244479e-06\n",
      "[step: 594] loss: 8.64773369357863e-07\n",
      "[step: 595] loss: 9.069324846677773e-07\n",
      "[step: 596] loss: 1.070760959009931e-06\n",
      "[step: 597] loss: 1.097267386285239e-06\n",
      "[step: 598] loss: 9.59795102062344e-07\n",
      "[step: 599] loss: 8.508436621923465e-07\n",
      "[step: 600] loss: 8.900048555915419e-07\n",
      "[step: 601] loss: 9.860366390057607e-07\n",
      "[step: 602] loss: 9.951876336344867e-07\n",
      "[step: 603] loss: 9.117447916651145e-07\n",
      "[step: 604] loss: 8.466439567200723e-07\n",
      "[step: 605] loss: 8.671869977661117e-07\n",
      "[step: 606] loss: 9.242384066965315e-07\n",
      "[step: 607] loss: 9.350025038656895e-07\n",
      "[step: 608] loss: 8.891018410395191e-07\n",
      "[step: 609] loss: 8.454097155663476e-07\n",
      "[step: 610] loss: 8.490454206366849e-07\n",
      "[step: 611] loss: 8.820155699140741e-07\n",
      "[step: 612] loss: 8.965077995526372e-07\n",
      "[step: 613] loss: 8.757759246691421e-07\n",
      "[step: 614] loss: 8.459110745206999e-07\n",
      "[step: 615] loss: 8.385647447539668e-07\n",
      "[step: 616] loss: 8.543360650037357e-07\n",
      "[step: 617] loss: 8.689802939443325e-07\n",
      "[step: 618] loss: 8.645143907415331e-07\n",
      "[step: 619] loss: 8.469851877634937e-07\n",
      "[step: 620] loss: 8.350045277438767e-07\n",
      "[step: 621] loss: 8.380609983760223e-07\n",
      "[step: 622] loss: 8.482945190735336e-07\n",
      "[step: 623] loss: 8.523293786311115e-07\n",
      "[step: 624] loss: 8.457795956928749e-07\n",
      "[step: 625] loss: 8.356563512279536e-07\n",
      "[step: 626] loss: 8.311494639201555e-07\n",
      "[step: 627] loss: 8.342870501110156e-07\n",
      "[step: 628] loss: 8.394672477152199e-07\n",
      "[step: 629] loss: 8.403347919738735e-07\n",
      "[step: 630] loss: 8.359028811355529e-07\n",
      "[step: 631] loss: 8.302163223561365e-07\n",
      "[step: 632] loss: 8.278429390884412e-07\n",
      "[step: 633] loss: 8.292872735182755e-07\n",
      "[step: 634] loss: 8.317975357385876e-07\n",
      "[step: 635] loss: 8.321973155034357e-07\n",
      "[step: 636] loss: 8.29924942991056e-07\n",
      "[step: 637] loss: 8.266100621767691e-07\n",
      "[step: 638] loss: 8.246099127973139e-07\n",
      "[step: 639] loss: 8.24555570488883e-07\n",
      "[step: 640] loss: 8.256899945990881e-07\n",
      "[step: 641] loss: 8.262342703346803e-07\n",
      "[step: 642] loss: 8.253438750216446e-07\n",
      "[step: 643] loss: 8.234553092734131e-07\n",
      "[step: 644] loss: 8.217302820412442e-07\n",
      "[step: 645] loss: 8.208789950003847e-07\n",
      "[step: 646] loss: 8.208722306335403e-07\n",
      "[step: 647] loss: 8.211517865674978e-07\n",
      "[step: 648] loss: 8.210029704969202e-07\n",
      "[step: 649] loss: 8.202381991395669e-07\n",
      "[step: 650] loss: 8.190165772248292e-07\n",
      "[step: 651] loss: 8.179391102203226e-07\n",
      "[step: 652] loss: 8.17213276604889e-07\n",
      "[step: 653] loss: 8.169491252374428e-07\n",
      "[step: 654] loss: 8.168253771145828e-07\n",
      "[step: 655] loss: 8.165574740814918e-07\n",
      "[step: 656] loss: 8.159952358255396e-07\n",
      "[step: 657] loss: 8.152610462275334e-07\n",
      "[step: 658] loss: 8.144192520376237e-07\n",
      "[step: 659] loss: 8.137164400068286e-07\n",
      "[step: 660] loss: 8.132566904350824e-07\n",
      "[step: 661] loss: 8.128613444569055e-07\n",
      "[step: 662] loss: 8.125433055283793e-07\n",
      "[step: 663] loss: 8.120816801238107e-07\n",
      "[step: 664] loss: 8.116386993606284e-07\n",
      "[step: 665] loss: 8.110906151159725e-07\n",
      "[step: 666] loss: 8.104266271402594e-07\n",
      "[step: 667] loss: 8.09857510830625e-07\n",
      "[step: 668] loss: 8.093034580269887e-07\n",
      "[step: 669] loss: 8.088044864962285e-07\n",
      "[step: 670] loss: 8.084068099378783e-07\n",
      "[step: 671] loss: 8.080186262304778e-07\n",
      "[step: 672] loss: 8.07569335847802e-07\n",
      "[step: 673] loss: 8.071246497820539e-07\n",
      "[step: 674] loss: 8.066217560553923e-07\n",
      "[step: 675] loss: 8.06102036676748e-07\n",
      "[step: 676] loss: 8.055575904108991e-07\n",
      "[step: 677] loss: 8.050883479882032e-07\n",
      "[step: 678] loss: 8.045863069128245e-07\n",
      "[step: 679] loss: 8.04134856480232e-07\n",
      "[step: 680] loss: 8.037077350309119e-07\n",
      "[step: 681] loss: 8.032636173993524e-07\n",
      "[step: 682] loss: 8.028470119825215e-07\n",
      "[step: 683] loss: 8.023844202398323e-07\n",
      "[step: 684] loss: 8.019004553716513e-07\n",
      "[step: 685] loss: 8.014347372409247e-07\n",
      "[step: 686] loss: 8.010309784367564e-07\n",
      "[step: 687] loss: 8.005618497008982e-07\n",
      "[step: 688] loss: 8.000982347766694e-07\n",
      "[step: 689] loss: 7.997065267772996e-07\n",
      "[step: 690] loss: 7.991498591763957e-07\n",
      "[step: 691] loss: 7.987640628925874e-07\n",
      "[step: 692] loss: 7.982585543686582e-07\n",
      "[step: 693] loss: 7.978509302120074e-07\n",
      "[step: 694] loss: 7.974284699230338e-07\n",
      "[step: 695] loss: 7.969744615365926e-07\n",
      "[step: 696] loss: 7.965456916281255e-07\n",
      "[step: 697] loss: 7.961304504533473e-07\n",
      "[step: 698] loss: 7.957700063343509e-07\n",
      "[step: 699] loss: 7.952938858579728e-07\n",
      "[step: 700] loss: 7.948553957248805e-07\n",
      "[step: 701] loss: 7.944454978314752e-07\n",
      "[step: 702] loss: 7.94089885403082e-07\n",
      "[step: 703] loss: 7.936533847896499e-07\n",
      "[step: 704] loss: 7.932566745694203e-07\n",
      "[step: 705] loss: 7.92836146956688e-07\n",
      "[step: 706] loss: 7.924432452455221e-07\n",
      "[step: 707] loss: 7.920841085251595e-07\n",
      "[step: 708] loss: 7.917213906694087e-07\n",
      "[step: 709] loss: 7.913757258393161e-07\n",
      "[step: 710] loss: 7.9101926075964e-07\n",
      "[step: 711] loss: 7.908082579888287e-07\n",
      "[step: 712] loss: 7.905268262220488e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 713] loss: 7.904162657723646e-07\n",
      "[step: 714] loss: 7.905067036517721e-07\n",
      "[step: 715] loss: 7.907909207460762e-07\n",
      "[step: 716] loss: 7.915000423963647e-07\n",
      "[step: 717] loss: 7.927986871436588e-07\n",
      "[step: 718] loss: 7.951230145408772e-07\n",
      "[step: 719] loss: 7.991310440047528e-07\n",
      "[step: 720] loss: 8.059469109866768e-07\n",
      "[step: 721] loss: 8.175666152965277e-07\n",
      "[step: 722] loss: 8.373414743800822e-07\n",
      "[step: 723] loss: 8.712826797818707e-07\n",
      "[step: 724] loss: 9.300267151957087e-07\n",
      "[step: 725] loss: 1.03285049135593e-06\n",
      "[step: 726] loss: 1.214339249600016e-06\n",
      "[step: 727] loss: 1.5383054687845288e-06\n",
      "[step: 728] loss: 2.1218320398475043e-06\n",
      "[step: 729] loss: 3.184989054716425e-06\n",
      "[step: 730] loss: 5.136425897944719e-06\n",
      "[step: 731] loss: 8.75523346621776e-06\n",
      "[step: 732] loss: 1.5482182789128274e-05\n",
      "[step: 733] loss: 2.805952499329578e-05\n",
      "[step: 734] loss: 5.129606506670825e-05\n",
      "[step: 735] loss: 9.368864994030446e-05\n",
      "[step: 736] loss: 0.0001666435127845034\n",
      "[step: 737] loss: 0.0002821369271259755\n",
      "[step: 738] loss: 0.0004278011037968099\n",
      "[step: 739] loss: 0.0005453713238239288\n",
      "[step: 740] loss: 0.0005134314415045083\n",
      "[step: 741] loss: 0.00030037559918127954\n",
      "[step: 742] loss: 6.122995546320453e-05\n",
      "[step: 743] loss: 1.0813145308929961e-05\n",
      "[step: 744] loss: 0.00014407280832529068\n",
      "[step: 745] loss: 0.0002520236885175109\n",
      "[step: 746] loss: 0.0001859828771557659\n",
      "[step: 747] loss: 3.9715007005725056e-05\n",
      "[step: 748] loss: 9.422043149243109e-06\n",
      "[step: 749] loss: 9.98311152216047e-05\n",
      "[step: 750] loss: 0.00014362894580699503\n",
      "[step: 751] loss: 7.178908708738163e-05\n",
      "[step: 752] loss: 2.6550653728918405e-06\n",
      "[step: 753] loss: 3.241041849832982e-05\n",
      "[step: 754] loss: 8.617185085313395e-05\n",
      "[step: 755] loss: 6.513392872875556e-05\n",
      "[step: 756] loss: 9.722240974952001e-06\n",
      "[step: 757] loss: 9.934002264344599e-06\n",
      "[step: 758] loss: 4.8684793000575155e-05\n",
      "[step: 759] loss: 4.887947579845786e-05\n",
      "[step: 760] loss: 1.2365505426714662e-05\n",
      "[step: 761] loss: 3.0859134767524665e-06\n",
      "[step: 762] loss: 2.7484911697683856e-05\n",
      "[step: 763] loss: 3.4142205549869686e-05\n",
      "[step: 764] loss: 1.184892789751757e-05\n",
      "[step: 765] loss: 1.144743350778299e-06\n",
      "[step: 766] loss: 1.5512747268076055e-05\n",
      "[step: 767] loss: 2.3269079974852502e-05\n",
      "[step: 768] loss: 1.022505239234306e-05\n",
      "[step: 769] loss: 7.719325481048145e-07\n",
      "[step: 770] loss: 8.69257655722322e-06\n",
      "[step: 771] loss: 1.5585848814225756e-05\n",
      "[step: 772] loss: 8.480189535475802e-06\n",
      "[step: 773] loss: 9.014362944981258e-07\n",
      "[step: 774] loss: 4.754027486342238e-06\n",
      "[step: 775] loss: 1.0313729035260621e-05\n",
      "[step: 776] loss: 6.89861462888075e-06\n",
      "[step: 777] loss: 1.1861544635394239e-06\n",
      "[step: 778] loss: 2.536876309022773e-06\n",
      "[step: 779] loss: 6.678771569568198e-06\n",
      "[step: 780] loss: 5.536316166399047e-06\n",
      "[step: 781] loss: 1.4896575066813966e-06\n",
      "[step: 782] loss: 1.372672386423801e-06\n",
      "[step: 783] loss: 4.211022769595729e-06\n",
      "[step: 784] loss: 4.353145413915627e-06\n",
      "[step: 785] loss: 1.7295036514042295e-06\n",
      "[step: 786] loss: 8.695413384884887e-07\n",
      "[step: 787] loss: 2.5702897801238578e-06\n",
      "[step: 788] loss: 3.32255513058044e-06\n",
      "[step: 789] loss: 1.8579255538497819e-06\n",
      "[step: 790] loss: 7.631360290361044e-07\n",
      "[step: 791] loss: 1.5498529819524265e-06\n",
      "[step: 792] loss: 2.4380967715842417e-06\n",
      "[step: 793] loss: 1.8447071852278896e-06\n",
      "[step: 794] loss: 8.602806929047802e-07\n",
      "[step: 795] loss: 9.96642597783648e-07\n",
      "[step: 796] loss: 1.720909040159313e-06\n",
      "[step: 797] loss: 1.693510284894728e-06\n",
      "[step: 798] loss: 1.0135421462109662e-06\n",
      "[step: 799] loss: 7.798443562023749e-07\n",
      "[step: 800] loss: 1.1989262702627457e-06\n",
      "[step: 801] loss: 1.4432147281695507e-06\n",
      "[step: 802] loss: 1.117196347877325e-06\n",
      "[step: 803] loss: 7.728204991508392e-07\n",
      "[step: 804] loss: 8.873421961652639e-07\n",
      "[step: 805] loss: 1.1616375559242442e-06\n",
      "[step: 806] loss: 1.1204549537069397e-06\n",
      "[step: 807] loss: 8.515631861882866e-07\n",
      "[step: 808] loss: 7.644973152309831e-07\n",
      "[step: 809] loss: 9.25813708363421e-07\n",
      "[step: 810] loss: 1.0298118695573066e-06\n",
      "[step: 811] loss: 9.154606459560455e-07\n",
      "[step: 812] loss: 7.692121926083928e-07\n",
      "[step: 813] loss: 7.887137485340645e-07\n",
      "[step: 814] loss: 8.992232665150368e-07\n",
      "[step: 815] loss: 9.134240599451005e-07\n",
      "[step: 816] loss: 8.169828902282461e-07\n",
      "[step: 817] loss: 7.538438353549282e-07\n",
      "[step: 818] loss: 7.948564757498389e-07\n",
      "[step: 819] loss: 8.545285936634173e-07\n",
      "[step: 820] loss: 8.394175665671355e-07\n",
      "[step: 821] loss: 7.767690135551675e-07\n",
      "[step: 822] loss: 7.532319727943104e-07\n",
      "[step: 823] loss: 7.868765692364832e-07\n",
      "[step: 824] loss: 8.164566906998516e-07\n",
      "[step: 825] loss: 7.987264893927204e-07\n",
      "[step: 826] loss: 7.613475645484868e-07\n",
      "[step: 827] loss: 7.527234515691816e-07\n",
      "[step: 828] loss: 7.748674875074357e-07\n",
      "[step: 829] loss: 7.900979994701629e-07\n",
      "[step: 830] loss: 7.772983394715993e-07\n",
      "[step: 831] loss: 7.553122145509406e-07\n",
      "[step: 832] loss: 7.50903552670934e-07\n",
      "[step: 833] loss: 7.640091439498065e-07\n",
      "[step: 834] loss: 7.730595257271489e-07\n",
      "[step: 835] loss: 7.658077834094001e-07\n",
      "[step: 836] loss: 7.524774900957709e-07\n",
      "[step: 837] loss: 7.487886364287988e-07\n",
      "[step: 838] loss: 7.559424943792692e-07\n",
      "[step: 839] loss: 7.620233191119041e-07\n",
      "[step: 840] loss: 7.588511152789579e-07\n",
      "[step: 841] loss: 7.50788785808254e-07\n",
      "[step: 842] loss: 7.471285243809689e-07\n",
      "[step: 843] loss: 7.503084020754613e-07\n",
      "[step: 844] loss: 7.545096423200448e-07\n",
      "[step: 845] loss: 7.539485409324698e-07\n",
      "[step: 846] loss: 7.494636520277709e-07\n",
      "[step: 847] loss: 7.460449182872253e-07\n",
      "[step: 848] loss: 7.466400120392791e-07\n",
      "[step: 849] loss: 7.492351414839504e-07\n",
      "[step: 850] loss: 7.500576089114475e-07\n",
      "[step: 851] loss: 7.48020795526827e-07\n",
      "[step: 852] loss: 7.45334887142235e-07\n",
      "[step: 853] loss: 7.444961056535249e-07\n",
      "[step: 854] loss: 7.455483910234761e-07\n",
      "[step: 855] loss: 7.466371130249172e-07\n",
      "[step: 856] loss: 7.462342068720318e-07\n",
      "[step: 857] loss: 7.4469943456279e-07\n",
      "[step: 858] loss: 7.43357475130324e-07\n",
      "[step: 859] loss: 7.43178190987237e-07\n",
      "[step: 860] loss: 7.438168836415571e-07\n",
      "[step: 861] loss: 7.441404363817128e-07\n",
      "[step: 862] loss: 7.43596785923728e-07\n",
      "[step: 863] loss: 7.425556987072923e-07\n",
      "[step: 864] loss: 7.418576046802627e-07\n",
      "[step: 865] loss: 7.417911547236145e-07\n",
      "[step: 866] loss: 7.420520091727667e-07\n",
      "[step: 867] loss: 7.420080123665684e-07\n",
      "[step: 868] loss: 7.416122684844595e-07\n",
      "[step: 869] loss: 7.409912541334052e-07\n",
      "[step: 870] loss: 7.40521272746264e-07\n",
      "[step: 871] loss: 7.403202175737533e-07\n",
      "[step: 872] loss: 7.4033584951394e-07\n",
      "[step: 873] loss: 7.403053814414307e-07\n",
      "[step: 874] loss: 7.40009568289679e-07\n",
      "[step: 875] loss: 7.395587431346939e-07\n",
      "[step: 876] loss: 7.392268912553845e-07\n",
      "[step: 877] loss: 7.389409120150958e-07\n",
      "[step: 878] loss: 7.388060225821391e-07\n",
      "[step: 879] loss: 7.387073992504156e-07\n",
      "[step: 880] loss: 7.385374942714407e-07\n",
      "[step: 881] loss: 7.381833597719378e-07\n",
      "[step: 882] loss: 7.379170483545749e-07\n",
      "[step: 883] loss: 7.376154371740995e-07\n",
      "[step: 884] loss: 7.374189863185165e-07\n",
      "[step: 885] loss: 7.37261984795623e-07\n",
      "[step: 886] loss: 7.371141919065849e-07\n",
      "[step: 887] loss: 7.368804517682292e-07\n",
      "[step: 888] loss: 7.366366503447352e-07\n",
      "[step: 889] loss: 7.363513532254728e-07\n",
      "[step: 890] loss: 7.361136908912158e-07\n",
      "[step: 891] loss: 7.359508913395985e-07\n",
      "[step: 892] loss: 7.357576237154717e-07\n",
      "[step: 893] loss: 7.355693583122047e-07\n",
      "[step: 894] loss: 7.353664273068716e-07\n",
      "[step: 895] loss: 7.351206932071364e-07\n",
      "[step: 896] loss: 7.348903068304935e-07\n",
      "[step: 897] loss: 7.346779966610484e-07\n",
      "[step: 898] loss: 7.344888786064985e-07\n",
      "[step: 899] loss: 7.342549679378862e-07\n",
      "[step: 900] loss: 7.340701699831698e-07\n",
      "[step: 901] loss: 7.338713317039947e-07\n",
      "[step: 902] loss: 7.336615226449794e-07\n",
      "[step: 903] loss: 7.335175951084238e-07\n",
      "[step: 904] loss: 7.332457698794315e-07\n",
      "[step: 905] loss: 7.330654057113861e-07\n",
      "[step: 906] loss: 7.328479796342435e-07\n",
      "[step: 907] loss: 7.326589752665313e-07\n",
      "[step: 908] loss: 7.324789521589992e-07\n",
      "[step: 909] loss: 7.322706210288743e-07\n",
      "[step: 910] loss: 7.320671784327715e-07\n",
      "[step: 911] loss: 7.318838015635265e-07\n",
      "[step: 912] loss: 7.316456844819186e-07\n",
      "[step: 913] loss: 7.314378080991446e-07\n",
      "[step: 914] loss: 7.312630145861476e-07\n",
      "[step: 915] loss: 7.310927685466595e-07\n",
      "[step: 916] loss: 7.309300826818799e-07\n",
      "[step: 917] loss: 7.306866791623179e-07\n",
      "[step: 918] loss: 7.305209237529198e-07\n",
      "[step: 919] loss: 7.30328906684008e-07\n",
      "[step: 920] loss: 7.301086952793412e-07\n",
      "[step: 921] loss: 7.299161097762408e-07\n",
      "[step: 922] loss: 7.297115871551796e-07\n",
      "[step: 923] loss: 7.295398063433822e-07\n",
      "[step: 924] loss: 7.2936427386594e-07\n",
      "[step: 925] loss: 7.291494625860651e-07\n",
      "[step: 926] loss: 7.290025223483099e-07\n",
      "[step: 927] loss: 7.288512620107213e-07\n",
      "[step: 928] loss: 7.286004120032885e-07\n",
      "[step: 929] loss: 7.284104981408746e-07\n",
      "[step: 930] loss: 7.282176852640987e-07\n",
      "[step: 931] loss: 7.280295335476694e-07\n",
      "[step: 932] loss: 7.278756584128132e-07\n",
      "[step: 933] loss: 7.276628366525983e-07\n",
      "[step: 934] loss: 7.275090752045799e-07\n",
      "[step: 935] loss: 7.27287272184185e-07\n",
      "[step: 936] loss: 7.270951414284355e-07\n",
      "[step: 937] loss: 7.269267712217697e-07\n",
      "[step: 938] loss: 7.267516366482596e-07\n",
      "[step: 939] loss: 7.265777526299644e-07\n",
      "[step: 940] loss: 7.264206374202331e-07\n",
      "[step: 941] loss: 7.262669896590523e-07\n",
      "[step: 942] loss: 7.260677534759452e-07\n",
      "[step: 943] loss: 7.258773848661804e-07\n",
      "[step: 944] loss: 7.257023071360891e-07\n",
      "[step: 945] loss: 7.255566742969677e-07\n",
      "[step: 946] loss: 7.253619855873694e-07\n",
      "[step: 947] loss: 7.251553029163915e-07\n",
      "[step: 948] loss: 7.250212661347177e-07\n",
      "[step: 949] loss: 7.247980420288513e-07\n",
      "[step: 950] loss: 7.246636073432455e-07\n",
      "[step: 951] loss: 7.244816515594721e-07\n",
      "[step: 952] loss: 7.242946935548389e-07\n",
      "[step: 953] loss: 7.241080766107189e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 954] loss: 7.239666501845932e-07\n",
      "[step: 955] loss: 7.238097055051185e-07\n",
      "[step: 956] loss: 7.236206442939874e-07\n",
      "[step: 957] loss: 7.234676218104141e-07\n",
      "[step: 958] loss: 7.232882239804894e-07\n",
      "[step: 959] loss: 7.2313400778512e-07\n",
      "[step: 960] loss: 7.229768357319699e-07\n",
      "[step: 961] loss: 7.228516665236384e-07\n",
      "[step: 962] loss: 7.226342972899147e-07\n",
      "[step: 963] loss: 7.224500677693868e-07\n",
      "[step: 964] loss: 7.222994327094057e-07\n",
      "[step: 965] loss: 7.221578357530234e-07\n",
      "[step: 966] loss: 7.219668987090699e-07\n",
      "[step: 967] loss: 7.217853408292285e-07\n",
      "[step: 968] loss: 7.216675612653489e-07\n",
      "[step: 969] loss: 7.214848096737114e-07\n",
      "[step: 970] loss: 7.213205890366226e-07\n",
      "[step: 971] loss: 7.21150115623459e-07\n",
      "[step: 972] loss: 7.210161356852041e-07\n",
      "[step: 973] loss: 7.208779493339534e-07\n",
      "[step: 974] loss: 7.207558496702404e-07\n",
      "[step: 975] loss: 7.205586598502123e-07\n",
      "[step: 976] loss: 7.205567840173899e-07\n",
      "[step: 977] loss: 7.205549081845675e-07\n",
      "[step: 978] loss: 7.206573968687735e-07\n",
      "[step: 979] loss: 7.208679448922339e-07\n",
      "[step: 980] loss: 7.213931780825078e-07\n",
      "[step: 981] loss: 7.223688953672536e-07\n",
      "[step: 982] loss: 7.24233814253239e-07\n",
      "[step: 983] loss: 7.275364168890519e-07\n",
      "[step: 984] loss: 7.333923122132546e-07\n",
      "[step: 985] loss: 7.440301601491228e-07\n",
      "[step: 986] loss: 7.630231948496657e-07\n",
      "[step: 987] loss: 7.974762752382958e-07\n",
      "[step: 988] loss: 8.605778134551656e-07\n",
      "[step: 989] loss: 9.768281188371475e-07\n",
      "[step: 990] loss: 1.1926597380806925e-06\n",
      "[step: 991] loss: 1.5966893442964647e-06\n",
      "[step: 992] loss: 2.359010750296875e-06\n",
      "[step: 993] loss: 3.810079761024099e-06\n",
      "[step: 994] loss: 6.589534223166993e-06\n",
      "[step: 995] loss: 1.1954971341765486e-05\n",
      "[step: 996] loss: 2.2335874746204354e-05\n",
      "[step: 997] loss: 4.2492396460147575e-05\n",
      "[step: 998] loss: 8.123507723212242e-05\n",
      "[step: 999] loss: 0.00015464413445442915\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "LSTM realforecast : [293369.89422385639, 18652475.137587946, 1263.6629504415446]\n",
      "Bayseian realforecast : [17001.131946273275, 22276.375241595382, 29180.097344759895]\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,0,forecastDay,'month') #7은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17001.131946273275, 22276.375241595382, 29180.097344759895]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawArrayDatas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
