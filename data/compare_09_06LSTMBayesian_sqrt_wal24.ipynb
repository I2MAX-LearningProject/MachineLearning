{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list(np.sqrt(rawArrayDatas[1]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list(np.sqrt(rawArrayDatas[1][:-forecastDay] ))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-(mockForcastDay+forecastDay)] & np.log\n",
    "    ds = rawArrayDatas[0][:-(mockForcastDay+forecastDay)]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-(mockForcastDay+forecastDay)]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of (mockForcastDay+forecastDay)  rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "    testY= rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출       \n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'day')\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "    \n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    print('LSTM realforecast :',realForecastDictionary['LSTM'])\n",
    "    print('Bayseian realforecast :',realForecastDictionary['Bayseian'] ) \n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "#         listedLogPredict=test_predict[-1].tolist()\n",
    "#     return [np.exp(y) for y in listedLogPredict]\n",
    "    return np.square(test_predict[-1]).tolist()\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM testforecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian testforecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 4\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('walMonth24.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.2)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2010-02-01',\n",
       "  '2010-03-01',\n",
       "  '2010-04-01',\n",
       "  '2010-05-01',\n",
       "  '2010-06-01',\n",
       "  '2010-07-01',\n",
       "  '2010-08-01',\n",
       "  '2010-09-01',\n",
       "  '2010-10-01',\n",
       "  '2010-11-01',\n",
       "  '2010-12-01',\n",
       "  '2011-01-01',\n",
       "  '2011-02-01',\n",
       "  '2011-03-01',\n",
       "  '2011-04-01',\n",
       "  '2011-05-01',\n",
       "  '2011-06-01',\n",
       "  '2011-07-01',\n",
       "  '2011-08-01',\n",
       "  '2011-09-01',\n",
       "  '2011-10-01',\n",
       "  '2011-11-01',\n",
       "  '2011-12-01',\n",
       "  '2012-01-01'],\n",
       " [32990.769999999997,\n",
       "  22809.285,\n",
       "  30103.352000000003,\n",
       "  16673.537499999999,\n",
       "  16685.174999999999,\n",
       "  16383.002,\n",
       "  16144.702499999999,\n",
       "  17978.317500000001,\n",
       "  26928.906000000003,\n",
       "  23040.349999999999,\n",
       "  34796.775999999998,\n",
       "  17286.647499999999,\n",
       "  31440.657500000001,\n",
       "  20705.834999999999,\n",
       "  33011.389999999999,\n",
       "  17062.93,\n",
       "  15744.6425,\n",
       "  15771.246000000001,\n",
       "  14765.487499999999,\n",
       "  17551.281999999999,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArrayDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 0.24472439289093018\n",
      "[step: 1] loss: 0.12649907171726227\n",
      "[step: 2] loss: 0.05139520391821861\n",
      "[step: 3] loss: 0.012810866348445415\n",
      "[step: 4] loss: 0.003557896474376321\n",
      "[step: 5] loss: 0.013666285201907158\n",
      "[step: 6] loss: 0.029137030243873596\n",
      "[step: 7] loss: 0.038231976330280304\n",
      "[step: 8] loss: 0.038096074014902115\n",
      "[step: 9] loss: 0.03156135603785515\n",
      "[step: 10] loss: 0.0224484633654356\n",
      "[step: 11] loss: 0.013684406876564026\n",
      "[step: 12] loss: 0.006947314366698265\n",
      "[step: 13] loss: 0.0028865919448435307\n",
      "[step: 14] loss: 0.0014300041366368532\n",
      "[step: 15] loss: 0.002036311663687229\n",
      "[step: 16] loss: 0.00389351905323565\n",
      "[step: 17] loss: 0.006115085911005735\n",
      "[step: 18] loss: 0.007955591194331646\n",
      "[step: 19] loss: 0.008974166586995125\n",
      "[step: 20] loss: 0.009067922830581665\n",
      "[step: 21] loss: 0.00838521495461464\n",
      "[step: 22] loss: 0.007196047343313694\n",
      "[step: 23] loss: 0.005785008426755667\n",
      "[step: 24] loss: 0.004389102570712566\n",
      "[step: 25] loss: 0.0031755699310451746\n",
      "[step: 26] loss: 0.0022448976524174213\n",
      "[step: 27] loss: 0.001643782714381814\n",
      "[step: 28] loss: 0.0013766470365226269\n",
      "[step: 29] loss: 0.0014104167930781841\n",
      "[step: 30] loss: 0.0016752452356740832\n",
      "[step: 31] loss: 0.002069229958578944\n",
      "[step: 32] loss: 0.002473925007507205\n",
      "[step: 33] loss: 0.002780317096039653\n",
      "[step: 34] loss: 0.0029163509607315063\n",
      "[step: 35] loss: 0.002863403409719467\n",
      "[step: 36] loss: 0.0026536923833191395\n",
      "[step: 37] loss: 0.0023509967140853405\n",
      "[step: 38] loss: 0.002025432186201215\n",
      "[step: 39] loss: 0.001733641023747623\n",
      "[step: 40] loss: 0.0015101140597835183\n",
      "[step: 41] loss: 0.0013682199642062187\n",
      "[step: 42] loss: 0.0013059324119240046\n",
      "[step: 43] loss: 0.0013115985784679651\n",
      "[step: 44] loss: 0.0013676746748387814\n",
      "[step: 45] loss: 0.0014527823077514768\n",
      "[step: 46] loss: 0.0015435419045388699\n",
      "[step: 47] loss: 0.0016173970652744174\n",
      "[step: 48] loss: 0.001656719483435154\n",
      "[step: 49] loss: 0.0016529001295566559\n",
      "[step: 50] loss: 0.0016085971146821976\n",
      "[step: 51] loss: 0.0015364529099315405\n",
      "[step: 52] loss: 0.001454398618079722\n",
      "[step: 53] loss: 0.001379508525133133\n",
      "[step: 54] loss: 0.0013232110068202019\n",
      "[step: 55] loss: 0.0012898158747702837\n",
      "[step: 56] loss: 0.0012779710814356804\n",
      "[step: 57] loss: 0.0012833077926188707\n",
      "[step: 58] loss: 0.001300376490689814\n",
      "[step: 59] loss: 0.0013232831843197346\n",
      "[step: 60] loss: 0.001345817930996418\n",
      "[step: 61] loss: 0.0013620455283671618\n",
      "[step: 62] loss: 0.0013676215894520283\n",
      "[step: 63] loss: 0.0013611169997602701\n",
      "[step: 64] loss: 0.0013445571530610323\n",
      "[step: 65] loss: 0.0013225993607193232\n",
      "[step: 66] loss: 0.0013007712550461292\n",
      "[step: 67] loss: 0.0012835818342864513\n",
      "[step: 68] loss: 0.0012733343755826354\n",
      "[step: 69] loss: 0.0012700079241767526\n",
      "[step: 70] loss: 0.001272024353966117\n",
      "[step: 71] loss: 0.0012772358022630215\n",
      "[step: 72] loss: 0.0012835826491937041\n",
      "[step: 73] loss: 0.0012893176171928644\n",
      "[step: 74] loss: 0.0012930275406688452\n",
      "[step: 75] loss: 0.0012937667779624462\n",
      "[step: 76] loss: 0.0012912731617689133\n",
      "[step: 77] loss: 0.0012861595023423433\n",
      "[step: 78] loss: 0.0012797585222870111\n",
      "[step: 79] loss: 0.0012736738426610827\n",
      "[step: 80] loss: 0.0012692178133875132\n",
      "[step: 81] loss: 0.0012669828720390797\n",
      "[step: 82] loss: 0.0012668126728385687\n",
      "[step: 83] loss: 0.001268021296709776\n",
      "[step: 84] loss: 0.0012697987258434296\n",
      "[step: 85] loss: 0.0012714748736470938\n",
      "[step: 86] loss: 0.001272604102268815\n",
      "[step: 87] loss: 0.0012729148147627711\n",
      "[step: 88] loss: 0.0012722972314804792\n",
      "[step: 89] loss: 0.0012708459980785847\n",
      "[step: 90] loss: 0.0012688873102888465\n",
      "[step: 91] loss: 0.0012669116258621216\n",
      "[step: 92] loss: 0.0012653693556785583\n",
      "[step: 93] loss: 0.0012645157985389233\n",
      "[step: 94] loss: 0.0012643315130844712\n",
      "[step: 95] loss: 0.0012646112591028214\n",
      "[step: 96] loss: 0.0012650660937651992\n",
      "[step: 97] loss: 0.001265455037355423\n",
      "[step: 98] loss: 0.0012656233739107847\n",
      "[step: 99] loss: 0.0012655071914196014\n",
      "[step: 100] loss: 0.0012651177821680903\n",
      "[step: 101] loss: 0.001264502527192235\n",
      "[step: 102] loss: 0.0012637756299227476\n",
      "[step: 103] loss: 0.0012630673591047525\n",
      "[step: 104] loss: 0.0012625142699107528\n",
      "[step: 105] loss: 0.0012621799251064658\n",
      "[step: 106] loss: 0.0012620529159903526\n",
      "[step: 107] loss: 0.0012620496563613415\n",
      "[step: 108] loss: 0.0012620689813047647\n",
      "[step: 109] loss: 0.0012620278866961598\n",
      "[step: 110] loss: 0.0012618941254913807\n",
      "[step: 111] loss: 0.001261663855984807\n",
      "[step: 112] loss: 0.0012613548897206783\n",
      "[step: 113] loss: 0.0012609963305294514\n",
      "[step: 114] loss: 0.0012606310192495584\n",
      "[step: 115] loss: 0.0012602999340742826\n",
      "[step: 116] loss: 0.0012600289192050695\n",
      "[step: 117] loss: 0.0012598292669281363\n",
      "[step: 118] loss: 0.0012596827000379562\n",
      "[step: 119] loss: 0.0012595505686476827\n",
      "[step: 120] loss: 0.0012594060972332954\n",
      "[step: 121] loss: 0.001259226817637682\n",
      "[step: 122] loss: 0.0012590100523084402\n",
      "[step: 123] loss: 0.00125876534730196\n",
      "[step: 124] loss: 0.0012585094664245844\n",
      "[step: 125] loss: 0.0012582475319504738\n",
      "[step: 126] loss: 0.0012579963076859713\n",
      "[step: 127] loss: 0.0012577653396874666\n",
      "[step: 128] loss: 0.0012575550936162472\n",
      "[step: 129] loss: 0.0012573609128594398\n",
      "[step: 130] loss: 0.0012571741826832294\n",
      "[step: 131] loss: 0.001256980816833675\n",
      "[step: 132] loss: 0.0012567747617140412\n",
      "[step: 133] loss: 0.0012565548531711102\n",
      "[step: 134] loss: 0.001256326911970973\n",
      "[step: 135] loss: 0.0012560910545289516\n",
      "[step: 136] loss: 0.0012558542657643557\n",
      "[step: 137] loss: 0.001255628652870655\n",
      "[step: 138] loss: 0.0012554003624245524\n",
      "[step: 139] loss: 0.0012551841791719198\n",
      "[step: 140] loss: 0.0012549720704555511\n",
      "[step: 141] loss: 0.001254758914001286\n",
      "[step: 142] loss: 0.001254541683010757\n",
      "[step: 143] loss: 0.0012543206103146076\n",
      "[step: 144] loss: 0.0012540945317596197\n",
      "[step: 145] loss: 0.0012538640294224024\n",
      "[step: 146] loss: 0.0012536357389762998\n",
      "[step: 147] loss: 0.0012534030247479677\n",
      "[step: 148] loss: 0.0012531729880720377\n",
      "[step: 149] loss: 0.001252944115549326\n",
      "[step: 150] loss: 0.0012527169892564416\n",
      "[step: 151] loss: 0.0012524919584393501\n",
      "[step: 152] loss: 0.0012522669276222587\n",
      "[step: 153] loss: 0.0012520368909463286\n",
      "[step: 154] loss: 0.0012518065050244331\n",
      "[step: 155] loss: 0.0012515716953203082\n",
      "[step: 156] loss: 0.0012513335095718503\n",
      "[step: 157] loss: 0.0012511000968515873\n",
      "[step: 158] loss: 0.0012508661020547152\n",
      "[step: 159] loss: 0.0012506295461207628\n",
      "[step: 160] loss: 0.0012503954349085689\n",
      "[step: 161] loss: 0.0012501582968980074\n",
      "[step: 162] loss: 0.0012499222066253424\n",
      "[step: 163] loss: 0.0012496847193688154\n",
      "[step: 164] loss: 0.0012494451366364956\n",
      "[step: 165] loss: 0.0012492055539041758\n",
      "[step: 166] loss: 0.0012489616638049483\n",
      "[step: 167] loss: 0.0012487219646573067\n",
      "[step: 168] loss: 0.0012484791222959757\n",
      "[step: 169] loss: 0.0012482373276725411\n",
      "[step: 170] loss: 0.001247993903234601\n",
      "[step: 171] loss: 0.0012477502459660172\n",
      "[step: 172] loss: 0.0012475044932216406\n",
      "[step: 173] loss: 0.001247262698598206\n",
      "[step: 174] loss: 0.0012470143847167492\n",
      "[step: 175] loss: 0.0012467660708352923\n",
      "[step: 176] loss: 0.0012465196195989847\n",
      "[step: 177] loss: 0.0012462697923183441\n",
      "[step: 178] loss: 0.0012460221769288182\n",
      "[step: 179] loss: 0.0012457720004022121\n",
      "[step: 180] loss: 0.001245519844815135\n",
      "[step: 181] loss: 0.0012452718801796436\n",
      "[step: 182] loss: 0.0012450176291167736\n",
      "[step: 183] loss: 0.0012447650078684092\n",
      "[step: 184] loss: 0.0012445129686966538\n",
      "[step: 185] loss: 0.001244258601218462\n",
      "[step: 186] loss: 0.0012440037680789828\n",
      "[step: 187] loss: 0.00124374870210886\n",
      "[step: 188] loss: 0.0012434918899089098\n",
      "[step: 189] loss: 0.0012432346120476723\n",
      "[step: 190] loss: 0.001242977217771113\n",
      "[step: 191] loss: 0.0012427180772647262\n",
      "[step: 192] loss: 0.0012424599844962358\n",
      "[step: 193] loss: 0.0012421999126672745\n",
      "[step: 194] loss: 0.0012419414706528187\n",
      "[step: 195] loss: 0.0012416790705174208\n",
      "[step: 196] loss: 0.001241418533027172\n",
      "[step: 197] loss: 0.0012411554343998432\n",
      "[step: 198] loss: 0.0012408896582201123\n",
      "[step: 199] loss: 0.0012406266760081053\n",
      "[step: 200] loss: 0.0012403607834130526\n",
      "[step: 201] loss: 0.0012400946579873562\n",
      "[step: 202] loss: 0.0012398306280374527\n",
      "[step: 203] loss: 0.0012395628727972507\n",
      "[step: 204] loss: 0.0012392951175570488\n",
      "[step: 205] loss: 0.0012390294577926397\n",
      "[step: 206] loss: 0.001238759490661323\n",
      "[step: 207] loss: 0.0012384874280542135\n",
      "[step: 208] loss: 0.0012382178101688623\n",
      "[step: 209] loss: 0.0012379486579447985\n",
      "[step: 210] loss: 0.0012376754311844707\n",
      "[step: 211] loss: 0.0012374036014080048\n",
      "[step: 212] loss: 0.0012371304910629988\n",
      "[step: 213] loss: 0.0012368569150567055\n",
      "[step: 214] loss: 0.001236582058481872\n",
      "[step: 215] loss: 0.001236307667568326\n",
      "[step: 216] loss: 0.0012360326945781708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 217] loss: 0.0012357558589428663\n",
      "[step: 218] loss: 0.0012354813516139984\n",
      "[step: 219] loss: 0.0012352027697488666\n",
      "[step: 220] loss: 0.0012349230237305164\n",
      "[step: 221] loss: 0.001234645489603281\n",
      "[step: 222] loss: 0.0012343658600002527\n",
      "[step: 223] loss: 0.0012340834364295006\n",
      "[step: 224] loss: 0.0012338024098426104\n",
      "[step: 225] loss: 0.0012335206847637892\n",
      "[step: 226] loss: 0.0012332384940236807\n",
      "[step: 227] loss: 0.0012329549062997103\n",
      "[step: 228] loss: 0.0012326715514063835\n",
      "[step: 229] loss: 0.0012323884293437004\n",
      "[step: 230] loss: 0.0012321012327447534\n",
      "[step: 231] loss: 0.0012318184599280357\n",
      "[step: 232] loss: 0.0012315307976678014\n",
      "[step: 233] loss: 0.0012312415055930614\n",
      "[step: 234] loss: 0.001230957917869091\n",
      "[step: 235] loss: 0.001230668742209673\n",
      "[step: 236] loss: 0.0012303771218284965\n",
      "[step: 237] loss: 0.001230088178999722\n",
      "[step: 238] loss: 0.0012297978391870856\n",
      "[step: 239] loss: 0.0012295072665438056\n",
      "[step: 240] loss: 0.001229214365594089\n",
      "[step: 241] loss: 0.0012289222795516253\n",
      "[step: 242] loss: 0.0012286300770938396\n",
      "[step: 243] loss: 0.001228334498591721\n",
      "[step: 244] loss: 0.0012280406663194299\n",
      "[step: 245] loss: 0.0012277467176318169\n",
      "[step: 246] loss: 0.0012274495093151927\n",
      "[step: 247] loss: 0.0012271536979824305\n",
      "[step: 248] loss: 0.0012268568389117718\n",
      "[step: 249] loss: 0.0012265604455024004\n",
      "[step: 250] loss: 0.001226261374540627\n",
      "[step: 251] loss: 0.00122596207074821\n",
      "[step: 252] loss: 0.0012256607878953218\n",
      "[step: 253] loss: 0.0012253602035343647\n",
      "[step: 254] loss: 0.0012250598520040512\n",
      "[step: 255] loss: 0.0012247591512277722\n",
      "[step: 256] loss: 0.0012244557728990912\n",
      "[step: 257] loss: 0.001224152627401054\n",
      "[step: 258] loss: 0.0012238484341651201\n",
      "[step: 259] loss: 0.00122354575432837\n",
      "[step: 260] loss: 0.001223242375999689\n",
      "[step: 261] loss: 0.001222935738041997\n",
      "[step: 262] loss: 0.0012226277031004429\n",
      "[step: 263] loss: 0.0012223217636346817\n",
      "[step: 264] loss: 0.001222012797370553\n",
      "[step: 265] loss: 0.0012217055773362517\n",
      "[step: 266] loss: 0.001221397309564054\n",
      "[step: 267] loss: 0.0012210861314088106\n",
      "[step: 268] loss: 0.001220776466652751\n",
      "[step: 269] loss: 0.0012204658705741167\n",
      "[step: 270] loss: 0.0012201534118503332\n",
      "[step: 271] loss: 0.0012198425829410553\n",
      "[step: 272] loss: 0.0012195296585559845\n",
      "[step: 273] loss: 0.0012192162685096264\n",
      "[step: 274] loss: 0.0012188998516649008\n",
      "[step: 275] loss: 0.0012185845989733934\n",
      "[step: 276] loss: 0.001218270743265748\n",
      "[step: 277] loss: 0.001217954559251666\n",
      "[step: 278] loss: 0.0012176375603303313\n",
      "[step: 279] loss: 0.001217321609146893\n",
      "[step: 280] loss: 0.0012170011177659035\n",
      "[step: 281] loss: 0.001216684002429247\n",
      "[step: 282] loss: 0.0012163631618022919\n",
      "[step: 283] loss: 0.0012160425540059805\n",
      "[step: 284] loss: 0.0012157222954556346\n",
      "[step: 285] loss: 0.001215403201058507\n",
      "[step: 286] loss: 0.0012150779366493225\n",
      "[step: 287] loss: 0.0012147566303610802\n",
      "[step: 288] loss: 0.0012144322972744703\n",
      "[step: 289] loss: 0.001214109594002366\n",
      "[step: 290] loss: 0.0012137857265770435\n",
      "[step: 291] loss: 0.0012134597636759281\n",
      "[step: 292] loss: 0.0012131331022828817\n",
      "[step: 293] loss: 0.001212805276736617\n",
      "[step: 294] loss: 0.0012124765198677778\n",
      "[step: 295] loss: 0.0012121489271521568\n",
      "[step: 296] loss: 0.001211821218021214\n",
      "[step: 297] loss: 0.001211490947753191\n",
      "[step: 298] loss: 0.0012111628893762827\n",
      "[step: 299] loss: 0.0012108318042010069\n",
      "[step: 300] loss: 0.0012105011846870184\n",
      "[step: 301] loss: 0.0012101682368665934\n",
      "[step: 302] loss: 0.0012098324950784445\n",
      "[step: 303] loss: 0.001209502574056387\n",
      "[step: 304] loss: 0.0012091684620827436\n",
      "[step: 305] loss: 0.0012088324874639511\n",
      "[step: 306] loss: 0.0012084972113370895\n",
      "[step: 307] loss: 0.0012081614695489407\n",
      "[step: 308] loss: 0.00120782561134547\n",
      "[step: 309] loss: 0.0012074895203113556\n",
      "[step: 310] loss: 0.0012071493547409773\n",
      "[step: 311] loss: 0.0012068112846463919\n",
      "[step: 312] loss: 0.0012064706534147263\n",
      "[step: 313] loss: 0.0012061346787959337\n",
      "[step: 314] loss: 0.001205789390951395\n",
      "[step: 315] loss: 0.0012054507387802005\n",
      "[step: 316] loss: 0.0012051074299961329\n",
      "[step: 317] loss: 0.001204767031595111\n",
      "[step: 318] loss: 0.001204421161673963\n",
      "[step: 319] loss: 0.0012040791334584355\n",
      "[step: 320] loss: 0.0012037304695695639\n",
      "[step: 321] loss: 0.0012033864622935653\n",
      "[step: 322] loss: 0.0012030403595417738\n",
      "[step: 323] loss: 0.0012026948388665915\n",
      "[step: 324] loss: 0.0012023476883769035\n",
      "[step: 325] loss: 0.0012019990244880319\n",
      "[step: 326] loss: 0.001201652456074953\n",
      "[step: 327] loss: 0.001201301347464323\n",
      "[step: 328] loss: 0.0012009511701762676\n",
      "[step: 329] loss: 0.00120059703476727\n",
      "[step: 330] loss: 0.0012002476723864675\n",
      "[step: 331] loss: 0.0011998980771750212\n",
      "[step: 332] loss: 0.001199542311951518\n",
      "[step: 333] loss: 0.0011991900391876698\n",
      "[step: 334] loss: 0.0011988356709480286\n",
      "[step: 335] loss: 0.0011984800221398473\n",
      "[step: 336] loss: 0.0011981241405010223\n",
      "[step: 337] loss: 0.0011977694230154157\n",
      "[step: 338] loss: 0.0011974090011790395\n",
      "[step: 339] loss: 0.00119705218821764\n",
      "[step: 340] loss: 0.0011966931633651257\n",
      "[step: 341] loss: 0.001196336350403726\n",
      "[step: 342] loss: 0.0011959767434746027\n",
      "[step: 343] loss: 0.0011956149246543646\n",
      "[step: 344] loss: 0.0011952539207413793\n",
      "[step: 345] loss: 0.0011948943138122559\n",
      "[step: 346] loss: 0.0011945299338549376\n",
      "[step: 347] loss: 0.0011941688135266304\n",
      "[step: 348] loss: 0.0011938023380935192\n",
      "[step: 349] loss: 0.0011934384237974882\n",
      "[step: 350] loss: 0.0011930724140256643\n",
      "[step: 351] loss: 0.0011927059385925531\n",
      "[step: 352] loss: 0.0011923386482521892\n",
      "[step: 353] loss: 0.0011919725220650434\n",
      "[step: 354] loss: 0.001191604882478714\n",
      "[step: 355] loss: 0.0011912371264770627\n",
      "[step: 356] loss: 0.001190865645185113\n",
      "[step: 357] loss: 0.0011904968414455652\n",
      "[step: 358] loss: 0.001190125709399581\n",
      "[step: 359] loss: 0.0011897541116923094\n",
      "[step: 360] loss: 0.0011893813498318195\n",
      "[step: 361] loss: 0.0011890088208019733\n",
      "[step: 362] loss: 0.0011886339634656906\n",
      "[step: 363] loss: 0.001188260386697948\n",
      "[step: 364] loss: 0.0011878854129463434\n",
      "[step: 365] loss: 0.001187509624287486\n",
      "[step: 366] loss: 0.0011871315073221922\n",
      "[step: 367] loss: 0.0011867557186633348\n",
      "[step: 368] loss: 0.0011863751569762826\n",
      "[step: 369] loss: 0.0011859964579343796\n",
      "[step: 370] loss: 0.001185618108138442\n",
      "[step: 371] loss: 0.0011852376628667116\n",
      "[step: 372] loss: 0.001184855354949832\n",
      "[step: 373] loss: 0.0011844730470329523\n",
      "[step: 374] loss: 0.0011840929510071874\n",
      "[step: 375] loss: 0.0011837078491225839\n",
      "[step: 376] loss: 0.0011833251919597387\n",
      "[step: 377] loss: 0.0011829410213977098\n",
      "[step: 378] loss: 0.0011825549881905317\n",
      "[step: 379] loss: 0.0011821689549833536\n",
      "[step: 380] loss: 0.0011817801278084517\n",
      "[step: 381] loss: 0.0011813940946012735\n",
      "[step: 382] loss: 0.0011810061987489462\n",
      "[step: 383] loss: 0.001180614111945033\n",
      "[step: 384] loss: 0.0011802242370322347\n",
      "[step: 385] loss: 0.001179832499474287\n",
      "[step: 386] loss: 0.001179443090222776\n",
      "[step: 387] loss: 0.0011790510034188628\n",
      "[step: 388] loss: 0.0011786555405706167\n",
      "[step: 389] loss: 0.0011782650835812092\n",
      "[step: 390] loss: 0.0011778705520555377\n",
      "[step: 391] loss: 0.0011774752056226134\n",
      "[step: 392] loss: 0.0011770788114517927\n",
      "[step: 393] loss: 0.0011766807874664664\n",
      "[step: 394] loss: 0.001176282879896462\n",
      "[step: 395] loss: 0.0011758847394958138\n",
      "[step: 396] loss: 0.001175487064756453\n",
      "[step: 397] loss: 0.0011750860139727592\n",
      "[step: 398] loss: 0.00117468589451164\n",
      "[step: 399] loss: 0.0011742864735424519\n",
      "[step: 400] loss: 0.0011738839093595743\n",
      "[step: 401] loss: 0.0011734827421605587\n",
      "[step: 402] loss: 0.0011730779660865664\n",
      "[step: 403] loss: 0.0011726722586899996\n",
      "[step: 404] loss: 0.0011722655035555363\n",
      "[step: 405] loss: 0.0011718603782355785\n",
      "[step: 406] loss: 0.0011714519932866096\n",
      "[step: 407] loss: 0.001171046169474721\n",
      "[step: 408] loss: 0.0011706368532031775\n",
      "[step: 409] loss: 0.0011702276533469558\n",
      "[step: 410] loss: 0.0011698168236762285\n",
      "[step: 411] loss: 0.0011694064596667886\n",
      "[step: 412] loss: 0.0011689945822581649\n",
      "[step: 413] loss: 0.001168581424281001\n",
      "[step: 414] loss: 0.0011681660544127226\n",
      "[step: 415] loss: 0.0011677560396492481\n",
      "[step: 416] loss: 0.001167338341474533\n",
      "[step: 417] loss: 0.0011669218074530363\n",
      "[step: 418] loss: 0.0011665073689073324\n",
      "[step: 419] loss: 0.001166091300547123\n",
      "[step: 420] loss: 0.001165671506896615\n",
      "[step: 421] loss: 0.0011652514804154634\n",
      "[step: 422] loss: 0.001164833316579461\n",
      "[step: 423] loss: 0.0011644127080217004\n",
      "[step: 424] loss: 0.0011639896547421813\n",
      "[step: 425] loss: 0.0011635663686320186\n",
      "[step: 426] loss: 0.001163144363090396\n",
      "[step: 427] loss: 0.001162719912827015\n",
      "[step: 428] loss: 0.0011622956953942776\n",
      "[step: 429] loss: 0.00116186891682446\n",
      "[step: 430] loss: 0.0011614423710852861\n",
      "[step: 431] loss: 0.0011610165238380432\n",
      "[step: 432] loss: 0.0011605864856392145\n",
      "[step: 433] loss: 0.0011601578444242477\n",
      "[step: 434] loss: 0.0011597273405641317\n",
      "[step: 435] loss: 0.0011592959053814411\n",
      "[step: 436] loss: 0.001158863422460854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 437] loss: 0.0011584318708628416\n",
      "[step: 438] loss: 0.0011579979909583926\n",
      "[step: 439] loss: 0.001157563179731369\n",
      "[step: 440] loss: 0.0011571261566132307\n",
      "[step: 441] loss: 0.00115669216029346\n",
      "[step: 442] loss: 0.0011562567669898272\n",
      "[step: 443] loss: 0.0011558164842426777\n",
      "[step: 444] loss: 0.0011553775984793901\n",
      "[step: 445] loss: 0.00115493917837739\n",
      "[step: 446] loss: 0.001154496567323804\n",
      "[step: 447] loss: 0.0011540558189153671\n",
      "[step: 448] loss: 0.001153611927293241\n",
      "[step: 449] loss: 0.0011531682685017586\n",
      "[step: 450] loss: 0.0011527251917868853\n",
      "[step: 451] loss: 0.0011522802524268627\n",
      "[step: 452] loss: 0.0011518358951434493\n",
      "[step: 453] loss: 0.0011513869976624846\n",
      "[step: 454] loss: 0.0011509375181049109\n",
      "[step: 455] loss: 0.0011504903668537736\n",
      "[step: 456] loss: 0.0011500392574816942\n",
      "[step: 457] loss: 0.0011495882645249367\n",
      "[step: 458] loss: 0.0011491378536447883\n",
      "[step: 459] loss: 0.0011486838338896632\n",
      "[step: 460] loss: 0.0011482315603643656\n",
      "[step: 461] loss: 0.0011477770749479532\n",
      "[step: 462] loss: 0.00114732189103961\n",
      "[step: 463] loss: 0.0011468634475022554\n",
      "[step: 464] loss: 0.0011464066337794065\n",
      "[step: 465] loss: 0.0011459491215646267\n",
      "[step: 466] loss: 0.0011454904451966286\n",
      "[step: 467] loss: 0.0011450301390141249\n",
      "[step: 468] loss: 0.0011445691343396902\n",
      "[step: 469] loss: 0.0011441046372056007\n",
      "[step: 470] loss: 0.0011436414206400514\n",
      "[step: 471] loss: 0.001143177505582571\n",
      "[step: 472] loss: 0.0011427139397710562\n",
      "[step: 473] loss: 0.0011422468814998865\n",
      "[step: 474] loss: 0.0011417786590754986\n",
      "[step: 475] loss: 0.0011413099709898233\n",
      "[step: 476] loss: 0.0011408411664888263\n",
      "[step: 477] loss: 0.0011403730604797602\n",
      "[step: 478] loss: 0.0011398997157812119\n",
      "[step: 479] loss: 0.0011394263710826635\n",
      "[step: 480] loss: 0.0011389548890292645\n",
      "[step: 481] loss: 0.0011384778190404177\n",
      "[step: 482] loss: 0.001138004707172513\n",
      "[step: 483] loss: 0.0011375256581231952\n",
      "[step: 484] loss: 0.0011370494030416012\n",
      "[step: 485] loss: 0.0011365704704076052\n",
      "[step: 486] loss: 0.0011360917706042528\n",
      "[step: 487] loss: 0.0011356121394783258\n",
      "[step: 488] loss: 0.0011351276189088821\n",
      "[step: 489] loss: 0.0011346429819241166\n",
      "[step: 490] loss: 0.0011341621866449714\n",
      "[step: 491] loss: 0.001133677433244884\n",
      "[step: 492] loss: 0.0011331918649375439\n",
      "[step: 493] loss: 0.0011327029205858707\n",
      "[step: 494] loss: 0.0011322179343551397\n",
      "[step: 495] loss: 0.0011317257303744555\n",
      "[step: 496] loss: 0.0011312351562082767\n",
      "[step: 497] loss: 0.0011307431850582361\n",
      "[step: 498] loss: 0.001130252960138023\n",
      "[step: 499] loss: 0.0011297594755887985\n",
      "[step: 500] loss: 0.001129262731410563\n",
      "[step: 501] loss: 0.0011287666857242584\n",
      "[step: 502] loss: 0.0011282707564532757\n",
      "[step: 503] loss: 0.0011277713347226381\n",
      "[step: 504] loss: 0.0011272707488387823\n",
      "[step: 505] loss: 0.0011267702793702483\n",
      "[step: 506] loss: 0.0011262696934863925\n",
      "[step: 507] loss: 0.0011257678270339966\n",
      "[step: 508] loss: 0.0011252624681219459\n",
      "[step: 509] loss: 0.0011247568763792515\n",
      "[step: 510] loss: 0.0011242525652050972\n",
      "[step: 511] loss: 0.0011237435974180698\n",
      "[step: 512] loss: 0.0011232350952923298\n",
      "[step: 513] loss: 0.0011227261275053024\n",
      "[step: 514] loss: 0.0011222128523513675\n",
      "[step: 515] loss: 0.0011217007413506508\n",
      "[step: 516] loss: 0.0011211865348741412\n",
      "[step: 517] loss: 0.0011206723283976316\n",
      "[step: 518] loss: 0.0011201573070138693\n",
      "[step: 519] loss: 0.0011196397244930267\n",
      "[step: 520] loss: 0.001119119580835104\n",
      "[step: 521] loss: 0.0011186018818989396\n",
      "[step: 522] loss: 0.0011180806905031204\n",
      "[step: 523] loss: 0.0011175584513694048\n",
      "[step: 524] loss: 0.00111703434959054\n",
      "[step: 525] loss: 0.0011165097821503878\n",
      "[step: 526] loss: 0.0011159831192344427\n",
      "[step: 527] loss: 0.0011154571548104286\n",
      "[step: 528] loss: 0.0011149296769872308\n",
      "[step: 529] loss: 0.0011143985902890563\n",
      "[step: 530] loss: 0.0011138683184981346\n",
      "[step: 531] loss: 0.0011133347870782018\n",
      "[step: 532] loss: 0.001112800557166338\n",
      "[step: 533] loss: 0.001112267142161727\n",
      "[step: 534] loss: 0.00111172825563699\n",
      "[step: 535] loss: 0.0011111918138340116\n",
      "[step: 536] loss: 0.001110654091462493\n",
      "[step: 537] loss: 0.0011101134587079287\n",
      "[step: 538] loss: 0.0011095704976469278\n",
      "[step: 539] loss: 0.0011090287007391453\n",
      "[step: 540] loss: 0.0011084843426942825\n",
      "[step: 541] loss: 0.0011079376563429832\n",
      "[step: 542] loss: 0.0011073912028223276\n",
      "[step: 543] loss: 0.0011068428866565228\n",
      "[step: 544] loss: 0.0011062915436923504\n",
      "[step: 545] loss: 0.0011057407828047872\n",
      "[step: 546] loss: 0.0011051868787035346\n",
      "[step: 547] loss: 0.0011046330910176039\n",
      "[step: 548] loss: 0.0011040768586099148\n",
      "[step: 549] loss: 0.0011035187635570765\n",
      "[step: 550] loss: 0.0011029617162421346\n",
      "[step: 551] loss: 0.0011024012928828597\n",
      "[step: 552] loss: 0.0011018386576324701\n",
      "[step: 553] loss: 0.001101275673136115\n",
      "[step: 554] loss: 0.0011007108259946108\n",
      "[step: 555] loss: 0.0011001457460224628\n",
      "[step: 556] loss: 0.001099578570574522\n",
      "[step: 557] loss: 0.0010990099981427193\n",
      "[step: 558] loss: 0.0010984379332512617\n",
      "[step: 559] loss: 0.0010978667996823788\n",
      "[step: 560] loss: 0.001097292872145772\n",
      "[step: 561] loss: 0.001096717664040625\n",
      "[step: 562] loss: 0.0010961426887661219\n",
      "[step: 563] loss: 0.0010955643374472857\n",
      "[step: 564] loss: 0.001094983657822013\n",
      "[step: 565] loss: 0.0010944022797048092\n",
      "[step: 566] loss: 0.001093819155357778\n",
      "[step: 567] loss: 0.0010932348668575287\n",
      "[step: 568] loss: 0.0010926492977887392\n",
      "[step: 569] loss: 0.0010920600034296513\n",
      "[step: 570] loss: 0.0010914710583165288\n",
      "[step: 571] loss: 0.0010908800177276134\n",
      "[step: 572] loss: 0.0010902893263846636\n",
      "[step: 573] loss: 0.001089693047106266\n",
      "[step: 574] loss: 0.0010890975827351213\n",
      "[step: 575] loss: 0.001088496996089816\n",
      "[step: 576] loss: 0.001087899785488844\n",
      "[step: 577] loss: 0.0010872986167669296\n",
      "[step: 578] loss: 0.0010866959346458316\n",
      "[step: 579] loss: 0.0010860917391255498\n",
      "[step: 580] loss: 0.0010854846332222223\n",
      "[step: 581] loss: 0.0010848802048712969\n",
      "[step: 582] loss: 0.0010842687916010618\n",
      "[step: 583] loss: 0.0010836548171937466\n",
      "[step: 584] loss: 0.0010830429382622242\n",
      "[step: 585] loss: 0.001082426868379116\n",
      "[step: 586] loss: 0.0010818133596330881\n",
      "[step: 587] loss: 0.0010811928659677505\n",
      "[step: 588] loss: 0.0010805752826854587\n",
      "[step: 589] loss: 0.0010799497831612825\n",
      "[step: 590] loss: 0.0010793270776048303\n",
      "[step: 591] loss: 0.0010787013452500105\n",
      "[step: 592] loss: 0.0010780736338347197\n",
      "[step: 593] loss: 0.0010774442926049232\n",
      "[step: 594] loss: 0.0010768112260848284\n",
      "[step: 595] loss: 0.0010761783923953772\n",
      "[step: 596] loss: 0.0010755453258752823\n",
      "[step: 597] loss: 0.0010749066714197397\n",
      "[step: 598] loss: 0.0010742658050730824\n",
      "[step: 599] loss: 0.001073623774573207\n",
      "[step: 600] loss: 0.001072980696335435\n",
      "[step: 601] loss: 0.0010723359882831573\n",
      "[step: 602] loss: 0.001071689184755087\n",
      "[step: 603] loss: 0.0010710416827350855\n",
      "[step: 604] loss: 0.0010703894076868892\n",
      "[step: 605] loss: 0.001069736434146762\n",
      "[step: 606] loss: 0.0010690826456993818\n",
      "[step: 607] loss: 0.001068424666300416\n",
      "[step: 608] loss: 0.0010677652899175882\n",
      "[step: 609] loss: 0.0010671019554138184\n",
      "[step: 610] loss: 0.001066437573172152\n",
      "[step: 611] loss: 0.0010657738894224167\n",
      "[step: 612] loss: 0.0010651045013219118\n",
      "[step: 613] loss: 0.0010644369758665562\n",
      "[step: 614] loss: 0.0010637658415362239\n",
      "[step: 615] loss: 0.0010630900505930185\n",
      "[step: 616] loss: 0.0010624132119119167\n",
      "[step: 617] loss: 0.0010617328807711601\n",
      "[step: 618] loss: 0.0010610544122755527\n",
      "[step: 619] loss: 0.0010603696573525667\n",
      "[step: 620] loss: 0.0010596830397844315\n",
      "[step: 621] loss: 0.0010589968878775835\n",
      "[step: 622] loss: 0.0010583067778497934\n",
      "[step: 623] loss: 0.0010576145723462105\n",
      "[step: 624] loss: 0.0010569191072136164\n",
      "[step: 625] loss: 0.001056221779435873\n",
      "[step: 626] loss: 0.001055523520335555\n",
      "[step: 627] loss: 0.0010548214195296168\n",
      "[step: 628] loss: 0.0010541173396632075\n",
      "[step: 629] loss: 0.0010534118628129363\n",
      "[step: 630] loss: 0.0010527026606723666\n",
      "[step: 631] loss: 0.0010519912466406822\n",
      "[step: 632] loss: 0.0010512762237340212\n",
      "[step: 633] loss: 0.0010505617829039693\n",
      "[step: 634] loss: 0.0010498426854610443\n",
      "[step: 635] loss: 0.0010491183493286371\n",
      "[step: 636] loss: 0.001048397389240563\n",
      "[step: 637] loss: 0.0010476714232936502\n",
      "[step: 638] loss: 0.001046941732056439\n",
      "[step: 639] loss: 0.001046209828928113\n",
      "[step: 640] loss: 0.0010454785078763962\n",
      "[step: 641] loss: 0.0010447405511513352\n",
      "[step: 642] loss: 0.0010439995676279068\n",
      "[step: 643] loss: 0.0010432584676891565\n",
      "[step: 644] loss: 0.0010425120126456022\n",
      "[step: 645] loss: 0.0010417678859084845\n",
      "[step: 646] loss: 0.0010410158429294825\n",
      "[step: 647] loss: 0.0010402626357972622\n",
      "[step: 648] loss: 0.0010395077988505363\n",
      "[step: 649] loss: 0.0010387483052909374\n",
      "[step: 650] loss: 0.0010379862505942583\n",
      "[step: 651] loss: 0.0010372218675911427\n",
      "[step: 652] loss: 0.0010364552726969123\n",
      "[step: 653] loss: 0.0010356849525123835\n",
      "[step: 654] loss: 0.001034911023452878\n",
      "[step: 655] loss: 0.001034136163070798\n",
      "[step: 656] loss: 0.0010333575773984194\n",
      "[step: 657] loss: 0.001032577478326857\n",
      "[step: 658] loss: 0.0010317908599972725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 659] loss: 0.001031004823744297\n",
      "[step: 660] loss: 0.0010302134323865175\n",
      "[step: 661] loss: 0.001029420644044876\n",
      "[step: 662] loss: 0.0010286231990903616\n",
      "[step: 663] loss: 0.001027823076583445\n",
      "[step: 664] loss: 0.0010270203929394484\n",
      "[step: 665] loss: 0.0010262143332511187\n",
      "[step: 666] loss: 0.001025402219966054\n",
      "[step: 667] loss: 0.0010245905723422766\n",
      "[step: 668] loss: 0.0010237740352749825\n",
      "[step: 669] loss: 0.0010229558683931828\n",
      "[step: 670] loss: 0.0010221336269751191\n",
      "[step: 671] loss: 0.001021307660266757\n",
      "[step: 672] loss: 0.0010204779682680964\n",
      "[step: 673] loss: 0.0010196459479629993\n",
      "[step: 674] loss: 0.0010188099695369601\n",
      "[step: 675] loss: 0.0010179686360061169\n",
      "[step: 676] loss: 0.00101712706964463\n",
      "[step: 677] loss: 0.0010162802645936608\n",
      "[step: 678] loss: 0.0010154314804822206\n",
      "[step: 679] loss: 0.0010145781561732292\n",
      "[step: 680] loss: 0.0010137201752513647\n",
      "[step: 681] loss: 0.001012858934700489\n",
      "[step: 682] loss: 0.001011995249427855\n",
      "[step: 683] loss: 0.0010111266747117043\n",
      "[step: 684] loss: 0.001010257052257657\n",
      "[step: 685] loss: 0.001009379862807691\n",
      "[step: 686] loss: 0.0010084998793900013\n",
      "[step: 687] loss: 0.0010076167527586222\n",
      "[step: 688] loss: 0.0010067312978208065\n",
      "[step: 689] loss: 0.0010058413026854396\n",
      "[step: 690] loss: 0.0010049473494291306\n",
      "[step: 691] loss: 0.0010040475754067302\n",
      "[step: 692] loss: 0.0010031445417553186\n",
      "[step: 693] loss: 0.0010022384813055396\n",
      "[step: 694] loss: 0.0010013296268880367\n",
      "[step: 695] loss: 0.0010004143696278334\n",
      "[step: 696] loss: 0.000999497133307159\n",
      "[step: 697] loss: 0.000998571515083313\n",
      "[step: 698] loss: 0.0009976467117667198\n",
      "[step: 699] loss: 0.0009967151563614607\n",
      "[step: 700] loss: 0.0009957815054804087\n",
      "[step: 701] loss: 0.0009948413353413343\n",
      "[step: 702] loss: 0.0009938967414200306\n",
      "[step: 703] loss: 0.0009929481893777847\n",
      "[step: 704] loss: 0.000991995446383953\n",
      "[step: 705] loss: 0.0009910383960232139\n",
      "[step: 706] loss: 0.0009900798322632909\n",
      "[step: 707] loss: 0.0009891113732010126\n",
      "[step: 708] loss: 0.0009881422156468034\n",
      "[step: 709] loss: 0.0009871665388345718\n",
      "[step: 710] loss: 0.0009861874859780073\n",
      "[step: 711] loss: 0.0009852026123553514\n",
      "[step: 712] loss: 0.0009842122672125697\n",
      "[step: 713] loss: 0.000983221223577857\n",
      "[step: 714] loss: 0.0009822219144552946\n",
      "[step: 715] loss: 0.0009812189964577556\n",
      "[step: 716] loss: 0.0009802107233554125\n",
      "[step: 717] loss: 0.0009791997727006674\n",
      "[step: 718] loss: 0.000978181604295969\n",
      "[step: 719] loss: 0.0009771576151251793\n",
      "[step: 720] loss: 0.0009761302499100566\n",
      "[step: 721] loss: 0.0009750986355356872\n",
      "[step: 722] loss: 0.0009740610257722437\n",
      "[step: 723] loss: 0.0009730164892971516\n",
      "[step: 724] loss: 0.0009719686349853873\n",
      "[step: 725] loss: 0.000970916822552681\n",
      "[step: 726] loss: 0.0009698565118014812\n",
      "[step: 727] loss: 0.0009687936399132013\n",
      "[step: 728] loss: 0.0009677240159362555\n",
      "[step: 729] loss: 0.0009666506666690111\n",
      "[step: 730] loss: 0.0009655674220994115\n",
      "[step: 731] loss: 0.0009644819074310362\n",
      "[step: 732] loss: 0.0009633898735046387\n",
      "[step: 733] loss: 0.0009622948709875345\n",
      "[step: 734] loss: 0.0009611922432668507\n",
      "[step: 735] loss: 0.0009600849589332938\n",
      "[step: 736] loss: 0.0009589710971340537\n",
      "[step: 737] loss: 0.0009578501340001822\n",
      "[step: 738] loss: 0.0009567234083078802\n",
      "[step: 739] loss: 0.0009555933647789061\n",
      "[step: 740] loss: 0.0009544554050080478\n",
      "[step: 741] loss: 0.0009533117990940809\n",
      "[step: 742] loss: 0.0009521640022285283\n",
      "[step: 743] loss: 0.0009510086383670568\n",
      "[step: 744] loss: 0.0009498461149632931\n",
      "[step: 745] loss: 0.0009486774215474725\n",
      "[step: 746] loss: 0.0009475016850046813\n",
      "[step: 747] loss: 0.0009463248425163329\n",
      "[step: 748] loss: 0.0009451385121792555\n",
      "[step: 749] loss: 0.0009439443238079548\n",
      "[step: 750] loss: 0.0009427431505173445\n",
      "[step: 751] loss: 0.0009415367967449129\n",
      "[step: 752] loss: 0.0009403245057910681\n",
      "[step: 753] loss: 0.0009391064522787929\n",
      "[step: 754] loss: 0.0009378795512020588\n",
      "[step: 755] loss: 0.0009366475278511643\n",
      "[step: 756] loss: 0.000935408053919673\n",
      "[step: 757] loss: 0.000934160896576941\n",
      "[step: 758] loss: 0.0009329061722382903\n",
      "[step: 759] loss: 0.0009316456853412092\n",
      "[step: 760] loss: 0.0009303826955147088\n",
      "[step: 761] loss: 0.0009291056194342673\n",
      "[step: 762] loss: 0.0009278234210796654\n",
      "[step: 763] loss: 0.0009265383123420179\n",
      "[step: 764] loss: 0.0009252380696125329\n",
      "[step: 765] loss: 0.0009239374194294214\n",
      "[step: 766] loss: 0.0009226250695064664\n",
      "[step: 767] loss: 0.0009213085286319256\n",
      "[step: 768] loss: 0.0009199840133078396\n",
      "[step: 769] loss: 0.0009186480310745537\n",
      "[step: 770] loss: 0.0009173101279884577\n",
      "[step: 771] loss: 0.0009159609326161444\n",
      "[step: 772] loss: 0.0009146067895926535\n",
      "[step: 773] loss: 0.0009132428094744682\n",
      "[step: 774] loss: 0.000911870039999485\n",
      "[step: 775] loss: 0.0009104917990043759\n",
      "[step: 776] loss: 0.0009091038955375552\n",
      "[step: 777] loss: 0.0009077106951735914\n",
      "[step: 778] loss: 0.0009063056786544621\n",
      "[step: 779] loss: 0.0009048930369317532\n",
      "[step: 780] loss: 0.0009034756803885102\n",
      "[step: 781] loss: 0.0009020466823130846\n",
      "[step: 782] loss: 0.0009006111649796367\n",
      "[step: 783] loss: 0.000899167382158339\n",
      "[step: 784] loss: 0.0008977135294117033\n",
      "[step: 785] loss: 0.0008962519350461662\n",
      "[step: 786] loss: 0.0008947813184931874\n",
      "[step: 787] loss: 0.0008933022036217153\n",
      "[step: 788] loss: 0.0008918153471313417\n",
      "[step: 789] loss: 0.0008903215057216585\n",
      "[step: 790] loss: 0.0008888168958947062\n",
      "[step: 791] loss: 0.0008873031474649906\n",
      "[step: 792] loss: 0.0008857797947712243\n",
      "[step: 793] loss: 0.0008842481183819473\n",
      "[step: 794] loss: 0.0008827082929201424\n",
      "[step: 795] loss: 0.0008811572333797812\n",
      "[step: 796] loss: 0.0008795990725047886\n",
      "[step: 797] loss: 0.0008780319476500154\n",
      "[step: 798] loss: 0.0008764557424001396\n",
      "[step: 799] loss: 0.0008748689433559775\n",
      "[step: 800] loss: 0.0008732719579711556\n",
      "[step: 801] loss: 0.0008716683369129896\n",
      "[step: 802] loss: 0.0008700554608367383\n",
      "[step: 803] loss: 0.0008684304775670171\n",
      "[step: 804] loss: 0.0008667957736179233\n",
      "[step: 805] loss: 0.0008651523385196924\n",
      "[step: 806] loss: 0.0008634988917037845\n",
      "[step: 807] loss: 0.0008618372376076877\n",
      "[step: 808] loss: 0.0008601646986790001\n",
      "[step: 809] loss: 0.0008584805182181299\n",
      "[step: 810] loss: 0.0008567885961383581\n",
      "[step: 811] loss: 0.0008550862548872828\n",
      "[step: 812] loss: 0.0008533751824870706\n",
      "[step: 813] loss: 0.000851652876008302\n",
      "[step: 814] loss: 0.0008499196264892817\n",
      "[step: 815] loss: 0.0008481786935590208\n",
      "[step: 816] loss: 0.0008464254206046462\n",
      "[step: 817] loss: 0.0008446610881946981\n",
      "[step: 818] loss: 0.0008428886067122221\n",
      "[step: 819] loss: 0.0008411041926592588\n",
      "[step: 820] loss: 0.0008393092430196702\n",
      "[step: 821] loss: 0.0008375055040232837\n",
      "[step: 822] loss: 0.0008356904145330191\n",
      "[step: 823] loss: 0.0008338629268109798\n",
      "[step: 824] loss: 0.0008320292108692229\n",
      "[step: 825] loss: 0.0008301818743348122\n",
      "[step: 826] loss: 0.0008283252827823162\n",
      "[step: 827] loss: 0.0008264604839496315\n",
      "[step: 828] loss: 0.0008245835779234767\n",
      "[step: 829] loss: 0.0008227046928368509\n",
      "[step: 830] loss: 0.0008208478102460504\n",
      "[step: 831] loss: 0.0008191034430637956\n",
      "[step: 832] loss: 0.0008178651332855225\n",
      "[step: 833] loss: 0.0008188695064745843\n",
      "[step: 834] loss: 0.0008287143427878618\n",
      "[step: 835] loss: 0.0008719126926735044\n",
      "[step: 836] loss: 0.0009933356195688248\n",
      "[step: 837] loss: 0.001186219509691\n",
      "[step: 838] loss: 0.0011647131759673357\n",
      "[step: 839] loss: 0.0008751052082516253\n",
      "[step: 840] loss: 0.0008498191018588841\n",
      "[step: 841] loss: 0.0010407079244032502\n",
      "[step: 842] loss: 0.000926964683458209\n",
      "[step: 843] loss: 0.0008054455392993987\n",
      "[step: 844] loss: 0.0009472623351030052\n",
      "[step: 845] loss: 0.0009002067963592708\n",
      "[step: 846] loss: 0.0007998109795153141\n",
      "[step: 847] loss: 0.0009029256762005389\n",
      "[step: 848] loss: 0.0008606630144640803\n",
      "[step: 849] loss: 0.0007992669707164168\n",
      "[step: 850] loss: 0.0008765681413933635\n",
      "[step: 851] loss: 0.0008261626353487372\n",
      "[step: 852] loss: 0.0008021687972359359\n",
      "[step: 853] loss: 0.0008537873509339988\n",
      "[step: 854] loss: 0.0008016545325517654\n",
      "[step: 855] loss: 0.0008058497915044427\n",
      "[step: 856] loss: 0.0008311278652399778\n",
      "[step: 857] loss: 0.0007875782321207225\n",
      "[step: 858] loss: 0.0008067801245488226\n",
      "[step: 859] loss: 0.0008100329432636499\n",
      "[step: 860] loss: 0.0007814780110493302\n",
      "[step: 861] loss: 0.0008031968027353287\n",
      "[step: 862] loss: 0.0007930684951134026\n",
      "[step: 863] loss: 0.0007792700780555606\n",
      "[step: 864] loss: 0.0007960206130519509\n",
      "[step: 865] loss: 0.0007809395319782197\n",
      "[step: 866] loss: 0.0007777157588861883\n",
      "[step: 867] loss: 0.0007871766574680805\n",
      "[step: 868] loss: 0.000772863975726068\n",
      "[step: 869] loss: 0.0007750888471491635\n",
      "[step: 870] loss: 0.0007784808985888958\n",
      "[step: 871] loss: 0.0007672386127524078\n",
      "[step: 872] loss: 0.0007711867801845074\n",
      "[step: 873] loss: 0.0007707740878686309\n",
      "[step: 874] loss: 0.0007627301383763552\n",
      "[step: 875] loss: 0.0007663904689252377\n",
      "[step: 876] loss: 0.0007641848642379045\n",
      "[step: 877] loss: 0.0007585081621073186\n",
      "[step: 878] loss: 0.0007612030021846294\n",
      "[step: 879] loss: 0.0007584548438899219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 880] loss: 0.0007542454986833036\n",
      "[step: 881] loss: 0.0007559215882793069\n",
      "[step: 882] loss: 0.0007532855961471796\n",
      "[step: 883] loss: 0.000749852042645216\n",
      "[step: 884] loss: 0.0007506865658797324\n",
      "[step: 885] loss: 0.000748421298339963\n",
      "[step: 886] loss: 0.0007453711004927754\n",
      "[step: 887] loss: 0.0007455243612639606\n",
      "[step: 888] loss: 0.0007437010062858462\n",
      "[step: 889] loss: 0.0007408509263768792\n",
      "[step: 890] loss: 0.0007404139614664018\n",
      "[step: 891] loss: 0.0007390093524008989\n",
      "[step: 892] loss: 0.0007363441400229931\n",
      "[step: 893] loss: 0.0007353475084528327\n",
      "[step: 894] loss: 0.0007342554163187742\n",
      "[step: 895] loss: 0.0007318667485378683\n",
      "[step: 896] loss: 0.0007303573656827211\n",
      "[step: 897] loss: 0.0007293744711205363\n",
      "[step: 898] loss: 0.0007273794617503881\n",
      "[step: 899] loss: 0.0007254896918311715\n",
      "[step: 900] loss: 0.0007243636646308005\n",
      "[step: 901] loss: 0.0007227860623970628\n",
      "[step: 902] loss: 0.0007207826711237431\n",
      "[step: 903] loss: 0.0007193080964498222\n",
      "[step: 904] loss: 0.0007179768290370703\n",
      "[step: 905] loss: 0.0007161588873714209\n",
      "[step: 906] loss: 0.0007143663242459297\n",
      "[step: 907] loss: 0.0007129519362933934\n",
      "[step: 908] loss: 0.0007114188047125936\n",
      "[step: 909] loss: 0.0007095993496477604\n",
      "[step: 910] loss: 0.000707911211065948\n",
      "[step: 911] loss: 0.0007064280798658729\n",
      "[step: 912] loss: 0.0007048139814287424\n",
      "[step: 913] loss: 0.0007030322449281812\n",
      "[step: 914] loss: 0.0007013464346528053\n",
      "[step: 915] loss: 0.0006997933378443122\n",
      "[step: 916] loss: 0.0006981645128689706\n",
      "[step: 917] loss: 0.0006964128697291017\n",
      "[step: 918] loss: 0.0006946935318410397\n",
      "[step: 919] loss: 0.0006930677336640656\n",
      "[step: 920] loss: 0.000691434892360121\n",
      "[step: 921] loss: 0.0006897185230627656\n",
      "[step: 922] loss: 0.0006879676366224885\n",
      "[step: 923] loss: 0.000686265469994396\n",
      "[step: 924] loss: 0.0006846023024991155\n",
      "[step: 925] loss: 0.0006829169578850269\n",
      "[step: 926] loss: 0.0006811819039285183\n",
      "[step: 927] loss: 0.0006794268265366554\n",
      "[step: 928] loss: 0.000677694333717227\n",
      "[step: 929] loss: 0.0006759822135791183\n",
      "[step: 930] loss: 0.0006742660189047456\n",
      "[step: 931] loss: 0.000672527588903904\n",
      "[step: 932] loss: 0.0006707661086693406\n",
      "[step: 933] loss: 0.0006689943838864565\n",
      "[step: 934] loss: 0.0006672291783615947\n",
      "[step: 935] loss: 0.0006654700264334679\n",
      "[step: 936] loss: 0.0006637120968662202\n",
      "[step: 937] loss: 0.0006619499763473868\n",
      "[step: 938] loss: 0.000660176738165319\n",
      "[step: 939] loss: 0.0006583889480680227\n",
      "[step: 940] loss: 0.0006565942312590778\n",
      "[step: 941] loss: 0.000654790666885674\n",
      "[step: 942] loss: 0.0006529833772219718\n",
      "[step: 943] loss: 0.0006511749234050512\n",
      "[step: 944] loss: 0.0006493555847555399\n",
      "[step: 945] loss: 0.0006475398549810052\n",
      "[step: 946] loss: 0.0006457134149968624\n",
      "[step: 947] loss: 0.000643885345198214\n",
      "[step: 948] loss: 0.0006420486606657505\n",
      "[step: 949] loss: 0.0006402130238711834\n",
      "[step: 950] loss: 0.0006383743602782488\n",
      "[step: 951] loss: 0.000636530399788171\n",
      "[step: 952] loss: 0.0006346968584693968\n",
      "[step: 953] loss: 0.0006328863091766834\n",
      "[step: 954] loss: 0.0006311486940830946\n",
      "[step: 955] loss: 0.0006296175997704268\n",
      "[step: 956] loss: 0.0006286720745265484\n",
      "[step: 957] loss: 0.0006294535123743117\n",
      "[step: 958] loss: 0.0006354746874421835\n",
      "[step: 959] loss: 0.0006577561143785715\n",
      "[step: 960] loss: 0.0007299166172742844\n",
      "[step: 961] loss: 0.0009396085515618324\n",
      "[step: 962] loss: 0.0014007481513544917\n",
      "[step: 963] loss: 0.001845896360464394\n",
      "[step: 964] loss: 0.0013993692118674517\n",
      "[step: 965] loss: 0.0006396317621693015\n",
      "[step: 966] loss: 0.0009781867265701294\n",
      "[step: 967] loss: 0.001260316465049982\n",
      "[step: 968] loss: 0.0006953227566555142\n",
      "[step: 969] loss: 0.0008106526802293956\n",
      "[step: 970] loss: 0.0010387812508270144\n",
      "[step: 971] loss: 0.0006364709697663784\n",
      "[step: 972] loss: 0.0008194746915251017\n",
      "[step: 973] loss: 0.0008486653678119183\n",
      "[step: 974] loss: 0.0006124494830146432\n",
      "[step: 975] loss: 0.0008316935854963958\n",
      "[step: 976] loss: 0.0006821051938459277\n",
      "[step: 977] loss: 0.0006683670217171311\n",
      "[step: 978] loss: 0.0007650331826880574\n",
      "[step: 979] loss: 0.0006071244133636355\n",
      "[step: 980] loss: 0.0007232874631881714\n",
      "[step: 981] loss: 0.0006534364074468613\n",
      "[step: 982] loss: 0.0006371614872477949\n",
      "[step: 983] loss: 0.0006929070223122835\n",
      "[step: 984] loss: 0.0006021750159561634\n",
      "[step: 985] loss: 0.0006730494787916541\n",
      "[step: 986] loss: 0.0006216165493242443\n",
      "[step: 987] loss: 0.0006256587221287191\n",
      "[step: 988] loss: 0.0006457081763073802\n",
      "[step: 989] loss: 0.0005980097921565175\n",
      "[step: 990] loss: 0.0006412408547475934\n",
      "[step: 991] loss: 0.0006019412539899349\n",
      "[step: 992] loss: 0.0006153359427116811\n",
      "[step: 993] loss: 0.0006161302444525063\n",
      "[step: 994] loss: 0.0005939446273259819\n",
      "[step: 995] loss: 0.0006175334565341473\n",
      "[step: 996] loss: 0.0005907835438847542\n",
      "[step: 997] loss: 0.0006035708356648684\n",
      "[step: 998] loss: 0.0005979703855700791\n",
      "[step: 999] loss: 0.0005881842225790024\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "testY is:  [15744.6425, 15771.246000000001, 14765.487499999999, 17551.281999999999]\n",
      "\n",
      "\n",
      "LSTM testforecast : [34515.77734375, 14878.9560546875, 28234.25390625, 18409.744140625] \n",
      "@@@@@LSTM rmse:  11568.2357171\n",
      "Bayseian testforecast : [20250.530192238555, 20602.191713573942, 20531.075513562486, 23107.247237178413] \n",
      "@@@@@Bayseian rmse:  5190.17859137\n",
      "\n",
      "\n",
      "Bayseian WON!!!!!!\n",
      "[step: 0] loss: 0.5123435854911804\n",
      "[step: 1] loss: 0.2538360059261322\n",
      "[step: 2] loss: 0.0952458456158638\n",
      "[step: 3] loss: 0.02065078541636467\n",
      "[step: 4] loss: 0.010768206790089607\n",
      "[step: 5] loss: 0.03930509090423584\n",
      "[step: 6] loss: 0.07239124923944473\n",
      "[step: 7] loss: 0.08705854415893555\n",
      "[step: 8] loss: 0.08159947395324707\n",
      "[step: 9] loss: 0.06418481469154358\n",
      "[step: 10] loss: 0.043285071849823\n",
      "[step: 11] loss: 0.02480696327984333\n",
      "[step: 12] loss: 0.011801982298493385\n",
      "[step: 13] loss: 0.005093918181955814\n",
      "[step: 14] loss: 0.004001750145107508\n",
      "[step: 15] loss: 0.006920722778886557\n",
      "[step: 16] loss: 0.01180326845496893\n",
      "[step: 17] loss: 0.01665617525577545\n",
      "[step: 18] loss: 0.020025955513119698\n",
      "[step: 19] loss: 0.021265815943479538\n",
      "[step: 20] loss: 0.020464157685637474\n",
      "[step: 21] loss: 0.01815798319876194\n",
      "[step: 22] loss: 0.01503375731408596\n",
      "[step: 23] loss: 0.0117233507335186\n",
      "[step: 24] loss: 0.00870734266936779\n",
      "[step: 25] loss: 0.006296277977526188\n",
      "[step: 26] loss: 0.004652203992009163\n",
      "[step: 27] loss: 0.003818693570792675\n",
      "[step: 28] loss: 0.0037395041435956955\n",
      "[step: 29] loss: 0.004264669492840767\n",
      "[step: 30] loss: 0.005160216707736254\n",
      "[step: 31] loss: 0.006140769459307194\n",
      "[step: 32] loss: 0.0069295079447329044\n",
      "[step: 33] loss: 0.007328475825488567\n",
      "[step: 34] loss: 0.007267989218235016\n",
      "[step: 35] loss: 0.006809824611991644\n",
      "[step: 36] loss: 0.0061041503213346004\n",
      "[step: 37] loss: 0.0053252773359417915\n",
      "[step: 38] loss: 0.004617697559297085\n",
      "[step: 39] loss: 0.0040710377506911755\n",
      "[step: 40] loss: 0.0037218972574919462\n",
      "[step: 41] loss: 0.00356829771772027\n",
      "[step: 42] loss: 0.0035835469607263803\n",
      "[step: 43] loss: 0.003724447451531887\n",
      "[step: 44] loss: 0.003936083521693945\n",
      "[step: 45] loss: 0.004157707095146179\n",
      "[step: 46] loss: 0.004332008771598339\n",
      "[step: 47] loss: 0.004416666924953461\n",
      "[step: 48] loss: 0.004393917508423328\n",
      "[step: 49] loss: 0.004274260252714157\n",
      "[step: 50] loss: 0.004091543145477772\n",
      "[step: 51] loss: 0.003890690393745899\n",
      "[step: 52] loss: 0.003712900448590517\n",
      "[step: 53] loss: 0.003584636840969324\n",
      "[step: 54] loss: 0.0035145035944879055\n",
      "[step: 55] loss: 0.003497219178825617\n",
      "[step: 56] loss: 0.0035202777944505215\n",
      "[step: 57] loss: 0.003568753134459257\n",
      "[step: 58] loss: 0.00362696242518723\n",
      "[step: 59] loss: 0.0036790906451642513\n",
      "[step: 60] loss: 0.003710963763296604\n",
      "[step: 61] loss: 0.0037136529572308064\n",
      "[step: 62] loss: 0.0036865542642772198\n",
      "[step: 63] loss: 0.0036376696079969406\n",
      "[step: 64] loss: 0.0035804989747703075\n",
      "[step: 65] loss: 0.0035288671497255564\n",
      "[step: 66] loss: 0.0034923641942441463\n",
      "[step: 67] loss: 0.0034743044525384903\n",
      "[step: 68] loss: 0.0034725498408079147\n",
      "[step: 69] loss: 0.0034819929860532284\n",
      "[step: 70] loss: 0.003496953286230564\n",
      "[step: 71] loss: 0.003512379713356495\n",
      "[step: 72] loss: 0.0035240896977484226\n",
      "[step: 73] loss: 0.0035289120860397816\n",
      "[step: 74] loss: 0.003525287378579378\n",
      "[step: 75] loss: 0.0035138889215886593\n",
      "[step: 76] loss: 0.003497636877000332\n",
      "[step: 77] loss: 0.003480698447674513\n",
      "[step: 78] loss: 0.0034669428132474422\n",
      "[step: 79] loss: 0.0034586433321237564\n",
      "[step: 80] loss: 0.0034560381900519133\n",
      "[step: 81] loss: 0.0034577432088553905\n",
      "[step: 82] loss: 0.0034616284538060427\n",
      "[step: 83] loss: 0.003465693211182952\n",
      "[step: 84] loss: 0.0034684878773987293\n",
      "[step: 85] loss: 0.0034691737964749336\n",
      "[step: 86] loss: 0.003467400325462222\n",
      "[step: 87] loss: 0.0034633490722626448\n",
      "[step: 88] loss: 0.0034577646292746067\n",
      "[step: 89] loss: 0.003451874479651451\n",
      "[step: 90] loss: 0.003446961985900998\n",
      "[step: 91] loss: 0.003443836234509945\n",
      "[step: 92] loss: 0.0034425684716552496\n",
      "[step: 93] loss: 0.0034426201600581408\n",
      "[step: 94] loss: 0.003443201305344701\n",
      "[step: 95] loss: 0.003443629015237093\n",
      "[step: 96] loss: 0.0034434706903994083\n",
      "[step: 97] loss: 0.0034425444900989532\n",
      "[step: 98] loss: 0.0034408478531986475\n",
      "[step: 99] loss: 0.0034385421313345432\n",
      "[step: 100] loss: 0.0034359346609562635\n",
      "[step: 101] loss: 0.003433405188843608\n",
      "[step: 102] loss: 0.0034313052892684937\n",
      "[step: 103] loss: 0.0034298112150281668\n",
      "[step: 104] loss: 0.0034288635943084955\n",
      "[step: 105] loss: 0.003428221447393298\n",
      "[step: 106] loss: 0.00342760793864727\n",
      "[step: 107] loss: 0.0034268179442733526\n",
      "[step: 108] loss: 0.003425746923312545\n",
      "[step: 109] loss: 0.0034244079142808914\n",
      "[step: 110] loss: 0.0034228628501296043\n",
      "[step: 111] loss: 0.0034212067257612944\n",
      "[step: 112] loss: 0.00341955479234457\n",
      "[step: 113] loss: 0.0034180162474513054\n",
      "[step: 114] loss: 0.003416660940274596\n",
      "[step: 115] loss: 0.0034154937602579594\n",
      "[step: 116] loss: 0.003414441831409931\n",
      "[step: 117] loss: 0.0034134122543036938\n",
      "[step: 118] loss: 0.0034123060759156942\n",
      "[step: 119] loss: 0.0034110993146896362\n",
      "[step: 120] loss: 0.003409793134778738\n",
      "[step: 121] loss: 0.0034084185026586056\n",
      "[step: 122] loss: 0.003407015698030591\n",
      "[step: 123] loss: 0.0034056161530315876\n",
      "[step: 124] loss: 0.0034042545594274998\n",
      "[step: 125] loss: 0.0034029558300971985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 126] loss: 0.0034017120487987995\n",
      "[step: 127] loss: 0.003400502260774374\n",
      "[step: 128] loss: 0.0033992871176451445\n",
      "[step: 129] loss: 0.0033980391453951597\n",
      "[step: 130] loss: 0.003396750194951892\n",
      "[step: 131] loss: 0.003395422827452421\n",
      "[step: 132] loss: 0.0033940779976546764\n",
      "[step: 133] loss: 0.0033927266485989094\n",
      "[step: 134] loss: 0.0033913799561560154\n",
      "[step: 135] loss: 0.003390049561858177\n",
      "[step: 136] loss: 0.00338873453438282\n",
      "[step: 137] loss: 0.003387431614100933\n",
      "[step: 138] loss: 0.003386132884770632\n",
      "[step: 139] loss: 0.003384823678061366\n",
      "[step: 140] loss: 0.003383506555110216\n",
      "[step: 141] loss: 0.0033821621909737587\n",
      "[step: 142] loss: 0.003380811307579279\n",
      "[step: 143] loss: 0.003379451110959053\n",
      "[step: 144] loss: 0.0033780885860323906\n",
      "[step: 145] loss: 0.003376731416210532\n",
      "[step: 146] loss: 0.0033753737807273865\n",
      "[step: 147] loss: 0.0033740221988409758\n",
      "[step: 148] loss: 0.0033726671244949102\n",
      "[step: 149] loss: 0.003371313214302063\n",
      "[step: 150] loss: 0.003369948361068964\n",
      "[step: 151] loss: 0.00336857745423913\n",
      "[step: 152] loss: 0.003367198398336768\n",
      "[step: 153] loss: 0.003365809563547373\n",
      "[step: 154] loss: 0.0033644200302660465\n",
      "[step: 155] loss: 0.0033630302641540766\n",
      "[step: 156] loss: 0.0033616407308727503\n",
      "[step: 157] loss: 0.003360249102115631\n",
      "[step: 158] loss: 0.0033588549122214317\n",
      "[step: 159] loss: 0.003357457462698221\n",
      "[step: 160] loss: 0.003356053726747632\n",
      "[step: 161] loss: 0.0033546430058777332\n",
      "[step: 162] loss: 0.0033532334491610527\n",
      "[step: 163] loss: 0.0033518136478960514\n",
      "[step: 164] loss: 0.0033503943122923374\n",
      "[step: 165] loss: 0.003348971949890256\n",
      "[step: 166] loss: 0.003347547259181738\n",
      "[step: 167] loss: 0.003346117213368416\n",
      "[step: 168] loss: 0.0033446853049099445\n",
      "[step: 169] loss: 0.0033432538621127605\n",
      "[step: 170] loss: 0.003341817297041416\n",
      "[step: 171] loss: 0.003340373281389475\n",
      "[step: 172] loss: 0.003338929731398821\n",
      "[step: 173] loss: 0.003337477333843708\n",
      "[step: 174] loss: 0.0033360226079821587\n",
      "[step: 175] loss: 0.003334572073072195\n",
      "[step: 176] loss: 0.0033331112936139107\n",
      "[step: 177] loss: 0.003331652609631419\n",
      "[step: 178] loss: 0.003330190433189273\n",
      "[step: 179] loss: 0.0033287175465375185\n",
      "[step: 180] loss: 0.0033272476866841316\n",
      "[step: 181] loss: 0.003325773635879159\n",
      "[step: 182] loss: 0.003324297722429037\n",
      "[step: 183] loss: 0.0033228143583983183\n",
      "[step: 184] loss: 0.0033213351853191853\n",
      "[step: 185] loss: 0.0033198497258126736\n",
      "[step: 186] loss: 0.0033183584455400705\n",
      "[step: 187] loss: 0.0033168629743158817\n",
      "[step: 188] loss: 0.0033153656404465437\n",
      "[step: 189] loss: 0.0033138697035610676\n",
      "[step: 190] loss: 0.0033123635221272707\n",
      "[step: 191] loss: 0.0033108596689999104\n",
      "[step: 192] loss: 0.0033093520905822515\n",
      "[step: 193] loss: 0.0033078438136726618\n",
      "[step: 194] loss: 0.003306324128061533\n",
      "[step: 195] loss: 0.003304811893031001\n",
      "[step: 196] loss: 0.003303288947790861\n",
      "[step: 197] loss: 0.0033017711248248816\n",
      "[step: 198] loss: 0.0033002467826008797\n",
      "[step: 199] loss: 0.0032987112645059824\n",
      "[step: 200] loss: 0.0032971801701933146\n",
      "[step: 201] loss: 0.003295646049082279\n",
      "[step: 202] loss: 0.0032941100653260946\n",
      "[step: 203] loss: 0.0032925691921263933\n",
      "[step: 204] loss: 0.003291024826467037\n",
      "[step: 205] loss: 0.003289483953267336\n",
      "[step: 206] loss: 0.003287930740043521\n",
      "[step: 207] loss: 0.0032863765954971313\n",
      "[step: 208] loss: 0.003284825012087822\n",
      "[step: 209] loss: 0.003283269237726927\n",
      "[step: 210] loss: 0.0032817120663821697\n",
      "[step: 211] loss: 0.0032801481429487467\n",
      "[step: 212] loss: 0.003278581891208887\n",
      "[step: 213] loss: 0.0032770121470093727\n",
      "[step: 214] loss: 0.0032754421699792147\n",
      "[step: 215] loss: 0.0032738654408603907\n",
      "[step: 216] loss: 0.003272289177402854\n",
      "[step: 217] loss: 0.003270711051300168\n",
      "[step: 218] loss: 0.003269130364060402\n",
      "[step: 219] loss: 0.003267548978328705\n",
      "[step: 220] loss: 0.0032659589778631926\n",
      "[step: 221] loss: 0.0032643720041960478\n",
      "[step: 222] loss: 0.00326278037391603\n",
      "[step: 223] loss: 0.0032611838541924953\n",
      "[step: 224] loss: 0.0032595829106867313\n",
      "[step: 225] loss: 0.0032579838298261166\n",
      "[step: 226] loss: 0.003256380558013916\n",
      "[step: 227] loss: 0.003254778916016221\n",
      "[step: 228] loss: 0.003253166563808918\n",
      "[step: 229] loss: 0.0032515597995370626\n",
      "[step: 230] loss: 0.003249943722039461\n",
      "[step: 231] loss: 0.0032483311370015144\n",
      "[step: 232] loss: 0.0032467094715684652\n",
      "[step: 233] loss: 0.003245092462748289\n",
      "[step: 234] loss: 0.0032434689346700907\n",
      "[step: 235] loss: 0.0032418412156403065\n",
      "[step: 236] loss: 0.003240215126425028\n",
      "[step: 237] loss: 0.003238579025492072\n",
      "[step: 238] loss: 0.0032369496766477823\n",
      "[step: 239] loss: 0.0032353128772228956\n",
      "[step: 240] loss: 0.0032336758449673653\n",
      "[step: 241] loss: 0.003232039511203766\n",
      "[step: 242] loss: 0.0032303929328918457\n",
      "[step: 243] loss: 0.003228745423257351\n",
      "[step: 244] loss: 0.003227099310606718\n",
      "[step: 245] loss: 0.0032254476100206375\n",
      "[step: 246] loss: 0.0032237935811281204\n",
      "[step: 247] loss: 0.0032221381552517414\n",
      "[step: 248] loss: 0.0032204831950366497\n",
      "[step: 249] loss: 0.0032188198529183865\n",
      "[step: 250] loss: 0.003217157442122698\n",
      "[step: 251] loss: 0.0032154954969882965\n",
      "[step: 252] loss: 0.0032138267997652292\n",
      "[step: 253] loss: 0.0032121590338647366\n",
      "[step: 254] loss: 0.0032104847487062216\n",
      "[step: 255] loss: 0.0032088097650557756\n",
      "[step: 256] loss: 0.0032071340829133987\n",
      "[step: 257] loss: 0.003205457702279091\n",
      "[step: 258] loss: 0.0032037761993706226\n",
      "[step: 259] loss: 0.0032020937651395798\n",
      "[step: 260] loss: 0.0032004057429730892\n",
      "[step: 261] loss: 0.0031987156253308058\n",
      "[step: 262] loss: 0.0031970252748578787\n",
      "[step: 263] loss: 0.0031953356228768826\n",
      "[step: 264] loss: 0.0031936392188072205\n",
      "[step: 265] loss: 0.0031919453758746386\n",
      "[step: 266] loss: 0.003190241754055023\n",
      "[step: 267] loss: 0.0031885437201708555\n",
      "[step: 268] loss: 0.003186839632689953\n",
      "[step: 269] loss: 0.0031851313542574644\n",
      "[step: 270] loss: 0.003183421678841114\n",
      "[step: 271] loss: 0.003181711072102189\n",
      "[step: 272] loss: 0.003180001163855195\n",
      "[step: 273] loss: 0.0031782840378582478\n",
      "[step: 274] loss: 0.0031765676103532314\n",
      "[step: 275] loss: 0.0031748453620821238\n",
      "[step: 276] loss: 0.003173123113811016\n",
      "[step: 277] loss: 0.0031714008655399084\n",
      "[step: 278] loss: 0.0031696753576397896\n",
      "[step: 279] loss: 0.003167948452755809\n",
      "[step: 280] loss: 0.003166214097291231\n",
      "[step: 281] loss: 0.0031644809059798717\n",
      "[step: 282] loss: 0.0031627477146685123\n",
      "[step: 283] loss: 0.0031610135920345783\n",
      "[step: 284] loss: 0.0031592727173119783\n",
      "[step: 285] loss: 0.003157529979944229\n",
      "[step: 286] loss: 0.0031557860784232616\n",
      "[step: 287] loss: 0.0031540412455797195\n",
      "[step: 288] loss: 0.0031522936187684536\n",
      "[step: 289] loss: 0.003150543663650751\n",
      "[step: 290] loss: 0.003148790216073394\n",
      "[step: 291] loss: 0.0031470367684960365\n",
      "[step: 292] loss: 0.0031452812254428864\n",
      "[step: 293] loss: 0.003143520560115576\n",
      "[step: 294] loss: 0.00314176082611084\n",
      "[step: 295] loss: 0.003139999695122242\n",
      "[step: 296] loss: 0.0031382334418594837\n",
      "[step: 297] loss: 0.0031364681199193\n",
      "[step: 298] loss: 0.00313470046967268\n",
      "[step: 299] loss: 0.003132926532998681\n",
      "[step: 300] loss: 0.003131152829155326\n",
      "[step: 301] loss: 0.003129381686449051\n",
      "[step: 302] loss: 0.0031276026275008917\n",
      "[step: 303] loss: 0.003125821240246296\n",
      "[step: 304] loss: 0.0031240424141287804\n",
      "[step: 305] loss: 0.0031222610268741846\n",
      "[step: 306] loss: 0.003120474051684141\n",
      "[step: 307] loss: 0.0031186859123408794\n",
      "[step: 308] loss: 0.0031169001013040543\n",
      "[step: 309] loss: 0.0031151026487350464\n",
      "[step: 310] loss: 0.0031133133452385664\n",
      "[step: 311] loss: 0.003111517056822777\n",
      "[step: 312] loss: 0.0031097184401005507\n",
      "[step: 313] loss: 0.003107919590547681\n",
      "[step: 314] loss: 0.0031061191111803055\n",
      "[step: 315] loss: 0.003104316769167781\n",
      "[step: 316] loss: 0.0031025055795907974\n",
      "[step: 317] loss: 0.003100702539086342\n",
      "[step: 318] loss: 0.0030988927464932203\n",
      "[step: 319] loss: 0.0030970775987952948\n",
      "[step: 320] loss: 0.0030952643137425184\n",
      "[step: 321] loss: 0.003093448933213949\n",
      "[step: 322] loss: 0.0030916333198547363\n",
      "[step: 323] loss: 0.003089815378189087\n",
      "[step: 324] loss: 0.003087995806708932\n",
      "[step: 325] loss: 0.003086167387664318\n",
      "[step: 326] loss: 0.0030843436252325773\n",
      "[step: 327] loss: 0.0030825159046798944\n",
      "[step: 328] loss: 0.003080687951296568\n",
      "[step: 329] loss: 0.0030788551084697247\n",
      "[step: 330] loss: 0.003077021101489663\n",
      "[step: 331] loss: 0.003075184766203165\n",
      "[step: 332] loss: 0.00307335052639246\n",
      "[step: 333] loss: 0.0030715125612914562\n",
      "[step: 334] loss: 0.0030696685425937176\n",
      "[step: 335] loss: 0.00306782778352499\n",
      "[step: 336] loss: 0.0030659851618111134\n",
      "[step: 337] loss: 0.0030641360208392143\n",
      "[step: 338] loss: 0.003062286414206028\n",
      "[step: 339] loss: 0.0030604354105889797\n",
      "[step: 340] loss: 0.003058583941310644\n",
      "[step: 341] loss: 0.003056728048250079\n",
      "[step: 342] loss: 0.0030548744834959507\n",
      "[step: 343] loss: 0.0030530153308063745\n",
      "[step: 344] loss: 0.003051153616979718\n",
      "[step: 345] loss: 0.0030492939986288548\n",
      "[step: 346] loss: 0.003047430654987693\n",
      "[step: 347] loss: 0.003045563353225589\n",
      "[step: 348] loss: 0.003043695818632841\n",
      "[step: 349] loss: 0.0030418233945965767\n",
      "[step: 350] loss: 0.003039953764528036\n",
      "[step: 351] loss: 0.0030380748212337494\n",
      "[step: 352] loss: 0.003036204259842634\n",
      "[step: 353] loss: 0.0030343295074999332\n",
      "[step: 354] loss: 0.0030324470717459917\n",
      "[step: 355] loss: 0.0030305632390081882\n",
      "[step: 356] loss: 0.003028680570423603\n",
      "[step: 357] loss: 0.003026799764484167\n",
      "[step: 358] loss: 0.0030249119736254215\n",
      "[step: 359] loss: 0.003023024881258607\n",
      "[step: 360] loss: 0.0030211322009563446\n",
      "[step: 361] loss: 0.0030192395206540823\n",
      "[step: 362] loss: 0.0030173466075211763\n",
      "[step: 363] loss: 0.0030154474079608917\n",
      "[step: 364] loss: 0.0030135500710457563\n",
      "[step: 365] loss: 0.0030116538982838392\n",
      "[step: 366] loss: 0.003009747713804245\n",
      "[step: 367] loss: 0.0030078457202762365\n",
      "[step: 368] loss: 0.003005941165611148\n",
      "[step: 369] loss: 0.003004034049808979\n",
      "[step: 370] loss: 0.003002125071361661\n",
      "[step: 371] loss: 0.0030002149287611246\n",
      "[step: 372] loss: 0.002998298965394497\n",
      "[step: 373] loss: 0.0029963848646730185\n",
      "[step: 374] loss: 0.0029944702982902527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 375] loss: 0.002992551075294614\n",
      "[step: 376] loss: 0.002990629058331251\n",
      "[step: 377] loss: 0.002988707274198532\n",
      "[step: 378] loss: 0.002986784791573882\n",
      "[step: 379] loss: 0.0029848581179976463\n",
      "[step: 380] loss: 0.002982932608574629\n",
      "[step: 381] loss: 0.002981003141030669\n",
      "[step: 382] loss: 0.0029790718108415604\n",
      "[step: 383] loss: 0.002977140247821808\n",
      "[step: 384] loss: 0.002975204959511757\n",
      "[step: 385] loss: 0.0029732640832662582\n",
      "[step: 386] loss: 0.002971323672682047\n",
      "[step: 387] loss: 0.002969387685880065\n",
      "[step: 388] loss: 0.0029674433171749115\n",
      "[step: 389] loss: 0.0029654991813004017\n",
      "[step: 390] loss: 0.0029635531827807426\n",
      "[step: 391] loss: 0.002961604855954647\n",
      "[step: 392] loss: 0.0029596579261124134\n",
      "[step: 393] loss: 0.0029577049426734447\n",
      "[step: 394] loss: 0.0029557514935731888\n",
      "[step: 395] loss: 0.0029537982773035765\n",
      "[step: 396] loss: 0.002951841102913022\n",
      "[step: 397] loss: 0.0029498785734176636\n",
      "[step: 398] loss: 0.0029479211661964655\n",
      "[step: 399] loss: 0.0029459635261446238\n",
      "[step: 400] loss: 0.002943997271358967\n",
      "[step: 401] loss: 0.0029420312494039536\n",
      "[step: 402] loss: 0.0029400624334812164\n",
      "[step: 403] loss: 0.002938089892268181\n",
      "[step: 404] loss: 0.0029361206106841564\n",
      "[step: 405] loss: 0.002934146672487259\n",
      "[step: 406] loss: 0.0029321731999516487\n",
      "[step: 407] loss: 0.0029301969334483147\n",
      "[step: 408] loss: 0.0029282185714691877\n",
      "[step: 409] loss: 0.002926237415522337\n",
      "[step: 410] loss: 0.0029242560267448425\n",
      "[step: 411] loss: 0.0029222704470157623\n",
      "[step: 412] loss: 0.0029202853329479694\n",
      "[step: 413] loss: 0.002918296493589878\n",
      "[step: 414] loss: 0.002916308119893074\n",
      "[step: 415] loss: 0.0029143178835511208\n",
      "[step: 416] loss: 0.0029123255517333746\n",
      "[step: 417] loss: 0.0029103332199156284\n",
      "[step: 418] loss: 0.002908333670347929\n",
      "[step: 419] loss: 0.002906334586441517\n",
      "[step: 420] loss: 0.0029043368995189667\n",
      "[step: 421] loss: 0.002902334090322256\n",
      "[step: 422] loss: 0.002900330349802971\n",
      "[step: 423] loss: 0.0028983205556869507\n",
      "[step: 424] loss: 0.0028963154181838036\n",
      "[step: 425] loss: 0.0028943056240677834\n",
      "[step: 426] loss: 0.002892298623919487\n",
      "[step: 427] loss: 0.0028902797494083643\n",
      "[step: 428] loss: 0.0028882711194455624\n",
      "[step: 429] loss: 0.002886253409087658\n",
      "[step: 430] loss: 0.0028842343017458916\n",
      "[step: 431] loss: 0.0028822175227105618\n",
      "[step: 432] loss: 0.0028801944572478533\n",
      "[step: 433] loss: 0.002878172555938363\n",
      "[step: 434] loss: 0.0028761443682014942\n",
      "[step: 435] loss: 0.002874118508771062\n",
      "[step: 436] loss: 0.002872087527066469\n",
      "[step: 437] loss: 0.002870060270652175\n",
      "[step: 438] loss: 0.0028680278919637203\n",
      "[step: 439] loss: 0.002865990623831749\n",
      "[step: 440] loss: 0.002863955684006214\n",
      "[step: 441] loss: 0.0028619193471968174\n",
      "[step: 442] loss: 0.002859879517927766\n",
      "[step: 443] loss: 0.0028578382916748524\n",
      "[step: 444] loss: 0.002855792874470353\n",
      "[step: 445] loss: 0.002853750716894865\n",
      "[step: 446] loss: 0.002851703204214573\n",
      "[step: 447] loss: 0.0028496552258729935\n",
      "[step: 448] loss: 0.0028476037550717592\n",
      "[step: 449] loss: 0.0028455546125769615\n",
      "[step: 450] loss: 0.002843497321009636\n",
      "[step: 451] loss: 0.002841442357748747\n",
      "[step: 452] loss: 0.002839387394487858\n",
      "[step: 453] loss: 0.0028373259119689465\n",
      "[step: 454] loss: 0.002835262333974242\n",
      "[step: 455] loss: 0.002833204809576273\n",
      "[step: 456] loss: 0.0028311405330896378\n",
      "[step: 457] loss: 0.0028290729969739914\n",
      "[step: 458] loss: 0.002827005460858345\n",
      "[step: 459] loss: 0.002824935829266906\n",
      "[step: 460] loss: 0.002822859212756157\n",
      "[step: 461] loss: 0.002820789348334074\n",
      "[step: 462] loss: 0.0028187136631458998\n",
      "[step: 463] loss: 0.0028166326228529215\n",
      "[step: 464] loss: 0.0028145594988018274\n",
      "[step: 465] loss: 0.0028124796226620674\n",
      "[step: 466] loss: 0.002810396021232009\n",
      "[step: 467] loss: 0.0028083105571568012\n",
      "[step: 468] loss: 0.0028062262572348118\n",
      "[step: 469] loss: 0.0028041377663612366\n",
      "[step: 470] loss: 0.0028020464815199375\n",
      "[step: 471] loss: 0.0027999570593237877\n",
      "[step: 472] loss: 0.0027978606522083282\n",
      "[step: 473] loss: 0.0027957626152783632\n",
      "[step: 474] loss: 0.0027936697006225586\n",
      "[step: 475] loss: 0.0027915718965232372\n",
      "[step: 476] loss: 0.0027894708327949047\n",
      "[step: 477] loss: 0.002787366509437561\n",
      "[step: 478] loss: 0.002785265911370516\n",
      "[step: 479] loss: 0.0027831606566905975\n",
      "[step: 480] loss: 0.0027810540050268173\n",
      "[step: 481] loss: 0.002778944093734026\n",
      "[step: 482] loss: 0.002776828594505787\n",
      "[step: 483] loss: 0.00277471961453557\n",
      "[step: 484] loss: 0.002772602718323469\n",
      "[step: 485] loss: 0.0027704851236194372\n",
      "[step: 486] loss: 0.002768370322883129\n",
      "[step: 487] loss: 0.0027662492357194424\n",
      "[step: 488] loss: 0.002764127915725112\n",
      "[step: 489] loss: 0.0027620058972388506\n",
      "[step: 490] loss: 0.002759880619123578\n",
      "[step: 491] loss: 0.0027577518485486507\n",
      "[step: 492] loss: 0.002755623310804367\n",
      "[step: 493] loss: 0.0027534901164472103\n",
      "[step: 494] loss: 0.0027513583190739155\n",
      "[step: 495] loss: 0.0027492274530231953\n",
      "[step: 496] loss: 0.002747089136391878\n",
      "[step: 497] loss: 0.0027449505869299173\n",
      "[step: 498] loss: 0.002742812503129244\n",
      "[step: 499] loss: 0.0027406709268689156\n",
      "[step: 500] loss: 0.002738527487963438\n",
      "[step: 501] loss: 0.002736380323767662\n",
      "[step: 502] loss: 0.0027342326939105988\n",
      "[step: 503] loss: 0.002732083899900317\n",
      "[step: 504] loss: 0.0027299332432448864\n",
      "[step: 505] loss: 0.002727781655266881\n",
      "[step: 506] loss: 0.0027256286703050137\n",
      "[step: 507] loss: 0.002723473124206066\n",
      "[step: 508] loss: 0.0027213143184781075\n",
      "[step: 509] loss: 0.0027191562112420797\n",
      "[step: 510] loss: 0.002716991351917386\n",
      "[step: 511] loss: 0.0027148276567459106\n",
      "[step: 512] loss: 0.002712666755542159\n",
      "[step: 513] loss: 0.002710498869419098\n",
      "[step: 514] loss: 0.0027083265595138073\n",
      "[step: 515] loss: 0.002706159371882677\n",
      "[step: 516] loss: 0.002703986596316099\n",
      "[step: 517] loss: 0.0027018114924430847\n",
      "[step: 518] loss: 0.0026996349915862083\n",
      "[step: 519] loss: 0.0026974580250680447\n",
      "[step: 520] loss: 0.0026952794287353754\n",
      "[step: 521] loss: 0.002693096175789833\n",
      "[step: 522] loss: 0.0026909136213362217\n",
      "[step: 523] loss: 0.002688730601221323\n",
      "[step: 524] loss: 0.002686545718461275\n",
      "[step: 525] loss: 0.002684356179088354\n",
      "[step: 526] loss: 0.0026821657083928585\n",
      "[step: 527] loss: 0.002679969882592559\n",
      "[step: 528] loss: 0.0026777773164212704\n",
      "[step: 529] loss: 0.0026755814906209707\n",
      "[step: 530] loss: 0.002673384267836809\n",
      "[step: 531] loss: 0.0026711814571172\n",
      "[step: 532] loss: 0.002668980974704027\n",
      "[step: 533] loss: 0.0026667779311537743\n",
      "[step: 534] loss: 0.0026645739562809467\n",
      "[step: 535] loss: 0.002662363927811384\n",
      "[step: 536] loss: 0.0026601606514304876\n",
      "[step: 537] loss: 0.002657945267856121\n",
      "[step: 538] loss: 0.002655736170709133\n",
      "[step: 539] loss: 0.0026535200886428356\n",
      "[step: 540] loss: 0.002651302609592676\n",
      "[step: 541] loss: 0.00264908978715539\n",
      "[step: 542] loss: 0.002646869048476219\n",
      "[step: 543] loss: 0.0026446469128131866\n",
      "[step: 544] loss: 0.0026424191892147064\n",
      "[step: 545] loss: 0.002640198916196823\n",
      "[step: 546] loss: 0.0026379695627838373\n",
      "[step: 547] loss: 0.0026357420720160007\n",
      "[step: 548] loss: 0.0026335096918046474\n",
      "[step: 549] loss: 0.002631280105561018\n",
      "[step: 550] loss: 0.0026290423702448606\n",
      "[step: 551] loss: 0.0026268064975738525\n",
      "[step: 552] loss: 0.002624566201120615\n",
      "[step: 553] loss: 0.002622326835989952\n",
      "[step: 554] loss: 0.002620086306706071\n",
      "[step: 555] loss: 0.002617842983454466\n",
      "[step: 556] loss: 0.002615597564727068\n",
      "[step: 557] loss: 0.002613348886370659\n",
      "[step: 558] loss: 0.002611103467643261\n",
      "[step: 559] loss: 0.0026088522281497717\n",
      "[step: 560] loss: 0.0026065977290272713\n",
      "[step: 561] loss: 0.0026043406687676907\n",
      "[step: 562] loss: 0.002602085703983903\n",
      "[step: 563] loss: 0.0025998293422162533\n",
      "[step: 564] loss: 0.0025975648313760757\n",
      "[step: 565] loss: 0.002595301251858473\n",
      "[step: 566] loss: 0.00259303767234087\n",
      "[step: 567] loss: 0.00259077618829906\n",
      "[step: 568] loss: 0.002588501665741205\n",
      "[step: 569] loss: 0.002586236922070384\n",
      "[step: 570] loss: 0.002583964727818966\n",
      "[step: 571] loss: 0.002581692300736904\n",
      "[step: 572] loss: 0.002579414751380682\n",
      "[step: 573] loss: 0.002577139064669609\n",
      "[step: 574] loss: 0.002574857324361801\n",
      "[step: 575] loss: 0.0025725774466991425\n",
      "[step: 576] loss: 0.002570295473560691\n",
      "[step: 577] loss: 0.00256800907664001\n",
      "[step: 578] loss: 0.0025657224468886852\n",
      "[step: 579] loss: 0.002563433488830924\n",
      "[step: 580] loss: 0.002561145694926381\n",
      "[step: 581] loss: 0.0025588516145944595\n",
      "[step: 582] loss: 0.0025565605610609055\n",
      "[step: 583] loss: 0.0025542634539306164\n",
      "[step: 584] loss: 0.0025519654154777527\n",
      "[step: 585] loss: 0.0025496657472103834\n",
      "[step: 586] loss: 0.002547363517805934\n",
      "[step: 587] loss: 0.0025450580287724733\n",
      "[step: 588] loss: 0.002542755100876093\n",
      "[step: 589] loss: 0.002540446352213621\n",
      "[step: 590] loss: 0.0025381366722285748\n",
      "[step: 591] loss: 0.0025358241982758045\n",
      "[step: 592] loss: 0.0025335110258311033\n",
      "[step: 593] loss: 0.0025311976205557585\n",
      "[step: 594] loss: 0.002528882585465908\n",
      "[step: 595] loss: 0.0025265610311180353\n",
      "[step: 596] loss: 0.002524241339415312\n",
      "[step: 597] loss: 0.0025219195522367954\n",
      "[step: 598] loss: 0.002519593108445406\n",
      "[step: 599] loss: 0.002517265733331442\n",
      "[step: 600] loss: 0.002514936961233616\n",
      "[step: 601] loss: 0.0025126098189502954\n",
      "[step: 602] loss: 0.0025102777872234583\n",
      "[step: 603] loss: 0.0025079436600208282\n",
      "[step: 604] loss: 0.0025056079030036926\n",
      "[step: 605] loss: 0.002503269352018833\n",
      "[step: 606] loss: 0.0025009282398968935\n",
      "[step: 607] loss: 0.0024985838681459427\n",
      "[step: 608] loss: 0.0024962425231933594\n",
      "[step: 609] loss: 0.0024938974529504776\n",
      "[step: 610] loss: 0.00249154819175601\n",
      "[step: 611] loss: 0.0024892031215131283\n",
      "[step: 612] loss: 0.0024868492037057877\n",
      "[step: 613] loss: 0.0024844948202371597\n",
      "[step: 614] loss: 0.0024821418337523937\n",
      "[step: 615] loss: 0.002479781862348318\n",
      "[step: 616] loss: 0.0024774211924523115\n",
      "[step: 617] loss: 0.0024750607553869486\n",
      "[step: 618] loss: 0.002472697291523218\n",
      "[step: 619] loss: 0.002470334991812706\n",
      "[step: 620] loss: 0.0024679661728441715\n",
      "[step: 621] loss: 0.002465598052367568\n",
      "[step: 622] loss: 0.0024632264394313097\n",
      "[step: 623] loss: 0.0024608548264950514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 624] loss: 0.0024584794882684946\n",
      "[step: 625] loss: 0.0024561025202274323\n",
      "[step: 626] loss: 0.002453723456710577\n",
      "[step: 627] loss: 0.0024513406679034233\n",
      "[step: 628] loss: 0.0024489564821124077\n",
      "[step: 629] loss: 0.0024465746246278286\n",
      "[step: 630] loss: 0.0024441878776997328\n",
      "[step: 631] loss: 0.002441796939820051\n",
      "[step: 632] loss: 0.0024394053034484386\n",
      "[step: 633] loss: 0.0024370127357542515\n",
      "[step: 634] loss: 0.0024346215650439262\n",
      "[step: 635] loss: 0.002432223642244935\n",
      "[step: 636] loss: 0.002429824322462082\n",
      "[step: 637] loss: 0.002427423372864723\n",
      "[step: 638] loss: 0.002425020094960928\n",
      "[step: 639] loss: 0.0024226189125329256\n",
      "[step: 640] loss: 0.0024202135391533375\n",
      "[step: 641] loss: 0.0024178025778383017\n",
      "[step: 642] loss: 0.0024153939448297024\n",
      "[step: 643] loss: 0.0024129794910550117\n",
      "[step: 644] loss: 0.0024105655029416084\n",
      "[step: 645] loss: 0.002408148255199194\n",
      "[step: 646] loss: 0.0024057317059487104\n",
      "[step: 647] loss: 0.0024033072404563427\n",
      "[step: 648] loss: 0.0024008839391171932\n",
      "[step: 649] loss: 0.002398463897407055\n",
      "[step: 650] loss: 0.0023960345424711704\n",
      "[step: 651] loss: 0.0023936075158417225\n",
      "[step: 652] loss: 0.0023911762982606888\n",
      "[step: 653] loss: 0.002388742519542575\n",
      "[step: 654] loss: 0.0023863064125180244\n",
      "[step: 655] loss: 0.002383871003985405\n",
      "[step: 656] loss: 0.002381435129791498\n",
      "[step: 657] loss: 0.0023789936676621437\n",
      "[step: 658] loss: 0.00237655034288764\n",
      "[step: 659] loss: 0.002374105155467987\n",
      "[step: 660] loss: 0.002371659269556403\n",
      "[step: 661] loss: 0.002369210124015808\n",
      "[step: 662] loss: 0.002366758417338133\n",
      "[step: 663] loss: 0.0023643076419830322\n",
      "[step: 664] loss: 0.0023618536069989204\n",
      "[step: 665] loss: 0.002359394682571292\n",
      "[step: 666] loss: 0.0023569324985146523\n",
      "[step: 667] loss: 0.0023544742725789547\n",
      "[step: 668] loss: 0.002352012787014246\n",
      "[step: 669] loss: 0.0023495492059737444\n",
      "[step: 670] loss: 0.0023470805026590824\n",
      "[step: 671] loss: 0.002344609936699271\n",
      "[step: 672] loss: 0.0023421398364007473\n",
      "[step: 673] loss: 0.002339664613828063\n",
      "[step: 674] loss: 0.002337189158424735\n",
      "[step: 675] loss: 0.0023347134701907635\n",
      "[step: 676] loss: 0.0023322345223277807\n",
      "[step: 677] loss: 0.0023297532461583614\n",
      "[step: 678] loss: 0.0023272731341421604\n",
      "[step: 679] loss: 0.002324787899851799\n",
      "[step: 680] loss: 0.0023223015014082193\n",
      "[step: 681] loss: 0.0023198069538921118\n",
      "[step: 682] loss: 0.0023173161316663027\n",
      "[step: 683] loss: 0.0023148213513195515\n",
      "[step: 684] loss: 0.0023123284336179495\n",
      "[step: 685] loss: 0.0023098308593034744\n",
      "[step: 686] loss: 0.0023073311895132065\n",
      "[step: 687] loss: 0.0023048294242471457\n",
      "[step: 688] loss: 0.0023023257963359356\n",
      "[step: 689] loss: 0.0022998182103037834\n",
      "[step: 690] loss: 0.002297311555594206\n",
      "[step: 691] loss: 0.002294801641255617\n",
      "[step: 692] loss: 0.002292294055223465\n",
      "[step: 693] loss: 0.0022897778544574976\n",
      "[step: 694] loss: 0.002287262585014105\n",
      "[step: 695] loss: 0.002284744055941701\n",
      "[step: 696] loss: 0.002282223664224148\n",
      "[step: 697] loss: 0.0022797025740146637\n",
      "[step: 698] loss: 0.0022771800868213177\n",
      "[step: 699] loss: 0.0022746515460312366\n",
      "[step: 700] loss: 0.0022721278946846724\n",
      "[step: 701] loss: 0.002269596327096224\n",
      "[step: 702] loss: 0.0022670645266771317\n",
      "[step: 703] loss: 0.0022645294666290283\n",
      "[step: 704] loss: 0.0022619934752583504\n",
      "[step: 705] loss: 0.002259457018226385\n",
      "[step: 706] loss: 0.0022569161374121904\n",
      "[step: 707] loss: 0.002254377119243145\n",
      "[step: 708] loss: 0.0022518341429531574\n",
      "[step: 709] loss: 0.002249290468171239\n",
      "[step: 710] loss: 0.002246738411486149\n",
      "[step: 711] loss: 0.002244191709905863\n",
      "[step: 712] loss: 0.0022416403517127037\n",
      "[step: 713] loss: 0.0022390889935195446\n",
      "[step: 714] loss: 0.0022365322802215815\n",
      "[step: 715] loss: 0.002233976498246193\n",
      "[step: 716] loss: 0.002231415593996644\n",
      "[step: 717] loss: 0.0022288570180535316\n",
      "[step: 718] loss: 0.002226294483989477\n",
      "[step: 719] loss: 0.0022237300872802734\n",
      "[step: 720] loss: 0.0022211659234017134\n",
      "[step: 721] loss: 0.0022185996640473604\n",
      "[step: 722] loss: 0.002216028980910778\n",
      "[step: 723] loss: 0.002213454805314541\n",
      "[step: 724] loss: 0.0022108820267021656\n",
      "[step: 725] loss: 0.0022083083167672157\n",
      "[step: 726] loss: 0.002205732511356473\n",
      "[step: 727] loss: 0.0022031511180102825\n",
      "[step: 728] loss: 0.002200573682785034\n",
      "[step: 729] loss: 0.002197990892454982\n",
      "[step: 730] loss: 0.002195406472310424\n",
      "[step: 731] loss: 0.002192821353673935\n",
      "[step: 732] loss: 0.0021902371663600206\n",
      "[step: 733] loss: 0.0021876483224332333\n",
      "[step: 734] loss: 0.0021850564517080784\n",
      "[step: 735] loss: 0.00218246690928936\n",
      "[step: 736] loss: 0.0021798755042254925\n",
      "[step: 737] loss: 0.002177281305193901\n",
      "[step: 738] loss: 0.002174684777855873\n",
      "[step: 739] loss: 0.0021720880176872015\n",
      "[step: 740] loss: 0.002169489860534668\n",
      "[step: 741] loss: 0.0021668889094144106\n",
      "[step: 742] loss: 0.002164286794140935\n",
      "[step: 743] loss: 0.0021616825833916664\n",
      "[step: 744] loss: 0.0021590818651020527\n",
      "[step: 745] loss: 0.002156477887183428\n",
      "[step: 746] loss: 0.0021538662258535624\n",
      "[step: 747] loss: 0.00215126178227365\n",
      "[step: 748] loss: 0.002148655243217945\n",
      "[step: 749] loss: 0.0021460438147187233\n",
      "[step: 750] loss: 0.002143434016034007\n",
      "[step: 751] loss: 0.002140820026397705\n",
      "[step: 752] loss: 0.0021382104605436325\n",
      "[step: 753] loss: 0.0021355936769396067\n",
      "[step: 754] loss: 0.0021329799201339483\n",
      "[step: 755] loss: 0.002130364766344428\n",
      "[step: 756] loss: 0.0021277472842484713\n",
      "[step: 757] loss: 0.002125129569321871\n",
      "[step: 758] loss: 0.0021225144155323505\n",
      "[step: 759] loss: 0.0021198957692831755\n",
      "[step: 760] loss: 0.0021172785200178623\n",
      "[step: 761] loss: 0.0021146582439541817\n",
      "[step: 762] loss: 0.002112037967890501\n",
      "[step: 763] loss: 0.002109416527673602\n",
      "[step: 764] loss: 0.002106800675392151\n",
      "[step: 765] loss: 0.002104175742715597\n",
      "[step: 766] loss: 0.0021015566308051348\n",
      "[step: 767] loss: 0.0020989333279430866\n",
      "[step: 768] loss: 0.0020963153801858425\n",
      "[step: 769] loss: 0.002093690913170576\n",
      "[step: 770] loss: 0.002091070404276252\n",
      "[step: 771] loss: 0.0020884531550109386\n",
      "[step: 772] loss: 0.0020858291536569595\n",
      "[step: 773] loss: 0.0020832130685448647\n",
      "[step: 774] loss: 0.002080592792481184\n",
      "[step: 775] loss: 0.0020779715850949287\n",
      "[step: 776] loss: 0.0020753522403538227\n",
      "[step: 777] loss: 0.002072734758257866\n",
      "[step: 778] loss: 0.002070116112008691\n",
      "[step: 779] loss: 0.0020674990955740213\n",
      "[step: 780] loss: 0.0020648841746151447\n",
      "[step: 781] loss: 0.0020622694864869118\n",
      "[step: 782] loss: 0.0020596510730683804\n",
      "[step: 783] loss: 0.0020570424385368824\n",
      "[step: 784] loss: 0.002054431941360235\n",
      "[step: 785] loss: 0.0020518209785223007\n",
      "[step: 786] loss: 0.0020492137409746647\n",
      "[step: 787] loss: 0.002046604175120592\n",
      "[step: 788] loss: 0.00204399973154068\n",
      "[step: 789] loss: 0.002041399013251066\n",
      "[step: 790] loss: 0.002038798062130809\n",
      "[step: 791] loss: 0.002036196179687977\n",
      "[step: 792] loss: 0.0020335998851805925\n",
      "[step: 793] loss: 0.00203100498765707\n",
      "[step: 794] loss: 0.0020284128841012716\n",
      "[step: 795] loss: 0.0020258305594325066\n",
      "[step: 796] loss: 0.0020232656970620155\n",
      "[step: 797] loss: 0.002020746935158968\n",
      "[step: 798] loss: 0.0020183890592306852\n",
      "[step: 799] loss: 0.0020166090689599514\n",
      "[step: 800] loss: 0.0020169324707239866\n",
      "[step: 801] loss: 0.0020252796821296215\n",
      "[step: 802] loss: 0.002065145643427968\n",
      "[step: 803] loss: 0.0022152860183268785\n",
      "[step: 804] loss: 0.002680709818378091\n",
      "[step: 805] loss: 0.0033711614087224007\n",
      "[step: 806] loss: 0.003283554222434759\n",
      "[step: 807] loss: 0.002158907474949956\n",
      "[step: 808] loss: 0.0023004019167274237\n",
      "[step: 809] loss: 0.002902335487306118\n",
      "[step: 810] loss: 0.002192440675571561\n",
      "[step: 811] loss: 0.002185283461585641\n",
      "[step: 812] loss: 0.0025986104737967253\n",
      "[step: 813] loss: 0.0020502405241131783\n",
      "[step: 814] loss: 0.002248502802103758\n",
      "[step: 815] loss: 0.002343551255762577\n",
      "[step: 816] loss: 0.0019895939622074366\n",
      "[step: 817] loss: 0.0022985003888607025\n",
      "[step: 818] loss: 0.002099234377965331\n",
      "[step: 819] loss: 0.0020674956031143665\n",
      "[step: 820] loss: 0.0022104838863015175\n",
      "[step: 821] loss: 0.001982796937227249\n",
      "[step: 822] loss: 0.0021493167150765657\n",
      "[step: 823] loss: 0.002043907530605793\n",
      "[step: 824] loss: 0.002029568422585726\n",
      "[step: 825] loss: 0.002100171521306038\n",
      "[step: 826] loss: 0.0019750837236642838\n",
      "[step: 827] loss: 0.0020795969758182764\n",
      "[step: 828] loss: 0.001995677826926112\n",
      "[step: 829] loss: 0.0020154821686446667\n",
      "[step: 830] loss: 0.0020300063770264387\n",
      "[step: 831] loss: 0.001970810815691948\n",
      "[step: 832] loss: 0.0020305514335632324\n",
      "[step: 833] loss: 0.001968282274901867\n",
      "[step: 834] loss: 0.001999367494136095\n",
      "[step: 835] loss: 0.0019863946363329887\n",
      "[step: 836] loss: 0.0019664280116558075\n",
      "[step: 837] loss: 0.0019939288031309843\n",
      "[step: 838] loss: 0.001954542240127921\n",
      "[step: 839] loss: 0.001980083528906107\n",
      "[step: 840] loss: 0.001960903871804476\n",
      "[step: 841] loss: 0.001957991626113653\n",
      "[step: 842] loss: 0.0019672184716910124\n",
      "[step: 843] loss: 0.0019450073596090078\n",
      "[step: 844] loss: 0.001961206551641226\n",
      "[step: 845] loss: 0.0019451463595032692\n",
      "[step: 846] loss: 0.0019466513767838478\n",
      "[step: 847] loss: 0.001948414254002273\n",
      "[step: 848] loss: 0.0019356256816536188\n",
      "[step: 849] loss: 0.0019445668440312147\n",
      "[step: 850] loss: 0.001933367340825498\n",
      "[step: 851] loss: 0.0019343062303960323\n",
      "[step: 852] loss: 0.0019341197330504656\n",
      "[step: 853] loss: 0.0019257344538345933\n",
      "[step: 854] loss: 0.0019303247099742293\n",
      "[step: 855] loss: 0.001923045376315713\n",
      "[step: 856] loss: 0.0019222096307203174\n",
      "[step: 857] loss: 0.0019220615504309535\n",
      "[step: 858] loss: 0.0019157330971211195\n",
      "[step: 859] loss: 0.0019175732741132379\n",
      "[step: 860] loss: 0.0019132138695567846\n",
      "[step: 861] loss: 0.001910766470246017\n",
      "[step: 862] loss: 0.0019108636770397425\n",
      "[step: 863] loss: 0.001905993209220469\n",
      "[step: 864] loss: 0.0019057556055486202\n",
      "[step: 865] loss: 0.0019034912111237645\n",
      "[step: 866] loss: 0.001900128903798759\n",
      "[step: 867] loss: 0.001899906899780035\n",
      "[step: 868] loss: 0.001896578585729003\n",
      "[step: 869] loss: 0.0018946569180116057\n",
      "[step: 870] loss: 0.0018935478292405605\n",
      "[step: 871] loss: 0.0018903057789430022\n",
      "[step: 872] loss: 0.0018890469800680876\n",
      "[step: 873] loss: 0.001887158490717411\n",
      "[step: 874] loss: 0.0018844247097149491\n",
      "[step: 875] loss: 0.0018832272617146373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 876] loss: 0.0018809587927535176\n",
      "[step: 877] loss: 0.0018786832224577665\n",
      "[step: 878] loss: 0.0018773155752569437\n",
      "[step: 879] loss: 0.0018749615410342813\n",
      "[step: 880] loss: 0.0018729653675109148\n",
      "[step: 881] loss: 0.0018714123871177435\n",
      "[step: 882] loss: 0.001869112835265696\n",
      "[step: 883] loss: 0.0018672436708584428\n",
      "[step: 884] loss: 0.00186556251719594\n",
      "[step: 885] loss: 0.0018633584259077907\n",
      "[step: 886] loss: 0.0018615317530930042\n",
      "[step: 887] loss: 0.0018597810994833708\n",
      "[step: 888] loss: 0.0018576699076220393\n",
      "[step: 889] loss: 0.00185584113933146\n",
      "[step: 890] loss: 0.001854069298133254\n",
      "[step: 891] loss: 0.001852030516602099\n",
      "[step: 892] loss: 0.0018501896411180496\n",
      "[step: 893] loss: 0.0018484167521819472\n",
      "[step: 894] loss: 0.001846440602093935\n",
      "[step: 895] loss: 0.0018445819150656462\n",
      "[step: 896] loss: 0.0018428121693432331\n",
      "[step: 897] loss: 0.001840893761254847\n",
      "[step: 898] loss: 0.0018390242476016283\n",
      "[step: 899] loss: 0.0018372524064034224\n",
      "[step: 900] loss: 0.0018353909254074097\n",
      "[step: 901] loss: 0.0018335182685405016\n",
      "[step: 902] loss: 0.0018317425856366754\n",
      "[step: 903] loss: 0.0018299242947250605\n",
      "[step: 904] loss: 0.0018280722433701158\n",
      "[step: 905] loss: 0.001826276071369648\n",
      "[step: 906] loss: 0.0018244966631755233\n",
      "[step: 907] loss: 0.0018226673128083348\n",
      "[step: 908] loss: 0.001820864388719201\n",
      "[step: 909] loss: 0.0018191009294241667\n",
      "[step: 910] loss: 0.0018173102289438248\n",
      "[step: 911] loss: 0.0018155149882659316\n",
      "[step: 912] loss: 0.0018137522274628282\n",
      "[step: 913] loss: 0.0018119921442121267\n",
      "[step: 914] loss: 0.001810217509046197\n",
      "[step: 915] loss: 0.0018084512557834387\n",
      "[step: 916] loss: 0.0018067080527544022\n",
      "[step: 917] loss: 0.0018049655482172966\n",
      "[step: 918] loss: 0.0018032133812084794\n",
      "[step: 919] loss: 0.0018014743691310287\n",
      "[step: 920] loss: 0.001799746765755117\n",
      "[step: 921] loss: 0.0017980232369154692\n",
      "[step: 922] loss: 0.0017962936544790864\n",
      "[step: 923] loss: 0.0017945758299902081\n",
      "[step: 924] loss: 0.0017928681336343288\n",
      "[step: 925] loss: 0.001791162882000208\n",
      "[step: 926] loss: 0.001789455651305616\n",
      "[step: 927] loss: 0.001787757035344839\n",
      "[step: 928] loss: 0.0017860737862065434\n",
      "[step: 929] loss: 0.0017843894893303514\n",
      "[step: 930] loss: 0.0017827056581154466\n",
      "[step: 931] loss: 0.001781028462573886\n",
      "[step: 932] loss: 0.001779362210072577\n",
      "[step: 933] loss: 0.0017776964232325554\n",
      "[step: 934] loss: 0.001776039251126349\n",
      "[step: 935] loss: 0.0017743837088346481\n",
      "[step: 936] loss: 0.0017727341037243605\n",
      "[step: 937] loss: 0.001771093113347888\n",
      "[step: 938] loss: 0.0017694550333544612\n",
      "[step: 939] loss: 0.0017678230069577694\n",
      "[step: 940] loss: 0.0017661928432062268\n",
      "[step: 941] loss: 0.0017645694315433502\n",
      "[step: 942] loss: 0.001762953121215105\n",
      "[step: 943] loss: 0.0017613422824069858\n",
      "[step: 944] loss: 0.0017597352853044868\n",
      "[step: 945] loss: 0.001758132828399539\n",
      "[step: 946] loss: 0.0017565363086760044\n",
      "[step: 947] loss: 0.0017549449112266302\n",
      "[step: 948] loss: 0.0017533599166199565\n",
      "[step: 949] loss: 0.001751780859194696\n",
      "[step: 950] loss: 0.0017502042464911938\n",
      "[step: 951] loss: 0.0017486331053078175\n",
      "[step: 952] loss: 0.0017470670863986015\n",
      "[step: 953] loss: 0.0017455073539167643\n",
      "[step: 954] loss: 0.0017439553048461676\n",
      "[step: 955] loss: 0.00174240383785218\n",
      "[step: 956] loss: 0.0017408588901162148\n",
      "[step: 957] loss: 0.0017393198795616627\n",
      "[step: 958] loss: 0.0017377848271280527\n",
      "[step: 959] loss: 0.001736255595460534\n",
      "[step: 960] loss: 0.0017347319517284632\n",
      "[step: 961] loss: 0.001733211101964116\n",
      "[step: 962] loss: 0.0017316994490101933\n",
      "[step: 963] loss: 0.001730189542286098\n",
      "[step: 964] loss: 0.0017286855727434158\n",
      "[step: 965] loss: 0.0017271862598136067\n",
      "[step: 966] loss: 0.0017256963765248656\n",
      "[step: 967] loss: 0.0017242036992684007\n",
      "[step: 968] loss: 0.0017227237112820148\n",
      "[step: 969] loss: 0.001721243024803698\n",
      "[step: 970] loss: 0.00171976862475276\n",
      "[step: 971] loss: 0.0017183013260364532\n",
      "[step: 972] loss: 0.0017168375197798014\n",
      "[step: 973] loss: 0.001715377438813448\n",
      "[step: 974] loss: 0.0017139222472906113\n",
      "[step: 975] loss: 0.0017124720616266131\n",
      "[step: 976] loss: 0.0017110295593738556\n",
      "[step: 977] loss: 0.0017095883376896381\n",
      "[step: 978] loss: 0.001708153635263443\n",
      "[step: 979] loss: 0.0017067278968170285\n",
      "[step: 980] loss: 0.0017053094925358891\n",
      "[step: 981] loss: 0.001703896326944232\n",
      "[step: 982] loss: 0.0017024995759129524\n",
      "[step: 983] loss: 0.0017011251766234636\n",
      "[step: 984] loss: 0.0016998082865029573\n",
      "[step: 985] loss: 0.0016985927941277623\n",
      "[step: 986] loss: 0.0016976265469565988\n",
      "[step: 987] loss: 0.0016972413286566734\n",
      "[step: 988] loss: 0.0016982476226985455\n",
      "[step: 989] loss: 0.0017027303110808134\n",
      "[step: 990] loss: 0.0017159059643745422\n",
      "[step: 991] loss: 0.0017516679363325238\n",
      "[step: 992] loss: 0.0018430749187245965\n",
      "[step: 993] loss: 0.002075116615742445\n",
      "[step: 994] loss: 0.0025699641555547714\n",
      "[step: 995] loss: 0.003434371203184128\n",
      "[step: 996] loss: 0.003861229168251157\n",
      "[step: 997] loss: 0.0030293725430965424\n",
      "[step: 998] loss: 0.0017489604651927948\n",
      "[step: 999] loss: 0.002187766134738922\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "LSTM realforecast : [24197.77734375, 7926.76025390625, 22488.498046875, 22451.146484375]\n",
      "Bayseian realforecast : [757618.93777588557, 761722.2857351097, 921883.73829621298, 329503.70969187602]\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,0,forecastDay,'month') #0은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[757618.93777588557, 761722.2857351097, 921883.73829621298, 329503.70969187602]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawArrayDatas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
