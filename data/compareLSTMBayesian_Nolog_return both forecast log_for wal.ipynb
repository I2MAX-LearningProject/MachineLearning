{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "tf.set_random_seed(77)\n",
    "\n",
    "mockForecastDictionary = {}\n",
    "realForecastDictionary = {}\n",
    "\n",
    "def LearningModuleRunner(rawArrayDatas, processId, forecastDay,dayOrWeekOrMonth):\n",
    "    #TODO make dayOrWeekOrMonth parameter\n",
    "    dayOrWeekOrMonth=dayOrWeekOrMonth\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "    feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "#     LoggingManager.PrintLogMessage(\"LearningManager\", \"LearningModuleRunner\", \"start of learning #\" + str(processId), DefineManager.LOG_LEVEL_INFO)\n",
    "\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "    mockForcastDay=forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "    ds = rawArrayDatas[0]\n",
    "    y = list((rawArrayDatas[1]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    y= list((rawArrayDatas[1][:-forecastDay] ))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "    ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    ##Make txsForMockForecastBayseian   [:-(mockForcastDay+forecastDay)] & np.log\n",
    "    ds = rawArrayDatas[0][:-(mockForcastDay+forecastDay)]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "    y= list(np.log(rawArrayDatas[1][:-(mockForcastDay+forecastDay)]))\n",
    "    sales = list(zip(ds, y))\n",
    "    txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "\n",
    "    #testY for algorithm compare has size of (mockForcastDay+forecastDay)  rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "    testY= rawArrayDatas[1][-(mockForcastDay+forecastDay):-forecastDay]\n",
    "\n",
    "\n",
    "    if dayOrWeekOrMonth is 'day':\n",
    "        ####LSTM_day\n",
    "\n",
    "        #select feature module\n",
    "        feature='DayOfWeek_WeekNumber_Month_Season'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay,feature)\n",
    "\n",
    "      \n",
    "        ####Bayseian_day\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'day')\n",
    "\n",
    "        #알고리즘 비교\n",
    "        nameOfBestAlgorithm= AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출       \n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'day')\n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'week':\n",
    "        \n",
    "        ####LSTM_week\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)\n",
    "\n",
    "\n",
    "        ####Bayseian_week\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'week')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'week')\n",
    "    \n",
    "            \n",
    "    elif dayOrWeekOrMonth is 'month':\n",
    "       \n",
    "        ####LSTM_month\n",
    "\n",
    "        # select feature module\n",
    "        feature = 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "        mockForecastDictionary['LSTM'] = LSTM(txsForMockForecastLstm, mockForcastDay, feature)    \n",
    "\n",
    "        ####Bayseian_month\n",
    "\n",
    "        mockForecastDictionary['Bayseian'] = Bayseian(txsForMockForecastBayseian, mockForcastDay, 'month')\n",
    "\n",
    "\n",
    "        # 알고리즘 비교\n",
    "        nameOfBestAlgorithm = AlgorithmCompare(testY)\n",
    "        \n",
    "        ####더 좋은 알고리즘 호출\n",
    "        tf.reset_default_graph()\n",
    "        realForecastDictionary['LSTM'] = LSTM(txsForRealForecastLstm, forecastDay,feature)\n",
    "        realForecastDictionary['Bayseian'] = Bayseian(txsForRealForecastBayesian, forecastDay, 'month')\n",
    "\n",
    "        ####################################################################################BAYSEIAN\n",
    "    print('LSTM realforecast :',realForecastDictionary['LSTM'])\n",
    "    print('Bayseian realforecast :',realForecastDictionary['Bayseian'] ) \n",
    "    data = rawArrayDatas[1][:-forecastDay] + realForecastDictionary[nameOfBestAlgorithm]\n",
    "    date= rawArrayDatas[0]\n",
    "    print(\"===================================THE END===================================================\")\n",
    "    return realForecastDictionary[nameOfBestAlgorithm]\n",
    "\n",
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['ds'].map(year)\n",
    "    txs['month'] = txs['ds'].map(month)\n",
    "    txs['weekNumber'] = txs['ds'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['ds'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['ds'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['ds'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['y'])\n",
    "    sales = list(txs['y'])\n",
    "    \n",
    "    #week number는 경계부분에서 약간 오류가 있다.\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "    \n",
    "   \n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "    \n",
    "\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    iterations=1000\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "    # train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "#         listedLogPredict=test_predict[-1].tolist()\n",
    "#     return [np.exp(y) for y in listedLogPredict]\n",
    "    return test_predict[-1].tolist()\n",
    "\n",
    "def Bayseian(txs, forecastDay, unit):\n",
    "    global mockForecastDictionary\n",
    "    global realForecastDictionary\n",
    "\n",
    "    if unit is 'day':\n",
    "        if (len(txs) < 366):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay)\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    elif unit is 'week':\n",
    "        if(len(txs)<53):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='w')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "    elif unit is 'month':\n",
    "        if(len(txs)<12):\n",
    "            model = Prophet()\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "        else:\n",
    "            model = Prophet(yearly_seasonality=True)\n",
    "            model.fit(txs)\n",
    "            future = model.make_future_dataframe(periods=forecastDay,freq='m')\n",
    "            forecastProphetTable = model.predict(future)\n",
    "\n",
    "\n",
    "    #date = [d.strftime('%Y-%m-%d') for d in forecastProphetTable['ds']]\n",
    "    return [np.exp(y) for y in forecastProphetTable['yhat'][-forecastDay:]]\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n",
    "\n",
    "def AlgorithmCompare(testY):\n",
    "    global mockForecastDictionary\n",
    "    nameOfBestAlgorithm = 'LSTM'\n",
    "    minData = rmse(testY, mockForecastDictionary[nameOfBestAlgorithm])\n",
    "    rms = 0\n",
    "    for algorithm in mockForecastDictionary.keys():\n",
    "        rms = rmse(testY, mockForecastDictionary[algorithm])\n",
    "        if rms < minData:\n",
    "            nameOfBestAlgorithm = algorithm\n",
    "    print('testY is: ', testY)\n",
    "    print('\\n')\n",
    "    print('LSTM testforecast :',mockForecastDictionary['LSTM'] , '\\n@@@@@LSTM rmse: ',rmse(testY, mockForecastDictionary['LSTM']) )\n",
    "    print('Bayseian testforecast :',mockForecastDictionary['Bayseian'], '\\n@@@@@Bayseian rmse: ',rmse(testY, mockForecastDictionary['Bayseian']) ) \n",
    "    print('\\n')\n",
    "    print(nameOfBestAlgorithm, 'WON!!!!!!')\n",
    "    return nameOfBestAlgorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def LSTM(txs, forecastDay, features)\n",
    "# def Bayseian(txs, forecastDay, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 6\n"
     ]
    }
   ],
   "source": [
    "columns=['ds','y']\n",
    "df=pd.read_table('walMonth.csv', sep=',',header=None,names=columns )\n",
    "forecastDay=int(len(df)*0.2)\n",
    "print(len(df), forecastDay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow35\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['y'][-forecastDay:]=[1]*forecastDay\n",
    "rawArrayDatas=[[i for i in df['ds']],[i for i in df['y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2010-02-01',\n",
       "  '2010-03-01',\n",
       "  '2010-04-01',\n",
       "  '2010-05-01',\n",
       "  '2010-06-01',\n",
       "  '2010-07-01',\n",
       "  '2010-08-01',\n",
       "  '2010-09-01',\n",
       "  '2010-10-01',\n",
       "  '2010-11-01',\n",
       "  '2010-12-01',\n",
       "  '2011-01-01',\n",
       "  '2011-02-01',\n",
       "  '2011-03-01',\n",
       "  '2011-04-01',\n",
       "  '2011-05-01',\n",
       "  '2011-06-01',\n",
       "  '2011-07-01',\n",
       "  '2011-08-01',\n",
       "  '2011-09-01',\n",
       "  '2011-10-01',\n",
       "  '2011-11-01',\n",
       "  '2011-12-01',\n",
       "  '2012-01-01',\n",
       "  '2012-02-01',\n",
       "  '2012-03-01',\n",
       "  '2012-04-01',\n",
       "  '2012-05-01',\n",
       "  '2012-06-01',\n",
       "  '2012-07-01',\n",
       "  '2012-08-01',\n",
       "  '2012-09-01',\n",
       "  '2012-10-01'],\n",
       " [32990.769999999997,\n",
       "  22809.285,\n",
       "  30103.352000000003,\n",
       "  16673.537499999999,\n",
       "  16685.174999999999,\n",
       "  16383.002,\n",
       "  16144.702499999999,\n",
       "  17978.317500000001,\n",
       "  26928.906000000003,\n",
       "  23040.349999999999,\n",
       "  34796.775999999998,\n",
       "  17286.647499999999,\n",
       "  31440.657500000001,\n",
       "  20705.834999999999,\n",
       "  33011.389999999999,\n",
       "  17062.93,\n",
       "  15744.6425,\n",
       "  15771.246000000001,\n",
       "  14765.487499999999,\n",
       "  17551.281999999999,\n",
       "  24701.7075,\n",
       "  24634.377499999999,\n",
       "  34902.413999999997,\n",
       "  17551.337500000001,\n",
       "  33670.824999999997,\n",
       "  22936.107999999997,\n",
       "  31400.029999999999,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawArrayDatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 14.355506896972656\n",
      "[step: 1] loss: 13.232269287109375\n",
      "[step: 2] loss: 12.165979385375977\n",
      "[step: 3] loss: 11.140503883361816\n",
      "[step: 4] loss: 10.141263961791992\n",
      "[step: 5] loss: 9.155973434448242\n",
      "[step: 6] loss: 8.173894882202148\n",
      "[step: 7] loss: 7.186500549316406\n",
      "[step: 8] loss: 6.189240455627441\n",
      "[step: 9] loss: 5.184516429901123\n",
      "[step: 10] loss: 4.1863813400268555\n",
      "[step: 11] loss: 3.228079319000244\n",
      "[step: 12] loss: 2.373772382736206\n",
      "[step: 13] loss: 1.733634352684021\n",
      "[step: 14] loss: 1.4662210941314697\n",
      "[step: 15] loss: 1.6913750171661377\n",
      "[step: 16] loss: 2.2171549797058105\n",
      "[step: 17] loss: 2.550845146179199\n",
      "[step: 18] loss: 2.475527763366699\n",
      "[step: 19] loss: 2.135228157043457\n",
      "[step: 20] loss: 1.7567733526229858\n",
      "[step: 21] loss: 1.4872534275054932\n",
      "[step: 22] loss: 1.3684765100479126\n",
      "[step: 23] loss: 1.3732736110687256\n",
      "[step: 24] loss: 1.4497278928756714\n",
      "[step: 25] loss: 1.5494227409362793\n",
      "[step: 26] loss: 1.6382442712783813\n",
      "[step: 27] loss: 1.6969311237335205\n",
      "[step: 28] loss: 1.717909812927246\n",
      "[step: 29] loss: 1.701794147491455\n",
      "[step: 30] loss: 1.6546640396118164\n",
      "[step: 31] loss: 1.586146354675293\n",
      "[step: 32] loss: 1.5079677104949951\n",
      "[step: 33] loss: 1.4325926303863525\n",
      "[step: 34] loss: 1.3716206550598145\n",
      "[step: 35] loss: 1.3338124752044678\n",
      "[step: 36] loss: 1.3229584693908691\n",
      "[step: 37] loss: 1.3363057374954224\n",
      "[step: 38] loss: 1.3646552562713623\n",
      "[step: 39] loss: 1.3949463367462158\n",
      "[step: 40] loss: 1.4148787260055542\n",
      "[step: 41] loss: 1.4174692630767822\n",
      "[step: 42] loss: 1.403080701828003\n",
      "[step: 43] loss: 1.378009557723999\n",
      "[step: 44] loss: 1.3508660793304443\n",
      "[step: 45] loss: 1.3289474248886108\n",
      "[step: 46] loss: 1.3161532878875732\n",
      "[step: 47] loss: 1.3127275705337524\n",
      "[step: 48] loss: 1.3162641525268555\n",
      "[step: 49] loss: 1.3231441974639893\n",
      "[step: 50] loss: 1.3298234939575195\n",
      "[step: 51] loss: 1.3336865901947021\n",
      "[step: 52] loss: 1.3334434032440186\n",
      "[step: 53] loss: 1.3291356563568115\n",
      "[step: 54] loss: 1.321873426437378\n",
      "[step: 55] loss: 1.3134002685546875\n",
      "[step: 56] loss: 1.3055737018585205\n",
      "[step: 57] loss: 1.2998679876327515\n",
      "[step: 58] loss: 1.2969934940338135\n",
      "[step: 59] loss: 1.2967512607574463\n",
      "[step: 60] loss: 1.2981622219085693\n",
      "[step: 61] loss: 1.299858808517456\n",
      "[step: 62] loss: 1.3005870580673218\n",
      "[step: 63] loss: 1.2996315956115723\n",
      "[step: 64] loss: 1.2969964742660522\n",
      "[step: 65] loss: 1.2932881116867065\n",
      "[step: 66] loss: 1.2893962860107422\n",
      "[step: 67] loss: 1.286123275756836\n",
      "[step: 68] loss: 1.2839256525039673\n",
      "[step: 69] loss: 1.2828269004821777\n",
      "[step: 70] loss: 1.2824976444244385\n",
      "[step: 71] loss: 1.2824335098266602\n",
      "[step: 72] loss: 1.282147765159607\n",
      "[step: 73] loss: 1.281319499015808\n",
      "[step: 74] loss: 1.2798619270324707\n",
      "[step: 75] loss: 1.2779088020324707\n",
      "[step: 76] loss: 1.2757362127304077\n",
      "[step: 77] loss: 1.273653507232666\n",
      "[step: 78] loss: 1.2718980312347412\n",
      "[step: 79] loss: 1.2705624103546143\n",
      "[step: 80] loss: 1.269586205482483\n",
      "[step: 81] loss: 1.268792748451233\n",
      "[step: 82] loss: 1.2679706811904907\n",
      "[step: 83] loss: 1.2669556140899658\n",
      "[step: 84] loss: 1.2656817436218262\n",
      "[step: 85] loss: 1.264194130897522\n",
      "[step: 86] loss: 1.2626092433929443\n",
      "[step: 87] loss: 1.2610630989074707\n",
      "[step: 88] loss: 1.2596521377563477\n",
      "[step: 89] loss: 1.2584059238433838\n",
      "[step: 90] loss: 1.2572885751724243\n",
      "[step: 91] loss: 1.2562215328216553\n",
      "[step: 92] loss: 1.2551236152648926\n",
      "[step: 93] loss: 1.25393807888031\n",
      "[step: 94] loss: 1.2526495456695557\n",
      "[step: 95] loss: 1.2512853145599365\n",
      "[step: 96] loss: 1.249894380569458\n",
      "[step: 97] loss: 1.248528003692627\n",
      "[step: 98] loss: 1.2472175359725952\n",
      "[step: 99] loss: 1.2459676265716553\n",
      "[step: 100] loss: 1.244755506515503\n",
      "[step: 101] loss: 1.2435473203659058\n",
      "[step: 102] loss: 1.2423102855682373\n",
      "[step: 103] loss: 1.2410283088684082\n",
      "[step: 104] loss: 1.2397056818008423\n",
      "[step: 105] loss: 1.2383594512939453\n",
      "[step: 106] loss: 1.2370123863220215\n",
      "[step: 107] loss: 1.2356829643249512\n",
      "[step: 108] loss: 1.234375238418579\n",
      "[step: 109] loss: 1.233083724975586\n",
      "[step: 110] loss: 1.23179292678833\n",
      "[step: 111] loss: 1.2304881811141968\n",
      "[step: 112] loss: 1.2291607856750488\n",
      "[step: 113] loss: 1.2278110980987549\n",
      "[step: 114] loss: 1.226444125175476\n",
      "[step: 115] loss: 1.2250702381134033\n",
      "[step: 116] loss: 1.2236974239349365\n",
      "[step: 117] loss: 1.222328543663025\n",
      "[step: 118] loss: 1.220961570739746\n",
      "[step: 119] loss: 1.2195900678634644\n",
      "[step: 120] loss: 1.2182077169418335\n",
      "[step: 121] loss: 1.2168117761611938\n",
      "[step: 122] loss: 1.2154003381729126\n",
      "[step: 123] loss: 1.2139769792556763\n",
      "[step: 124] loss: 1.2125459909439087\n",
      "[step: 125] loss: 1.2111091613769531\n",
      "[step: 126] loss: 1.2096670866012573\n",
      "[step: 127] loss: 1.2082184553146362\n",
      "[step: 128] loss: 1.2067598104476929\n",
      "[step: 129] loss: 1.205289602279663\n",
      "[step: 130] loss: 1.203805685043335\n",
      "[step: 131] loss: 1.2023086547851562\n",
      "[step: 132] loss: 1.2007991075515747\n",
      "[step: 133] loss: 1.199280023574829\n",
      "[step: 134] loss: 1.1977503299713135\n",
      "[step: 135] loss: 1.196210265159607\n",
      "[step: 136] loss: 1.194657325744629\n",
      "[step: 137] loss: 1.1930911540985107\n",
      "[step: 138] loss: 1.1915100812911987\n",
      "[step: 139] loss: 1.1899141073226929\n",
      "[step: 140] loss: 1.188302993774414\n",
      "[step: 141] loss: 1.1866778135299683\n",
      "[step: 142] loss: 1.1850379705429077\n",
      "[step: 143] loss: 1.1833828687667847\n",
      "[step: 144] loss: 1.181712031364441\n",
      "[step: 145] loss: 1.1800241470336914\n",
      "[step: 146] loss: 1.1783181428909302\n",
      "[step: 147] loss: 1.176593542098999\n",
      "[step: 148] loss: 1.174849510192871\n",
      "[step: 149] loss: 1.1730865240097046\n",
      "[step: 150] loss: 1.1713039875030518\n",
      "[step: 151] loss: 1.1695008277893066\n",
      "[step: 152] loss: 1.1676766872406006\n",
      "[step: 153] loss: 1.165830135345459\n",
      "[step: 154] loss: 1.1639602184295654\n",
      "[step: 155] loss: 1.1620659828186035\n",
      "[step: 156] loss: 1.1601471900939941\n",
      "[step: 157] loss: 1.1582019329071045\n",
      "[step: 158] loss: 1.1562309265136719\n",
      "[step: 159] loss: 1.1542315483093262\n",
      "[step: 160] loss: 1.1522036790847778\n",
      "[step: 161] loss: 1.1501448154449463\n",
      "[step: 162] loss: 1.148055076599121\n",
      "[step: 163] loss: 1.1459321975708008\n",
      "[step: 164] loss: 1.143775463104248\n",
      "[step: 165] loss: 1.1415833234786987\n",
      "[step: 166] loss: 1.1393537521362305\n",
      "[step: 167] loss: 1.1370859146118164\n",
      "[step: 168] loss: 1.134777545928955\n",
      "[step: 169] loss: 1.1324266195297241\n",
      "[step: 170] loss: 1.1300311088562012\n",
      "[step: 171] loss: 1.127589225769043\n",
      "[step: 172] loss: 1.1250983476638794\n",
      "[step: 173] loss: 1.1225571632385254\n",
      "[step: 174] loss: 1.1199620962142944\n",
      "[step: 175] loss: 1.1173107624053955\n",
      "[step: 176] loss: 1.1146005392074585\n",
      "[step: 177] loss: 1.111828327178955\n",
      "[step: 178] loss: 1.1089904308319092\n",
      "[step: 179] loss: 1.1060847043991089\n",
      "[step: 180] loss: 1.1031056642532349\n",
      "[step: 181] loss: 1.1000510454177856\n",
      "[step: 182] loss: 1.0969161987304688\n",
      "[step: 183] loss: 1.0936963558197021\n",
      "[step: 184] loss: 1.0903866291046143\n",
      "[step: 185] loss: 1.0869832038879395\n",
      "[step: 186] loss: 1.083479642868042\n",
      "[step: 187] loss: 1.079871416091919\n",
      "[step: 188] loss: 1.0761522054672241\n",
      "[step: 189] loss: 1.0723159313201904\n",
      "[step: 190] loss: 1.0683557987213135\n",
      "[step: 191] loss: 1.0642659664154053\n",
      "[step: 192] loss: 1.0600385665893555\n",
      "[step: 193] loss: 1.055666208267212\n",
      "[step: 194] loss: 1.0511406660079956\n",
      "[step: 195] loss: 1.0464550256729126\n",
      "[step: 196] loss: 1.0416009426116943\n",
      "[step: 197] loss: 1.036569356918335\n",
      "[step: 198] loss: 1.0313514471054077\n",
      "[step: 199] loss: 1.025940179824829\n",
      "[step: 200] loss: 1.0203251838684082\n",
      "[step: 201] loss: 1.0144996643066406\n",
      "[step: 202] loss: 1.0084543228149414\n",
      "[step: 203] loss: 1.0021804571151733\n",
      "[step: 204] loss: 0.9956714510917664\n",
      "[step: 205] loss: 0.9889196157455444\n",
      "[step: 206] loss: 0.9819199442863464\n",
      "[step: 207] loss: 0.9746654033660889\n",
      "[step: 208] loss: 0.967153012752533\n",
      "[step: 209] loss: 0.9593791961669922\n",
      "[step: 210] loss: 0.9513429403305054\n",
      "[step: 211] loss: 0.9430437088012695\n",
      "[step: 212] loss: 0.9344838857650757\n",
      "[step: 213] loss: 0.925666332244873\n",
      "[step: 214] loss: 0.9165986776351929\n",
      "[step: 215] loss: 0.9072896242141724\n",
      "[step: 216] loss: 0.8977485299110413\n",
      "[step: 217] loss: 0.8879913091659546\n",
      "[step: 218] loss: 0.8780347108840942\n",
      "[step: 219] loss: 0.8678990006446838\n",
      "[step: 220] loss: 0.8576083779335022\n",
      "[step: 221] loss: 0.8471918106079102\n",
      "[step: 222] loss: 0.8366826772689819\n",
      "[step: 223] loss: 0.8261182308197021\n",
      "[step: 224] loss: 0.8155430555343628\n",
      "[step: 225] loss: 0.8050055503845215\n",
      "[step: 226] loss: 0.7945619821548462\n",
      "[step: 227] loss: 0.7842742204666138\n",
      "[step: 228] loss: 0.77420973777771\n",
      "[step: 229] loss: 0.764441967010498\n",
      "[step: 230] loss: 0.7550469636917114\n",
      "[step: 231] loss: 0.7461037635803223\n",
      "[step: 232] loss: 0.7376884818077087\n",
      "[step: 233] loss: 0.7298715114593506\n",
      "[step: 234] loss: 0.7227116227149963\n",
      "[step: 235] loss: 0.716248095035553\n",
      "[step: 236] loss: 0.7104954123497009\n",
      "[step: 237] loss: 0.7054370641708374\n",
      "[step: 238] loss: 0.7010178565979004\n",
      "[step: 239] loss: 0.6971431970596313\n",
      "[step: 240] loss: 0.6936819553375244\n",
      "[step: 241] loss: 0.6904740929603577\n",
      "[step: 242] loss: 0.6873440742492676\n",
      "[step: 243] loss: 0.684118390083313\n",
      "[step: 244] loss: 0.6806457042694092\n",
      "[step: 245] loss: 0.676814079284668\n",
      "[step: 246] loss: 0.6725658178329468\n",
      "[step: 247] loss: 0.6679763793945312\n",
      "[step: 248] loss: 0.6636587381362915\n",
      "[step: 249] loss: 0.6622782349586487\n",
      "[step: 250] loss: 0.6664769649505615\n",
      "[step: 251] loss: 0.6604604721069336\n",
      "[step: 252] loss: 0.6419882774353027\n",
      "[step: 253] loss: 0.6437551975250244\n",
      "[step: 254] loss: 0.6388428807258606\n",
      "[step: 255] loss: 0.6262767910957336\n",
      "[step: 256] loss: 0.6283347606658936\n",
      "[step: 257] loss: 0.6174579858779907\n",
      "[step: 258] loss: 0.613633394241333\n",
      "[step: 259] loss: 0.6096449494361877\n",
      "[step: 260] loss: 0.6004536747932434\n",
      "[step: 261] loss: 0.5991131067276001\n",
      "[step: 262] loss: 0.5901371240615845\n",
      "[step: 263] loss: 0.5857986211776733\n",
      "[step: 264] loss: 0.5807464718818665\n",
      "[step: 265] loss: 0.5724887847900391\n",
      "[step: 266] loss: 0.5688024759292603\n",
      "[step: 267] loss: 0.5619053840637207\n",
      "[step: 268] loss: 0.554024875164032\n",
      "[step: 269] loss: 0.5500831007957458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 270] loss: 0.5430952906608582\n",
      "[step: 271] loss: 0.5346294045448303\n",
      "[step: 272] loss: 0.5290062427520752\n",
      "[step: 273] loss: 0.5242760181427002\n",
      "[step: 274] loss: 0.5176475048065186\n",
      "[step: 275] loss: 0.5096523761749268\n",
      "[step: 276] loss: 0.5016870498657227\n",
      "[step: 277] loss: 0.4942476153373718\n",
      "[step: 278] loss: 0.48731061816215515\n",
      "[step: 279] loss: 0.4805811643600464\n",
      "[step: 280] loss: 0.4746474027633667\n",
      "[step: 281] loss: 0.47308623790740967\n",
      "[step: 282] loss: 0.49993348121643066\n",
      "[step: 283] loss: 0.601479709148407\n",
      "[step: 284] loss: 0.5748312473297119\n",
      "[step: 285] loss: 0.44964832067489624\n",
      "[step: 286] loss: 0.5716143846511841\n",
      "[step: 287] loss: 0.46086621284484863\n",
      "[step: 288] loss: 0.5040069818496704\n",
      "[step: 289] loss: 0.4774826467037201\n",
      "[step: 290] loss: 0.45776334404945374\n",
      "[step: 291] loss: 0.48143795132637024\n",
      "[step: 292] loss: 0.4337202310562134\n",
      "[step: 293] loss: 0.47327518463134766\n",
      "[step: 294] loss: 0.4200282394886017\n",
      "[step: 295] loss: 0.4605700671672821\n",
      "[step: 296] loss: 0.41309595108032227\n",
      "[step: 297] loss: 0.4459823668003082\n",
      "[step: 298] loss: 0.4083602726459503\n",
      "[step: 299] loss: 0.4306272864341736\n",
      "[step: 300] loss: 0.40555933117866516\n",
      "[step: 301] loss: 0.41547691822052\n",
      "[step: 302] loss: 0.40318408608436584\n",
      "[step: 303] loss: 0.4012082815170288\n",
      "[step: 304] loss: 0.4008784890174866\n",
      "[step: 305] loss: 0.388610303401947\n",
      "[step: 306] loss: 0.3966180086135864\n",
      "[step: 307] loss: 0.37932974100112915\n",
      "[step: 308] loss: 0.38976478576660156\n",
      "[step: 309] loss: 0.3740728497505188\n",
      "[step: 310] loss: 0.3793168067932129\n",
      "[step: 311] loss: 0.3717624545097351\n",
      "[step: 312] loss: 0.3677912950515747\n",
      "[step: 313] loss: 0.36941051483154297\n",
      "[step: 314] loss: 0.3592095971107483\n",
      "[step: 315] loss: 0.36296525597572327\n",
      "[step: 316] loss: 0.35573792457580566\n",
      "[step: 317] loss: 0.3528960943222046\n",
      "[step: 318] loss: 0.3534920811653137\n",
      "[step: 319] loss: 0.3456249535083771\n",
      "[step: 320] loss: 0.34639859199523926\n",
      "[step: 321] loss: 0.34340670704841614\n",
      "[step: 322] loss: 0.3377792537212372\n",
      "[step: 323] loss: 0.338674932718277\n",
      "[step: 324] loss: 0.33450788259506226\n",
      "[step: 325] loss: 0.33051568269729614\n",
      "[step: 326] loss: 0.3304625153541565\n",
      "[step: 327] loss: 0.32701539993286133\n",
      "[step: 328] loss: 0.3230245113372803\n",
      "[step: 329] loss: 0.3225274085998535\n",
      "[step: 330] loss: 0.31992992758750916\n",
      "[step: 331] loss: 0.31604766845703125\n",
      "[step: 332] loss: 0.3144073784351349\n",
      "[step: 333] loss: 0.3130059838294983\n",
      "[step: 334] loss: 0.3095938563346863\n",
      "[step: 335] loss: 0.3067920506000519\n",
      "[step: 336] loss: 0.30536532402038574\n",
      "[step: 337] loss: 0.303363174200058\n",
      "[step: 338] loss: 0.30022695660591125\n",
      "[step: 339] loss: 0.29772403836250305\n",
      "[step: 340] loss: 0.2960503101348877\n",
      "[step: 341] loss: 0.29406481981277466\n",
      "[step: 342] loss: 0.2913135290145874\n",
      "[step: 343] loss: 0.2887193560600281\n",
      "[step: 344] loss: 0.2867107391357422\n",
      "[step: 345] loss: 0.28486672043800354\n",
      "[step: 346] loss: 0.28256216645240784\n",
      "[step: 347] loss: 0.2800080180168152\n",
      "[step: 348] loss: 0.27761608362197876\n",
      "[step: 349] loss: 0.2755904793739319\n",
      "[step: 350] loss: 0.27360886335372925\n",
      "[step: 351] loss: 0.27143457531929016\n",
      "[step: 352] loss: 0.269050657749176\n",
      "[step: 353] loss: 0.2667071223258972\n",
      "[step: 354] loss: 0.26452773809432983\n",
      "[step: 355] loss: 0.26249438524246216\n",
      "[step: 356] loss: 0.26045072078704834\n",
      "[step: 357] loss: 0.25831133127212524\n",
      "[step: 358] loss: 0.2560850977897644\n",
      "[step: 359] loss: 0.25384998321533203\n",
      "[step: 360] loss: 0.2516862452030182\n",
      "[step: 361] loss: 0.24959996342658997\n",
      "[step: 362] loss: 0.24757465720176697\n",
      "[step: 363] loss: 0.24555164575576782\n",
      "[step: 364] loss: 0.24351385235786438\n",
      "[step: 365] loss: 0.24144577980041504\n",
      "[step: 366] loss: 0.23936712741851807\n",
      "[step: 367] loss: 0.23729348182678223\n",
      "[step: 368] loss: 0.2352391630411148\n",
      "[step: 369] loss: 0.23321664333343506\n",
      "[step: 370] loss: 0.23122060298919678\n",
      "[step: 371] loss: 0.22925607860088348\n",
      "[step: 372] loss: 0.22731463611125946\n",
      "[step: 373] loss: 0.22539836168289185\n",
      "[step: 374] loss: 0.2235071063041687\n",
      "[step: 375] loss: 0.22164490818977356\n",
      "[step: 376] loss: 0.2198239266872406\n",
      "[step: 377] loss: 0.21806028485298157\n",
      "[step: 378] loss: 0.21639569103717804\n",
      "[step: 379] loss: 0.21491456031799316\n",
      "[step: 380] loss: 0.21377721428871155\n",
      "[step: 381] loss: 0.21338817477226257\n",
      "[step: 382] loss: 0.21446260809898376\n",
      "[step: 383] loss: 0.21883271634578705\n",
      "[step: 384] loss: 0.2283630669116974\n",
      "[step: 385] loss: 0.24655181169509888\n",
      "[step: 386] loss: 0.2607533931732178\n",
      "[step: 387] loss: 0.2579098641872406\n",
      "[step: 388] loss: 0.22063925862312317\n",
      "[step: 389] loss: 0.19800995290279388\n",
      "[step: 390] loss: 0.2124728262424469\n",
      "[step: 391] loss: 0.22593533992767334\n",
      "[step: 392] loss: 0.21017292141914368\n",
      "[step: 393] loss: 0.19290104508399963\n",
      "[step: 394] loss: 0.20358875393867493\n",
      "[step: 395] loss: 0.21051838994026184\n",
      "[step: 396] loss: 0.1942482739686966\n",
      "[step: 397] loss: 0.18989335000514984\n",
      "[step: 398] loss: 0.19987019896507263\n",
      "[step: 399] loss: 0.19457130134105682\n",
      "[step: 400] loss: 0.18472865223884583\n",
      "[step: 401] loss: 0.18901926279067993\n",
      "[step: 402] loss: 0.19118046760559082\n",
      "[step: 403] loss: 0.18326929211616516\n",
      "[step: 404] loss: 0.18128365278244019\n",
      "[step: 405] loss: 0.1853305548429489\n",
      "[step: 406] loss: 0.1823953241109848\n",
      "[step: 407] loss: 0.1770753264427185\n",
      "[step: 408] loss: 0.17858192324638367\n",
      "[step: 409] loss: 0.17984578013420105\n",
      "[step: 410] loss: 0.1755586862564087\n",
      "[step: 411] loss: 0.17314520478248596\n",
      "[step: 412] loss: 0.17474718391895294\n",
      "[step: 413] loss: 0.1742059886455536\n",
      "[step: 414] loss: 0.1708841174840927\n",
      "[step: 415] loss: 0.16945642232894897\n",
      "[step: 416] loss: 0.17032723128795624\n",
      "[step: 417] loss: 0.1698196977376938\n",
      "[step: 418] loss: 0.16736003756523132\n",
      "[step: 419] loss: 0.16584137082099915\n",
      "[step: 420] loss: 0.16603554785251617\n",
      "[step: 421] loss: 0.1659545600414276\n",
      "[step: 422] loss: 0.16451752185821533\n",
      "[step: 423] loss: 0.162784144282341\n",
      "[step: 424] loss: 0.16206057369709015\n",
      "[step: 425] loss: 0.1620522439479828\n",
      "[step: 426] loss: 0.16165480017662048\n",
      "[step: 427] loss: 0.16052642464637756\n",
      "[step: 428] loss: 0.15920943021774292\n",
      "[step: 429] loss: 0.15836700797080994\n",
      "[step: 430] loss: 0.15802502632141113\n",
      "[step: 431] loss: 0.15773341059684753\n",
      "[step: 432] loss: 0.1571345031261444\n",
      "[step: 433] loss: 0.15620417892932892\n",
      "[step: 434] loss: 0.15521901845932007\n",
      "[step: 435] loss: 0.15442144870758057\n",
      "[step: 436] loss: 0.15387095510959625\n",
      "[step: 437] loss: 0.15345656871795654\n",
      "[step: 438] loss: 0.15301960706710815\n",
      "[step: 439] loss: 0.15246781706809998\n",
      "[step: 440] loss: 0.15178556740283966\n",
      "[step: 441] loss: 0.15103860199451447\n",
      "[step: 442] loss: 0.15028786659240723\n",
      "[step: 443] loss: 0.14958703517913818\n",
      "[step: 444] loss: 0.14895370602607727\n",
      "[step: 445] loss: 0.14838120341300964\n",
      "[step: 446] loss: 0.14785236120224\n",
      "[step: 447] loss: 0.14734894037246704\n",
      "[step: 448] loss: 0.14686191082000732\n",
      "[step: 449] loss: 0.14638733863830566\n",
      "[step: 450] loss: 0.14593809843063354\n",
      "[step: 451] loss: 0.14552518725395203\n",
      "[step: 452] loss: 0.14519086480140686\n",
      "[step: 453] loss: 0.14497342705726624\n",
      "[step: 454] loss: 0.14498846232891083\n",
      "[step: 455] loss: 0.14534813165664673\n",
      "[step: 456] loss: 0.14637205004692078\n",
      "[step: 457] loss: 0.14826489984989166\n",
      "[step: 458] loss: 0.15165606141090393\n",
      "[step: 459] loss: 0.15595579147338867\n",
      "[step: 460] loss: 0.1611347198486328\n",
      "[step: 461] loss: 0.16252313554286957\n",
      "[step: 462] loss: 0.15949922800064087\n",
      "[step: 463] loss: 0.14963924884796143\n",
      "[step: 464] loss: 0.14043964445590973\n",
      "[step: 465] loss: 0.13742248713970184\n",
      "[step: 466] loss: 0.14096331596374512\n",
      "[step: 467] loss: 0.14556044340133667\n",
      "[step: 468] loss: 0.1450447291135788\n",
      "[step: 469] loss: 0.14008846879005432\n",
      "[step: 470] loss: 0.13527540862560272\n",
      "[step: 471] loss: 0.13493439555168152\n",
      "[step: 472] loss: 0.1376260221004486\n",
      "[step: 473] loss: 0.13877235352993011\n",
      "[step: 474] loss: 0.13663595914840698\n",
      "[step: 475] loss: 0.13312995433807373\n",
      "[step: 476] loss: 0.13161247968673706\n",
      "[step: 477] loss: 0.13252246379852295\n",
      "[step: 478] loss: 0.13364127278327942\n",
      "[step: 479] loss: 0.133107990026474\n",
      "[step: 480] loss: 0.13096782565116882\n",
      "[step: 481] loss: 0.12901689112186432\n",
      "[step: 482] loss: 0.12839367985725403\n",
      "[step: 483] loss: 0.12879569828510284\n",
      "[step: 484] loss: 0.12913520634174347\n",
      "[step: 485] loss: 0.12858639657497406\n",
      "[step: 486] loss: 0.12729179859161377\n",
      "[step: 487] loss: 0.1258305013179779\n",
      "[step: 488] loss: 0.12481717020273209\n",
      "[step: 489] loss: 0.12439559400081635\n",
      "[step: 490] loss: 0.12432195991277695\n",
      "[step: 491] loss: 0.12425656616687775\n",
      "[step: 492] loss: 0.12393797934055328\n",
      "[step: 493] loss: 0.12333635985851288\n",
      "[step: 494] loss: 0.12248653173446655\n",
      "[step: 495] loss: 0.12155178189277649\n",
      "[step: 496] loss: 0.12061865627765656\n",
      "[step: 497] loss: 0.11976221948862076\n",
      "[step: 498] loss: 0.11899535357952118\n",
      "[step: 499] loss: 0.11831032484769821\n",
      "[step: 500] loss: 0.11768773943185806\n",
      "[step: 501] loss: 0.11710958182811737\n",
      "[step: 502] loss: 0.11656996607780457\n",
      "[step: 503] loss: 0.11608079820871353\n",
      "[step: 504] loss: 0.11568143218755722\n",
      "[step: 505] loss: 0.11545582115650177\n",
      "[step: 506] loss: 0.11560367792844772\n",
      "[step: 507] loss: 0.11650268733501434\n",
      "[step: 508] loss: 0.11902789771556854\n",
      "[step: 509] loss: 0.12444045394659042\n",
      "[step: 510] loss: 0.1348998248577118\n",
      "[step: 511] loss: 0.14832520484924316\n",
      "[step: 512] loss: 0.16037499904632568\n",
      "[step: 513] loss: 0.15132519602775574\n",
      "[step: 514] loss: 0.129827618598938\n",
      "[step: 515] loss: 0.11104343086481094\n",
      "[step: 516] loss: 0.11457796394824982\n",
      "[step: 517] loss: 0.12869501113891602\n",
      "[step: 518] loss: 0.12649299204349518\n",
      "[step: 519] loss: 0.11183862388134003\n",
      "[step: 520] loss: 0.1069779247045517\n",
      "[step: 521] loss: 0.11616505682468414\n",
      "[step: 522] loss: 0.11870762705802917\n",
      "[step: 523] loss: 0.10818451642990112\n",
      "[step: 524] loss: 0.10451802611351013\n",
      "[step: 525] loss: 0.11072953045368195\n",
      "[step: 526] loss: 0.11091722548007965\n",
      "[step: 527] loss: 0.10431891679763794\n",
      "[step: 528] loss: 0.10237185657024384\n",
      "[step: 529] loss: 0.10583393275737762\n",
      "[step: 530] loss: 0.10578878223896027\n",
      "[step: 531] loss: 0.10153606534004211\n",
      "[step: 532] loss: 0.10002449154853821\n",
      "[step: 533] loss: 0.10146856307983398\n",
      "[step: 534] loss: 0.10140803456306458\n",
      "[step: 535] loss: 0.09939421713352203\n",
      "[step: 536] loss: 0.09766097366809845\n",
      "[step: 537] loss: 0.09722922742366791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 538] loss: 0.09744267165660858\n",
      "[step: 539] loss: 0.0972408652305603\n",
      "[step: 540] loss: 0.09605677425861359\n",
      "[step: 541] loss: 0.09433252364397049\n",
      "[step: 542] loss: 0.09342801570892334\n",
      "[step: 543] loss: 0.09373338520526886\n",
      "[step: 544] loss: 0.09401610493659973\n",
      "[step: 545] loss: 0.09316407889127731\n",
      "[step: 546] loss: 0.09163489192724228\n",
      "[step: 547] loss: 0.09053961932659149\n",
      "[step: 548] loss: 0.09014056622982025\n",
      "[step: 549] loss: 0.08996792882680893\n",
      "[step: 550] loss: 0.0897412896156311\n",
      "[step: 551] loss: 0.08949653059244156\n",
      "[step: 552] loss: 0.08917514979839325\n",
      "[step: 553] loss: 0.08854030072689056\n",
      "[step: 554] loss: 0.08768892288208008\n",
      "[step: 555] loss: 0.08685237169265747\n",
      "[step: 556] loss: 0.08621063828468323\n",
      "[step: 557] loss: 0.0856621116399765\n",
      "[step: 558] loss: 0.08506797254085541\n",
      "[step: 559] loss: 0.08445703983306885\n",
      "[step: 560] loss: 0.08393584191799164\n",
      "[step: 561] loss: 0.08352356404066086\n",
      "[step: 562] loss: 0.08314673602581024\n",
      "[step: 563] loss: 0.08277535438537598\n",
      "[step: 564] loss: 0.08252798765897751\n",
      "[step: 565] loss: 0.0826348140835762\n",
      "[step: 566] loss: 0.08353839814662933\n",
      "[step: 567] loss: 0.08620011806488037\n",
      "[step: 568] loss: 0.09292156994342804\n",
      "[step: 569] loss: 0.10735417157411575\n",
      "[step: 570] loss: 0.13295720517635345\n",
      "[step: 571] loss: 0.1523597687482834\n",
      "[step: 572] loss: 0.14709991216659546\n",
      "[step: 573] loss: 0.10228589177131653\n",
      "[step: 574] loss: 0.08352292329072952\n",
      "[step: 575] loss: 0.10062725096940994\n",
      "[step: 576] loss: 0.11023732274770737\n",
      "[step: 577] loss: 0.09688176214694977\n",
      "[step: 578] loss: 0.08240354806184769\n",
      "[step: 579] loss: 0.09131002426147461\n",
      "[step: 580] loss: 0.10090148448944092\n",
      "[step: 581] loss: 0.08603565394878387\n",
      "[step: 582] loss: 0.07993792742490768\n",
      "[step: 583] loss: 0.09172292798757553\n",
      "[step: 584] loss: 0.08761946856975555\n",
      "[step: 585] loss: 0.07701835036277771\n",
      "[step: 586] loss: 0.08300872892141342\n",
      "[step: 587] loss: 0.08600923418998718\n",
      "[step: 588] loss: 0.07690414041280746\n",
      "[step: 589] loss: 0.07731369137763977\n",
      "[step: 590] loss: 0.08258607983589172\n",
      "[step: 591] loss: 0.07805037498474121\n",
      "[step: 592] loss: 0.07453066110610962\n",
      "[step: 593] loss: 0.0782875046133995\n",
      "[step: 594] loss: 0.07801232486963272\n",
      "[step: 595] loss: 0.07424160838127136\n",
      "[step: 596] loss: 0.07475936412811279\n",
      "[step: 597] loss: 0.0759371966123581\n",
      "[step: 598] loss: 0.07475882768630981\n",
      "[step: 599] loss: 0.07305996119976044\n",
      "[step: 600] loss: 0.07289750874042511\n",
      "[step: 601] loss: 0.07386236637830734\n",
      "[step: 602] loss: 0.07335124909877777\n",
      "[step: 603] loss: 0.07139827311038971\n",
      "[step: 604] loss: 0.07137122750282288\n",
      "[step: 605] loss: 0.07258076965808868\n",
      "[step: 606] loss: 0.07170119136571884\n",
      "[step: 607] loss: 0.07034064829349518\n",
      "[step: 608] loss: 0.07037988305091858\n",
      "[step: 609] loss: 0.07079915702342987\n",
      "[step: 610] loss: 0.07052803039550781\n",
      "[step: 611] loss: 0.06985105574131012\n",
      "[step: 612] loss: 0.06923539191484451\n",
      "[step: 613] loss: 0.06915626674890518\n",
      "[step: 614] loss: 0.06945370137691498\n",
      "[step: 615] loss: 0.06914597749710083\n",
      "[step: 616] loss: 0.0683993399143219\n",
      "[step: 617] loss: 0.06810443103313446\n",
      "[step: 618] loss: 0.06816300004720688\n",
      "[step: 619] loss: 0.06806632876396179\n",
      "[step: 620] loss: 0.06779895722866058\n",
      "[step: 621] loss: 0.06744809448719025\n",
      "[step: 622] loss: 0.06705847382545471\n",
      "[step: 623] loss: 0.06690233200788498\n",
      "[step: 624] loss: 0.06690873205661774\n",
      "[step: 625] loss: 0.0667366161942482\n",
      "[step: 626] loss: 0.06639331579208374\n",
      "[step: 627] loss: 0.06609469652175903\n",
      "[step: 628] loss: 0.06590047478675842\n",
      "[step: 629] loss: 0.06574617326259613\n",
      "[step: 630] loss: 0.06563477963209152\n",
      "[step: 631] loss: 0.0654708668589592\n",
      "[step: 632] loss: 0.0652020126581192\n",
      "[step: 633] loss: 0.0649338960647583\n",
      "[step: 634] loss: 0.06474419683218002\n",
      "[step: 635] loss: 0.06459096074104309\n",
      "[step: 636] loss: 0.0644310861825943\n",
      "[step: 637] loss: 0.06427180767059326\n",
      "[step: 638] loss: 0.06408967077732086\n",
      "[step: 639] loss: 0.06386387348175049\n",
      "[step: 640] loss: 0.06364434957504272\n",
      "[step: 641] loss: 0.06345784664154053\n",
      "[step: 642] loss: 0.06328818202018738\n",
      "[step: 643] loss: 0.06311985105276108\n",
      "[step: 644] loss: 0.06295900046825409\n",
      "[step: 645] loss: 0.06279503554105759\n",
      "[step: 646] loss: 0.06261022388935089\n",
      "[step: 647] loss: 0.06241629645228386\n",
      "[step: 648] loss: 0.06222806125879288\n",
      "[step: 649] loss: 0.062041815370321274\n",
      "[step: 650] loss: 0.061854857951402664\n",
      "[step: 651] loss: 0.06167379394173622\n",
      "[step: 652] loss: 0.06150287389755249\n",
      "[step: 653] loss: 0.061332955956459045\n",
      "[step: 654] loss: 0.06116199493408203\n",
      "[step: 655] loss: 0.06099501997232437\n",
      "[step: 656] loss: 0.06083116680383682\n",
      "[step: 657] loss: 0.06066761910915375\n",
      "[step: 658] loss: 0.060504913330078125\n",
      "[step: 659] loss: 0.060349494218826294\n",
      "[step: 660] loss: 0.060204118490219116\n",
      "[step: 661] loss: 0.060072675347328186\n",
      "[step: 662] loss: 0.05996556580066681\n",
      "[step: 663] loss: 0.05990735813975334\n",
      "[step: 664] loss: 0.059929847717285156\n",
      "[step: 665] loss: 0.06011350452899933\n",
      "[step: 666] loss: 0.06057809665799141\n",
      "[step: 667] loss: 0.06164399906992912\n",
      "[step: 668] loss: 0.0637315958738327\n",
      "[step: 669] loss: 0.06806877255439758\n",
      "[step: 670] loss: 0.07555742561817169\n",
      "[step: 671] loss: 0.08954218775033951\n",
      "[step: 672] loss: 0.10511963814496994\n",
      "[step: 673] loss: 0.12157666683197021\n",
      "[step: 674] loss: 0.10888098180294037\n",
      "[step: 675] loss: 0.08169849216938019\n",
      "[step: 676] loss: 0.05914445221424103\n",
      "[step: 677] loss: 0.06646986305713654\n",
      "[step: 678] loss: 0.0848410427570343\n",
      "[step: 679] loss: 0.07968668639659882\n",
      "[step: 680] loss: 0.06250219792127609\n",
      "[step: 681] loss: 0.059153154492378235\n",
      "[step: 682] loss: 0.07061419636011124\n",
      "[step: 683] loss: 0.07241785526275635\n",
      "[step: 684] loss: 0.05998135730624199\n",
      "[step: 685] loss: 0.05904253572225571\n",
      "[step: 686] loss: 0.06746681034564972\n",
      "[step: 687] loss: 0.06482098996639252\n",
      "[step: 688] loss: 0.057209499180316925\n",
      "[step: 689] loss: 0.058912333101034164\n",
      "[step: 690] loss: 0.06354217976331711\n",
      "[step: 691] loss: 0.06090463697910309\n",
      "[step: 692] loss: 0.05601974576711655\n",
      "[step: 693] loss: 0.05822526663541794\n",
      "[step: 694] loss: 0.061085544526576996\n",
      "[step: 695] loss: 0.058046258985996246\n",
      "[step: 696] loss: 0.05520445853471756\n",
      "[step: 697] loss: 0.05709239840507507\n",
      "[step: 698] loss: 0.058539219200611115\n",
      "[step: 699] loss: 0.05661826953291893\n",
      "[step: 700] loss: 0.054526664316654205\n",
      "[step: 701] loss: 0.05551846697926521\n",
      "[step: 702] loss: 0.05676604062318802\n",
      "[step: 703] loss: 0.05572076514363289\n",
      "[step: 704] loss: 0.054004646837711334\n",
      "[step: 705] loss: 0.05409675091505051\n",
      "[step: 706] loss: 0.05503670498728752\n",
      "[step: 707] loss: 0.05500665307044983\n",
      "[step: 708] loss: 0.053768426179885864\n",
      "[step: 709] loss: 0.053065113723278046\n",
      "[step: 710] loss: 0.05341152846813202\n",
      "[step: 711] loss: 0.053861841559410095\n",
      "[step: 712] loss: 0.053528379648923874\n",
      "[step: 713] loss: 0.05269600823521614\n",
      "[step: 714] loss: 0.052269309759140015\n",
      "[step: 715] loss: 0.052424103021621704\n",
      "[step: 716] loss: 0.05264844745397568\n",
      "[step: 717] loss: 0.05244436115026474\n",
      "[step: 718] loss: 0.05191587656736374\n",
      "[step: 719] loss: 0.05147618427872658\n",
      "[step: 720] loss: 0.05138770490884781\n",
      "[step: 721] loss: 0.051470231264829636\n",
      "[step: 722] loss: 0.05144847184419632\n",
      "[step: 723] loss: 0.05117373913526535\n",
      "[step: 724] loss: 0.050804298371076584\n",
      "[step: 725] loss: 0.05050961673259735\n",
      "[step: 726] loss: 0.05038358271121979\n",
      "[step: 727] loss: 0.05034640431404114\n",
      "[step: 728] loss: 0.05028601363301277\n",
      "[step: 729] loss: 0.05012291297316551\n",
      "[step: 730] loss: 0.049870815128088\n",
      "[step: 731] loss: 0.04960387572646141\n",
      "[step: 732] loss: 0.04937850683927536\n",
      "[step: 733] loss: 0.049223221838474274\n",
      "[step: 734] loss: 0.049108825623989105\n",
      "[step: 735] loss: 0.04900948703289032\n",
      "[step: 736] loss: 0.048884984105825424\n",
      "[step: 737] loss: 0.048729389905929565\n",
      "[step: 738] loss: 0.04854320362210274\n",
      "[step: 739] loss: 0.04834207147359848\n",
      "[step: 740] loss: 0.04813801124691963\n",
      "[step: 741] loss: 0.04793846234679222\n",
      "[step: 742] loss: 0.04774976521730423\n",
      "[step: 743] loss: 0.047570109367370605\n",
      "[step: 744] loss: 0.0474000982940197\n",
      "[step: 745] loss: 0.04723454266786575\n",
      "[step: 746] loss: 0.047074802219867706\n",
      "[step: 747] loss: 0.046918369829654694\n",
      "[step: 748] loss: 0.046768732368946075\n",
      "[step: 749] loss: 0.046629033982753754\n",
      "[step: 750] loss: 0.04650891572237015\n",
      "[step: 751] loss: 0.04642760753631592\n",
      "[step: 752] loss: 0.0464225709438324\n",
      "[step: 753] loss: 0.04657965898513794\n",
      "[step: 754] loss: 0.0470711775124073\n",
      "[step: 755] loss: 0.048316359519958496\n",
      "[step: 756] loss: 0.05114619806408882\n",
      "[step: 757] loss: 0.05766492709517479\n",
      "[step: 758] loss: 0.0711359977722168\n",
      "[step: 759] loss: 0.09834569692611694\n",
      "[step: 760] loss: 0.13377316296100616\n",
      "[step: 761] loss: 0.16139715909957886\n",
      "[step: 762] loss: 0.12009213864803314\n",
      "[step: 763] loss: 0.06019025295972824\n",
      "[step: 764] loss: 0.05352315679192543\n",
      "[step: 765] loss: 0.08987577259540558\n",
      "[step: 766] loss: 0.09059165418148041\n",
      "[step: 767] loss: 0.05239785462617874\n",
      "[step: 768] loss: 0.058470867574214935\n",
      "[step: 769] loss: 0.07902562618255615\n",
      "[step: 770] loss: 0.05864237621426582\n",
      "[step: 771] loss: 0.0494517982006073\n",
      "[step: 772] loss: 0.06525840610265732\n",
      "[step: 773] loss: 0.05743865668773651\n",
      "[step: 774] loss: 0.047812044620513916\n",
      "[step: 775] loss: 0.05576346814632416\n",
      "[step: 776] loss: 0.05515942722558975\n",
      "[step: 777] loss: 0.04711011052131653\n",
      "[step: 778] loss: 0.04984438046813011\n",
      "[step: 779] loss: 0.05217716842889786\n",
      "[step: 780] loss: 0.04675045982003212\n",
      "[step: 781] loss: 0.04612784832715988\n",
      "[step: 782] loss: 0.04971034824848175\n",
      "[step: 783] loss: 0.0462590754032135\n",
      "[step: 784] loss: 0.04383711516857147\n",
      "[step: 785] loss: 0.047460488975048065\n",
      "[step: 786] loss: 0.045914243906736374\n",
      "[step: 787] loss: 0.042518049478530884\n",
      "[step: 788] loss: 0.04525955393910408\n",
      "[step: 789] loss: 0.045347534120082855\n",
      "[step: 790] loss: 0.04211840033531189\n",
      "[step: 791] loss: 0.043224822729825974\n",
      "[step: 792] loss: 0.044251926243305206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 793] loss: 0.04221367463469505\n",
      "[step: 794] loss: 0.041850894689559937\n",
      "[step: 795] loss: 0.042561694979667664\n",
      "[step: 796] loss: 0.04212377220392227\n",
      "[step: 797] loss: 0.04148608446121216\n",
      "[step: 798] loss: 0.04103603586554527\n",
      "[step: 799] loss: 0.0411820113658905\n",
      "[step: 800] loss: 0.04136502742767334\n",
      "[step: 801] loss: 0.0405580997467041\n",
      "[step: 802] loss: 0.03996749594807625\n",
      "[step: 803] loss: 0.04051238298416138\n",
      "[step: 804] loss: 0.04042293131351471\n",
      "[step: 805] loss: 0.039589907974004745\n",
      "[step: 806] loss: 0.03937147185206413\n",
      "[step: 807] loss: 0.039610493928194046\n",
      "[step: 808] loss: 0.03941396623849869\n",
      "[step: 809] loss: 0.03901136666536331\n",
      "[step: 810] loss: 0.03868287056684494\n",
      "[step: 811] loss: 0.03859235346317291\n",
      "[step: 812] loss: 0.03862025961279869\n",
      "[step: 813] loss: 0.03840318322181702\n",
      "[step: 814] loss: 0.03795282542705536\n",
      "[step: 815] loss: 0.03777953237295151\n",
      "[step: 816] loss: 0.03779929131269455\n",
      "[step: 817] loss: 0.03763725981116295\n",
      "[step: 818] loss: 0.03735728934407234\n",
      "[step: 819] loss: 0.037131693214178085\n",
      "[step: 820] loss: 0.03694215044379234\n",
      "[step: 821] loss: 0.03680805861949921\n",
      "[step: 822] loss: 0.03672012686729431\n",
      "[step: 823] loss: 0.03652800992131233\n",
      "[step: 824] loss: 0.03625946491956711\n",
      "[step: 825] loss: 0.03605325147509575\n",
      "[step: 826] loss: 0.03592207282781601\n",
      "[step: 827] loss: 0.03577803075313568\n",
      "[step: 828] loss: 0.03562700003385544\n",
      "[step: 829] loss: 0.03545455262064934\n",
      "[step: 830] loss: 0.0352410264313221\n",
      "[step: 831] loss: 0.03502354025840759\n",
      "[step: 832] loss: 0.034849345684051514\n",
      "[step: 833] loss: 0.03469138592481613\n",
      "[step: 834] loss: 0.03452755883336067\n",
      "[step: 835] loss: 0.03437238186597824\n",
      "[step: 836] loss: 0.03421778604388237\n",
      "[step: 837] loss: 0.03404301404953003\n",
      "[step: 838] loss: 0.03385768085718155\n",
      "[step: 839] loss: 0.03367944806814194\n",
      "[step: 840] loss: 0.033497899770736694\n",
      "[step: 841] loss: 0.03331154212355614\n",
      "[step: 842] loss: 0.03313160315155983\n",
      "[step: 843] loss: 0.032961614429950714\n",
      "[step: 844] loss: 0.032793059945106506\n",
      "[step: 845] loss: 0.032633550465106964\n",
      "[step: 846] loss: 0.03249947354197502\n",
      "[step: 847] loss: 0.03240906447172165\n",
      "[step: 848] loss: 0.03240625932812691\n",
      "[step: 849] loss: 0.03261032700538635\n",
      "[step: 850] loss: 0.03329385817050934\n",
      "[step: 851] loss: 0.03512360155582428\n",
      "[step: 852] loss: 0.03972287476062775\n",
      "[step: 853] loss: 0.05118098855018616\n",
      "[step: 854] loss: 0.07777542620897293\n",
      "[step: 855] loss: 0.1328762173652649\n",
      "[step: 856] loss: 0.2030056118965149\n",
      "[step: 857] loss: 0.21503612399101257\n",
      "[step: 858] loss: 0.09890323877334595\n",
      "[step: 859] loss: 0.034284282475709915\n",
      "[step: 860] loss: 0.10085874795913696\n",
      "[step: 861] loss: 0.10672318190336227\n",
      "[step: 862] loss: 0.042710136622190475\n",
      "[step: 863] loss: 0.0638631209731102\n",
      "[step: 864] loss: 0.08210255205631256\n",
      "[step: 865] loss: 0.04697348549962044\n",
      "[step: 866] loss: 0.05631987005472183\n",
      "[step: 867] loss: 0.06267453730106354\n",
      "[step: 868] loss: 0.04768792912364006\n",
      "[step: 869] loss: 0.04997880756855011\n",
      "[step: 870] loss: 0.05139533057808876\n",
      "[step: 871] loss: 0.04655952379107475\n",
      "[step: 872] loss: 0.041947141289711\n",
      "[step: 873] loss: 0.04813940078020096\n",
      "[step: 874] loss: 0.04095228388905525\n",
      "[step: 875] loss: 0.03808268532156944\n",
      "[step: 876] loss: 0.0457661934196949\n",
      "[step: 877] loss: 0.0333283394575119\n",
      "[step: 878] loss: 0.03978843614459038\n",
      "[step: 879] loss: 0.038326241075992584\n",
      "[step: 880] loss: 0.03172406181693077\n",
      "[step: 881] loss: 0.03930540382862091\n",
      "[step: 882] loss: 0.032139841467142105\n",
      "[step: 883] loss: 0.03375709056854248\n",
      "[step: 884] loss: 0.03508560732007027\n",
      "[step: 885] loss: 0.03079385682940483\n",
      "[step: 886] loss: 0.033671095967292786\n",
      "[step: 887] loss: 0.03138590231537819\n",
      "[step: 888] loss: 0.03180340677499771\n",
      "[step: 889] loss: 0.030935868620872498\n",
      "[step: 890] loss: 0.030805306509137154\n",
      "[step: 891] loss: 0.03118760511279106\n",
      "[step: 892] loss: 0.028999999165534973\n",
      "[step: 893] loss: 0.030916539952158928\n",
      "[step: 894] loss: 0.029534731060266495\n",
      "[step: 895] loss: 0.02857593074440956\n",
      "[step: 896] loss: 0.030021628364920616\n",
      "[step: 897] loss: 0.02842455357313156\n",
      "[step: 898] loss: 0.028486961498856544\n",
      "[step: 899] loss: 0.02855313941836357\n",
      "[step: 900] loss: 0.02807413786649704\n",
      "[step: 901] loss: 0.02793850377202034\n",
      "[step: 902] loss: 0.027395382523536682\n",
      "[step: 903] loss: 0.027795234695076942\n",
      "[step: 904] loss: 0.027139566838741302\n",
      "[step: 905] loss: 0.02674025669693947\n",
      "[step: 906] loss: 0.027177555486559868\n",
      "[step: 907] loss: 0.02647307515144348\n",
      "[step: 908] loss: 0.02631343901157379\n",
      "[step: 909] loss: 0.026362266391515732\n",
      "[step: 910] loss: 0.025965243577957153\n",
      "[step: 911] loss: 0.02590888738632202\n",
      "[step: 912] loss: 0.025586741045117378\n",
      "[step: 913] loss: 0.0254974365234375\n",
      "[step: 914] loss: 0.025434253737330437\n",
      "[step: 915] loss: 0.02499675191938877\n",
      "[step: 916] loss: 0.024979818612337112\n",
      "[step: 917] loss: 0.024889392778277397\n",
      "[step: 918] loss: 0.024551119655370712\n",
      "[step: 919] loss: 0.024454068392515182\n",
      "[step: 920] loss: 0.024292761459946632\n",
      "[step: 921] loss: 0.024130957201123238\n",
      "[step: 922] loss: 0.023968558758497238\n",
      "[step: 923] loss: 0.023725129663944244\n",
      "[step: 924] loss: 0.023638948798179626\n",
      "[step: 925] loss: 0.023494141176342964\n",
      "[step: 926] loss: 0.02324393019080162\n",
      "[step: 927] loss: 0.023118725046515465\n",
      "[step: 928] loss: 0.022968972101807594\n",
      "[step: 929] loss: 0.022792819887399673\n",
      "[step: 930] loss: 0.022646984085440636\n",
      "[step: 931] loss: 0.022448932752013206\n",
      "[step: 932] loss: 0.022294314578175545\n",
      "[step: 933] loss: 0.022171704098582268\n",
      "[step: 934] loss: 0.021985404193401337\n",
      "[step: 935] loss: 0.021815747022628784\n",
      "[step: 936] loss: 0.02166319265961647\n",
      "[step: 937] loss: 0.021496692672371864\n",
      "[step: 938] loss: 0.02135573700070381\n",
      "[step: 939] loss: 0.021196821704506874\n",
      "[step: 940] loss: 0.021017586812376976\n",
      "[step: 941] loss: 0.02086474373936653\n",
      "[step: 942] loss: 0.020710211247205734\n",
      "[step: 943] loss: 0.020550858229398727\n",
      "[step: 944] loss: 0.020404312759637833\n",
      "[step: 945] loss: 0.02024274691939354\n",
      "[step: 946] loss: 0.020076360553503036\n",
      "[step: 947] loss: 0.019922463223338127\n",
      "[step: 948] loss: 0.019764043390750885\n",
      "[step: 949] loss: 0.0196080282330513\n",
      "[step: 950] loss: 0.019459843635559082\n",
      "[step: 951] loss: 0.019303664565086365\n",
      "[step: 952] loss: 0.01914701610803604\n",
      "[step: 953] loss: 0.018994057551026344\n",
      "[step: 954] loss: 0.018835794180631638\n",
      "[step: 955] loss: 0.018678510561585426\n",
      "[step: 956] loss: 0.018525410443544388\n",
      "[step: 957] loss: 0.018369097262620926\n",
      "[step: 958] loss: 0.018214363604784012\n",
      "[step: 959] loss: 0.018062636256217957\n",
      "[step: 960] loss: 0.017908774316310883\n",
      "[step: 961] loss: 0.01775568351149559\n",
      "[step: 962] loss: 0.017605111002922058\n",
      "[step: 963] loss: 0.017453093081712723\n",
      "[step: 964] loss: 0.017302079126238823\n",
      "[step: 965] loss: 0.017153922468423843\n",
      "[step: 966] loss: 0.01700727641582489\n",
      "[step: 967] loss: 0.01686667650938034\n",
      "[step: 968] loss: 0.016741158440709114\n",
      "[step: 969] loss: 0.016649946570396423\n",
      "[step: 970] loss: 0.016648415476083755\n",
      "[step: 971] loss: 0.01689072698354721\n",
      "[step: 972] loss: 0.017810925841331482\n",
      "[step: 973] loss: 0.02068393863737583\n",
      "[step: 974] loss: 0.02932162396609783\n",
      "[step: 975] loss: 0.05470546707510948\n",
      "[step: 976] loss: 0.1252073496580124\n",
      "[step: 977] loss: 0.27850884199142456\n",
      "[step: 978] loss: 0.4370657801628113\n",
      "[step: 979] loss: 0.28020814061164856\n",
      "[step: 980] loss: 0.024231702089309692\n",
      "[step: 981] loss: 0.1581820547580719\n",
      "[step: 982] loss: 0.1795048862695694\n",
      "[step: 983] loss: 0.03129412233829498\n",
      "[step: 984] loss: 0.13767945766448975\n",
      "[step: 985] loss: 0.07107295840978622\n",
      "[step: 986] loss: 0.06528196483850479\n",
      "[step: 987] loss: 0.08445358276367188\n",
      "[step: 988] loss: 0.041657790541648865\n",
      "[step: 989] loss: 0.07556628435850143\n",
      "[step: 990] loss: 0.04037835821509361\n",
      "[step: 991] loss: 0.06310120224952698\n",
      "[step: 992] loss: 0.03802679479122162\n",
      "[step: 993] loss: 0.055897440761327744\n",
      "[step: 994] loss: 0.031467996537685394\n",
      "[step: 995] loss: 0.05289750546216965\n",
      "[step: 996] loss: 0.02550555020570755\n",
      "[step: 997] loss: 0.048932746052742004\n",
      "[step: 998] loss: 0.02267395332455635\n",
      "[step: 999] loss: 0.04384246841073036\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "testY is:  [24634.377499999999, 34902.413999999997, 17551.337500000001, 33670.824999999997, 22936.107999999997, 31400.029999999999]\n",
      "\n",
      "\n",
      "LSTM testforecast : [30229.880859375, 18688.056640625, 20792.294921875, 26841.2109375, 33232.8671875, 15683.8740234375] \n",
      "@@@@@LSTM rmse:  10834.9994933\n",
      "Bayseian testforecast : [32925.312791257173, 38196.754285232695, 15142.313646137402, 24837.956186690313, 18320.030051632868, 35148.519261413574] \n",
      "@@@@@Bayseian rmse:  5755.7884008\n",
      "\n",
      "\n",
      "Bayseian WON!!!!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 32.84620666503906\n",
      "[step: 1] loss: 30.399192810058594\n",
      "[step: 2] loss: 28.07622528076172\n",
      "[step: 3] loss: 25.85139274597168\n",
      "[step: 4] loss: 23.69237518310547\n",
      "[step: 5] loss: 21.563556671142578\n",
      "[step: 6] loss: 19.430923461914062\n",
      "[step: 7] loss: 17.264938354492188\n",
      "[step: 8] loss: 15.045271873474121\n",
      "[step: 9] loss: 12.767768859863281\n",
      "[step: 10] loss: 10.454533576965332\n",
      "[step: 11] loss: 8.17024040222168\n",
      "[step: 12] loss: 6.049325942993164\n",
      "[step: 13] loss: 4.3365654945373535\n",
      "[step: 14] loss: 3.4211089611053467\n",
      "[step: 15] loss: 3.7265615463256836\n",
      "[step: 16] loss: 5.061164855957031\n",
      "[step: 17] loss: 6.139828681945801\n",
      "[step: 18] loss: 6.134076118469238\n",
      "[step: 19] loss: 5.327476501464844\n",
      "[step: 20] loss: 4.337134838104248\n",
      "[step: 21] loss: 3.5942859649658203\n",
      "[step: 22] loss: 3.2418370246887207\n",
      "[step: 23] loss: 3.223176956176758\n",
      "[step: 24] loss: 3.4044289588928223\n",
      "[step: 25] loss: 3.6563451290130615\n",
      "[step: 26] loss: 3.8869247436523438\n",
      "[step: 27] loss: 4.043954372406006\n",
      "[step: 28] loss: 4.1064229011535645\n",
      "[step: 29] loss: 4.074883460998535\n",
      "[step: 30] loss: 3.9640703201293945\n",
      "[step: 31] loss: 3.797861099243164\n",
      "[step: 32] loss: 3.6056313514709473\n",
      "[step: 33] loss: 3.4189186096191406\n",
      "[step: 34] loss: 3.2674551010131836\n",
      "[step: 35] loss: 3.1741151809692383\n",
      "[step: 36] loss: 3.1492626667022705\n",
      "[step: 37] loss: 3.1863231658935547\n",
      "[step: 38] loss: 3.2616472244262695\n",
      "[step: 39] loss: 3.3411173820495605\n",
      "[step: 40] loss: 3.3924551010131836\n",
      "[step: 41] loss: 3.397559881210327\n",
      "[step: 42] loss: 3.3579790592193604\n",
      "[step: 43] loss: 3.29095721244812\n",
      "[step: 44] loss: 3.219576835632324\n",
      "[step: 45] loss: 3.1630377769470215\n",
      "[step: 46] loss: 3.1312613487243652\n",
      "[step: 47] loss: 3.1244466304779053\n",
      "[step: 48] loss: 3.1359028816223145\n",
      "[step: 49] loss: 3.15592360496521\n",
      "[step: 50] loss: 3.1751508712768555\n",
      "[step: 51] loss: 3.186753749847412\n",
      "[step: 52] loss: 3.187394618988037\n",
      "[step: 53] loss: 3.1772069931030273\n",
      "[step: 54] loss: 3.159097909927368\n",
      "[step: 55] loss: 3.1376168727874756\n",
      "[step: 56] loss: 3.117617130279541\n",
      "[step: 57] loss: 3.102969169616699\n",
      "[step: 58] loss: 3.095580577850342\n",
      "[step: 59] loss: 3.094994068145752\n",
      "[step: 60] loss: 3.0987329483032227\n",
      "[step: 61] loss: 3.1032967567443848\n",
      "[step: 62] loss: 3.1054749488830566\n",
      "[step: 63] loss: 3.1034350395202637\n",
      "[step: 64] loss: 3.0971908569335938\n",
      "[step: 65] loss: 3.0882842540740967\n",
      "[step: 66] loss: 3.0789566040039062\n",
      "[step: 67] loss: 3.0712122917175293\n",
      "[step: 68] loss: 3.066159248352051\n",
      "[step: 69] loss: 3.063816785812378\n",
      "[step: 70] loss: 3.0633254051208496\n",
      "[step: 71] loss: 3.063408374786377\n",
      "[step: 72] loss: 3.0628528594970703\n",
      "[step: 73] loss: 3.060879945755005\n",
      "[step: 74] loss: 3.0573039054870605\n",
      "[step: 75] loss: 3.052494525909424\n",
      "[step: 76] loss: 3.0471744537353516\n",
      "[step: 77] loss: 3.0421366691589355\n",
      "[step: 78] loss: 3.0379748344421387\n",
      "[step: 79] loss: 3.0349135398864746\n",
      "[step: 80] loss: 3.0327720642089844\n",
      "[step: 81] loss: 3.031081438064575\n",
      "[step: 82] loss: 3.029287815093994\n",
      "[step: 83] loss: 3.026965618133545\n",
      "[step: 84] loss: 3.023953914642334\n",
      "[step: 85] loss: 3.0203804969787598\n",
      "[step: 86] loss: 3.0165557861328125\n",
      "[step: 87] loss: 3.0128321647644043\n",
      "[step: 88] loss: 3.0094621181488037\n",
      "[step: 89] loss: 3.006518840789795\n",
      "[step: 90] loss: 3.00390625\n",
      "[step: 91] loss: 3.0014233589172363\n",
      "[step: 92] loss: 2.998856782913208\n",
      "[step: 93] loss: 2.9960618019104004\n",
      "[step: 94] loss: 2.993001699447632\n",
      "[step: 95] loss: 2.9897408485412598\n",
      "[step: 96] loss: 2.9864072799682617\n",
      "[step: 97] loss: 2.9831299781799316\n",
      "[step: 98] loss: 2.9799907207489014\n",
      "[step: 99] loss: 2.9769985675811768\n",
      "[step: 100] loss: 2.974097967147827\n",
      "[step: 101] loss: 2.9712016582489014\n",
      "[step: 102] loss: 2.968229293823242\n",
      "[step: 103] loss: 2.9651389122009277\n",
      "[step: 104] loss: 2.961940050125122\n",
      "[step: 105] loss: 2.958676815032959\n",
      "[step: 106] loss: 2.955408811569214\n",
      "[step: 107] loss: 2.9521777629852295\n",
      "[step: 108] loss: 2.9489970207214355\n",
      "[step: 109] loss: 2.9458487033843994\n",
      "[step: 110] loss: 2.942695140838623\n",
      "[step: 111] loss: 2.939499616622925\n",
      "[step: 112] loss: 2.93624210357666\n",
      "[step: 113] loss: 2.9329190254211426\n",
      "[step: 114] loss: 2.929548740386963\n",
      "[step: 115] loss: 2.9261538982391357\n",
      "[step: 116] loss: 2.92275333404541\n",
      "[step: 117] loss: 2.9193553924560547\n",
      "[step: 118] loss: 2.915952205657959\n",
      "[step: 119] loss: 2.9125289916992188\n",
      "[step: 120] loss: 2.909069538116455\n",
      "[step: 121] loss: 2.905564546585083\n",
      "[step: 122] loss: 2.9020140171051025\n",
      "[step: 123] loss: 2.898425817489624\n",
      "[step: 124] loss: 2.894808053970337\n",
      "[step: 125] loss: 2.8911681175231934\n",
      "[step: 126] loss: 2.887504816055298\n",
      "[step: 127] loss: 2.883814811706543\n",
      "[step: 128] loss: 2.880089044570923\n",
      "[step: 129] loss: 2.87632155418396\n",
      "[step: 130] loss: 2.8725109100341797\n",
      "[step: 131] loss: 2.8686580657958984\n",
      "[step: 132] loss: 2.8647632598876953\n",
      "[step: 133] loss: 2.860833168029785\n",
      "[step: 134] loss: 2.856865406036377\n",
      "[step: 135] loss: 2.852858781814575\n",
      "[step: 136] loss: 2.848809242248535\n",
      "[step: 137] loss: 2.844712257385254\n",
      "[step: 138] loss: 2.8405673503875732\n",
      "[step: 139] loss: 2.8363747596740723\n",
      "[step: 140] loss: 2.832134246826172\n",
      "[step: 141] loss: 2.827847719192505\n",
      "[step: 142] loss: 2.8235135078430176\n",
      "[step: 143] loss: 2.8191299438476562\n",
      "[step: 144] loss: 2.8146939277648926\n",
      "[step: 145] loss: 2.8102049827575684\n",
      "[step: 146] loss: 2.8056607246398926\n",
      "[step: 147] loss: 2.8010611534118652\n",
      "[step: 148] loss: 2.796405792236328\n",
      "[step: 149] loss: 2.7916946411132812\n",
      "[step: 150] loss: 2.786923408508301\n",
      "[step: 151] loss: 2.7820918560028076\n",
      "[step: 152] loss: 2.77719783782959\n",
      "[step: 153] loss: 2.7722384929656982\n",
      "[step: 154] loss: 2.767214775085449\n",
      "[step: 155] loss: 2.762124538421631\n",
      "[step: 156] loss: 2.7569687366485596\n",
      "[step: 157] loss: 2.7517428398132324\n",
      "[step: 158] loss: 2.7464473247528076\n",
      "[step: 159] loss: 2.7410781383514404\n",
      "[step: 160] loss: 2.7356350421905518\n",
      "[step: 161] loss: 2.7301151752471924\n",
      "[step: 162] loss: 2.724518299102783\n",
      "[step: 163] loss: 2.7188405990600586\n",
      "[step: 164] loss: 2.713080883026123\n",
      "[step: 165] loss: 2.7072362899780273\n",
      "[step: 166] loss: 2.701303005218506\n",
      "[step: 167] loss: 2.695279598236084\n",
      "[step: 168] loss: 2.68916392326355\n",
      "[step: 169] loss: 2.6829535961151123\n",
      "[step: 170] loss: 2.676645278930664\n",
      "[step: 171] loss: 2.670234203338623\n",
      "[step: 172] loss: 2.6637187004089355\n",
      "[step: 173] loss: 2.657094717025757\n",
      "[step: 174] loss: 2.6503591537475586\n",
      "[step: 175] loss: 2.643507957458496\n",
      "[step: 176] loss: 2.636536121368408\n",
      "[step: 177] loss: 2.6294398307800293\n",
      "[step: 178] loss: 2.62221360206604\n",
      "[step: 179] loss: 2.6148552894592285\n",
      "[step: 180] loss: 2.6073567867279053\n",
      "[step: 181] loss: 2.599714756011963\n",
      "[step: 182] loss: 2.591923236846924\n",
      "[step: 183] loss: 2.5839760303497314\n",
      "[step: 184] loss: 2.575866937637329\n",
      "[step: 185] loss: 2.5675911903381348\n",
      "[step: 186] loss: 2.559140205383301\n",
      "[step: 187] loss: 2.550508499145508\n",
      "[step: 188] loss: 2.5416884422302246\n",
      "[step: 189] loss: 2.532672882080078\n",
      "[step: 190] loss: 2.523456573486328\n",
      "[step: 191] loss: 2.5140295028686523\n",
      "[step: 192] loss: 2.5043859481811523\n",
      "[step: 193] loss: 2.4945170879364014\n",
      "[step: 194] loss: 2.4844160079956055\n",
      "[step: 195] loss: 2.4740753173828125\n",
      "[step: 196] loss: 2.463489055633545\n",
      "[step: 197] loss: 2.4526467323303223\n",
      "[step: 198] loss: 2.4415462017059326\n",
      "[step: 199] loss: 2.4301791191101074\n",
      "[step: 200] loss: 2.418539524078369\n",
      "[step: 201] loss: 2.406625270843506\n",
      "[step: 202] loss: 2.394430637359619\n",
      "[step: 203] loss: 2.3819544315338135\n",
      "[step: 204] loss: 2.3691959381103516\n",
      "[step: 205] loss: 2.356154203414917\n",
      "[step: 206] loss: 2.3428337574005127\n",
      "[step: 207] loss: 2.329237937927246\n",
      "[step: 208] loss: 2.315373182296753\n",
      "[step: 209] loss: 2.3012495040893555\n",
      "[step: 210] loss: 2.286879539489746\n",
      "[step: 211] loss: 2.272275924682617\n",
      "[step: 212] loss: 2.257457733154297\n",
      "[step: 213] loss: 2.242443561553955\n",
      "[step: 214] loss: 2.227257251739502\n",
      "[step: 215] loss: 2.2119226455688477\n",
      "[step: 216] loss: 2.1964683532714844\n",
      "[step: 217] loss: 2.180922031402588\n",
      "[step: 218] loss: 2.1653125286102295\n",
      "[step: 219] loss: 2.1496665477752686\n",
      "[step: 220] loss: 2.134010076522827\n",
      "[step: 221] loss: 2.1183664798736572\n",
      "[step: 222] loss: 2.102750301361084\n",
      "[step: 223] loss: 2.0871729850769043\n",
      "[step: 224] loss: 2.07163405418396\n",
      "[step: 225] loss: 2.056123971939087\n",
      "[step: 226] loss: 2.0406227111816406\n",
      "[step: 227] loss: 2.025099515914917\n",
      "[step: 228] loss: 2.009514331817627\n",
      "[step: 229] loss: 1.9938209056854248\n",
      "[step: 230] loss: 1.977970004081726\n",
      "[step: 231] loss: 1.9619178771972656\n",
      "[step: 232] loss: 1.945631504058838\n",
      "[step: 233] loss: 1.9290990829467773\n",
      "[step: 234] loss: 1.912335753440857\n",
      "[step: 235] loss: 1.89539635181427\n",
      "[step: 236] loss: 1.8783845901489258\n",
      "[step: 237] loss: 1.8614563941955566\n",
      "[step: 238] loss: 1.8448207378387451\n",
      "[step: 239] loss: 1.8287391662597656\n",
      "[step: 240] loss: 1.8135102987289429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 241] loss: 1.7995173931121826\n",
      "[step: 242] loss: 1.7891708612442017\n",
      "[step: 243] loss: 1.8182246685028076\n",
      "[step: 244] loss: 1.9055407047271729\n",
      "[step: 245] loss: 1.7912843227386475\n",
      "[step: 246] loss: 1.8108000755310059\n",
      "[step: 247] loss: 1.8162797689437866\n",
      "[step: 248] loss: 1.7536201477050781\n",
      "[step: 249] loss: 1.8134446144104004\n",
      "[step: 250] loss: 1.7341547012329102\n",
      "[step: 251] loss: 1.7936004400253296\n",
      "[step: 252] loss: 1.7316577434539795\n",
      "[step: 253] loss: 1.7658039331436157\n",
      "[step: 254] loss: 1.7334001064300537\n",
      "[step: 255] loss: 1.7415599822998047\n",
      "[step: 256] loss: 1.7342950105667114\n",
      "[step: 257] loss: 1.7237935066223145\n",
      "[step: 258] loss: 1.7327704429626465\n",
      "[step: 259] loss: 1.7108652591705322\n",
      "[step: 260] loss: 1.7285047769546509\n",
      "[step: 261] loss: 1.7036700248718262\n",
      "[step: 262] loss: 1.721993327140808\n",
      "[step: 263] loss: 1.6994993686676025\n",
      "[step: 264] loss: 1.714137315750122\n",
      "[step: 265] loss: 1.6972885131835938\n",
      "[step: 266] loss: 1.7055836915969849\n",
      "[step: 267] loss: 1.6961867809295654\n",
      "[step: 268] loss: 1.697465181350708\n",
      "[step: 269] loss: 1.694616436958313\n",
      "[step: 270] loss: 1.690371036529541\n",
      "[step: 271] loss: 1.6924594640731812\n",
      "[step: 272] loss: 1.6847158670425415\n",
      "[step: 273] loss: 1.689091682434082\n",
      "[step: 274] loss: 1.6807466745376587\n",
      "[step: 275] loss: 1.6846070289611816\n",
      "[step: 276] loss: 1.677945852279663\n",
      "[step: 277] loss: 1.6795600652694702\n",
      "[step: 278] loss: 1.6758575439453125\n",
      "[step: 279] loss: 1.6744017601013184\n",
      "[step: 280] loss: 1.6736421585083008\n",
      "[step: 281] loss: 1.6699198484420776\n",
      "[step: 282] loss: 1.6706634759902954\n",
      "[step: 283] loss: 1.666337251663208\n",
      "[step: 284] loss: 1.6669061183929443\n",
      "[step: 285] loss: 1.663465976715088\n",
      "[step: 286] loss: 1.66258704662323\n",
      "[step: 287] loss: 1.6608057022094727\n",
      "[step: 288] loss: 1.6583516597747803\n",
      "[step: 289] loss: 1.657702088356018\n",
      "[step: 290] loss: 1.6546504497528076\n",
      "[step: 291] loss: 1.6539788246154785\n",
      "[step: 292] loss: 1.651404857635498\n",
      "[step: 293] loss: 1.6498656272888184\n",
      "[step: 294] loss: 1.64823579788208\n",
      "[step: 295] loss: 1.6458356380462646\n",
      "[step: 296] loss: 1.6446778774261475\n",
      "[step: 297] loss: 1.6421879529953003\n",
      "[step: 298] loss: 1.6406630277633667\n",
      "[step: 299] loss: 1.6387168169021606\n",
      "[step: 300] loss: 1.6365478038787842\n",
      "[step: 301] loss: 1.6350005865097046\n",
      "[step: 302] loss: 1.6326533555984497\n",
      "[step: 303] loss: 1.6308903694152832\n",
      "[step: 304] loss: 1.6288785934448242\n",
      "[step: 305] loss: 1.626659870147705\n",
      "[step: 306] loss: 1.624866008758545\n",
      "[step: 307] loss: 1.622588038444519\n",
      "[step: 308] loss: 1.6205389499664307\n",
      "[step: 309] loss: 1.6185189485549927\n",
      "[step: 310] loss: 1.6161967515945435\n",
      "[step: 311] loss: 1.6141624450683594\n",
      "[step: 312] loss: 1.611952543258667\n",
      "[step: 313] loss: 1.6096440553665161\n",
      "[step: 314] loss: 1.6075255870819092\n",
      "[step: 315] loss: 1.6051990985870361\n",
      "[step: 316] loss: 1.602872610092163\n",
      "[step: 317] loss: 1.600644588470459\n",
      "[step: 318] loss: 1.5982413291931152\n",
      "[step: 319] loss: 1.5958585739135742\n",
      "[step: 320] loss: 1.5935277938842773\n",
      "[step: 321] loss: 1.5910576581954956\n",
      "[step: 322] loss: 1.588594913482666\n",
      "[step: 323] loss: 1.5861670970916748\n",
      "[step: 324] loss: 1.583632230758667\n",
      "[step: 325] loss: 1.581078290939331\n",
      "[step: 326] loss: 1.578553318977356\n",
      "[step: 327] loss: 1.5759532451629639\n",
      "[step: 328] loss: 1.5733039379119873\n",
      "[step: 329] loss: 1.5706709623336792\n",
      "[step: 330] loss: 1.5679984092712402\n",
      "[step: 331] loss: 1.565260887145996\n",
      "[step: 332] loss: 1.5625118017196655\n",
      "[step: 333] loss: 1.5597481727600098\n",
      "[step: 334] loss: 1.5569301843643188\n",
      "[step: 335] loss: 1.5540664196014404\n",
      "[step: 336] loss: 1.551188349723816\n",
      "[step: 337] loss: 1.5482815504074097\n",
      "[step: 338] loss: 1.5453218221664429\n",
      "[step: 339] loss: 1.5423201322555542\n",
      "[step: 340] loss: 1.5392930507659912\n",
      "[step: 341] loss: 1.536233901977539\n",
      "[step: 342] loss: 1.533128261566162\n",
      "[step: 343] loss: 1.5299742221832275\n",
      "[step: 344] loss: 1.5267832279205322\n",
      "[step: 345] loss: 1.5235575437545776\n",
      "[step: 346] loss: 1.5202893018722534\n",
      "[step: 347] loss: 1.5169739723205566\n",
      "[step: 348] loss: 1.5136096477508545\n",
      "[step: 349] loss: 1.5102002620697021\n",
      "[step: 350] loss: 1.506748080253601\n",
      "[step: 351] loss: 1.5032503604888916\n",
      "[step: 352] loss: 1.4997048377990723\n",
      "[step: 353] loss: 1.4961081743240356\n",
      "[step: 354] loss: 1.4924590587615967\n",
      "[step: 355] loss: 1.4887580871582031\n",
      "[step: 356] loss: 1.48500394821167\n",
      "[step: 357] loss: 1.4811965227127075\n",
      "[step: 358] loss: 1.4773337841033936\n",
      "[step: 359] loss: 1.4734159708023071\n",
      "[step: 360] loss: 1.4694414138793945\n",
      "[step: 361] loss: 1.4654083251953125\n",
      "[step: 362] loss: 1.461316704750061\n",
      "[step: 363] loss: 1.4571642875671387\n",
      "[step: 364] loss: 1.4529502391815186\n",
      "[step: 365] loss: 1.4486759901046753\n",
      "[step: 366] loss: 1.4443457126617432\n",
      "[step: 367] loss: 1.4399759769439697\n",
      "[step: 368] loss: 1.4356257915496826\n",
      "[step: 369] loss: 1.431495189666748\n",
      "[step: 370] loss: 1.4283092021942139\n",
      "[step: 371] loss: 1.4286057949066162\n",
      "[step: 372] loss: 1.4415820837020874\n",
      "[step: 373] loss: 1.4882220029830933\n",
      "[step: 374] loss: 1.5854461193084717\n",
      "[step: 375] loss: 1.5729120969772339\n",
      "[step: 376] loss: 1.4315481185913086\n",
      "[step: 377] loss: 1.4244771003723145\n",
      "[step: 378] loss: 1.4996854066848755\n",
      "[step: 379] loss: 1.4166215658187866\n",
      "[step: 380] loss: 1.4093984365463257\n",
      "[step: 381] loss: 1.452399492263794\n",
      "[step: 382] loss: 1.3829892873764038\n",
      "[step: 383] loss: 1.4120959043502808\n",
      "[step: 384] loss: 1.4032868146896362\n",
      "[step: 385] loss: 1.3701727390289307\n",
      "[step: 386] loss: 1.4009060859680176\n",
      "[step: 387] loss: 1.3627800941467285\n",
      "[step: 388] loss: 1.3716915845870972\n",
      "[step: 389] loss: 1.3705976009368896\n",
      "[step: 390] loss: 1.3445546627044678\n",
      "[step: 391] loss: 1.3625705242156982\n",
      "[step: 392] loss: 1.3404130935668945\n",
      "[step: 393] loss: 1.335336446762085\n",
      "[step: 394] loss: 1.3419404029846191\n",
      "[step: 395] loss: 1.319374442100525\n",
      "[step: 396] loss: 1.3229758739471436\n",
      "[step: 397] loss: 1.3203303813934326\n",
      "[step: 398] loss: 1.3031492233276367\n",
      "[step: 399] loss: 1.3068039417266846\n",
      "[step: 400] loss: 1.3014694452285767\n",
      "[step: 401] loss: 1.287593960762024\n",
      "[step: 402] loss: 1.2886841297149658\n",
      "[step: 403] loss: 1.2842469215393066\n",
      "[step: 404] loss: 1.2717959880828857\n",
      "[step: 405] loss: 1.2694650888442993\n",
      "[step: 406] loss: 1.2668741941452026\n",
      "[step: 407] loss: 1.2561184167861938\n",
      "[step: 408] loss: 1.2499102354049683\n",
      "[step: 409] loss: 1.2479844093322754\n",
      "[step: 410] loss: 1.2404147386550903\n",
      "[step: 411] loss: 1.2313997745513916\n",
      "[step: 412] loss: 1.2273269891738892\n",
      "[step: 413] loss: 1.2230387926101685\n",
      "[step: 414] loss: 1.2147252559661865\n",
      "[step: 415] loss: 1.206956148147583\n",
      "[step: 416] loss: 1.202265977859497\n",
      "[step: 417] loss: 1.197235107421875\n",
      "[step: 418] loss: 1.1896681785583496\n",
      "[step: 419] loss: 1.181638479232788\n",
      "[step: 420] loss: 1.1752270460128784\n",
      "[step: 421] loss: 1.1699103116989136\n",
      "[step: 422] loss: 1.1641566753387451\n",
      "[step: 423] loss: 1.157260775566101\n",
      "[step: 424] loss: 1.1496803760528564\n",
      "[step: 425] loss: 1.1419655084609985\n",
      "[step: 426] loss: 1.1344754695892334\n",
      "[step: 427] loss: 1.1271793842315674\n",
      "[step: 428] loss: 1.1199848651885986\n",
      "[step: 429] loss: 1.1128160953521729\n",
      "[step: 430] loss: 1.1056972742080688\n",
      "[step: 431] loss: 1.0989269018173218\n",
      "[step: 432] loss: 1.0937235355377197\n",
      "[step: 433] loss: 1.0960198640823364\n",
      "[step: 434] loss: 1.1309850215911865\n",
      "[step: 435] loss: 1.2741022109985352\n",
      "[step: 436] loss: 1.4527504444122314\n",
      "[step: 437] loss: 1.3467307090759277\n",
      "[step: 438] loss: 1.0649446249008179\n",
      "[step: 439] loss: 1.2779390811920166\n",
      "[step: 440] loss: 1.2398755550384521\n",
      "[step: 441] loss: 1.0586822032928467\n",
      "[step: 442] loss: 1.257045030593872\n",
      "[step: 443] loss: 1.0675933361053467\n",
      "[step: 444] loss: 1.138150930404663\n",
      "[step: 445] loss: 1.113525152206421\n",
      "[step: 446] loss: 1.0524617433547974\n",
      "[step: 447] loss: 1.1173226833343506\n",
      "[step: 448] loss: 1.0235400199890137\n",
      "[step: 449] loss: 1.0967743396759033\n",
      "[step: 450] loss: 1.022141456604004\n",
      "[step: 451] loss: 1.0532910823822021\n",
      "[step: 452] loss: 1.0260975360870361\n",
      "[step: 453] loss: 1.0111956596374512\n",
      "[step: 454] loss: 1.028010368347168\n",
      "[step: 455] loss: 0.9852158427238464\n",
      "[step: 456] loss: 1.0168695449829102\n",
      "[step: 457] loss: 0.9763928055763245\n",
      "[step: 458] loss: 0.9870316982269287\n",
      "[step: 459] loss: 0.9780086278915405\n",
      "[step: 460] loss: 0.9577994346618652\n",
      "[step: 461] loss: 0.973537027835846\n",
      "[step: 462] loss: 0.9436079859733582\n",
      "[step: 463] loss: 0.9543442726135254\n",
      "[step: 464] loss: 0.9446138143539429\n",
      "[step: 465] loss: 0.9299768209457397\n",
      "[step: 466] loss: 0.938866913318634\n",
      "[step: 467] loss: 0.9222680330276489\n",
      "[step: 468] loss: 0.9155627489089966\n",
      "[step: 469] loss: 0.9206185340881348\n",
      "[step: 470] loss: 0.9023511409759521\n",
      "[step: 471] loss: 0.9012709856033325\n",
      "[step: 472] loss: 0.8996649980545044\n",
      "[step: 473] loss: 0.888276219367981\n",
      "[step: 474] loss: 0.8814729452133179\n",
      "[step: 475] loss: 0.8834930062294006\n",
      "[step: 476] loss: 0.8726398944854736\n",
      "[step: 477] loss: 0.8650226593017578\n",
      "[step: 478] loss: 0.8640334606170654\n",
      "[step: 479] loss: 0.8605340123176575\n",
      "[step: 480] loss: 0.8498864769935608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 481] loss: 0.8446171283721924\n",
      "[step: 482] loss: 0.8430737257003784\n",
      "[step: 483] loss: 0.8392200469970703\n",
      "[step: 484] loss: 0.8318261504173279\n",
      "[step: 485] loss: 0.8240559697151184\n",
      "[step: 486] loss: 0.81932532787323\n",
      "[step: 487] loss: 0.8162580728530884\n",
      "[step: 488] loss: 0.8142091631889343\n",
      "[step: 489] loss: 0.8117830753326416\n",
      "[step: 490] loss: 0.8099356293678284\n",
      "[step: 491] loss: 0.8095616698265076\n",
      "[step: 492] loss: 0.8128510117530823\n",
      "[step: 493] loss: 0.8274366855621338\n",
      "[step: 494] loss: 0.8579116463661194\n",
      "[step: 495] loss: 0.9308547973632812\n",
      "[step: 496] loss: 0.9870738387107849\n",
      "[step: 497] loss: 1.0199291706085205\n",
      "[step: 498] loss: 0.8489252328872681\n",
      "[step: 499] loss: 0.7681407928466797\n",
      "[step: 500] loss: 0.8473777174949646\n",
      "[step: 501] loss: 0.8903800845146179\n",
      "[step: 502] loss: 0.827391505241394\n",
      "[step: 503] loss: 0.756142258644104\n",
      "[step: 504] loss: 0.8108535408973694\n",
      "[step: 505] loss: 0.8601551055908203\n",
      "[step: 506] loss: 0.780002236366272\n",
      "[step: 507] loss: 0.7503805160522461\n",
      "[step: 508] loss: 0.8051156997680664\n",
      "[step: 509] loss: 0.7956737875938416\n",
      "[step: 510] loss: 0.7447934746742249\n",
      "[step: 511] loss: 0.7469959259033203\n",
      "[step: 512] loss: 0.7785782217979431\n",
      "[step: 513] loss: 0.7676883339881897\n",
      "[step: 514] loss: 0.7293960452079773\n",
      "[step: 515] loss: 0.7405319809913635\n",
      "[step: 516] loss: 0.7636090517044067\n",
      "[step: 517] loss: 0.7423557043075562\n",
      "[step: 518] loss: 0.7175664901733398\n",
      "[step: 519] loss: 0.7284034490585327\n",
      "[step: 520] loss: 0.7403690814971924\n",
      "[step: 521] loss: 0.731199324131012\n",
      "[step: 522] loss: 0.7093452215194702\n",
      "[step: 523] loss: 0.7098879814147949\n",
      "[step: 524] loss: 0.7212182283401489\n",
      "[step: 525] loss: 0.7218321561813354\n",
      "[step: 526] loss: 0.70744788646698\n",
      "[step: 527] loss: 0.696526050567627\n",
      "[step: 528] loss: 0.6965808868408203\n",
      "[step: 529] loss: 0.7043750882148743\n",
      "[step: 530] loss: 0.7048897743225098\n",
      "[step: 531] loss: 0.699882984161377\n",
      "[step: 532] loss: 0.6898691654205322\n",
      "[step: 533] loss: 0.683000922203064\n",
      "[step: 534] loss: 0.6799638867378235\n",
      "[step: 535] loss: 0.6812835335731506\n",
      "[step: 536] loss: 0.684307336807251\n",
      "[step: 537] loss: 0.688113272190094\n",
      "[step: 538] loss: 0.6934529542922974\n",
      "[step: 539] loss: 0.699931263923645\n",
      "[step: 540] loss: 0.7137567400932312\n",
      "[step: 541] loss: 0.7290505766868591\n",
      "[step: 542] loss: 0.7587097883224487\n",
      "[step: 543] loss: 0.7780360579490662\n",
      "[step: 544] loss: 0.8028628826141357\n",
      "[step: 545] loss: 0.7665313482284546\n",
      "[step: 546] loss: 0.7168807983398438\n",
      "[step: 547] loss: 0.6668090224266052\n",
      "[step: 548] loss: 0.6580281257629395\n",
      "[step: 549] loss: 0.6845848560333252\n",
      "[step: 550] loss: 0.7132555842399597\n",
      "[step: 551] loss: 0.7261031270027161\n",
      "[step: 552] loss: 0.6983529329299927\n",
      "[step: 553] loss: 0.6646378636360168\n",
      "[step: 554] loss: 0.6473981142044067\n",
      "[step: 555] loss: 0.6561464071273804\n",
      "[step: 556] loss: 0.6764018535614014\n",
      "[step: 557] loss: 0.6830043792724609\n",
      "[step: 558] loss: 0.6746660470962524\n",
      "[step: 559] loss: 0.6540810465812683\n",
      "[step: 560] loss: 0.6403232216835022\n",
      "[step: 561] loss: 0.6399354934692383\n",
      "[step: 562] loss: 0.6487953662872314\n",
      "[step: 563] loss: 0.6579928398132324\n",
      "[step: 564] loss: 0.6583802700042725\n",
      "[step: 565] loss: 0.652344822883606\n",
      "[step: 566] loss: 0.6416490077972412\n",
      "[step: 567] loss: 0.6330578327178955\n",
      "[step: 568] loss: 0.6286943554878235\n",
      "[step: 569] loss: 0.628734290599823\n",
      "[step: 570] loss: 0.6317658424377441\n",
      "[step: 571] loss: 0.6360480785369873\n",
      "[step: 572] loss: 0.6411622762680054\n",
      "[step: 573] loss: 0.6453083753585815\n",
      "[step: 574] loss: 0.6508382558822632\n",
      "[step: 575] loss: 0.6546680927276611\n",
      "[step: 576] loss: 0.6612439155578613\n",
      "[step: 577] loss: 0.6649297475814819\n",
      "[step: 578] loss: 0.6724270582199097\n",
      "[step: 579] loss: 0.673518180847168\n",
      "[step: 580] loss: 0.675953209400177\n",
      "[step: 581] loss: 0.6670646071434021\n",
      "[step: 582] loss: 0.656742513179779\n",
      "[step: 583] loss: 0.6391298770904541\n",
      "[step: 584] loss: 0.6240357160568237\n",
      "[step: 585] loss: 0.613073468208313\n",
      "[step: 586] loss: 0.6086353063583374\n",
      "[step: 587] loss: 0.6098114848136902\n",
      "[step: 588] loss: 0.6147047281265259\n",
      "[step: 589] loss: 0.6220254302024841\n",
      "[step: 590] loss: 0.6296645402908325\n",
      "[step: 591] loss: 0.6388781070709229\n",
      "[step: 592] loss: 0.645108699798584\n",
      "[step: 593] loss: 0.6522460579872131\n",
      "[step: 594] loss: 0.6516914367675781\n",
      "[step: 595] loss: 0.6495368480682373\n",
      "[step: 596] loss: 0.6380256414413452\n",
      "[step: 597] loss: 0.6255775690078735\n",
      "[step: 598] loss: 0.6114012002944946\n",
      "[step: 599] loss: 0.6012011170387268\n",
      "[step: 600] loss: 0.5959380269050598\n",
      "[step: 601] loss: 0.5955606698989868\n",
      "[step: 602] loss: 0.5987507104873657\n",
      "[step: 603] loss: 0.6039414405822754\n",
      "[step: 604] loss: 0.6107147932052612\n",
      "[step: 605] loss: 0.6173849105834961\n",
      "[step: 606] loss: 0.6256967782974243\n",
      "[step: 607] loss: 0.6315040588378906\n",
      "[step: 608] loss: 0.6382506489753723\n",
      "[step: 609] loss: 0.6379032135009766\n",
      "[step: 610] loss: 0.6356191635131836\n",
      "[step: 611] loss: 0.6243473291397095\n",
      "[step: 612] loss: 0.6118438243865967\n",
      "[step: 613] loss: 0.5979812145233154\n",
      "[step: 614] loss: 0.5881580710411072\n",
      "[step: 615] loss: 0.5833346843719482\n",
      "[step: 616] loss: 0.5833126306533813\n",
      "[step: 617] loss: 0.5867186784744263\n",
      "[step: 618] loss: 0.5919487476348877\n",
      "[step: 619] loss: 0.5984383821487427\n",
      "[step: 620] loss: 0.6043676137924194\n",
      "[step: 621] loss: 0.6109535694122314\n",
      "[step: 622] loss: 0.6144692897796631\n",
      "[step: 623] loss: 0.6175800561904907\n",
      "[step: 624] loss: 0.6149187088012695\n",
      "[step: 625] loss: 0.6106762290000916\n",
      "[step: 626] loss: 0.6014578938484192\n",
      "[step: 627] loss: 0.5922079682350159\n",
      "[step: 628] loss: 0.5829280614852905\n",
      "[step: 629] loss: 0.576221227645874\n",
      "[step: 630] loss: 0.5722664594650269\n",
      "[step: 631] loss: 0.5709085464477539\n",
      "[step: 632] loss: 0.5715140700340271\n",
      "[step: 633] loss: 0.5734900236129761\n",
      "[step: 634] loss: 0.5766463279724121\n",
      "[step: 635] loss: 0.580722451210022\n",
      "[step: 636] loss: 0.5865803956985474\n",
      "[step: 637] loss: 0.593497633934021\n",
      "[step: 638] loss: 0.6037819981575012\n",
      "[step: 639] loss: 0.6143597960472107\n",
      "[step: 640] loss: 0.6291947960853577\n",
      "[step: 641] loss: 0.6384493112564087\n",
      "[step: 642] loss: 0.6469553709030151\n",
      "[step: 643] loss: 0.637421727180481\n",
      "[step: 644] loss: 0.6199411153793335\n",
      "[step: 645] loss: 0.5911322832107544\n",
      "[step: 646] loss: 0.5686240196228027\n",
      "[step: 647] loss: 0.5598331689834595\n",
      "[step: 648] loss: 0.5655752420425415\n",
      "[step: 649] loss: 0.5786588191986084\n",
      "[step: 650] loss: 0.5883792638778687\n",
      "[step: 651] loss: 0.590211033821106\n",
      "[step: 652] loss: 0.5805801153182983\n",
      "[step: 653] loss: 0.5675177574157715\n",
      "[step: 654] loss: 0.5573265552520752\n",
      "[step: 655] loss: 0.5549948215484619\n",
      "[step: 656] loss: 0.5593751072883606\n",
      "[step: 657] loss: 0.5655794143676758\n",
      "[step: 658] loss: 0.5693761706352234\n",
      "[step: 659] loss: 0.5677205324172974\n",
      "[step: 660] loss: 0.562567949295044\n",
      "[step: 661] loss: 0.5559335350990295\n",
      "[step: 662] loss: 0.5510770082473755\n",
      "[step: 663] loss: 0.5492290258407593\n",
      "[step: 664] loss: 0.5501068830490112\n",
      "[step: 665] loss: 0.5524399280548096\n",
      "[step: 666] loss: 0.5546899437904358\n",
      "[step: 667] loss: 0.5561040639877319\n",
      "[step: 668] loss: 0.5559455156326294\n",
      "[step: 669] loss: 0.5548135042190552\n",
      "[step: 670] loss: 0.5526385307312012\n",
      "[step: 671] loss: 0.5502554178237915\n",
      "[step: 672] loss: 0.5477564334869385\n",
      "[step: 673] loss: 0.5456097722053528\n",
      "[step: 674] loss: 0.5437995195388794\n",
      "[step: 675] loss: 0.5423725843429565\n",
      "[step: 676] loss: 0.5412076711654663\n",
      "[step: 677] loss: 0.5402445793151855\n",
      "[step: 678] loss: 0.5394216775894165\n",
      "[step: 679] loss: 0.5387197136878967\n",
      "[step: 680] loss: 0.5381237268447876\n",
      "[step: 681] loss: 0.5376824736595154\n",
      "[step: 682] loss: 0.5375182628631592\n",
      "[step: 683] loss: 0.5379294157028198\n",
      "[step: 684] loss: 0.5394936203956604\n",
      "[step: 685] loss: 0.5437082052230835\n",
      "[step: 686] loss: 0.5534895658493042\n",
      "[step: 687] loss: 0.5767580270767212\n",
      "[step: 688] loss: 0.62577223777771\n",
      "[step: 689] loss: 0.7324256896972656\n",
      "[step: 690] loss: 0.8815255761146545\n",
      "[step: 691] loss: 1.040121078491211\n",
      "[step: 692] loss: 0.869577944278717\n",
      "[step: 693] loss: 0.5950222015380859\n",
      "[step: 694] loss: 0.5754578113555908\n",
      "[step: 695] loss: 0.7489856481552124\n",
      "[step: 696] loss: 0.7130019664764404\n",
      "[step: 697] loss: 0.5353622436523438\n",
      "[step: 698] loss: 0.6403635740280151\n",
      "[step: 699] loss: 0.7013846635818481\n",
      "[step: 700] loss: 0.5496623516082764\n",
      "[step: 701] loss: 0.5949967503547668\n",
      "[step: 702] loss: 0.6588246822357178\n",
      "[step: 703] loss: 0.537670373916626\n",
      "[step: 704] loss: 0.5889458060264587\n",
      "[step: 705] loss: 0.6093891859054565\n",
      "[step: 706] loss: 0.5313339233398438\n",
      "[step: 707] loss: 0.5908101201057434\n",
      "[step: 708] loss: 0.5715734958648682\n",
      "[step: 709] loss: 0.5338610410690308\n",
      "[step: 710] loss: 0.5866255760192871\n",
      "[step: 711] loss: 0.5404437780380249\n",
      "[step: 712] loss: 0.545844554901123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 713] loss: 0.5643657445907593\n",
      "[step: 714] loss: 0.5270862579345703\n",
      "[step: 715] loss: 0.5503684282302856\n",
      "[step: 716] loss: 0.5417195558547974\n",
      "[step: 717] loss: 0.5267772078514099\n",
      "[step: 718] loss: 0.5470768213272095\n",
      "[step: 719] loss: 0.5257828235626221\n",
      "[step: 720] loss: 0.5324280858039856\n",
      "[step: 721] loss: 0.5355688333511353\n",
      "[step: 722] loss: 0.521119236946106\n",
      "[step: 723] loss: 0.5329208970069885\n",
      "[step: 724] loss: 0.5252481698989868\n",
      "[step: 725] loss: 0.5209692716598511\n",
      "[step: 726] loss: 0.5291329622268677\n",
      "[step: 727] loss: 0.5186219811439514\n",
      "[step: 728] loss: 0.5210577249526978\n",
      "[step: 729] loss: 0.5227869749069214\n",
      "[step: 730] loss: 0.5152958631515503\n",
      "[step: 731] loss: 0.5192562341690063\n",
      "[step: 732] loss: 0.5176116824150085\n",
      "[step: 733] loss: 0.5131399631500244\n",
      "[step: 734] loss: 0.5167831182479858\n",
      "[step: 735] loss: 0.5134488940238953\n",
      "[step: 736] loss: 0.5114387273788452\n",
      "[step: 737] loss: 0.5136173963546753\n",
      "[step: 738] loss: 0.5104324817657471\n",
      "[step: 739] loss: 0.5093684196472168\n",
      "[step: 740] loss: 0.510708212852478\n",
      "[step: 741] loss: 0.5078967213630676\n",
      "[step: 742] loss: 0.5072005987167358\n",
      "[step: 743] loss: 0.5079090595245361\n",
      "[step: 744] loss: 0.5057268142700195\n",
      "[step: 745] loss: 0.5049288272857666\n",
      "[step: 746] loss: 0.5053900480270386\n",
      "[step: 747] loss: 0.5036911368370056\n",
      "[step: 748] loss: 0.5026876926422119\n",
      "[step: 749] loss: 0.502941370010376\n",
      "[step: 750] loss: 0.5017421841621399\n",
      "[step: 751] loss: 0.500556468963623\n",
      "[step: 752] loss: 0.5005455017089844\n",
      "[step: 753] loss: 0.4998360276222229\n",
      "[step: 754] loss: 0.4985780715942383\n",
      "[step: 755] loss: 0.49822571873664856\n",
      "[step: 756] loss: 0.49783092737197876\n",
      "[step: 757] loss: 0.49677300453186035\n",
      "[step: 758] loss: 0.4960305392742157\n",
      "[step: 759] loss: 0.49573519825935364\n",
      "[step: 760] loss: 0.4949936866760254\n",
      "[step: 761] loss: 0.49407699704170227\n",
      "[step: 762] loss: 0.4935648441314697\n",
      "[step: 763] loss: 0.493106484413147\n",
      "[step: 764] loss: 0.49230459332466125\n",
      "[step: 765] loss: 0.4915428161621094\n",
      "[step: 766] loss: 0.49105003476142883\n",
      "[step: 767] loss: 0.49049609899520874\n",
      "[step: 768] loss: 0.4897392988204956\n",
      "[step: 769] loss: 0.48904120922088623\n",
      "[step: 770] loss: 0.48851528763771057\n",
      "[step: 771] loss: 0.487943172454834\n",
      "[step: 772] loss: 0.48724016547203064\n",
      "[step: 773] loss: 0.4865570664405823\n",
      "[step: 774] loss: 0.48599064350128174\n",
      "[step: 775] loss: 0.4854222238063812\n",
      "[step: 776] loss: 0.48477113246917725\n",
      "[step: 777] loss: 0.4840954840183258\n",
      "[step: 778] loss: 0.4834834933280945\n",
      "[step: 779] loss: 0.4829132556915283\n",
      "[step: 780] loss: 0.482305645942688\n",
      "[step: 781] loss: 0.4816560447216034\n",
      "[step: 782] loss: 0.4810122549533844\n",
      "[step: 783] loss: 0.4804103672504425\n",
      "[step: 784] loss: 0.479822039604187\n",
      "[step: 785] loss: 0.47921136021614075\n",
      "[step: 786] loss: 0.4785762429237366\n",
      "[step: 787] loss: 0.47794318199157715\n",
      "[step: 788] loss: 0.4773303270339966\n",
      "[step: 789] loss: 0.47673219442367554\n",
      "[step: 790] loss: 0.4761275053024292\n",
      "[step: 791] loss: 0.47550705075263977\n",
      "[step: 792] loss: 0.474879652261734\n",
      "[step: 793] loss: 0.4742557406425476\n",
      "[step: 794] loss: 0.4736420512199402\n",
      "[step: 795] loss: 0.47303470969200134\n",
      "[step: 796] loss: 0.4724254310131073\n",
      "[step: 797] loss: 0.47180983424186707\n",
      "[step: 798] loss: 0.4711882472038269\n",
      "[step: 799] loss: 0.47056522965431213\n",
      "[step: 800] loss: 0.4699436128139496\n",
      "[step: 801] loss: 0.46932512521743774\n",
      "[step: 802] loss: 0.46870994567871094\n",
      "[step: 803] loss: 0.4680953025817871\n",
      "[step: 804] loss: 0.46747878193855286\n",
      "[step: 805] loss: 0.466860294342041\n",
      "[step: 806] loss: 0.46623948216438293\n",
      "[step: 807] loss: 0.46561649441719055\n",
      "[step: 808] loss: 0.46499282121658325\n",
      "[step: 809] loss: 0.4643684923648834\n",
      "[step: 810] loss: 0.4637429714202881\n",
      "[step: 811] loss: 0.4631188213825226\n",
      "[step: 812] loss: 0.46249353885650635\n",
      "[step: 813] loss: 0.4618687033653259\n",
      "[step: 814] loss: 0.46124231815338135\n",
      "[step: 815] loss: 0.46061670780181885\n",
      "[step: 816] loss: 0.4599888324737549\n",
      "[step: 817] loss: 0.45936158299446106\n",
      "[step: 818] loss: 0.4587327837944031\n",
      "[step: 819] loss: 0.45810452103614807\n",
      "[step: 820] loss: 0.45747631788253784\n",
      "[step: 821] loss: 0.4568485617637634\n",
      "[step: 822] loss: 0.4562242031097412\n",
      "[step: 823] loss: 0.45560625195503235\n",
      "[step: 824] loss: 0.45500338077545166\n",
      "[step: 825] loss: 0.4544334411621094\n",
      "[step: 826] loss: 0.45393770933151245\n",
      "[step: 827] loss: 0.4536116123199463\n",
      "[step: 828] loss: 0.4536837935447693\n",
      "[step: 829] loss: 0.45469868183135986\n",
      "[step: 830] loss: 0.4580250382423401\n",
      "[step: 831] loss: 0.4669857323169708\n",
      "[step: 832] loss: 0.4899929165840149\n",
      "[step: 833] loss: 0.5453236103057861\n",
      "[step: 834] loss: 0.6690607070922852\n",
      "[step: 835] loss: 0.871300220489502\n",
      "[step: 836] loss: 1.0504871606826782\n",
      "[step: 837] loss: 0.8445500135421753\n",
      "[step: 838] loss: 0.5016956329345703\n",
      "[step: 839] loss: 0.5386217832565308\n",
      "[step: 840] loss: 0.7309467792510986\n",
      "[step: 841] loss: 0.5860943794250488\n",
      "[step: 842] loss: 0.459400475025177\n",
      "[step: 843] loss: 0.6311588883399963\n",
      "[step: 844] loss: 0.5664471387863159\n",
      "[step: 845] loss: 0.4567921757698059\n",
      "[step: 846] loss: 0.5832675695419312\n",
      "[step: 847] loss: 0.5149993300437927\n",
      "[step: 848] loss: 0.46567416191101074\n",
      "[step: 849] loss: 0.5497686862945557\n",
      "[step: 850] loss: 0.4660384953022003\n",
      "[step: 851] loss: 0.48792529106140137\n",
      "[step: 852] loss: 0.5053378343582153\n",
      "[step: 853] loss: 0.4475446343421936\n",
      "[step: 854] loss: 0.5000073909759521\n",
      "[step: 855] loss: 0.45907819271087646\n",
      "[step: 856] loss: 0.46667858958244324\n",
      "[step: 857] loss: 0.47798433899879456\n",
      "[step: 858] loss: 0.4471292495727539\n",
      "[step: 859] loss: 0.4752596914768219\n",
      "[step: 860] loss: 0.4507426917552948\n",
      "[step: 861] loss: 0.45725351572036743\n",
      "[step: 862] loss: 0.46085354685783386\n",
      "[step: 863] loss: 0.4434022307395935\n",
      "[step: 864] loss: 0.4604474604129791\n",
      "[step: 865] loss: 0.4430958330631256\n",
      "[step: 866] loss: 0.45012542605400085\n",
      "[step: 867] loss: 0.44927361607551575\n",
      "[step: 868] loss: 0.44039806723594666\n",
      "[step: 869] loss: 0.44964224100112915\n",
      "[step: 870] loss: 0.439172625541687\n",
      "[step: 871] loss: 0.44214928150177\n",
      "[step: 872] loss: 0.44220417737960815\n",
      "[step: 873] loss: 0.4359303116798401\n",
      "[step: 874] loss: 0.44124892354011536\n",
      "[step: 875] loss: 0.4355102479457855\n",
      "[step: 876] loss: 0.4357161819934845\n",
      "[step: 877] loss: 0.4368951618671417\n",
      "[step: 878] loss: 0.4316873252391815\n",
      "[step: 879] loss: 0.4347633123397827\n",
      "[step: 880] loss: 0.43188953399658203\n",
      "[step: 881] loss: 0.4302809536457062\n",
      "[step: 882] loss: 0.4318804144859314\n",
      "[step: 883] loss: 0.4284924864768982\n",
      "[step: 884] loss: 0.42869749665260315\n",
      "[step: 885] loss: 0.4286919832229614\n",
      "[step: 886] loss: 0.42597827315330505\n",
      "[step: 887] loss: 0.4267353415489197\n",
      "[step: 888] loss: 0.42562514543533325\n",
      "[step: 889] loss: 0.42387092113494873\n",
      "[step: 890] loss: 0.42447376251220703\n",
      "[step: 891] loss: 0.4229815900325775\n",
      "[step: 892] loss: 0.42193603515625\n",
      "[step: 893] loss: 0.4220747947692871\n",
      "[step: 894] loss: 0.4206874966621399\n",
      "[step: 895] loss: 0.41984397172927856\n",
      "[step: 896] loss: 0.41979703307151794\n",
      "[step: 897] loss: 0.4185240864753723\n",
      "[step: 898] loss: 0.4177893400192261\n",
      "[step: 899] loss: 0.4175795316696167\n",
      "[step: 900] loss: 0.41646867990493774\n",
      "[step: 901] loss: 0.4157319962978363\n",
      "[step: 902] loss: 0.4154093861579895\n",
      "[step: 903] loss: 0.41449129581451416\n",
      "[step: 904] loss: 0.41367414593696594\n",
      "[step: 905] loss: 0.41330066323280334\n",
      "[step: 906] loss: 0.4125213027000427\n",
      "[step: 907] loss: 0.4116840958595276\n",
      "[step: 908] loss: 0.41122037172317505\n",
      "[step: 909] loss: 0.41056540608406067\n",
      "[step: 910] loss: 0.40973803400993347\n",
      "[step: 911] loss: 0.4091544449329376\n",
      "[step: 912] loss: 0.4086001515388489\n",
      "[step: 913] loss: 0.4078251123428345\n",
      "[step: 914] loss: 0.4071481227874756\n",
      "[step: 915] loss: 0.40660202503204346\n",
      "[step: 916] loss: 0.40592455863952637\n",
      "[step: 917] loss: 0.40519827604293823\n",
      "[step: 918] loss: 0.40459370613098145\n",
      "[step: 919] loss: 0.403995543718338\n",
      "[step: 920] loss: 0.40329140424728394\n",
      "[step: 921] loss: 0.40261924266815186\n",
      "[step: 922] loss: 0.40202245116233826\n",
      "[step: 923] loss: 0.4013838768005371\n",
      "[step: 924] loss: 0.40069690346717834\n",
      "[step: 925] loss: 0.40004560351371765\n",
      "[step: 926] loss: 0.3994351625442505\n",
      "[step: 927] loss: 0.3987860083580017\n",
      "[step: 928] loss: 0.3981103301048279\n",
      "[step: 929] loss: 0.3974659740924835\n",
      "[step: 930] loss: 0.39684152603149414\n",
      "[step: 931] loss: 0.3961927592754364\n",
      "[step: 932] loss: 0.3955245614051819\n",
      "[step: 933] loss: 0.3948747515678406\n",
      "[step: 934] loss: 0.3942407965660095\n",
      "[step: 935] loss: 0.3935931921005249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 936] loss: 0.3929312825202942\n",
      "[step: 937] loss: 0.3922739624977112\n",
      "[step: 938] loss: 0.39163100719451904\n",
      "[step: 939] loss: 0.39098480343818665\n",
      "[step: 940] loss: 0.3903258442878723\n",
      "[step: 941] loss: 0.389664888381958\n",
      "[step: 942] loss: 0.3890111446380615\n",
      "[step: 943] loss: 0.3883611857891083\n",
      "[step: 944] loss: 0.3877057433128357\n",
      "[step: 945] loss: 0.387043297290802\n",
      "[step: 946] loss: 0.3863816261291504\n",
      "[step: 947] loss: 0.3857230544090271\n",
      "[step: 948] loss: 0.3850656747817993\n",
      "[step: 949] loss: 0.38440483808517456\n",
      "[step: 950] loss: 0.3837393820285797\n",
      "[step: 951] loss: 0.38307294249534607\n",
      "[step: 952] loss: 0.3824089765548706\n",
      "[step: 953] loss: 0.38174498081207275\n",
      "[step: 954] loss: 0.38107919692993164\n",
      "[step: 955] loss: 0.38041040301322937\n",
      "[step: 956] loss: 0.37973886728286743\n",
      "[step: 957] loss: 0.3790682256221771\n",
      "[step: 958] loss: 0.3783974349498749\n",
      "[step: 959] loss: 0.3777257204055786\n",
      "[step: 960] loss: 0.3770526051521301\n",
      "[step: 961] loss: 0.37637731432914734\n",
      "[step: 962] loss: 0.3757009506225586\n",
      "[step: 963] loss: 0.3750224709510803\n",
      "[step: 964] loss: 0.3743438124656677\n",
      "[step: 965] loss: 0.3736642599105835\n",
      "[step: 966] loss: 0.37298423051834106\n",
      "[step: 967] loss: 0.3723025321960449\n",
      "[step: 968] loss: 0.3716188073158264\n",
      "[step: 969] loss: 0.3709336221218109\n",
      "[step: 970] loss: 0.3702473044395447\n",
      "[step: 971] loss: 0.3695601522922516\n",
      "[step: 972] loss: 0.3688715398311615\n",
      "[step: 973] loss: 0.36818188428878784\n",
      "[step: 974] loss: 0.36749136447906494\n",
      "[step: 975] loss: 0.36679914593696594\n",
      "[step: 976] loss: 0.3661057949066162\n",
      "[step: 977] loss: 0.3654101490974426\n",
      "[step: 978] loss: 0.3647139370441437\n",
      "[step: 979] loss: 0.36401593685150146\n",
      "[step: 980] loss: 0.36331701278686523\n",
      "[step: 981] loss: 0.3626158535480499\n",
      "[step: 982] loss: 0.3619143068790436\n",
      "[step: 983] loss: 0.3612111508846283\n",
      "[step: 984] loss: 0.3605068325996399\n",
      "[step: 985] loss: 0.35979998111724854\n",
      "[step: 986] loss: 0.35909202694892883\n",
      "[step: 987] loss: 0.3583831787109375\n",
      "[step: 988] loss: 0.35767245292663574\n",
      "[step: 989] loss: 0.35696089267730713\n",
      "[step: 990] loss: 0.3562466502189636\n",
      "[step: 991] loss: 0.35553205013275146\n",
      "[step: 992] loss: 0.35481545329093933\n",
      "[step: 993] loss: 0.3540968596935272\n",
      "[step: 994] loss: 0.3533768057823181\n",
      "[step: 995] loss: 0.35265615582466125\n",
      "[step: 996] loss: 0.351933091878891\n",
      "[step: 997] loss: 0.3512088656425476\n",
      "[step: 998] loss: 0.350483238697052\n",
      "[step: 999] loss: 0.34975677728652954\n",
      "Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "LSTM realforecast : [21349.103515625, 15779.19921875, 13242.3798828125, 12768.884765625, 18729.337890625, 18976.56640625]\n",
      "Bayseian realforecast : [18505.381228343118, 17436.523430305366, 19748.680869264314, 18776.571383034858, 23150.508624040402, 29887.249859914671]\n",
      "===================================THE END===================================================\n"
     ]
    }
   ],
   "source": [
    "answer=LearningModuleRunner(rawArrayDatas,0,forecastDay,'month') #0은 processId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18505.381228343118,\n",
       " 17436.523430305366,\n",
       " 19748.680869264314,\n",
       " 18776.571383034858,\n",
       " 23150.508624040402,\n",
       " 29887.249859914671]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawArrayDatas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
