{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fbprophet import Prophet\n",
    "forecastDay=7\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawArrayDatas=[\"2010-02-05\",\n",
    " \"2010-02-12\",\n",
    " \"2010-02-19\",\n",
    " \"2010-02-26\",\n",
    " \"2010-03-05\",\n",
    " \"2010-03-12\",\n",
    " \"2010-03-19\",\n",
    " \"2010-03-26\",\n",
    " \"2010-04-02\",\n",
    " \"2010-04-09\",\n",
    " \"2010-04-16\",\n",
    " \"2010-04-23\",\n",
    " \"2010-04-30\",\n",
    " \"2010-05-07\",\n",
    " \"2010-05-14\",\n",
    " \"2010-05-21\",\n",
    " \"2010-05-28\",\n",
    " \"2010-06-04\",\n",
    " \"2010-06-11\",\n",
    " \"2010-06-18\",\n",
    " \"2010-06-25\",\n",
    " \"2010-07-02\",\n",
    " \"2010-07-09\",\n",
    " \"2010-07-16\",\n",
    " \"2010-07-23\",\n",
    " \"2010-07-30\",\n",
    " \"2010-08-06\",\n",
    " \"2010-08-13\",\n",
    " \"2010-08-20\",\n",
    " \"2010-08-27\",\n",
    " \"2010-09-03\",\n",
    " \"2010-09-10\",\n",
    " \"2010-09-17\",\n",
    " \"2010-09-24\",\n",
    " \"2010-10-01\",\n",
    " \"2010-10-08\",\n",
    " \"2010-10-15\",\n",
    " \"2010-10-22\",\n",
    " \"2010-10-29\",\n",
    " \"2010-11-05\",\n",
    " \"2010-11-12\",\n",
    " \"2010-11-19\",\n",
    " \"2010-11-26\",\n",
    " \"2010-12-03\",\n",
    " \"2010-12-10\",\n",
    " \"2010-12-17\",\n",
    " \"2010-12-24\",\n",
    " \"2010-12-31\",\n",
    " \"2011-01-07\",\n",
    " \"2011-01-14\",\n",
    " \"2011-01-21\",\n",
    " \"2011-01-28\",\n",
    " \"2011-02-04\",\n",
    " \"2011-02-11\",\n",
    " \"2011-02-18\",\n",
    " \"2011-02-25\",\n",
    " \"2011-03-04\",\n",
    " \"2011-03-11\",\n",
    " \"2011-03-18\",\n",
    " \"2011-03-25\",\n",
    " \"2011-04-01\",\n",
    " \"2011-04-08\",\n",
    " \"2011-04-15\",\n",
    " \"2011-04-22\",\n",
    " \"2011-04-29\",\n",
    " \"2011-05-06\",\n",
    " \"2011-05-13\",\n",
    " \"2011-05-20\",\n",
    " \"2011-05-27\",\n",
    " \"2011-06-03\",\n",
    " \"2011-06-10\",\n",
    " \"2011-06-17\",\n",
    " \"2011-06-24\",\n",
    " \"2011-07-01\",\n",
    " \"2011-07-08\",\n",
    " \"2011-07-15\",\n",
    " \"2011-07-22\",\n",
    " \"2011-07-29\",\n",
    " \"2011-08-05\",\n",
    " \"2011-08-12\",\n",
    " \"2011-08-19\",\n",
    " \"2011-08-26\",\n",
    " \"2011-09-02\",\n",
    " \"2011-09-09\",\n",
    " \"2011-09-16\",\n",
    " \"2011-09-23\",\n",
    " \"2011-09-30\",\n",
    " \"2011-10-07\",\n",
    " \"2011-10-14\",\n",
    " \"2011-10-21\",\n",
    " \"2011-10-28\",\n",
    " \"2011-11-04\",\n",
    " \"2011-11-11\",\n",
    " \"2011-11-18\",\n",
    " \"2011-11-25\",\n",
    " \"2011-12-02\",\n",
    " \"2011-12-09\",\n",
    " \"2011-12-16\",\n",
    " \"2011-12-23\",\n",
    " \"2011-12-30\",\n",
    " \"2012-01-06\",\n",
    " \"2012-01-13\",\n",
    " \"2012-01-20\",\n",
    " \"2012-01-27\",\n",
    " \"2012-02-03\",\n",
    " \"2012-02-10\",\n",
    " \"2012-02-17\",\n",
    " \"2012-02-24\",\n",
    " \"2012-03-02\",\n",
    " \"2012-03-09\",\n",
    " \"2012-03-16\",\n",
    " \"2012-03-23\",\n",
    " \"2012-03-30\",\n",
    " \"2012-04-06\",\n",
    " \"2012-04-13\",\n",
    " \"2012-04-20\",\n",
    " \"2012-04-27\",\n",
    " \"2012-05-04\",\n",
    " \"2012-05-11\",\n",
    " \"2012-05-18\",\n",
    " \"2012-05-25\",\n",
    " \"2012-06-01\",\n",
    " \"2012-06-08\",\n",
    " \"2012-06-15\",\n",
    " \"2012-06-22\",\n",
    " \"2012-06-29\",\n",
    " \"2012-07-06\",\n",
    " \"2012-07-13\",\n",
    " \"2012-07-20\",\n",
    " \"2012-07-27\",\n",
    " \"2012-08-03\",\n",
    " \"2012-08-10\",\n",
    " \"2012-08-17\",\n",
    " \"2012-08-24\",\n",
    " \"2012-08-31\",\n",
    " \"2012-09-07\",\n",
    " \"2012-09-14\",\n",
    " \"2012-09-21\",\n",
    " \"2012-09-28\",\n",
    " \"2012-10-05\",\n",
    " \"2012-10-12\",\n",
    " \"2012-10-19\",\n",
    " \"2012-10-26\"],[24924.5,\n",
    " 46039.489999999998,\n",
    " 41595.550000000003,\n",
    " 19403.540000000001,\n",
    " 21827.900000000001,\n",
    " 21043.389999999999,\n",
    " 22136.639999999999,\n",
    " 26229.209999999999,\n",
    " 57258.43,\n",
    " 42960.910000000003,\n",
    " 17596.959999999999,\n",
    " 16145.35,\n",
    " 16555.110000000001,\n",
    " 17413.939999999999,\n",
    " 18926.740000000002,\n",
    " 14773.040000000001,\n",
    " 15580.43,\n",
    " 17558.09,\n",
    " 16637.619999999999,\n",
    " 16216.27,\n",
    " 16328.719999999999,\n",
    " 16333.139999999999,\n",
    " 17688.759999999998,\n",
    " 17150.84,\n",
    " 15360.450000000001,\n",
    " 15381.82,\n",
    " 17508.41,\n",
    " 15536.4,\n",
    " 15740.129999999999,\n",
    " 15793.870000000001,\n",
    " 16241.780000000001,\n",
    " 18194.740000000002,\n",
    " 19354.23,\n",
    " 18122.52,\n",
    " 20094.189999999999,\n",
    " 23388.029999999999,\n",
    " 26978.34,\n",
    " 25543.040000000001,\n",
    " 38640.93,\n",
    " 34238.879999999997,\n",
    " 19549.389999999999,\n",
    " 19552.84,\n",
    " 18820.290000000001,\n",
    " 22517.560000000001,\n",
    " 31497.650000000001,\n",
    " 44912.860000000001,\n",
    " 55931.230000000003,\n",
    " 19124.580000000002,\n",
    " 15984.24,\n",
    " 17359.700000000001,\n",
    " 17341.470000000001,\n",
    " 18461.18,\n",
    " 21665.759999999998,\n",
    " 37887.169999999998,\n",
    " 46845.870000000003,\n",
    " 19363.830000000002,\n",
    " 20327.610000000001,\n",
    " 21280.400000000001,\n",
    " 20334.23,\n",
    " 20881.099999999999,\n",
    " 20398.09,\n",
    " 23873.790000000001,\n",
    " 28762.369999999999,\n",
    " 50510.309999999998,\n",
    " 41512.389999999999,\n",
    " 20138.189999999999,\n",
    " 17235.150000000001,\n",
    " 15136.780000000001,\n",
    " 15741.6,\n",
    " 16434.150000000001,\n",
    " 15883.52,\n",
    " 14978.09,\n",
    " 15682.809999999999,\n",
    " 15363.5,\n",
    " 16148.870000000001,\n",
    " 15654.85,\n",
    " 15766.6,\n",
    " 15922.41,\n",
    " 15295.549999999999,\n",
    " 14539.790000000001,\n",
    " 14689.24,\n",
    " 14537.370000000001,\n",
    " 15277.27,\n",
    " 17746.68,\n",
    " 18535.48,\n",
    " 17859.299999999999,\n",
    " 18337.68,\n",
    " 20797.580000000002,\n",
    " 23077.549999999999,\n",
    " 23351.799999999999,\n",
    " 31579.900000000001,\n",
    " 39886.059999999998,\n",
    " 18689.540000000001,\n",
    " 19050.66,\n",
    " 20911.25,\n",
    " 25293.490000000002,\n",
    " 33305.919999999998,\n",
    " 45773.029999999999,\n",
    " 46788.75,\n",
    " 23350.880000000001,\n",
    " 16567.689999999999,\n",
    " 16894.400000000001,\n",
    " 18365.099999999999,\n",
    " 18378.16,\n",
    " 23510.490000000002,\n",
    " 36988.489999999998,\n",
    " 54060.099999999999,\n",
    " 20124.220000000001,\n",
    " 20113.029999999999,\n",
    " 21140.07,\n",
    " 22366.880000000001,\n",
    " 22107.700000000001,\n",
    " 28952.860000000001,\n",
    " 57592.120000000003,\n",
    " 34684.209999999999,\n",
    " 16976.189999999999,\n",
    " 16347.6,\n",
    " 17147.439999999999,\n",
    " 18164.200000000001,\n",
    " 18517.790000000001,\n",
    " 16963.549999999999,\n",
    " 16065.49,\n",
    " 17666.0,\n",
    " 17558.82,\n",
    " 16633.41,\n",
    " 15722.82,\n",
    " 17823.369999999999,\n",
    " 16566.18,\n",
    " 16348.059999999999,\n",
    " 15731.18,\n",
    " 16628.310000000001,\n",
    " 16119.92,\n",
    " 17330.700000000001,\n",
    " 16286.4,\n",
    " 16680.240000000002,\n",
    " 18322.369999999999,\n",
    " 19616.220000000001,\n",
    " 19251.5,\n",
    " 18947.810000000001,\n",
    " 0.0,\n",
    " 0.0,\n",
    " 0.0,\n",
    " 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dayOrWeekOrMonth='week'\n",
    "    # options:\n",
    "    # 'day', 'week', 'month'\n",
    "\n",
    "feature = 'DayOfWeek_WeekNumber_Month_Season'\n",
    "    # options:\n",
    "    # dayOrWeekOrMonth='day': 'DayOfWeek_WeekNumber_Month_Season','DayOfWeek01_WeekNumber_Month_Season'//\n",
    "    # dayOrWeekOrMonth='week': 'WeekNumber_Month_Season_Year'\n",
    "\n",
    "\n",
    "mockForcastDay=2*forecastDay\n",
    "\n",
    "    ##Make txsForRealForecastLstm   [:]\n",
    "ds = rawArrayDatas[0]\n",
    "y = list(rawArrayDatas[1])\n",
    "sales = list(zip(ds, y))\n",
    "txsForRealForecastLstm =pd.DataFrame(data=sales, columns=['date', 'sales'])\n",
    "   \n",
    "\n",
    "    ##Make txsForMockForecastLstm [:-forecastDay]\n",
    "ds = rawArrayDatas[0][:-forecastDay]\n",
    "y= list(rawArrayDatas[1][:-forecastDay] )\n",
    "sales = list(zip(ds, y))\n",
    "txsForMockForecastLstm =pd.DataFrame(data=sales, columns=['date', 'sales'])\n",
    "   \n",
    "    ##Make txsForRealForecastBayesian [:-forecastDay] & np.log\n",
    "ds = rawArrayDatas[0][:-forecastDay]\n",
    "    # TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "y = list(np.log(rawArrayDatas[1][:-forecastDay]))\n",
    "sales = list(zip(ds, y))\n",
    "txsForRealForecastBayesian = pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "    \n",
    "    ##Make txsForMockForecastBayseian   [:-3*forecastDay] & np.log\n",
    "ds = rawArrayDatas[0][:-3*forecastDay]\n",
    "    #TODO bayseian에 대해서는 input값이 0인 상황처리 필요\n",
    "y= list(np.log(rawArrayDatas[1][:-3*forecastDay]))\n",
    "sales = list(zip(ds, y))\n",
    "txsForMockForecastBayseian =pd.DataFrame(data=sales, columns=['ds', 'y'])\n",
    "    \n",
    "    #testY for algorithm compare has size of 2*forecastDay:  rawArrayDatas[1][-3*forecastDay:-forecastDay]\n",
    "testY= rawArrayDatas[1][-3*forecastDay:-forecastDay]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noOutlierSales(sales):\n",
    "    mean=np.mean(sales)\n",
    "    std=np.std(sales)\n",
    "    for i in range(len(sales)):\n",
    "        if (sales[i]<mean-2*std or sales[i]>mean+2*std):\n",
    "             sales[i]=int(mean)\n",
    "    return sales\n",
    "def logSales(sales):\n",
    "    for i in range(len(sales)):\n",
    "        if sales[i] is 0:\n",
    "            sales[i]=1\n",
    "    return np.log(sales)\n",
    "def sqrtSales(sales):\n",
    "    return np.sqrt(sales)\n",
    "\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(77)\n",
    "    # Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['date'].map(year)\n",
    "    txs['month'] = txs['date'].map(month)\n",
    "    txs['weekNumber'] = txs['date'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['date'].map(dayOfWeek)\n",
    "\n",
    "    # Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['date'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['date'].map(day_of_week01)\n",
    "\n",
    "    # Backup originalSales\n",
    "    originalSales = list(txs['sales'])\n",
    "    sales = list(txs['sales'])\n",
    "\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "    elif features is 'DayOfWeek01_WeekNumber_Month_Season':\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']), list(txs['month']), list(txs['season']), sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year':\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']), list(txs['year']), sales]\n",
    "\n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "\n",
    "    # Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    # TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay + 1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "\n",
    "    train_size = int(len(dataY) - forecastDay)\n",
    "#     train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "    denormalizedTestY = originalSales[train_size + seq_length:]\n",
    "    #     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "    count = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        while (1):\n",
    "            count = count + 1\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(count, step_loss))\n",
    "            if (step_loss < 0.5):\n",
    "                break\n",
    "\n",
    "        # Test step\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}), originalXY)\n",
    "        realSale = minMaxDeNormalizer(testY[-1], originalXY)\n",
    "    return test_predict[-1].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1] loss: 144.07269287109375\n",
      "[step: 2] loss: 138.5416259765625\n",
      "[step: 3] loss: 133.17449951171875\n",
      "[step: 4] loss: 127.94277954101562\n",
      "[step: 5] loss: 122.82670593261719\n",
      "[step: 6] loss: 117.80735778808594\n",
      "[step: 7] loss: 112.86554718017578\n",
      "[step: 8] loss: 107.9822998046875\n",
      "[step: 9] loss: 103.13963317871094\n",
      "[step: 10] loss: 98.32171630859375\n",
      "[step: 11] loss: 93.51560974121094\n",
      "[step: 12] loss: 88.7123031616211\n",
      "[step: 13] loss: 83.90681457519531\n",
      "[step: 14] loss: 79.09873962402344\n",
      "[step: 15] loss: 74.29310607910156\n",
      "[step: 16] loss: 69.50161743164062\n",
      "[step: 17] loss: 64.74412536621094\n",
      "[step: 18] loss: 60.050811767578125\n",
      "[step: 19] loss: 55.464210510253906\n",
      "[step: 20] loss: 51.04118728637695\n",
      "[step: 21] loss: 46.85404586791992\n",
      "[step: 22] loss: 42.98950958251953\n",
      "[step: 23] loss: 39.54285430908203\n",
      "[step: 24] loss: 36.60601043701172\n",
      "[step: 25] loss: 34.248756408691406\n",
      "[step: 26] loss: 32.495826721191406\n",
      "[step: 27] loss: 31.305259704589844\n",
      "[step: 28] loss: 30.55856704711914\n",
      "[step: 29] loss: 30.076345443725586\n",
      "[step: 30] loss: 29.66338348388672\n",
      "[step: 31] loss: 29.165603637695312\n",
      "[step: 32] loss: 28.510147094726562\n",
      "[step: 33] loss: 27.7120361328125\n",
      "[step: 34] loss: 26.851070404052734\n",
      "[step: 35] loss: 26.03398323059082\n",
      "[step: 36] loss: 25.358055114746094\n",
      "[step: 37] loss: 24.886402130126953\n",
      "[step: 38] loss: 24.638446807861328\n",
      "[step: 39] loss: 24.593446731567383\n",
      "[step: 40] loss: 24.70252227783203\n",
      "[step: 41] loss: 24.903589248657227\n",
      "[step: 42] loss: 25.13490867614746\n",
      "[step: 43] loss: 25.344703674316406\n",
      "[step: 44] loss: 25.496294021606445\n",
      "[step: 45] loss: 25.56926727294922\n",
      "[step: 46] loss: 25.558073043823242\n",
      "[step: 47] loss: 25.46910285949707\n",
      "[step: 48] loss: 25.317310333251953\n",
      "[step: 49] loss: 25.12272071838379\n",
      "[step: 50] loss: 24.907264709472656\n",
      "[step: 51] loss: 24.691972732543945\n",
      "[step: 52] loss: 24.49460792541504\n",
      "[step: 53] loss: 24.327960968017578\n",
      "[step: 54] loss: 24.198856353759766\n",
      "[step: 55] loss: 24.10797882080078\n",
      "[step: 56] loss: 24.050689697265625\n",
      "[step: 57] loss: 24.01861572265625\n",
      "[step: 58] loss: 24.00177001953125\n",
      "[step: 59] loss: 23.990673065185547\n",
      "[step: 60] loss: 23.97815704345703\n",
      "[step: 61] loss: 23.960256576538086\n",
      "[step: 62] loss: 23.936248779296875\n",
      "[step: 63] loss: 23.90789031982422\n",
      "[step: 64] loss: 23.878273010253906\n",
      "[step: 65] loss: 23.850595474243164\n",
      "[step: 66] loss: 23.82723617553711\n",
      "[step: 67] loss: 23.809249877929688\n",
      "[step: 68] loss: 23.796283721923828\n",
      "[step: 69] loss: 23.786903381347656\n",
      "[step: 70] loss: 23.779052734375\n",
      "[step: 71] loss: 23.77054786682129\n",
      "[step: 72] loss: 23.75959587097168\n",
      "[step: 73] loss: 23.74505043029785\n",
      "[step: 74] loss: 23.726577758789062\n",
      "[step: 75] loss: 23.70459747314453\n",
      "[step: 76] loss: 23.680145263671875\n",
      "[step: 77] loss: 23.654613494873047\n",
      "[step: 78] loss: 23.62943458557129\n",
      "[step: 79] loss: 23.605892181396484\n",
      "[step: 80] loss: 23.584827423095703\n",
      "[step: 81] loss: 23.566627502441406\n",
      "[step: 82] loss: 23.55116081237793\n",
      "[step: 83] loss: 23.537914276123047\n",
      "[step: 84] loss: 23.526140213012695\n",
      "[step: 85] loss: 23.51504898071289\n",
      "[step: 86] loss: 23.50396728515625\n",
      "[step: 87] loss: 23.492467880249023\n",
      "[step: 88] loss: 23.480365753173828\n",
      "[step: 89] loss: 23.46774673461914\n",
      "[step: 90] loss: 23.454837799072266\n",
      "[step: 91] loss: 23.441932678222656\n",
      "[step: 92] loss: 23.429264068603516\n",
      "[step: 93] loss: 23.41698455810547\n",
      "[step: 94] loss: 23.405128479003906\n",
      "[step: 95] loss: 23.393592834472656\n",
      "[step: 96] loss: 23.382225036621094\n",
      "[step: 97] loss: 23.370832443237305\n",
      "[step: 98] loss: 23.35927391052246\n",
      "[step: 99] loss: 23.34746742248535\n",
      "[step: 100] loss: 23.335407257080078\n",
      "[step: 101] loss: 23.323165893554688\n",
      "[step: 102] loss: 23.31085968017578\n",
      "[step: 103] loss: 23.29864501953125\n",
      "[step: 104] loss: 23.286630630493164\n",
      "[step: 105] loss: 23.274913787841797\n",
      "[step: 106] loss: 23.263534545898438\n",
      "[step: 107] loss: 23.252473831176758\n",
      "[step: 108] loss: 23.241666793823242\n",
      "[step: 109] loss: 23.231029510498047\n",
      "[step: 110] loss: 23.220478057861328\n",
      "[step: 111] loss: 23.20992660522461\n",
      "[step: 112] loss: 23.199337005615234\n",
      "[step: 113] loss: 23.18868637084961\n",
      "[step: 114] loss: 23.177982330322266\n",
      "[step: 115] loss: 23.167251586914062\n",
      "[step: 116] loss: 23.156524658203125\n",
      "[step: 117] loss: 23.14584732055664\n",
      "[step: 118] loss: 23.13522720336914\n",
      "[step: 119] loss: 23.12467384338379\n",
      "[step: 120] loss: 23.114198684692383\n",
      "[step: 121] loss: 23.10377311706543\n",
      "[step: 122] loss: 23.09339714050293\n",
      "[step: 123] loss: 23.083045959472656\n",
      "[step: 124] loss: 23.072715759277344\n",
      "[step: 125] loss: 23.062421798706055\n",
      "[step: 126] loss: 23.05214500427246\n",
      "[step: 127] loss: 23.041912078857422\n",
      "[step: 128] loss: 23.031715393066406\n",
      "[step: 129] loss: 23.021574020385742\n",
      "[step: 130] loss: 23.011470794677734\n",
      "[step: 131] loss: 23.001426696777344\n",
      "[step: 132] loss: 22.991411209106445\n",
      "[step: 133] loss: 22.981433868408203\n",
      "[step: 134] loss: 22.971467971801758\n",
      "[step: 135] loss: 22.96152114868164\n",
      "[step: 136] loss: 22.95157814025879\n",
      "[step: 137] loss: 22.941650390625\n",
      "[step: 138] loss: 22.93172836303711\n",
      "[step: 139] loss: 22.921817779541016\n",
      "[step: 140] loss: 22.911937713623047\n",
      "[step: 141] loss: 22.902063369750977\n",
      "[step: 142] loss: 22.892221450805664\n",
      "[step: 143] loss: 22.882402420043945\n",
      "[step: 144] loss: 22.872600555419922\n",
      "[step: 145] loss: 22.86280632019043\n",
      "[step: 146] loss: 22.853031158447266\n",
      "[step: 147] loss: 22.843265533447266\n",
      "[step: 148] loss: 22.83350944519043\n",
      "[step: 149] loss: 22.823755264282227\n",
      "[step: 150] loss: 22.814010620117188\n",
      "[step: 151] loss: 22.804264068603516\n",
      "[step: 152] loss: 22.794523239135742\n",
      "[step: 153] loss: 22.784786224365234\n",
      "[step: 154] loss: 22.77505111694336\n",
      "[step: 155] loss: 22.76531410217285\n",
      "[step: 156] loss: 22.755573272705078\n",
      "[step: 157] loss: 22.745824813842773\n",
      "[step: 158] loss: 22.736082077026367\n",
      "[step: 159] loss: 22.726322174072266\n",
      "[step: 160] loss: 22.71656036376953\n",
      "[step: 161] loss: 22.706777572631836\n",
      "[step: 162] loss: 22.696990966796875\n",
      "[step: 163] loss: 22.687191009521484\n",
      "[step: 164] loss: 22.677379608154297\n",
      "[step: 165] loss: 22.66754913330078\n",
      "[step: 166] loss: 22.65770721435547\n",
      "[step: 167] loss: 22.6478328704834\n",
      "[step: 168] loss: 22.637950897216797\n",
      "[step: 169] loss: 22.628036499023438\n",
      "[step: 170] loss: 22.618091583251953\n",
      "[step: 171] loss: 22.608125686645508\n",
      "[step: 172] loss: 22.59813117980957\n",
      "[step: 173] loss: 22.588102340698242\n",
      "[step: 174] loss: 22.578041076660156\n",
      "[step: 175] loss: 22.56793975830078\n",
      "[step: 176] loss: 22.557798385620117\n",
      "[step: 177] loss: 22.54762077331543\n",
      "[step: 178] loss: 22.537405014038086\n",
      "[step: 179] loss: 22.527137756347656\n",
      "[step: 180] loss: 22.51681900024414\n",
      "[step: 181] loss: 22.506458282470703\n",
      "[step: 182] loss: 22.496036529541016\n",
      "[step: 183] loss: 22.48556137084961\n",
      "[step: 184] loss: 22.475032806396484\n",
      "[step: 185] loss: 22.46444320678711\n",
      "[step: 186] loss: 22.453784942626953\n",
      "[step: 187] loss: 22.443065643310547\n",
      "[step: 188] loss: 22.432268142700195\n",
      "[step: 189] loss: 22.421401977539062\n",
      "[step: 190] loss: 22.410457611083984\n",
      "[step: 191] loss: 22.399429321289062\n",
      "[step: 192] loss: 22.388317108154297\n",
      "[step: 193] loss: 22.377120971679688\n",
      "[step: 194] loss: 22.36583137512207\n",
      "[step: 195] loss: 22.35445213317871\n",
      "[step: 196] loss: 22.34296417236328\n",
      "[step: 197] loss: 22.331378936767578\n",
      "[step: 198] loss: 22.319684982299805\n",
      "[step: 199] loss: 22.30787467956543\n",
      "[step: 200] loss: 22.29595184326172\n",
      "[step: 201] loss: 22.283905029296875\n",
      "[step: 202] loss: 22.271724700927734\n",
      "[step: 203] loss: 22.25942611694336\n",
      "[step: 204] loss: 22.246980667114258\n",
      "[step: 205] loss: 22.234394073486328\n",
      "[step: 206] loss: 22.221656799316406\n",
      "[step: 207] loss: 22.20876693725586\n",
      "[step: 208] loss: 22.195714950561523\n",
      "[step: 209] loss: 22.1825008392334\n",
      "[step: 210] loss: 22.169109344482422\n",
      "[step: 211] loss: 22.155532836914062\n",
      "[step: 212] loss: 22.141775131225586\n",
      "[step: 213] loss: 22.127822875976562\n",
      "[step: 214] loss: 22.113662719726562\n",
      "[step: 215] loss: 22.099292755126953\n",
      "[step: 216] loss: 22.084707260131836\n",
      "[step: 217] loss: 22.069889068603516\n",
      "[step: 218] loss: 22.054838180541992\n",
      "[step: 219] loss: 22.03954315185547\n",
      "[step: 220] loss: 22.023988723754883\n",
      "[step: 221] loss: 22.00817108154297\n",
      "[step: 222] loss: 21.992076873779297\n",
      "[step: 223] loss: 21.97570037841797\n",
      "[step: 224] loss: 21.959026336669922\n",
      "[step: 225] loss: 21.94204330444336\n",
      "[step: 226] loss: 21.92474365234375\n",
      "[step: 227] loss: 21.907108306884766\n",
      "[step: 228] loss: 21.889129638671875\n",
      "[step: 229] loss: 21.87079620361328\n",
      "[step: 230] loss: 21.852092742919922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 231] loss: 21.8330135345459\n",
      "[step: 232] loss: 21.81352996826172\n",
      "[step: 233] loss: 21.793638229370117\n",
      "[step: 234] loss: 21.7733154296875\n",
      "[step: 235] loss: 21.7525634765625\n",
      "[step: 236] loss: 21.731348037719727\n",
      "[step: 237] loss: 21.709674835205078\n",
      "[step: 238] loss: 21.68750762939453\n",
      "[step: 239] loss: 21.664844512939453\n",
      "[step: 240] loss: 21.641664505004883\n",
      "[step: 241] loss: 21.61795425415039\n",
      "[step: 242] loss: 21.59368896484375\n",
      "[step: 243] loss: 21.568872451782227\n",
      "[step: 244] loss: 21.543476104736328\n",
      "[step: 245] loss: 21.517488479614258\n",
      "[step: 246] loss: 21.49090003967285\n",
      "[step: 247] loss: 21.463693618774414\n",
      "[step: 248] loss: 21.435855865478516\n",
      "[step: 249] loss: 21.40737533569336\n",
      "[step: 250] loss: 21.378253936767578\n",
      "[step: 251] loss: 21.348472595214844\n",
      "[step: 252] loss: 21.318038940429688\n",
      "[step: 253] loss: 21.28692626953125\n",
      "[step: 254] loss: 21.255157470703125\n",
      "[step: 255] loss: 21.222726821899414\n",
      "[step: 256] loss: 21.18964195251465\n",
      "[step: 257] loss: 21.155914306640625\n",
      "[step: 258] loss: 21.12156105041504\n",
      "[step: 259] loss: 21.086597442626953\n",
      "[step: 260] loss: 21.051057815551758\n",
      "[step: 261] loss: 21.014976501464844\n",
      "[step: 262] loss: 20.978378295898438\n",
      "[step: 263] loss: 20.941329956054688\n",
      "[step: 264] loss: 20.90386962890625\n",
      "[step: 265] loss: 20.866065979003906\n",
      "[step: 266] loss: 20.827985763549805\n",
      "[step: 267] loss: 20.78972625732422\n",
      "[step: 268] loss: 20.751354217529297\n",
      "[step: 269] loss: 20.712966918945312\n",
      "[step: 270] loss: 20.674684524536133\n",
      "[step: 271] loss: 20.6365966796875\n",
      "[step: 272] loss: 20.598838806152344\n",
      "[step: 273] loss: 20.561500549316406\n",
      "[step: 274] loss: 20.52471160888672\n",
      "[step: 275] loss: 20.488582611083984\n",
      "[step: 276] loss: 20.453227996826172\n",
      "[step: 277] loss: 20.41872787475586\n",
      "[step: 278] loss: 20.385173797607422\n",
      "[step: 279] loss: 20.352622985839844\n",
      "[step: 280] loss: 20.321117401123047\n",
      "[step: 281] loss: 20.290672302246094\n",
      "[step: 282] loss: 20.26126480102539\n",
      "[step: 283] loss: 20.232866287231445\n",
      "[step: 284] loss: 20.205402374267578\n",
      "[step: 285] loss: 20.17878532409668\n",
      "[step: 286] loss: 20.152915954589844\n",
      "[step: 287] loss: 20.12765884399414\n",
      "[step: 288] loss: 20.1029052734375\n",
      "[step: 289] loss: 20.078523635864258\n",
      "[step: 290] loss: 20.05438995361328\n",
      "[step: 291] loss: 20.03040885925293\n",
      "[step: 292] loss: 20.006479263305664\n",
      "[step: 293] loss: 19.982524871826172\n",
      "[step: 294] loss: 19.95850372314453\n",
      "[step: 295] loss: 19.93438720703125\n",
      "[step: 296] loss: 19.91015625\n",
      "[step: 297] loss: 19.885822296142578\n",
      "[step: 298] loss: 19.861408233642578\n",
      "[step: 299] loss: 19.836925506591797\n",
      "[step: 300] loss: 19.812429428100586\n",
      "[step: 301] loss: 19.787960052490234\n",
      "[step: 302] loss: 19.7635440826416\n",
      "[step: 303] loss: 19.739234924316406\n",
      "[step: 304] loss: 19.71505355834961\n",
      "[step: 305] loss: 19.6910457611084\n",
      "[step: 306] loss: 19.667236328125\n",
      "[step: 307] loss: 19.643634796142578\n",
      "[step: 308] loss: 19.620267868041992\n",
      "[step: 309] loss: 19.597137451171875\n",
      "[step: 310] loss: 19.574264526367188\n",
      "[step: 311] loss: 19.55164909362793\n",
      "[step: 312] loss: 19.529298782348633\n",
      "[step: 313] loss: 19.507198333740234\n",
      "[step: 314] loss: 19.485366821289062\n",
      "[step: 315] loss: 19.463802337646484\n",
      "[step: 316] loss: 19.442508697509766\n",
      "[step: 317] loss: 19.421478271484375\n",
      "[step: 318] loss: 19.400718688964844\n",
      "[step: 319] loss: 19.380239486694336\n",
      "[step: 320] loss: 19.36003875732422\n",
      "[step: 321] loss: 19.340120315551758\n",
      "[step: 322] loss: 19.320499420166016\n",
      "[step: 323] loss: 19.30116844177246\n",
      "[step: 324] loss: 19.28213882446289\n",
      "[step: 325] loss: 19.263412475585938\n",
      "[step: 326] loss: 19.244998931884766\n",
      "[step: 327] loss: 19.226898193359375\n",
      "[step: 328] loss: 19.209110260009766\n",
      "[step: 329] loss: 19.191638946533203\n",
      "[step: 330] loss: 19.174484252929688\n",
      "[step: 331] loss: 19.157644271850586\n",
      "[step: 332] loss: 19.141124725341797\n",
      "[step: 333] loss: 19.12491226196289\n",
      "[step: 334] loss: 19.10900115966797\n",
      "[step: 335] loss: 19.093393325805664\n",
      "[step: 336] loss: 19.078083038330078\n",
      "[step: 337] loss: 19.063045501708984\n",
      "[step: 338] loss: 19.048294067382812\n",
      "[step: 339] loss: 19.033824920654297\n",
      "[step: 340] loss: 19.019609451293945\n",
      "[step: 341] loss: 19.005645751953125\n",
      "[step: 342] loss: 18.991939544677734\n",
      "[step: 343] loss: 18.978471755981445\n",
      "[step: 344] loss: 18.96523666381836\n",
      "[step: 345] loss: 18.952228546142578\n",
      "[step: 346] loss: 18.939441680908203\n",
      "[step: 347] loss: 18.926864624023438\n",
      "[step: 348] loss: 18.91449737548828\n",
      "[step: 349] loss: 18.902320861816406\n",
      "[step: 350] loss: 18.890335083007812\n",
      "[step: 351] loss: 18.87853240966797\n",
      "[step: 352] loss: 18.866910934448242\n",
      "[step: 353] loss: 18.855457305908203\n",
      "[step: 354] loss: 18.844167709350586\n",
      "[step: 355] loss: 18.833030700683594\n",
      "[step: 356] loss: 18.822036743164062\n",
      "[step: 357] loss: 18.811195373535156\n",
      "[step: 358] loss: 18.800479888916016\n",
      "[step: 359] loss: 18.789901733398438\n",
      "[step: 360] loss: 18.779438018798828\n",
      "[step: 361] loss: 18.769088745117188\n",
      "[step: 362] loss: 18.758846282958984\n",
      "[step: 363] loss: 18.748714447021484\n",
      "[step: 364] loss: 18.73867416381836\n",
      "[step: 365] loss: 18.728727340698242\n",
      "[step: 366] loss: 18.7188720703125\n",
      "[step: 367] loss: 18.709091186523438\n",
      "[step: 368] loss: 18.699390411376953\n",
      "[step: 369] loss: 18.68976593017578\n",
      "[step: 370] loss: 18.68020248413086\n",
      "[step: 371] loss: 18.67070770263672\n",
      "[step: 372] loss: 18.661270141601562\n",
      "[step: 373] loss: 18.65188980102539\n",
      "[step: 374] loss: 18.642547607421875\n",
      "[step: 375] loss: 18.633268356323242\n",
      "[step: 376] loss: 18.6240234375\n",
      "[step: 377] loss: 18.61481475830078\n",
      "[step: 378] loss: 18.60564422607422\n",
      "[step: 379] loss: 18.596508026123047\n",
      "[step: 380] loss: 18.587392807006836\n",
      "[step: 381] loss: 18.578292846679688\n",
      "[step: 382] loss: 18.5692195892334\n",
      "[step: 383] loss: 18.56015968322754\n",
      "[step: 384] loss: 18.551105499267578\n",
      "[step: 385] loss: 18.542064666748047\n",
      "[step: 386] loss: 18.533021926879883\n",
      "[step: 387] loss: 18.52397918701172\n",
      "[step: 388] loss: 18.51492691040039\n",
      "[step: 389] loss: 18.505878448486328\n",
      "[step: 390] loss: 18.496809005737305\n",
      "[step: 391] loss: 18.48773193359375\n",
      "[step: 392] loss: 18.4786376953125\n",
      "[step: 393] loss: 18.469520568847656\n",
      "[step: 394] loss: 18.46038055419922\n",
      "[step: 395] loss: 18.45121955871582\n",
      "[step: 396] loss: 18.44202995300293\n",
      "[step: 397] loss: 18.43281364440918\n",
      "[step: 398] loss: 18.423561096191406\n",
      "[step: 399] loss: 18.41427230834961\n",
      "[step: 400] loss: 18.404949188232422\n",
      "[step: 401] loss: 18.395591735839844\n",
      "[step: 402] loss: 18.38619041442871\n",
      "[step: 403] loss: 18.376747131347656\n",
      "[step: 404] loss: 18.367259979248047\n",
      "[step: 405] loss: 18.35773468017578\n",
      "[step: 406] loss: 18.34815788269043\n",
      "[step: 407] loss: 18.338537216186523\n",
      "[step: 408] loss: 18.328872680664062\n",
      "[step: 409] loss: 18.319154739379883\n",
      "[step: 410] loss: 18.309385299682617\n",
      "[step: 411] loss: 18.299571990966797\n",
      "[step: 412] loss: 18.289710998535156\n",
      "[step: 413] loss: 18.279796600341797\n",
      "[step: 414] loss: 18.26983642578125\n",
      "[step: 415] loss: 18.259830474853516\n",
      "[step: 416] loss: 18.249773025512695\n",
      "[step: 417] loss: 18.239665985107422\n",
      "[step: 418] loss: 18.22951889038086\n",
      "[step: 419] loss: 18.219318389892578\n",
      "[step: 420] loss: 18.20907211303711\n",
      "[step: 421] loss: 18.198780059814453\n",
      "[step: 422] loss: 18.188446044921875\n",
      "[step: 423] loss: 18.17807388305664\n",
      "[step: 424] loss: 18.16765785217285\n",
      "[step: 425] loss: 18.157188415527344\n",
      "[step: 426] loss: 18.146686553955078\n",
      "[step: 427] loss: 18.136144638061523\n",
      "[step: 428] loss: 18.125558853149414\n",
      "[step: 429] loss: 18.11493492126465\n",
      "[step: 430] loss: 18.104272842407227\n",
      "[step: 431] loss: 18.093564987182617\n",
      "[step: 432] loss: 18.082820892333984\n",
      "[step: 433] loss: 18.072032928466797\n",
      "[step: 434] loss: 18.06121063232422\n",
      "[step: 435] loss: 18.050352096557617\n",
      "[step: 436] loss: 18.039445877075195\n",
      "[step: 437] loss: 18.028505325317383\n",
      "[step: 438] loss: 18.017532348632812\n",
      "[step: 439] loss: 18.006511688232422\n",
      "[step: 440] loss: 17.99545669555664\n",
      "[step: 441] loss: 17.984371185302734\n",
      "[step: 442] loss: 17.973247528076172\n",
      "[step: 443] loss: 17.96209144592285\n",
      "[step: 444] loss: 17.950904846191406\n",
      "[step: 445] loss: 17.939685821533203\n",
      "[step: 446] loss: 17.928447723388672\n",
      "[step: 447] loss: 17.917171478271484\n",
      "[step: 448] loss: 17.905881881713867\n",
      "[step: 449] loss: 17.89456558227539\n",
      "[step: 450] loss: 17.88323211669922\n",
      "[step: 451] loss: 17.871883392333984\n",
      "[step: 452] loss: 17.860515594482422\n",
      "[step: 453] loss: 17.84913444519043\n",
      "[step: 454] loss: 17.837751388549805\n",
      "[step: 455] loss: 17.826353073120117\n",
      "[step: 456] loss: 17.81494903564453\n",
      "[step: 457] loss: 17.803543090820312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 458] loss: 17.79213523864746\n",
      "[step: 459] loss: 17.780731201171875\n",
      "[step: 460] loss: 17.769329071044922\n",
      "[step: 461] loss: 17.7579345703125\n",
      "[step: 462] loss: 17.74654769897461\n",
      "[step: 463] loss: 17.73517417907715\n",
      "[step: 464] loss: 17.72380828857422\n",
      "[step: 465] loss: 17.71246337890625\n",
      "[step: 466] loss: 17.70113754272461\n",
      "[step: 467] loss: 17.68982696533203\n",
      "[step: 468] loss: 17.678546905517578\n",
      "[step: 469] loss: 17.667278289794922\n",
      "[step: 470] loss: 17.65604591369629\n",
      "[step: 471] loss: 17.64483642578125\n",
      "[step: 472] loss: 17.633663177490234\n",
      "[step: 473] loss: 17.62251853942871\n",
      "[step: 474] loss: 17.611404418945312\n",
      "[step: 475] loss: 17.600322723388672\n",
      "[step: 476] loss: 17.589277267456055\n",
      "[step: 477] loss: 17.578275680541992\n",
      "[step: 478] loss: 17.567306518554688\n",
      "[step: 479] loss: 17.556373596191406\n",
      "[step: 480] loss: 17.545486450195312\n",
      "[step: 481] loss: 17.534635543823242\n",
      "[step: 482] loss: 17.52382469177246\n",
      "[step: 483] loss: 17.513057708740234\n",
      "[step: 484] loss: 17.502338409423828\n",
      "[step: 485] loss: 17.49169921875\n",
      "[step: 486] loss: 17.481306076049805\n",
      "[step: 487] loss: 17.471900939941406\n",
      "[step: 488] loss: 17.46514129638672\n",
      "[step: 489] loss: 17.461360931396484\n",
      "[step: 490] loss: 17.451452255249023\n",
      "[step: 491] loss: 17.433616638183594\n",
      "[step: 492] loss: 17.424909591674805\n",
      "[step: 493] loss: 17.419876098632812\n",
      "[step: 494] loss: 17.404918670654297\n",
      "[step: 495] loss: 17.39450454711914\n",
      "[step: 496] loss: 17.39052391052246\n",
      "[step: 497] loss: 17.377899169921875\n",
      "[step: 498] loss: 17.36644172668457\n",
      "[step: 499] loss: 17.36134147644043\n",
      "[step: 500] loss: 17.350540161132812\n",
      "[step: 501] loss: 17.339920043945312\n",
      "[step: 502] loss: 17.333934783935547\n",
      "[step: 503] loss: 17.323699951171875\n",
      "[step: 504] loss: 17.313356399536133\n",
      "[step: 505] loss: 17.306856155395508\n",
      "[step: 506] loss: 17.297565460205078\n",
      "[step: 507] loss: 17.28763771057129\n",
      "[step: 508] loss: 17.280519485473633\n",
      "[step: 509] loss: 17.271629333496094\n",
      "[step: 510] loss: 17.261863708496094\n",
      "[step: 511] loss: 17.25436782836914\n",
      "[step: 512] loss: 17.246074676513672\n",
      "[step: 513] loss: 17.236642837524414\n",
      "[step: 514] loss: 17.22871971130371\n",
      "[step: 515] loss: 17.220773696899414\n",
      "[step: 516] loss: 17.211624145507812\n",
      "[step: 517] loss: 17.2032470703125\n",
      "[step: 518] loss: 17.19542121887207\n",
      "[step: 519] loss: 17.186687469482422\n",
      "[step: 520] loss: 17.178024291992188\n",
      "[step: 521] loss: 17.17011260986328\n",
      "[step: 522] loss: 17.161880493164062\n",
      "[step: 523] loss: 17.153276443481445\n",
      "[step: 524] loss: 17.14527702331543\n",
      "[step: 525] loss: 17.13772964477539\n",
      "[step: 526] loss: 17.13031768798828\n",
      "[step: 527] loss: 17.12394142150879\n",
      "[step: 528] loss: 17.120010375976562\n",
      "[step: 529] loss: 17.117687225341797\n",
      "[step: 530] loss: 17.11448097229004\n",
      "[step: 531] loss: 17.10318946838379\n",
      "[step: 532] loss: 17.085693359375\n",
      "[step: 533] loss: 17.07178497314453\n",
      "[step: 534] loss: 17.06793975830078\n",
      "[step: 535] loss: 17.066539764404297\n",
      "[step: 536] loss: 17.057456970214844\n",
      "[step: 537] loss: 17.043418884277344\n",
      "[step: 538] loss: 17.03335952758789\n",
      "[step: 539] loss: 17.02914047241211\n",
      "[step: 540] loss: 17.024459838867188\n",
      "[step: 541] loss: 17.014442443847656\n",
      "[step: 542] loss: 17.002758026123047\n",
      "[step: 543] loss: 16.99500846862793\n",
      "[step: 544] loss: 16.990192413330078\n",
      "[step: 545] loss: 16.983327865600586\n",
      "[step: 546] loss: 16.973182678222656\n",
      "[step: 547] loss: 16.96341323852539\n",
      "[step: 548] loss: 16.95620346069336\n",
      "[step: 549] loss: 16.95005226135254\n",
      "[step: 550] loss: 16.942590713500977\n",
      "[step: 551] loss: 16.933273315429688\n",
      "[step: 552] loss: 16.923946380615234\n",
      "[step: 553] loss: 16.91610336303711\n",
      "[step: 554] loss: 16.909196853637695\n",
      "[step: 555] loss: 16.901777267456055\n",
      "[step: 556] loss: 16.893280029296875\n",
      "[step: 557] loss: 16.88427734375\n",
      "[step: 558] loss: 16.875497817993164\n",
      "[step: 559] loss: 16.867353439331055\n",
      "[step: 560] loss: 16.85968589782715\n",
      "[step: 561] loss: 16.851943969726562\n",
      "[step: 562] loss: 16.843751907348633\n",
      "[step: 563] loss: 16.835124969482422\n",
      "[step: 564] loss: 16.826282501220703\n",
      "[step: 565] loss: 16.81740951538086\n",
      "[step: 566] loss: 16.80863380432129\n",
      "[step: 567] loss: 16.80003547668457\n",
      "[step: 568] loss: 16.791549682617188\n",
      "[step: 569] loss: 16.783096313476562\n",
      "[step: 570] loss: 16.774669647216797\n",
      "[step: 571] loss: 16.76630401611328\n",
      "[step: 572] loss: 16.75806427001953\n",
      "[step: 573] loss: 16.750091552734375\n",
      "[step: 574] loss: 16.742597579956055\n",
      "[step: 575] loss: 16.73607635498047\n",
      "[step: 576] loss: 16.730735778808594\n",
      "[step: 577] loss: 16.727169036865234\n",
      "[step: 578] loss: 16.723241806030273\n",
      "[step: 579] loss: 16.717052459716797\n",
      "[step: 580] loss: 16.702960968017578\n",
      "[step: 581] loss: 16.684555053710938\n",
      "[step: 582] loss: 16.668394088745117\n",
      "[step: 583] loss: 16.660011291503906\n",
      "[step: 584] loss: 16.656997680664062\n",
      "[step: 585] loss: 16.65273094177246\n",
      "[step: 586] loss: 16.643238067626953\n",
      "[step: 587] loss: 16.62865447998047\n",
      "[step: 588] loss: 16.6148681640625\n",
      "[step: 589] loss: 16.60568618774414\n",
      "[step: 590] loss: 16.600051879882812\n",
      "[step: 591] loss: 16.594194412231445\n",
      "[step: 592] loss: 16.5849666595459\n",
      "[step: 593] loss: 16.572969436645508\n",
      "[step: 594] loss: 16.560543060302734\n",
      "[step: 595] loss: 16.550159454345703\n",
      "[step: 596] loss: 16.54206085205078\n",
      "[step: 597] loss: 16.534835815429688\n",
      "[step: 598] loss: 16.527009963989258\n",
      "[step: 599] loss: 16.517589569091797\n",
      "[step: 600] loss: 16.506916046142578\n",
      "[step: 601] loss: 16.4956111907959\n",
      "[step: 602] loss: 16.48464012145996\n",
      "[step: 603] loss: 16.47439956665039\n",
      "[step: 604] loss: 16.464893341064453\n",
      "[step: 605] loss: 16.455900192260742\n",
      "[step: 606] loss: 16.447195053100586\n",
      "[step: 607] loss: 16.438758850097656\n",
      "[step: 608] loss: 16.43058204650879\n",
      "[step: 609] loss: 16.42302703857422\n",
      "[step: 610] loss: 16.415977478027344\n",
      "[step: 611] loss: 16.410049438476562\n",
      "[step: 612] loss: 16.40397071838379\n",
      "[step: 613] loss: 16.397737503051758\n",
      "[step: 614] loss: 16.387561798095703\n",
      "[step: 615] loss: 16.374000549316406\n",
      "[step: 616] loss: 16.35668182373047\n",
      "[step: 617] loss: 16.340391159057617\n",
      "[step: 618] loss: 16.327926635742188\n",
      "[step: 619] loss: 16.319766998291016\n",
      "[step: 620] loss: 16.314208984375\n",
      "[step: 621] loss: 16.308883666992188\n",
      "[step: 622] loss: 16.302478790283203\n",
      "[step: 623] loss: 16.29299545288086\n",
      "[step: 624] loss: 16.281200408935547\n",
      "[step: 625] loss: 16.267194747924805\n",
      "[step: 626] loss: 16.253379821777344\n",
      "[step: 627] loss: 16.240985870361328\n",
      "[step: 628] loss: 16.230609893798828\n",
      "[step: 629] loss: 16.221906661987305\n",
      "[step: 630] loss: 16.214303970336914\n",
      "[step: 631] loss: 16.207595825195312\n",
      "[step: 632] loss: 16.201553344726562\n",
      "[step: 633] loss: 16.196744918823242\n",
      "[step: 634] loss: 16.19194793701172\n",
      "[step: 635] loss: 16.187419891357422\n",
      "[step: 636] loss: 16.17881202697754\n",
      "[step: 637] loss: 16.166717529296875\n",
      "[step: 638] loss: 16.149215698242188\n",
      "[step: 639] loss: 16.13154411315918\n",
      "[step: 640] loss: 16.11654281616211\n",
      "[step: 641] loss: 16.105979919433594\n",
      "[step: 642] loss: 16.099109649658203\n",
      "[step: 643] loss: 16.094282150268555\n",
      "[step: 644] loss: 16.09023094177246\n",
      "[step: 645] loss: 16.084667205810547\n",
      "[step: 646] loss: 16.077285766601562\n",
      "[step: 647] loss: 16.06561279296875\n",
      "[step: 648] loss: 16.051803588867188\n",
      "[step: 649] loss: 16.03672981262207\n",
      "[step: 650] loss: 16.023216247558594\n",
      "[step: 651] loss: 16.011783599853516\n",
      "[step: 652] loss: 16.00198745727539\n",
      "[step: 653] loss: 15.993014335632324\n",
      "[step: 654] loss: 15.984565734863281\n",
      "[step: 655] loss: 15.977145195007324\n",
      "[step: 656] loss: 15.971548080444336\n",
      "[step: 657] loss: 15.969552993774414\n",
      "[step: 658] loss: 15.972004890441895\n",
      "[step: 659] loss: 15.981196403503418\n",
      "[step: 660] loss: 15.988719940185547\n",
      "[step: 661] loss: 15.990673065185547\n",
      "[step: 662] loss: 15.966048240661621\n",
      "[step: 663] loss: 15.930115699768066\n",
      "[step: 664] loss: 15.896790504455566\n",
      "[step: 665] loss: 15.886895179748535\n",
      "[step: 666] loss: 15.897899627685547\n",
      "[step: 667] loss: 15.903728485107422\n",
      "[step: 668] loss: 15.887234687805176\n",
      "[step: 669] loss: 15.855088233947754\n",
      "[step: 670] loss: 15.837925910949707\n",
      "[step: 671] loss: 15.838812828063965\n",
      "[step: 672] loss: 15.83967399597168\n",
      "[step: 673] loss: 15.832982063293457\n",
      "[step: 674] loss: 15.819303512573242\n",
      "[step: 675] loss: 15.802005767822266\n",
      "[step: 676] loss: 15.785256385803223\n",
      "[step: 677] loss: 15.777563095092773\n",
      "[step: 678] loss: 15.777276039123535\n",
      "[step: 679] loss: 15.773746490478516\n",
      "[step: 680] loss: 15.763774871826172\n",
      "[step: 681] loss: 15.75108528137207\n",
      "[step: 682] loss: 15.738597869873047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 683] loss: 15.724625587463379\n",
      "[step: 684] loss: 15.712298393249512\n",
      "[step: 685] loss: 15.704421997070312\n",
      "[step: 686] loss: 15.698469161987305\n",
      "[step: 687] loss: 15.692054748535156\n",
      "[step: 688] loss: 15.68692398071289\n",
      "[step: 689] loss: 15.685925483703613\n",
      "[step: 690] loss: 15.687994003295898\n",
      "[step: 691] loss: 15.695728302001953\n",
      "[step: 692] loss: 15.706575393676758\n",
      "[step: 693] loss: 15.72348403930664\n",
      "[step: 694] loss: 15.716300010681152\n",
      "[step: 695] loss: 15.686100959777832\n",
      "[step: 696] loss: 15.631930351257324\n",
      "[step: 697] loss: 15.596626281738281\n",
      "[step: 698] loss: 15.594785690307617\n",
      "[step: 699] loss: 15.612104415893555\n",
      "[step: 700] loss: 15.624363899230957\n",
      "[step: 701] loss: 15.60750961303711\n",
      "[step: 702] loss: 15.57394027709961\n",
      "[step: 703] loss: 15.544515609741211\n",
      "[step: 704] loss: 15.536031723022461\n",
      "[step: 705] loss: 15.54228401184082\n",
      "[step: 706] loss: 15.547830581665039\n",
      "[step: 707] loss: 15.543598175048828\n",
      "[step: 708] loss: 15.523504257202148\n",
      "[step: 709] loss: 15.499074935913086\n",
      "[step: 710] loss: 15.479860305786133\n",
      "[step: 711] loss: 15.47052001953125\n",
      "[step: 712] loss: 15.468356132507324\n",
      "[step: 713] loss: 15.468894958496094\n",
      "[step: 714] loss: 15.46946907043457\n",
      "[step: 715] loss: 15.465486526489258\n",
      "[step: 716] loss: 15.45869255065918\n",
      "[step: 717] loss: 15.446271896362305\n",
      "[step: 718] loss: 15.432401657104492\n",
      "[step: 719] loss: 15.415375709533691\n",
      "[step: 720] loss: 15.399187088012695\n",
      "[step: 721] loss: 15.383953094482422\n",
      "[step: 722] loss: 15.370530128479004\n",
      "[step: 723] loss: 15.358264923095703\n",
      "[step: 724] loss: 15.347113609313965\n",
      "[step: 725] loss: 15.336719512939453\n",
      "[step: 726] loss: 15.326676368713379\n",
      "[step: 727] loss: 15.317010879516602\n",
      "[step: 728] loss: 15.308496475219727\n",
      "[step: 729] loss: 15.302936553955078\n",
      "[step: 730] loss: 15.30610466003418\n",
      "[step: 731] loss: 15.333986282348633\n",
      "[step: 732] loss: 15.438396453857422\n",
      "[step: 733] loss: 15.67566967010498\n",
      "[step: 734] loss: 16.031896591186523\n",
      "[step: 735] loss: 15.702046394348145\n",
      "[step: 736] loss: 15.24803352355957\n",
      "[step: 737] loss: 15.443873405456543\n",
      "[step: 738] loss: 15.568597793579102\n",
      "[step: 739] loss: 15.259687423706055\n",
      "[step: 740] loss: 15.30484676361084\n",
      "[step: 741] loss: 15.435215950012207\n",
      "[step: 742] loss: 15.220775604248047\n",
      "[step: 743] loss: 15.256425857543945\n",
      "[step: 744] loss: 15.33426570892334\n",
      "[step: 745] loss: 15.176824569702148\n",
      "[step: 746] loss: 15.225763320922852\n",
      "[step: 747] loss: 15.252618789672852\n",
      "[step: 748] loss: 15.141839981079102\n",
      "[step: 749] loss: 15.194887161254883\n",
      "[step: 750] loss: 15.184435844421387\n",
      "[step: 751] loss: 15.115654945373535\n",
      "[step: 752] loss: 15.160984992980957\n",
      "[step: 753] loss: 15.131988525390625\n",
      "[step: 754] loss: 15.09292221069336\n",
      "[step: 755] loss: 15.124958992004395\n",
      "[step: 756] loss: 15.09127426147461\n",
      "[step: 757] loss: 15.06909465789795\n",
      "[step: 758] loss: 15.088934898376465\n",
      "[step: 759] loss: 15.05817985534668\n",
      "[step: 760] loss: 15.041881561279297\n",
      "[step: 761] loss: 15.053793907165527\n",
      "[step: 762] loss: 15.029486656188965\n",
      "[step: 763] loss: 15.012334823608398\n",
      "[step: 764] loss: 15.018991470336914\n",
      "[step: 765] loss: 15.00208854675293\n",
      "[step: 766] loss: 14.982521057128906\n",
      "[step: 767] loss: 14.983454704284668\n",
      "[step: 768] loss: 14.974568367004395\n",
      "[step: 769] loss: 14.95455551147461\n",
      "[step: 770] loss: 14.947359085083008\n",
      "[step: 771] loss: 14.944032669067383\n",
      "[step: 772] loss: 14.929215431213379\n",
      "[step: 773] loss: 14.914050102233887\n",
      "[step: 774] loss: 14.908134460449219\n",
      "[step: 775] loss: 14.901514053344727\n",
      "[step: 776] loss: 14.887292861938477\n",
      "[step: 777] loss: 14.873250961303711\n",
      "[step: 778] loss: 14.864974975585938\n",
      "[step: 779] loss: 14.857873916625977\n",
      "[step: 780] loss: 14.846826553344727\n",
      "[step: 781] loss: 14.832942962646484\n",
      "[step: 782] loss: 14.82050895690918\n",
      "[step: 783] loss: 14.810880661010742\n",
      "[step: 784] loss: 14.802386283874512\n",
      "[step: 785] loss: 14.792966842651367\n",
      "[step: 786] loss: 14.781806945800781\n",
      "[step: 787] loss: 14.769609451293945\n",
      "[step: 788] loss: 14.757094383239746\n",
      "[step: 789] loss: 14.74480152130127\n",
      "[step: 790] loss: 14.732832908630371\n",
      "[step: 791] loss: 14.721113204956055\n",
      "[step: 792] loss: 14.709498405456543\n",
      "[step: 793] loss: 14.697885513305664\n",
      "[step: 794] loss: 14.68624496459961\n",
      "[step: 795] loss: 14.674560546875\n",
      "[step: 796] loss: 14.662843704223633\n",
      "[step: 797] loss: 14.651191711425781\n",
      "[step: 798] loss: 14.639984130859375\n",
      "[step: 799] loss: 14.630666732788086\n",
      "[step: 800] loss: 14.629264831542969\n",
      "[step: 801] loss: 14.66278076171875\n",
      "[step: 802] loss: 14.845102310180664\n",
      "[step: 803] loss: 15.637187957763672\n",
      "[step: 804] loss: 17.121376037597656\n",
      "[step: 805] loss: 17.27385711669922\n",
      "[step: 806] loss: 14.620182991027832\n",
      "[step: 807] loss: 16.972068786621094\n",
      "[step: 808] loss: 15.44941520690918\n",
      "[step: 809] loss: 15.708078384399414\n",
      "[step: 810] loss: 15.545642852783203\n",
      "[step: 811] loss: 15.085235595703125\n",
      "[step: 812] loss: 15.446134567260742\n",
      "[step: 813] loss: 14.974281311035156\n",
      "[step: 814] loss: 15.213566780090332\n",
      "[step: 815] loss: 14.869747161865234\n",
      "[step: 816] loss: 15.01632308959961\n",
      "[step: 817] loss: 14.89693832397461\n",
      "[step: 818] loss: 14.808303833007812\n",
      "[step: 819] loss: 14.890432357788086\n",
      "[step: 820] loss: 14.674623489379883\n",
      "[step: 821] loss: 14.887166023254395\n",
      "[step: 822] loss: 14.55772876739502\n",
      "[step: 823] loss: 14.860166549682617\n",
      "[step: 824] loss: 14.52224349975586\n",
      "[step: 825] loss: 14.784162521362305\n",
      "[step: 826] loss: 14.531427383422852\n",
      "[step: 827] loss: 14.690366744995117\n",
      "[step: 828] loss: 14.555070877075195\n",
      "[step: 829] loss: 14.59444808959961\n",
      "[step: 830] loss: 14.582060813903809\n",
      "[step: 831] loss: 14.519289016723633\n",
      "[step: 832] loss: 14.586448669433594\n",
      "[step: 833] loss: 14.477973937988281\n",
      "[step: 834] loss: 14.56501579284668\n",
      "[step: 835] loss: 14.457172393798828\n",
      "[step: 836] loss: 14.529613494873047\n",
      "[step: 837] loss: 14.450258255004883\n",
      "[step: 838] loss: 14.48785400390625\n",
      "[step: 839] loss: 14.448660850524902\n",
      "[step: 840] loss: 14.44986343383789\n",
      "[step: 841] loss: 14.442782402038574\n",
      "[step: 842] loss: 14.41895866394043\n",
      "[step: 843] loss: 14.431089401245117\n",
      "[step: 844] loss: 14.394332885742188\n",
      "[step: 845] loss: 14.414118766784668\n",
      "[step: 846] loss: 14.375429153442383\n",
      "[step: 847] loss: 14.393627166748047\n",
      "[step: 848] loss: 14.359472274780273\n",
      "[step: 849] loss: 14.372886657714844\n",
      "[step: 850] loss: 14.34437370300293\n",
      "[step: 851] loss: 14.351635932922363\n",
      "[step: 852] loss: 14.328994750976562\n",
      "[step: 853] loss: 14.331400871276855\n",
      "[step: 854] loss: 14.313032150268555\n",
      "[step: 855] loss: 14.31202220916748\n",
      "[step: 856] loss: 14.296804428100586\n",
      "[step: 857] loss: 14.293563842773438\n",
      "[step: 858] loss: 14.279870986938477\n",
      "[step: 859] loss: 14.275381088256836\n",
      "[step: 860] loss: 14.26253604888916\n",
      "[step: 861] loss: 14.257645606994629\n",
      "[step: 862] loss: 14.245037078857422\n",
      "[step: 863] loss: 14.239985466003418\n",
      "[step: 864] loss: 14.227483749389648\n",
      "[step: 865] loss: 14.222339630126953\n",
      "[step: 866] loss: 14.209911346435547\n",
      "[step: 867] loss: 14.20443058013916\n",
      "[step: 868] loss: 14.192438125610352\n",
      "[step: 869] loss: 14.186368942260742\n",
      "[step: 870] loss: 14.17512321472168\n",
      "[step: 871] loss: 14.167996406555176\n",
      "[step: 872] loss: 14.157883644104004\n",
      "[step: 873] loss: 14.149457931518555\n",
      "[step: 874] loss: 14.140562057495117\n",
      "[step: 875] loss: 14.130838394165039\n",
      "[step: 876] loss: 14.122943878173828\n",
      "[step: 877] loss: 14.112430572509766\n",
      "[step: 878] loss: 14.104818344116211\n",
      "[step: 879] loss: 14.094353675842285\n",
      "[step: 880] loss: 14.086153030395508\n",
      "[step: 881] loss: 14.076457977294922\n",
      "[step: 882] loss: 14.0671968460083\n",
      "[step: 883] loss: 14.058345794677734\n",
      "[step: 884] loss: 14.048423767089844\n",
      "[step: 885] loss: 14.039636611938477\n",
      "[step: 886] loss: 14.02995491027832\n",
      "[step: 887] loss: 14.020462036132812\n",
      "[step: 888] loss: 14.011368751525879\n",
      "[step: 889] loss: 14.001367568969727\n",
      "[step: 890] loss: 13.992208480834961\n",
      "[step: 891] loss: 13.982503890991211\n",
      "[step: 892] loss: 13.972684860229492\n",
      "[step: 893] loss: 13.963329315185547\n",
      "[step: 894] loss: 13.95335865020752\n",
      "[step: 895] loss: 13.943593978881836\n",
      "[step: 896] loss: 13.933950424194336\n",
      "[step: 897] loss: 13.923892974853516\n",
      "[step: 898] loss: 13.913978576660156\n",
      "[step: 899] loss: 13.904172897338867\n",
      "[step: 900] loss: 13.89397144317627\n",
      "[step: 901] loss: 13.8839111328125\n",
      "[step: 902] loss: 13.873887062072754\n",
      "[step: 903] loss: 13.863605499267578\n",
      "[step: 904] loss: 13.853304862976074\n",
      "[step: 905] loss: 13.843093872070312\n",
      "[step: 906] loss: 13.83272933959961\n",
      "[step: 907] loss: 13.822205543518066\n",
      "[step: 908] loss: 13.811752319335938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 909] loss: 13.801251411437988\n",
      "[step: 910] loss: 13.790608406066895\n",
      "[step: 911] loss: 13.779869079589844\n",
      "[step: 912] loss: 13.769129753112793\n",
      "[step: 913] loss: 13.758362770080566\n",
      "[step: 914] loss: 13.747472763061523\n",
      "[step: 915] loss: 13.736480712890625\n",
      "[step: 916] loss: 13.725418090820312\n",
      "[step: 917] loss: 13.71432876586914\n",
      "[step: 918] loss: 13.703178405761719\n",
      "[step: 919] loss: 13.691937446594238\n",
      "[step: 920] loss: 13.680601119995117\n",
      "[step: 921] loss: 13.669164657592773\n",
      "[step: 922] loss: 13.657658576965332\n",
      "[step: 923] loss: 13.646068572998047\n",
      "[step: 924] loss: 13.634395599365234\n",
      "[step: 925] loss: 13.622650146484375\n",
      "[step: 926] loss: 13.610811233520508\n",
      "[step: 927] loss: 13.598896026611328\n",
      "[step: 928] loss: 13.58688735961914\n",
      "[step: 929] loss: 13.574790954589844\n",
      "[step: 930] loss: 13.562612533569336\n",
      "[step: 931] loss: 13.55037784576416\n",
      "[step: 932] loss: 13.538172721862793\n",
      "[step: 933] loss: 13.526342391967773\n",
      "[step: 934] loss: 13.51634407043457\n",
      "[step: 935] loss: 13.514627456665039\n",
      "[step: 936] loss: 13.552292823791504\n",
      "[step: 937] loss: 13.774375915527344\n",
      "[step: 938] loss: 14.916816711425781\n",
      "[step: 939] loss: 18.242769241333008\n",
      "[step: 940] loss: 22.481525421142578\n",
      "[step: 941] loss: 13.831448554992676\n",
      "[step: 942] loss: 20.388418197631836\n",
      "[step: 943] loss: 16.002513885498047\n",
      "[step: 944] loss: 18.111309051513672\n",
      "[step: 945] loss: 14.455082893371582\n",
      "[step: 946] loss: 17.74585723876953\n",
      "[step: 947] loss: 13.968965530395508\n",
      "[step: 948] loss: 16.939645767211914\n",
      "[step: 949] loss: 13.971918106079102\n",
      "[step: 950] loss: 15.570699691772461\n",
      "[step: 951] loss: 14.773971557617188\n",
      "[step: 952] loss: 14.149169921875\n",
      "[step: 953] loss: 15.308216094970703\n",
      "[step: 954] loss: 13.797698974609375\n",
      "[step: 955] loss: 14.544021606445312\n",
      "[step: 956] loss: 14.438252449035645\n",
      "[step: 957] loss: 13.754706382751465\n",
      "[step: 958] loss: 14.489967346191406\n",
      "[step: 959] loss: 13.98155403137207\n",
      "[step: 960] loss: 13.813170433044434\n",
      "[step: 961] loss: 14.271801948547363\n",
      "[step: 962] loss: 13.786908149719238\n",
      "[step: 963] loss: 13.843772888183594\n",
      "[step: 964] loss: 14.071084976196289\n",
      "[step: 965] loss: 13.698992729187012\n",
      "[step: 966] loss: 13.808308601379395\n",
      "[step: 967] loss: 13.92711353302002\n",
      "[step: 968] loss: 13.635223388671875\n",
      "[step: 969] loss: 13.780794143676758\n",
      "[step: 970] loss: 13.800780296325684\n",
      "[step: 971] loss: 13.598991394042969\n",
      "[step: 972] loss: 13.737228393554688\n",
      "[step: 973] loss: 13.701601028442383\n",
      "[step: 974] loss: 13.571487426757812\n",
      "[step: 975] loss: 13.689172744750977\n",
      "[step: 976] loss: 13.628129959106445\n",
      "[step: 977] loss: 13.557043075561523\n",
      "[step: 978] loss: 13.64571762084961\n",
      "[step: 979] loss: 13.569292068481445\n",
      "[step: 980] loss: 13.542455673217773\n",
      "[step: 981] loss: 13.596972465515137\n",
      "[step: 982] loss: 13.525020599365234\n",
      "[step: 983] loss: 13.523614883422852\n",
      "[step: 984] loss: 13.549927711486816\n",
      "[step: 985] loss: 13.488157272338867\n",
      "[step: 986] loss: 13.504841804504395\n",
      "[step: 987] loss: 13.50651741027832\n",
      "[step: 988] loss: 13.460951805114746\n",
      "[step: 989] loss: 13.481874465942383\n",
      "[step: 990] loss: 13.464978218078613\n",
      "[step: 991] loss: 13.438655853271484\n",
      "[step: 992] loss: 13.45483684539795\n",
      "[step: 993] loss: 13.429010391235352\n",
      "[step: 994] loss: 13.418893814086914\n",
      "[step: 995] loss: 13.424052238464355\n",
      "[step: 996] loss: 13.398541450500488\n",
      "[step: 997] loss: 13.39869499206543\n",
      "[step: 998] loss: 13.39142894744873\n",
      "[step: 999] loss: 13.373296737670898\n",
      "[step: 1000] loss: 13.374654769897461\n",
      "[step: 1001] loss: 13.360084533691406\n",
      "[step: 1002] loss: 13.350666999816895\n",
      "[step: 1003] loss: 13.347111701965332\n",
      "[step: 1004] loss: 13.332100868225098\n",
      "[step: 1005] loss: 13.32733154296875\n",
      "[step: 1006] loss: 13.31848430633545\n",
      "[step: 1007] loss: 13.307005882263184\n",
      "[step: 1008] loss: 13.302106857299805\n",
      "[step: 1009] loss: 13.290824890136719\n",
      "[step: 1010] loss: 13.282586097717285\n",
      "[step: 1011] loss: 13.275559425354004\n",
      "[step: 1012] loss: 13.264522552490234\n",
      "[step: 1013] loss: 13.257749557495117\n",
      "[step: 1014] loss: 13.248620986938477\n",
      "[step: 1015] loss: 13.239089012145996\n",
      "[step: 1016] loss: 13.232120513916016\n",
      "[step: 1017] loss: 13.221855163574219\n",
      "[step: 1018] loss: 13.214111328125\n",
      "[step: 1019] loss: 13.205488204956055\n",
      "[step: 1020] loss: 13.195999145507812\n",
      "[step: 1021] loss: 13.18856143951416\n",
      "[step: 1022] loss: 13.178682327270508\n",
      "[step: 1023] loss: 13.17074966430664\n",
      "[step: 1024] loss: 13.161815643310547\n",
      "[step: 1025] loss: 13.152738571166992\n",
      "[step: 1026] loss: 13.144667625427246\n",
      "[step: 1027] loss: 13.135120391845703\n",
      "[step: 1028] loss: 13.12692642211914\n",
      "[step: 1029] loss: 13.117792129516602\n",
      "[step: 1030] loss: 13.108927726745605\n",
      "[step: 1031] loss: 13.100311279296875\n",
      "[step: 1032] loss: 13.09103775024414\n",
      "[step: 1033] loss: 13.082448959350586\n",
      "[step: 1034] loss: 13.073258399963379\n",
      "[step: 1035] loss: 13.06435775756836\n",
      "[step: 1036] loss: 13.055414199829102\n",
      "[step: 1037] loss: 13.046194076538086\n",
      "[step: 1038] loss: 13.037341117858887\n",
      "[step: 1039] loss: 13.028003692626953\n",
      "[step: 1040] loss: 13.019049644470215\n",
      "[step: 1041] loss: 13.00976276397705\n",
      "[step: 1042] loss: 13.000589370727539\n",
      "[step: 1043] loss: 12.99137020111084\n",
      "[step: 1044] loss: 12.982020378112793\n",
      "[step: 1045] loss: 12.972801208496094\n",
      "[step: 1046] loss: 12.963356018066406\n",
      "[step: 1047] loss: 12.954034805297852\n",
      "[step: 1048] loss: 12.94456958770752\n",
      "[step: 1049] loss: 12.93510913848877\n",
      "[step: 1050] loss: 12.925626754760742\n",
      "[step: 1051] loss: 12.916040420532227\n",
      "[step: 1052] loss: 12.906505584716797\n",
      "[step: 1053] loss: 12.896820068359375\n",
      "[step: 1054] loss: 12.887208938598633\n",
      "[step: 1055] loss: 12.877452850341797\n",
      "[step: 1056] loss: 12.86772632598877\n",
      "[step: 1057] loss: 12.857917785644531\n",
      "[step: 1058] loss: 12.848075866699219\n",
      "[step: 1059] loss: 12.838205337524414\n",
      "[step: 1060] loss: 12.828255653381348\n",
      "[step: 1061] loss: 12.818302154541016\n",
      "[step: 1062] loss: 12.808263778686523\n",
      "[step: 1063] loss: 12.7982177734375\n",
      "[step: 1064] loss: 12.788087844848633\n",
      "[step: 1065] loss: 12.777939796447754\n",
      "[step: 1066] loss: 12.767720222473145\n",
      "[step: 1067] loss: 12.75747013092041\n",
      "[step: 1068] loss: 12.747167587280273\n",
      "[step: 1069] loss: 12.73681640625\n",
      "[step: 1070] loss: 12.726418495178223\n",
      "[step: 1071] loss: 12.715970039367676\n",
      "[step: 1072] loss: 12.705469131469727\n",
      "[step: 1073] loss: 12.694915771484375\n",
      "[step: 1074] loss: 12.684316635131836\n",
      "[step: 1075] loss: 12.673664093017578\n",
      "[step: 1076] loss: 12.6629638671875\n",
      "[step: 1077] loss: 12.652205467224121\n",
      "[step: 1078] loss: 12.641403198242188\n",
      "[step: 1079] loss: 12.630545616149902\n",
      "[step: 1080] loss: 12.619634628295898\n",
      "[step: 1081] loss: 12.608673095703125\n",
      "[step: 1082] loss: 12.59765911102295\n",
      "[step: 1083] loss: 12.586585998535156\n",
      "[step: 1084] loss: 12.575467109680176\n",
      "[step: 1085] loss: 12.564292907714844\n",
      "[step: 1086] loss: 12.55306625366211\n",
      "[step: 1087] loss: 12.541780471801758\n",
      "[step: 1088] loss: 12.530439376831055\n",
      "[step: 1089] loss: 12.519046783447266\n",
      "[step: 1090] loss: 12.507604598999023\n",
      "[step: 1091] loss: 12.496097564697266\n",
      "[step: 1092] loss: 12.484539031982422\n",
      "[step: 1093] loss: 12.47293472290039\n",
      "[step: 1094] loss: 12.461261749267578\n",
      "[step: 1095] loss: 12.449541091918945\n",
      "[step: 1096] loss: 12.437765121459961\n",
      "[step: 1097] loss: 12.425931930541992\n",
      "[step: 1098] loss: 12.414041519165039\n",
      "[step: 1099] loss: 12.402098655700684\n",
      "[step: 1100] loss: 12.390098571777344\n",
      "[step: 1101] loss: 12.378040313720703\n",
      "[step: 1102] loss: 12.365930557250977\n",
      "[step: 1103] loss: 12.35376262664795\n",
      "[step: 1104] loss: 12.341541290283203\n",
      "[step: 1105] loss: 12.329257011413574\n",
      "[step: 1106] loss: 12.316926956176758\n",
      "[step: 1107] loss: 12.30453872680664\n",
      "[step: 1108] loss: 12.292084693908691\n",
      "[step: 1109] loss: 12.279590606689453\n",
      "[step: 1110] loss: 12.267035484313965\n",
      "[step: 1111] loss: 12.254426956176758\n",
      "[step: 1112] loss: 12.241764068603516\n",
      "[step: 1113] loss: 12.22906494140625\n",
      "[step: 1114] loss: 12.216360092163086\n",
      "[step: 1115] loss: 12.203770637512207\n",
      "[step: 1116] loss: 12.191798210144043\n",
      "[step: 1117] loss: 12.182584762573242\n",
      "[step: 1118] loss: 12.186017990112305\n",
      "[step: 1119] loss: 12.248326301574707\n",
      "[step: 1120] loss: 12.616783142089844\n",
      "[step: 1121] loss: 14.30871295928955\n",
      "[step: 1122] loss: 21.723047256469727\n",
      "[step: 1123] loss: 21.78314208984375\n",
      "[step: 1124] loss: 15.849212646484375\n",
      "[step: 1125] loss: 15.218876838684082\n",
      "[step: 1126] loss: 16.79806900024414\n",
      "[step: 1127] loss: 12.76634693145752\n",
      "[step: 1128] loss: 15.7670259475708\n",
      "[step: 1129] loss: 12.659881591796875\n",
      "[step: 1130] loss: 15.426582336425781\n",
      "[step: 1131] loss: 12.460031509399414\n",
      "[step: 1132] loss: 14.440723419189453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1133] loss: 12.526165008544922\n",
      "[step: 1134] loss: 14.11707592010498\n",
      "[step: 1135] loss: 12.562088966369629\n",
      "[step: 1136] loss: 13.474435806274414\n",
      "[step: 1137] loss: 12.718332290649414\n",
      "[step: 1138] loss: 12.995136260986328\n",
      "[step: 1139] loss: 12.902006149291992\n",
      "[step: 1140] loss: 12.694225311279297\n",
      "[step: 1141] loss: 12.985044479370117\n",
      "[step: 1142] loss: 12.437749862670898\n",
      "[step: 1143] loss: 12.952856063842773\n",
      "[step: 1144] loss: 12.44009780883789\n",
      "[step: 1145] loss: 12.755760192871094\n",
      "[step: 1146] loss: 12.511943817138672\n",
      "[step: 1147] loss: 12.529129028320312\n",
      "[step: 1148] loss: 12.56035041809082\n",
      "[step: 1149] loss: 12.40972900390625\n",
      "[step: 1150] loss: 12.55396842956543\n",
      "[step: 1151] loss: 12.332295417785645\n",
      "[step: 1152] loss: 12.492304801940918\n",
      "[step: 1153] loss: 12.319265365600586\n",
      "[step: 1154] loss: 12.41849422454834\n",
      "[step: 1155] loss: 12.335514068603516\n",
      "[step: 1156] loss: 12.336135864257812\n",
      "[step: 1157] loss: 12.332483291625977\n",
      "[step: 1158] loss: 12.276994705200195\n",
      "[step: 1159] loss: 12.321913719177246\n",
      "[step: 1160] loss: 12.233205795288086\n",
      "[step: 1161] loss: 12.293764114379883\n",
      "[step: 1162] loss: 12.201367378234863\n",
      "[step: 1163] loss: 12.259323120117188\n",
      "[step: 1164] loss: 12.187236785888672\n",
      "[step: 1165] loss: 12.222649574279785\n",
      "[step: 1166] loss: 12.172977447509766\n",
      "[step: 1167] loss: 12.183515548706055\n",
      "[step: 1168] loss: 12.157188415527344\n",
      "[step: 1169] loss: 12.150896072387695\n",
      "[step: 1170] loss: 12.139514923095703\n",
      "[step: 1171] loss: 12.12425422668457\n",
      "[step: 1172] loss: 12.11706256866455\n",
      "[step: 1173] loss: 12.100915908813477\n",
      "[step: 1174] loss: 12.093090057373047\n",
      "[step: 1175] loss: 12.080126762390137\n",
      "[step: 1176] loss: 12.068714141845703\n",
      "[step: 1177] loss: 12.058979988098145\n",
      "[step: 1178] loss: 12.045160293579102\n",
      "[step: 1179] loss: 12.038887977600098\n",
      "[step: 1180] loss: 12.023237228393555\n",
      "[step: 1181] loss: 12.01763916015625\n",
      "[step: 1182] loss: 12.001100540161133\n",
      "[step: 1183] loss: 11.996673583984375\n",
      "[step: 1184] loss: 11.980541229248047\n",
      "[step: 1185] loss: 11.976483345031738\n",
      "[step: 1186] loss: 11.96008586883545\n",
      "[step: 1187] loss: 11.955180168151855\n",
      "[step: 1188] loss: 11.940156936645508\n",
      "[step: 1189] loss: 11.934150695800781\n",
      "[step: 1190] loss: 11.921266555786133\n",
      "[step: 1191] loss: 11.912408828735352\n",
      "[step: 1192] loss: 11.902466773986816\n",
      "[step: 1193] loss: 11.891103744506836\n",
      "[step: 1194] loss: 11.883415222167969\n",
      "[step: 1195] loss: 11.870589256286621\n",
      "[step: 1196] loss: 11.862936973571777\n",
      "[step: 1197] loss: 11.851335525512695\n",
      "[step: 1198] loss: 11.841896057128906\n",
      "[step: 1199] loss: 11.832708358764648\n",
      "[step: 1200] loss: 11.821025848388672\n",
      "[step: 1201] loss: 11.81295394897461\n",
      "[step: 1202] loss: 11.801448822021484\n",
      "[step: 1203] loss: 11.792230606079102\n",
      "[step: 1204] loss: 11.782512664794922\n",
      "[step: 1205] loss: 11.771543502807617\n",
      "[step: 1206] loss: 11.762592315673828\n",
      "[step: 1207] loss: 11.752148628234863\n",
      "[step: 1208] loss: 11.741935729980469\n",
      "[step: 1209] loss: 11.732654571533203\n",
      "[step: 1210] loss: 11.722049713134766\n",
      "[step: 1211] loss: 11.712159156799316\n",
      "[step: 1212] loss: 11.70269775390625\n",
      "[step: 1213] loss: 11.692100524902344\n",
      "[step: 1214] loss: 11.6823148727417\n",
      "[step: 1215] loss: 11.67260456085205\n",
      "[step: 1216] loss: 11.662266731262207\n",
      "[step: 1217] loss: 11.65228271484375\n",
      "[step: 1218] loss: 11.642616271972656\n",
      "[step: 1219] loss: 11.632315635681152\n",
      "[step: 1220] loss: 11.622259140014648\n",
      "[step: 1221] loss: 11.612480163574219\n",
      "[step: 1222] loss: 11.60245132446289\n",
      "[step: 1223] loss: 11.592203140258789\n",
      "[step: 1224] loss: 11.58230209350586\n",
      "[step: 1225] loss: 11.572412490844727\n",
      "[step: 1226] loss: 11.56230640411377\n",
      "[step: 1227] loss: 11.552142143249512\n",
      "[step: 1228] loss: 11.542207717895508\n",
      "[step: 1229] loss: 11.532264709472656\n",
      "[step: 1230] loss: 11.522195816040039\n",
      "[step: 1231] loss: 11.512044906616211\n",
      "[step: 1232] loss: 11.5020170211792\n",
      "[step: 1233] loss: 11.492048263549805\n",
      "[step: 1234] loss: 11.482074737548828\n",
      "[step: 1235] loss: 11.47199821472168\n",
      "[step: 1236] loss: 11.461907386779785\n",
      "[step: 1237] loss: 11.451826095581055\n",
      "[step: 1238] loss: 11.441805839538574\n",
      "[step: 1239] loss: 11.431806564331055\n",
      "[step: 1240] loss: 11.421821594238281\n",
      "[step: 1241] loss: 11.411826133728027\n",
      "[step: 1242] loss: 11.40182876586914\n",
      "[step: 1243] loss: 11.391834259033203\n",
      "[step: 1244] loss: 11.381856918334961\n",
      "[step: 1245] loss: 11.37192153930664\n",
      "[step: 1246] loss: 11.362091064453125\n",
      "[step: 1247] loss: 11.35247802734375\n",
      "[step: 1248] loss: 11.343385696411133\n",
      "[step: 1249] loss: 11.335665702819824\n",
      "[step: 1250] loss: 11.331862449645996\n",
      "[step: 1251] loss: 11.339347839355469\n",
      "[step: 1252] loss: 11.384546279907227\n",
      "[step: 1253] loss: 11.544767379760742\n",
      "[step: 1254] loss: 12.14975643157959\n",
      "[step: 1255] loss: 13.837756156921387\n",
      "[step: 1256] loss: 19.373226165771484\n",
      "[step: 1257] loss: 19.371482849121094\n",
      "[step: 1258] loss: 15.996496200561523\n",
      "[step: 1259] loss: 12.290291786193848\n",
      "[step: 1260] loss: 17.45325469970703\n",
      "[step: 1261] loss: 13.826935768127441\n",
      "[step: 1262] loss: 13.446891784667969\n",
      "[step: 1263] loss: 15.198814392089844\n",
      "[step: 1264] loss: 11.584392547607422\n",
      "[step: 1265] loss: 14.158059120178223\n",
      "[step: 1266] loss: 11.559853553771973\n",
      "[step: 1267] loss: 13.686798095703125\n",
      "[step: 1268] loss: 11.753316879272461\n",
      "[step: 1269] loss: 13.08098316192627\n",
      "[step: 1270] loss: 11.6117525100708\n",
      "[step: 1271] loss: 12.810178756713867\n",
      "[step: 1272] loss: 11.634664535522461\n",
      "[step: 1273] loss: 12.541015625\n",
      "[step: 1274] loss: 11.57530689239502\n",
      "[step: 1275] loss: 12.383346557617188\n",
      "[step: 1276] loss: 11.553442001342773\n",
      "[step: 1277] loss: 12.191802978515625\n",
      "[step: 1278] loss: 11.550395965576172\n",
      "[step: 1279] loss: 12.057279586791992\n",
      "[step: 1280] loss: 11.529783248901367\n",
      "[step: 1281] loss: 11.899724960327148\n",
      "[step: 1282] loss: 11.540691375732422\n",
      "[step: 1283] loss: 11.790281295776367\n",
      "[step: 1284] loss: 11.527472496032715\n",
      "[step: 1285] loss: 11.686882019042969\n",
      "[step: 1286] loss: 11.526918411254883\n",
      "[step: 1287] loss: 11.604673385620117\n",
      "[step: 1288] loss: 11.515753746032715\n",
      "[step: 1289] loss: 11.539859771728516\n",
      "[step: 1290] loss: 11.500450134277344\n",
      "[step: 1291] loss: 11.485494613647461\n",
      "[step: 1292] loss: 11.474587440490723\n",
      "[step: 1293] loss: 11.449271202087402\n",
      "[step: 1294] loss: 11.446211814880371\n",
      "[step: 1295] loss: 11.415335655212402\n",
      "[step: 1296] loss: 11.413887977600098\n",
      "[step: 1297] loss: 11.391317367553711\n",
      "[step: 1298] loss: 11.383466720581055\n",
      "[step: 1299] loss: 11.368229866027832\n",
      "[step: 1300] loss: 11.350028991699219\n",
      "[step: 1301] loss: 11.348596572875977\n",
      "[step: 1302] loss: 11.319992065429688\n",
      "[step: 1303] loss: 11.331016540527344\n",
      "[step: 1304] loss: 11.291379928588867\n",
      "[step: 1305] loss: 11.31134033203125\n",
      "[step: 1306] loss: 11.267471313476562\n",
      "[step: 1307] loss: 11.289780616760254\n",
      "[step: 1308] loss: 11.247091293334961\n",
      "[step: 1309] loss: 11.264532089233398\n",
      "[step: 1310] loss: 11.231038093566895\n",
      "[step: 1311] loss: 11.237455368041992\n",
      "[step: 1312] loss: 11.218330383300781\n",
      "[step: 1313] loss: 11.20954704284668\n",
      "[step: 1314] loss: 11.205164909362793\n",
      "[step: 1315] loss: 11.185124397277832\n",
      "[step: 1316] loss: 11.188520431518555\n",
      "[step: 1317] loss: 11.166488647460938\n",
      "[step: 1318] loss: 11.167487144470215\n",
      "[step: 1319] loss: 11.15274429321289\n",
      "[step: 1320] loss: 11.143811225891113\n",
      "[step: 1321] loss: 11.139249801635742\n",
      "[step: 1322] loss: 11.122875213623047\n",
      "[step: 1323] loss: 11.121451377868652\n",
      "[step: 1324] loss: 11.107487678527832\n",
      "[step: 1325] loss: 11.099979400634766\n",
      "[step: 1326] loss: 11.093666076660156\n",
      "[step: 1327] loss: 11.080375671386719\n",
      "[step: 1328] loss: 11.07627010345459\n",
      "[step: 1329] loss: 11.065595626831055\n",
      "[step: 1330] loss: 11.05612564086914\n",
      "[step: 1331] loss: 11.050663948059082\n",
      "[step: 1332] loss: 11.039275169372559\n",
      "[step: 1333] loss: 11.031874656677246\n",
      "[step: 1334] loss: 11.02481460571289\n",
      "[step: 1335] loss: 11.014074325561523\n",
      "[step: 1336] loss: 11.007290840148926\n",
      "[step: 1337] loss: 10.999387741088867\n",
      "[step: 1338] loss: 10.98956298828125\n",
      "[step: 1339] loss: 10.982566833496094\n",
      "[step: 1340] loss: 10.974663734436035\n",
      "[step: 1341] loss: 10.965272903442383\n",
      "[step: 1342] loss: 10.958013534545898\n",
      "[step: 1343] loss: 10.950325965881348\n",
      "[step: 1344] loss: 10.941322326660156\n",
      "[step: 1345] loss: 10.933589935302734\n",
      "[step: 1346] loss: 10.926238059997559\n",
      "[step: 1347] loss: 10.917671203613281\n",
      "[step: 1348] loss: 10.909509658813477\n",
      "[step: 1349] loss: 10.902175903320312\n",
      "[step: 1350] loss: 10.894261360168457\n",
      "[step: 1351] loss: 10.885924339294434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1352] loss: 10.878229141235352\n",
      "[step: 1353] loss: 10.870786666870117\n",
      "[step: 1354] loss: 10.862836837768555\n",
      "[step: 1355] loss: 10.854779243469238\n",
      "[step: 1356] loss: 10.847160339355469\n",
      "[step: 1357] loss: 10.839709281921387\n",
      "[step: 1358] loss: 10.83195686340332\n",
      "[step: 1359] loss: 10.824071884155273\n",
      "[step: 1360] loss: 10.816396713256836\n",
      "[step: 1361] loss: 10.808952331542969\n",
      "[step: 1362] loss: 10.801457405090332\n",
      "[step: 1363] loss: 10.793806076049805\n",
      "[step: 1364] loss: 10.78611946105957\n",
      "[step: 1365] loss: 10.778570175170898\n",
      "[step: 1366] loss: 10.77114486694336\n",
      "[step: 1367] loss: 10.763772964477539\n",
      "[step: 1368] loss: 10.756354331970215\n",
      "[step: 1369] loss: 10.748882293701172\n",
      "[step: 1370] loss: 10.741405487060547\n",
      "[step: 1371] loss: 10.733966827392578\n",
      "[step: 1372] loss: 10.726591110229492\n",
      "[step: 1373] loss: 10.719278335571289\n",
      "[step: 1374] loss: 10.712011337280273\n",
      "[step: 1375] loss: 10.704782485961914\n",
      "[step: 1376] loss: 10.697590827941895\n",
      "[step: 1377] loss: 10.690442085266113\n",
      "[step: 1378] loss: 10.683368682861328\n",
      "[step: 1379] loss: 10.676420211791992\n",
      "[step: 1380] loss: 10.669710159301758\n",
      "[step: 1381] loss: 10.663511276245117\n",
      "[step: 1382] loss: 10.658447265625\n",
      "[step: 1383] loss: 10.65617561340332\n",
      "[step: 1384] loss: 10.660737991333008\n",
      "[step: 1385] loss: 10.684540748596191\n",
      "[step: 1386] loss: 10.757888793945312\n",
      "[step: 1387] loss: 10.99032974243164\n",
      "[step: 1388] loss: 11.601929664611816\n",
      "[step: 1389] loss: 13.542268753051758\n",
      "[step: 1390] loss: 16.49930191040039\n",
      "[step: 1391] loss: 21.967134475708008\n",
      "[step: 1392] loss: 14.235013961791992\n",
      "[step: 1393] loss: 11.5418701171875\n",
      "[step: 1394] loss: 15.53564453125\n",
      "[step: 1395] loss: 11.734672546386719\n",
      "[step: 1396] loss: 12.069847106933594\n",
      "[step: 1397] loss: 14.088788032531738\n",
      "[step: 1398] loss: 10.892074584960938\n",
      "[step: 1399] loss: 13.220355987548828\n",
      "[step: 1400] loss: 12.218162536621094\n",
      "[step: 1401] loss: 11.597484588623047\n",
      "[step: 1402] loss: 12.716750144958496\n",
      "[step: 1403] loss: 10.929865837097168\n",
      "[step: 1404] loss: 12.317270278930664\n",
      "[step: 1405] loss: 10.811344146728516\n",
      "[step: 1406] loss: 12.071401596069336\n",
      "[step: 1407] loss: 11.057731628417969\n",
      "[step: 1408] loss: 11.587811470031738\n",
      "[step: 1409] loss: 11.134546279907227\n",
      "[step: 1410] loss: 11.224416732788086\n",
      "[step: 1411] loss: 11.21578311920166\n",
      "[step: 1412] loss: 11.078483581542969\n",
      "[step: 1413] loss: 11.205459594726562\n",
      "[step: 1414] loss: 10.906573295593262\n",
      "[step: 1415] loss: 11.159294128417969\n",
      "[step: 1416] loss: 10.831457138061523\n",
      "[step: 1417] loss: 11.13984489440918\n",
      "[step: 1418] loss: 10.769657135009766\n",
      "[step: 1419] loss: 11.051801681518555\n",
      "[step: 1420] loss: 10.744722366333008\n",
      "[step: 1421] loss: 10.984061241149902\n",
      "[step: 1422] loss: 10.752055168151855\n",
      "[step: 1423] loss: 10.902169227600098\n",
      "[step: 1424] loss: 10.747303009033203\n",
      "[step: 1425] loss: 10.821517944335938\n",
      "[step: 1426] loss: 10.764864921569824\n",
      "[step: 1427] loss: 10.749856948852539\n",
      "[step: 1428] loss: 10.77305793762207\n",
      "[step: 1429] loss: 10.685872077941895\n",
      "[step: 1430] loss: 10.757966041564941\n",
      "[step: 1431] loss: 10.656587600708008\n",
      "[step: 1432] loss: 10.729707717895508\n",
      "[step: 1433] loss: 10.648526191711426\n",
      "[step: 1434] loss: 10.6802978515625\n",
      "[step: 1435] loss: 10.652310371398926\n",
      "[step: 1436] loss: 10.630979537963867\n",
      "[step: 1437] loss: 10.654256820678711\n",
      "[step: 1438] loss: 10.598567962646484\n",
      "[step: 1439] loss: 10.633480072021484\n",
      "[step: 1440] loss: 10.589969635009766\n",
      "[step: 1441] loss: 10.596054077148438\n",
      "[step: 1442] loss: 10.590971946716309\n",
      "[step: 1443] loss: 10.562108993530273\n",
      "[step: 1444] loss: 10.578706741333008\n",
      "[step: 1445] loss: 10.547971725463867\n",
      "[step: 1446] loss: 10.550956726074219\n",
      "[step: 1447] loss: 10.545984268188477\n",
      "[step: 1448] loss: 10.523852348327637\n",
      "[step: 1449] loss: 10.533000946044922\n",
      "[step: 1450] loss: 10.513864517211914\n",
      "[step: 1451] loss: 10.507290840148926\n",
      "[step: 1452] loss: 10.507951736450195\n",
      "[step: 1453] loss: 10.489070892333984\n",
      "[step: 1454] loss: 10.488600730895996\n",
      "[step: 1455] loss: 10.482720375061035\n",
      "[step: 1456] loss: 10.468210220336914\n",
      "[step: 1457] loss: 10.46830940246582\n",
      "[step: 1458] loss: 10.459356307983398\n",
      "[step: 1459] loss: 10.448654174804688\n",
      "[step: 1460] loss: 10.447356224060059\n",
      "[step: 1461] loss: 10.438386917114258\n",
      "[step: 1462] loss: 10.429088592529297\n",
      "[step: 1463] loss: 10.426637649536133\n",
      "[step: 1464] loss: 10.418495178222656\n",
      "[step: 1465] loss: 10.40985107421875\n",
      "[step: 1466] loss: 10.406234741210938\n",
      "[step: 1467] loss: 10.399454116821289\n",
      "[step: 1468] loss: 10.390922546386719\n",
      "[step: 1469] loss: 10.386302947998047\n",
      "[step: 1470] loss: 10.380749702453613\n",
      "[step: 1471] loss: 10.372701644897461\n",
      "[step: 1472] loss: 10.366811752319336\n",
      "[step: 1473] loss: 10.362030982971191\n",
      "[step: 1474] loss: 10.355125427246094\n",
      "[step: 1475] loss: 10.348174095153809\n",
      "[step: 1476] loss: 10.343074798583984\n",
      "[step: 1477] loss: 10.33752727508545\n",
      "[step: 1478] loss: 10.330703735351562\n",
      "[step: 1479] loss: 10.32451057434082\n",
      "[step: 1480] loss: 10.319340705871582\n",
      "[step: 1481] loss: 10.313614845275879\n",
      "[step: 1482] loss: 10.307186126708984\n",
      "[step: 1483] loss: 10.301200866699219\n",
      "[step: 1484] loss: 10.295893669128418\n",
      "[step: 1485] loss: 10.290351867675781\n",
      "[step: 1486] loss: 10.284290313720703\n",
      "[step: 1487] loss: 10.278327941894531\n",
      "[step: 1488] loss: 10.272838592529297\n",
      "[step: 1489] loss: 10.267492294311523\n",
      "[step: 1490] loss: 10.261859893798828\n",
      "[step: 1491] loss: 10.256032943725586\n",
      "[step: 1492] loss: 10.250333786010742\n",
      "[step: 1493] loss: 10.24490737915039\n",
      "[step: 1494] loss: 10.239580154418945\n",
      "[step: 1495] loss: 10.23414421081543\n",
      "[step: 1496] loss: 10.228570938110352\n",
      "[step: 1497] loss: 10.22298812866211\n",
      "[step: 1498] loss: 10.217529296875\n",
      "[step: 1499] loss: 10.212197303771973\n",
      "[step: 1500] loss: 10.206930160522461\n",
      "[step: 1501] loss: 10.201641082763672\n",
      "[step: 1502] loss: 10.196319580078125\n",
      "[step: 1503] loss: 10.190958976745605\n",
      "[step: 1504] loss: 10.185606002807617\n",
      "[step: 1505] loss: 10.180289268493652\n",
      "[step: 1506] loss: 10.175015449523926\n",
      "[step: 1507] loss: 10.169792175292969\n",
      "[step: 1508] loss: 10.164605140686035\n",
      "[step: 1509] loss: 10.159455299377441\n",
      "[step: 1510] loss: 10.154333114624023\n",
      "[step: 1511] loss: 10.149245262145996\n",
      "[step: 1512] loss: 10.144200325012207\n",
      "[step: 1513] loss: 10.139225006103516\n",
      "[step: 1514] loss: 10.134354591369629\n",
      "[step: 1515] loss: 10.129690170288086\n",
      "[step: 1516] loss: 10.125402450561523\n",
      "[step: 1517] loss: 10.121940612792969\n",
      "[step: 1518] loss: 10.120213508605957\n",
      "[step: 1519] loss: 10.122564315795898\n",
      "[step: 1520] loss: 10.134065628051758\n",
      "[step: 1521] loss: 10.1694917678833\n",
      "[step: 1522] loss: 10.259260177612305\n",
      "[step: 1523] loss: 10.507394790649414\n",
      "[step: 1524] loss: 11.074551582336426\n",
      "[step: 1525] loss: 12.649372100830078\n",
      "[step: 1526] loss: 14.890459060668945\n",
      "[step: 1527] loss: 18.999610900878906\n",
      "[step: 1528] loss: 14.829740524291992\n",
      "[step: 1529] loss: 10.69646167755127\n",
      "[step: 1530] loss: 11.866275787353516\n",
      "[step: 1531] loss: 13.243541717529297\n",
      "[step: 1532] loss: 11.14526653289795\n",
      "[step: 1533] loss: 10.579093933105469\n",
      "[step: 1534] loss: 12.348703384399414\n",
      "[step: 1535] loss: 11.226801872253418\n",
      "[step: 1536] loss: 10.415031433105469\n",
      "[step: 1537] loss: 11.776640892028809\n",
      "[step: 1538] loss: 10.598955154418945\n",
      "[step: 1539] loss: 10.784008026123047\n",
      "[step: 1540] loss: 11.108428001403809\n",
      "[step: 1541] loss: 10.201408386230469\n",
      "[step: 1542] loss: 10.937261581420898\n",
      "[step: 1543] loss: 10.442163467407227\n",
      "[step: 1544] loss: 10.424086570739746\n",
      "[step: 1545] loss: 10.690000534057617\n",
      "[step: 1546] loss: 10.195416450500488\n",
      "[step: 1547] loss: 10.630252838134766\n",
      "[step: 1548] loss: 10.311333656311035\n",
      "[step: 1549] loss: 10.3558988571167\n",
      "[step: 1550] loss: 10.444243431091309\n",
      "[step: 1551] loss: 10.16270637512207\n",
      "[step: 1552] loss: 10.415735244750977\n",
      "[step: 1553] loss: 10.199438095092773\n",
      "[step: 1554] loss: 10.253693580627441\n",
      "[step: 1555] loss: 10.293103218078613\n",
      "[step: 1556] loss: 10.134515762329102\n",
      "[step: 1557] loss: 10.289236068725586\n",
      "[step: 1558] loss: 10.154595375061035\n",
      "[step: 1559] loss: 10.175074577331543\n",
      "[step: 1560] loss: 10.210073471069336\n",
      "[step: 1561] loss: 10.095673561096191\n",
      "[step: 1562] loss: 10.17982292175293\n",
      "[step: 1563] loss: 10.124689102172852\n",
      "[step: 1564] loss: 10.0914306640625\n",
      "[step: 1565] loss: 10.142950057983398\n",
      "[step: 1566] loss: 10.06994342803955\n",
      "[step: 1567] loss: 10.091815948486328\n",
      "[step: 1568] loss: 10.099787712097168\n",
      "[step: 1569] loss: 10.044734001159668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1570] loss: 10.076889038085938\n",
      "[step: 1571] loss: 10.0614652633667\n",
      "[step: 1572] loss: 10.028177261352539\n",
      "[step: 1573] loss: 10.055001258850098\n",
      "[step: 1574] loss: 10.032663345336914\n",
      "[step: 1575] loss: 10.010320663452148\n",
      "[step: 1576] loss: 10.030292510986328\n",
      "[step: 1577] loss: 10.010564804077148\n",
      "[step: 1578] loss: 9.993081092834473\n",
      "[step: 1579] loss: 10.006311416625977\n",
      "[step: 1580] loss: 9.991883277893066\n",
      "[step: 1581] loss: 9.975667953491211\n",
      "[step: 1582] loss: 9.984349250793457\n",
      "[step: 1583] loss: 9.97579574584961\n",
      "[step: 1584] loss: 9.958902359008789\n",
      "[step: 1585] loss: 9.962430000305176\n",
      "[step: 1586] loss: 9.96007251739502\n",
      "[step: 1587] loss: 9.94466495513916\n",
      "[step: 1588] loss: 9.941417694091797\n",
      "[step: 1589] loss: 9.942683219909668\n",
      "[step: 1590] loss: 9.932214736938477\n",
      "[step: 1591] loss: 9.923437118530273\n",
      "[step: 1592] loss: 9.923446655273438\n",
      "[step: 1593] loss: 9.919227600097656\n",
      "[step: 1594] loss: 9.909422874450684\n",
      "[step: 1595] loss: 9.904448509216309\n",
      "[step: 1596] loss: 9.903139114379883\n",
      "[step: 1597] loss: 9.897419929504395\n",
      "[step: 1598] loss: 9.889366149902344\n",
      "[step: 1599] loss: 9.885174751281738\n",
      "[step: 1600] loss: 9.882772445678711\n",
      "[step: 1601] loss: 9.877411842346191\n",
      "[step: 1602] loss: 9.870481491088867\n",
      "[step: 1603] loss: 9.865789413452148\n",
      "[step: 1604] loss: 9.862780570983887\n",
      "[step: 1605] loss: 9.858386993408203\n",
      "[step: 1606] loss: 9.852365493774414\n",
      "[step: 1607] loss: 9.847017288208008\n",
      "[step: 1608] loss: 9.843205451965332\n",
      "[step: 1609] loss: 9.839503288269043\n",
      "[step: 1610] loss: 9.834693908691406\n",
      "[step: 1611] loss: 9.829275131225586\n",
      "[step: 1612] loss: 9.824434280395508\n",
      "[step: 1613] loss: 9.820430755615234\n",
      "[step: 1614] loss: 9.816514015197754\n",
      "[step: 1615] loss: 9.812066078186035\n",
      "[step: 1616] loss: 9.807161331176758\n",
      "[step: 1617] loss: 9.802352905273438\n",
      "[step: 1618] loss: 9.79797649383545\n",
      "[step: 1619] loss: 9.793928146362305\n",
      "[step: 1620] loss: 9.789854049682617\n",
      "[step: 1621] loss: 9.785537719726562\n",
      "[step: 1622] loss: 9.78099250793457\n",
      "[step: 1623] loss: 9.776416778564453\n",
      "[step: 1624] loss: 9.77196979522705\n",
      "[step: 1625] loss: 9.767715454101562\n",
      "[step: 1626] loss: 9.763603210449219\n",
      "[step: 1627] loss: 9.759512901306152\n",
      "[step: 1628] loss: 9.755367279052734\n",
      "[step: 1629] loss: 9.751163482666016\n",
      "[step: 1630] loss: 9.746895790100098\n",
      "[step: 1631] loss: 9.742599487304688\n",
      "[step: 1632] loss: 9.738304138183594\n",
      "[step: 1633] loss: 9.734034538269043\n",
      "[step: 1634] loss: 9.729789733886719\n",
      "[step: 1635] loss: 9.725587844848633\n",
      "[step: 1636] loss: 9.721414566040039\n",
      "[step: 1637] loss: 9.717255592346191\n",
      "[step: 1638] loss: 9.713119506835938\n",
      "[step: 1639] loss: 9.709003448486328\n",
      "[step: 1640] loss: 9.704891204833984\n",
      "[step: 1641] loss: 9.700811386108398\n",
      "[step: 1642] loss: 9.69675064086914\n",
      "[step: 1643] loss: 9.692731857299805\n",
      "[step: 1644] loss: 9.688791275024414\n",
      "[step: 1645] loss: 9.685002326965332\n",
      "[step: 1646] loss: 9.681520462036133\n",
      "[step: 1647] loss: 9.678696632385254\n",
      "[step: 1648] loss: 9.677355766296387\n",
      "[step: 1649] loss: 9.679313659667969\n",
      "[step: 1650] loss: 9.68932056427002\n",
      "[step: 1651] loss: 9.717823028564453\n",
      "[step: 1652] loss: 9.795126914978027\n",
      "[step: 1653] loss: 9.981866836547852\n",
      "[step: 1654] loss: 10.481464385986328\n",
      "[step: 1655] loss: 11.533361434936523\n",
      "[step: 1656] loss: 14.13810920715332\n",
      "[step: 1657] loss: 16.343856811523438\n",
      "[step: 1658] loss: 17.99005699157715\n",
      "[step: 1659] loss: 11.764280319213867\n",
      "[step: 1660] loss: 10.625070571899414\n",
      "[step: 1661] loss: 14.206877708435059\n",
      "[step: 1662] loss: 11.481931686401367\n",
      "[step: 1663] loss: 9.962093353271484\n",
      "[step: 1664] loss: 12.264091491699219\n",
      "[step: 1665] loss: 10.786384582519531\n",
      "[step: 1666] loss: 10.002496719360352\n",
      "[step: 1667] loss: 11.35772705078125\n",
      "[step: 1668] loss: 9.970991134643555\n",
      "[step: 1669] loss: 10.571731567382812\n",
      "[step: 1670] loss: 10.581801414489746\n",
      "[step: 1671] loss: 9.883758544921875\n",
      "[step: 1672] loss: 10.720518112182617\n",
      "[step: 1673] loss: 9.967975616455078\n",
      "[step: 1674] loss: 10.205493927001953\n",
      "[step: 1675] loss: 10.26512336730957\n",
      "[step: 1676] loss: 9.81219482421875\n",
      "[step: 1677] loss: 10.258522987365723\n",
      "[step: 1678] loss: 9.806794166564941\n",
      "[step: 1679] loss: 10.066051483154297\n",
      "[step: 1680] loss: 9.955877304077148\n",
      "[step: 1681] loss: 9.814855575561523\n",
      "[step: 1682] loss: 10.042814254760742\n",
      "[step: 1683] loss: 9.734655380249023\n",
      "[step: 1684] loss: 9.933771133422852\n",
      "[step: 1685] loss: 9.806556701660156\n",
      "[step: 1686] loss: 9.76764965057373\n",
      "[step: 1687] loss: 9.866189002990723\n",
      "[step: 1688] loss: 9.704167366027832\n",
      "[step: 1689] loss: 9.822334289550781\n",
      "[step: 1690] loss: 9.741338729858398\n",
      "[step: 1691] loss: 9.70926284790039\n",
      "[step: 1692] loss: 9.782144546508789\n",
      "[step: 1693] loss: 9.6653413772583\n",
      "[step: 1694] loss: 9.727169036865234\n",
      "[step: 1695] loss: 9.698493957519531\n",
      "[step: 1696] loss: 9.650690078735352\n",
      "[step: 1697] loss: 9.704082489013672\n",
      "[step: 1698] loss: 9.644824028015137\n",
      "[step: 1699] loss: 9.648344039916992\n",
      "[step: 1700] loss: 9.665714263916016\n",
      "[step: 1701] loss: 9.613347053527832\n",
      "[step: 1702] loss: 9.638731002807617\n",
      "[step: 1703] loss: 9.630386352539062\n",
      "[step: 1704] loss: 9.595945358276367\n",
      "[step: 1705] loss: 9.61943244934082\n",
      "[step: 1706] loss: 9.60114860534668\n",
      "[step: 1707] loss: 9.581892013549805\n",
      "[step: 1708] loss: 9.598136901855469\n",
      "[step: 1709] loss: 9.579841613769531\n",
      "[step: 1710] loss: 9.565746307373047\n",
      "[step: 1711] loss: 9.578083038330078\n",
      "[step: 1712] loss: 9.562155723571777\n",
      "[step: 1713] loss: 9.550708770751953\n",
      "[step: 1714] loss: 9.558003425598145\n",
      "[step: 1715] loss: 9.547077178955078\n",
      "[step: 1716] loss: 9.535384178161621\n",
      "[step: 1717] loss: 9.539728164672852\n",
      "[step: 1718] loss: 9.532856941223145\n",
      "[step: 1719] loss: 9.520736694335938\n",
      "[step: 1720] loss: 9.522011756896973\n",
      "[step: 1721] loss: 9.519140243530273\n",
      "[step: 1722] loss: 9.50819206237793\n",
      "[step: 1723] loss: 9.504754066467285\n",
      "[step: 1724] loss: 9.504514694213867\n",
      "[step: 1725] loss: 9.496411323547363\n",
      "[step: 1726] loss: 9.489765167236328\n",
      "[step: 1727] loss: 9.48872184753418\n",
      "[step: 1728] loss: 9.48453426361084\n",
      "[step: 1729] loss: 9.477104187011719\n",
      "[step: 1730] loss: 9.473207473754883\n",
      "[step: 1731] loss: 9.47118091583252\n",
      "[step: 1732] loss: 9.46578598022461\n",
      "[step: 1733] loss: 9.459814071655273\n",
      "[step: 1734] loss: 9.45655632019043\n",
      "[step: 1735] loss: 9.45365047454834\n",
      "[step: 1736] loss: 9.448434829711914\n",
      "[step: 1737] loss: 9.443188667297363\n",
      "[step: 1738] loss: 9.439852714538574\n",
      "[step: 1739] loss: 9.436559677124023\n",
      "[step: 1740] loss: 9.431877136230469\n",
      "[step: 1741] loss: 9.426960945129395\n",
      "[step: 1742] loss: 9.423269271850586\n",
      "[step: 1743] loss: 9.41991138458252\n",
      "[step: 1744] loss: 9.415750503540039\n",
      "[step: 1745] loss: 9.411118507385254\n",
      "[step: 1746] loss: 9.407037734985352\n",
      "[step: 1747] loss: 9.403543472290039\n",
      "[step: 1748] loss: 9.399795532226562\n",
      "[step: 1749] loss: 9.395566940307617\n",
      "[step: 1750] loss: 9.391302108764648\n",
      "[step: 1751] loss: 9.387457847595215\n",
      "[step: 1752] loss: 9.383846282958984\n",
      "[step: 1753] loss: 9.38004207611084\n",
      "[step: 1754] loss: 9.375980377197266\n",
      "[step: 1755] loss: 9.371896743774414\n",
      "[step: 1756] loss: 9.368051528930664\n",
      "[step: 1757] loss: 9.364364624023438\n",
      "[step: 1758] loss: 9.360625267028809\n",
      "[step: 1759] loss: 9.356741905212402\n",
      "[step: 1760] loss: 9.352781295776367\n",
      "[step: 1761] loss: 9.348886489868164\n",
      "[step: 1762] loss: 9.345102310180664\n",
      "[step: 1763] loss: 9.341388702392578\n",
      "[step: 1764] loss: 9.337655067443848\n",
      "[step: 1765] loss: 9.333845138549805\n",
      "[step: 1766] loss: 9.329992294311523\n",
      "[step: 1767] loss: 9.326148986816406\n",
      "[step: 1768] loss: 9.322355270385742\n",
      "[step: 1769] loss: 9.31861400604248\n",
      "[step: 1770] loss: 9.314888954162598\n",
      "[step: 1771] loss: 9.31116771697998\n",
      "[step: 1772] loss: 9.307413101196289\n",
      "[step: 1773] loss: 9.303644180297852\n",
      "[step: 1774] loss: 9.299867630004883\n",
      "[step: 1775] loss: 9.296091079711914\n",
      "[step: 1776] loss: 9.292325973510742\n",
      "[step: 1777] loss: 9.288589477539062\n",
      "[step: 1778] loss: 9.284862518310547\n",
      "[step: 1779] loss: 9.281140327453613\n",
      "[step: 1780] loss: 9.277427673339844\n",
      "[step: 1781] loss: 9.273714065551758\n",
      "[step: 1782] loss: 9.270002365112305\n",
      "[step: 1783] loss: 9.266291618347168\n",
      "[step: 1784] loss: 9.262578010559082\n",
      "[step: 1785] loss: 9.258872985839844\n",
      "[step: 1786] loss: 9.255172729492188\n",
      "[step: 1787] loss: 9.251480102539062\n",
      "[step: 1788] loss: 9.247804641723633\n",
      "[step: 1789] loss: 9.244155883789062\n",
      "[step: 1790] loss: 9.240548133850098\n",
      "[step: 1791] loss: 9.237011909484863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1792] loss: 9.233600616455078\n",
      "[step: 1793] loss: 9.230424880981445\n",
      "[step: 1794] loss: 9.227666854858398\n",
      "[step: 1795] loss: 9.225753784179688\n",
      "[step: 1796] loss: 9.22542953491211\n",
      "[step: 1797] loss: 9.228437423706055\n",
      "[step: 1798] loss: 9.23798942565918\n",
      "[step: 1799] loss: 9.262100219726562\n",
      "[step: 1800] loss: 9.31498908996582\n",
      "[step: 1801] loss: 9.436354637145996\n",
      "[step: 1802] loss: 9.684046745300293\n",
      "[step: 1803] loss: 10.248193740844727\n",
      "[step: 1804] loss: 11.211227416992188\n",
      "[step: 1805] loss: 13.090738296508789\n",
      "[step: 1806] loss: 14.130451202392578\n",
      "[step: 1807] loss: 14.148747444152832\n",
      "[step: 1808] loss: 10.632837295532227\n",
      "[step: 1809] loss: 9.4298095703125\n",
      "[step: 1810] loss: 11.366765975952148\n",
      "[step: 1811] loss: 11.690614700317383\n",
      "[step: 1812] loss: 9.974751472473145\n",
      "[step: 1813] loss: 9.375408172607422\n",
      "[step: 1814] loss: 10.610113143920898\n",
      "[step: 1815] loss: 10.395232200622559\n",
      "[step: 1816] loss: 9.274843215942383\n",
      "[step: 1817] loss: 10.111783981323242\n",
      "[step: 1818] loss: 10.180402755737305\n",
      "[step: 1819] loss: 9.25648307800293\n",
      "[step: 1820] loss: 9.971694946289062\n",
      "[step: 1821] loss: 9.973140716552734\n",
      "[step: 1822] loss: 9.263298034667969\n",
      "[step: 1823] loss: 9.841329574584961\n",
      "[step: 1824] loss: 9.715636253356934\n",
      "[step: 1825] loss: 9.266887664794922\n",
      "[step: 1826] loss: 9.770418167114258\n",
      "[step: 1827] loss: 9.492238998413086\n",
      "[step: 1828] loss: 9.276834487915039\n",
      "[step: 1829] loss: 9.65823745727539\n",
      "[step: 1830] loss: 9.350141525268555\n",
      "[step: 1831] loss: 9.290843963623047\n",
      "[step: 1832] loss: 9.54069995880127\n",
      "[step: 1833] loss: 9.272031784057617\n",
      "[step: 1834] loss: 9.281231880187988\n",
      "[step: 1835] loss: 9.435209274291992\n",
      "[step: 1836] loss: 9.21810531616211\n",
      "[step: 1837] loss: 9.259969711303711\n",
      "[step: 1838] loss: 9.352729797363281\n",
      "[step: 1839] loss: 9.188298225402832\n",
      "[step: 1840] loss: 9.223723411560059\n",
      "[step: 1841] loss: 9.285589218139648\n",
      "[step: 1842] loss: 9.169745445251465\n",
      "[step: 1843] loss: 9.186500549316406\n",
      "[step: 1844] loss: 9.238174438476562\n",
      "[step: 1845] loss: 9.154970169067383\n",
      "[step: 1846] loss: 9.15280532836914\n",
      "[step: 1847] loss: 9.199726104736328\n",
      "[step: 1848] loss: 9.145092964172363\n",
      "[step: 1849] loss: 9.124138832092285\n",
      "[step: 1850] loss: 9.163410186767578\n",
      "[step: 1851] loss: 9.138172149658203\n",
      "[step: 1852] loss: 9.102856636047363\n",
      "[step: 1853] loss: 9.128667831420898\n",
      "[step: 1854] loss: 9.128220558166504\n",
      "[step: 1855] loss: 9.091585159301758\n",
      "[step: 1856] loss: 9.096067428588867\n",
      "[step: 1857] loss: 9.11018180847168\n",
      "[step: 1858] loss: 9.087821960449219\n",
      "[step: 1859] loss: 9.071924209594727\n",
      "[step: 1860] loss: 9.083816528320312\n",
      "[step: 1861] loss: 9.081515312194824\n",
      "[step: 1862] loss: 9.061379432678223\n",
      "[step: 1863] loss: 9.05733871459961\n",
      "[step: 1864] loss: 9.064105987548828\n",
      "[step: 1865] loss: 9.057106971740723\n",
      "[step: 1866] loss: 9.042525291442871\n",
      "[step: 1867] loss: 9.040602684020996\n",
      "[step: 1868] loss: 9.043828964233398\n",
      "[step: 1869] loss: 9.036748886108398\n",
      "[step: 1870] loss: 9.025996208190918\n",
      "[step: 1871] loss: 9.0228271484375\n",
      "[step: 1872] loss: 9.023881912231445\n",
      "[step: 1873] loss: 9.01927375793457\n",
      "[step: 1874] loss: 9.010485649108887\n",
      "[step: 1875] loss: 9.005512237548828\n",
      "[step: 1876] loss: 9.004671096801758\n",
      "[step: 1877] loss: 9.002216339111328\n",
      "[step: 1878] loss: 8.99589729309082\n",
      "[step: 1879] loss: 8.98961067199707\n",
      "[step: 1880] loss: 8.98642349243164\n",
      "[step: 1881] loss: 8.984540939331055\n",
      "[step: 1882] loss: 8.980932235717773\n",
      "[step: 1883] loss: 8.975362777709961\n",
      "[step: 1884] loss: 8.97021198272705\n",
      "[step: 1885] loss: 8.966825485229492\n",
      "[step: 1886] loss: 8.964202880859375\n",
      "[step: 1887] loss: 8.960769653320312\n",
      "[step: 1888] loss: 8.956127166748047\n",
      "[step: 1889] loss: 8.951333045959473\n",
      "[step: 1890] loss: 8.947362899780273\n",
      "[step: 1891] loss: 8.944168090820312\n",
      "[step: 1892] loss: 8.941008567810059\n",
      "[step: 1893] loss: 8.937240600585938\n",
      "[step: 1894] loss: 8.932971954345703\n",
      "[step: 1895] loss: 8.92868423461914\n",
      "[step: 1896] loss: 8.92477798461914\n",
      "[step: 1897] loss: 8.921298027038574\n",
      "[step: 1898] loss: 8.9179105758667\n",
      "[step: 1899] loss: 8.91436767578125\n",
      "[step: 1900] loss: 8.910562515258789\n",
      "[step: 1901] loss: 8.90656852722168\n",
      "[step: 1902] loss: 8.90259075164795\n",
      "[step: 1903] loss: 8.898750305175781\n",
      "[step: 1904] loss: 8.895078659057617\n",
      "[step: 1905] loss: 8.891523361206055\n",
      "[step: 1906] loss: 8.887988090515137\n",
      "[step: 1907] loss: 8.884394645690918\n",
      "[step: 1908] loss: 8.880729675292969\n",
      "[step: 1909] loss: 8.876993179321289\n",
      "[step: 1910] loss: 8.873212814331055\n",
      "[step: 1911] loss: 8.869421005249023\n",
      "[step: 1912] loss: 8.865663528442383\n",
      "[step: 1913] loss: 8.861921310424805\n",
      "[step: 1914] loss: 8.858211517333984\n",
      "[step: 1915] loss: 8.854528427124023\n",
      "[step: 1916] loss: 8.850866317749023\n",
      "[step: 1917] loss: 8.847217559814453\n",
      "[step: 1918] loss: 8.843588829040527\n",
      "[step: 1919] loss: 8.839967727661133\n",
      "[step: 1920] loss: 8.83636474609375\n",
      "[step: 1921] loss: 8.832788467407227\n",
      "[step: 1922] loss: 8.829244613647461\n",
      "[step: 1923] loss: 8.825764656066895\n",
      "[step: 1924] loss: 8.822399139404297\n",
      "[step: 1925] loss: 8.81922721862793\n",
      "[step: 1926] loss: 8.816415786743164\n",
      "[step: 1927] loss: 8.814241409301758\n",
      "[step: 1928] loss: 8.813294410705566\n",
      "[step: 1929] loss: 8.81467056274414\n",
      "[step: 1930] loss: 8.82075309753418\n",
      "[step: 1931] loss: 8.835947036743164\n",
      "[step: 1932] loss: 8.870509147644043\n",
      "[step: 1933] loss: 8.94308853149414\n",
      "[step: 1934] loss: 9.099666595458984\n",
      "[step: 1935] loss: 9.41043472290039\n",
      "[step: 1936] loss: 10.054839134216309\n",
      "[step: 1937] loss: 11.097602844238281\n",
      "[step: 1938] loss: 12.728224754333496\n",
      "[step: 1939] loss: 13.275732040405273\n",
      "[step: 1940] loss: 12.336725234985352\n",
      "[step: 1941] loss: 9.5379638671875\n",
      "[step: 1942] loss: 9.10395622253418\n",
      "[step: 1943] loss: 10.871305465698242\n",
      "[step: 1944] loss: 11.125875473022461\n",
      "[step: 1945] loss: 9.586113929748535\n",
      "[step: 1946] loss: 8.87644100189209\n",
      "[step: 1947] loss: 9.932470321655273\n",
      "[step: 1948] loss: 9.98011589050293\n",
      "[step: 1949] loss: 8.899487495422363\n",
      "[step: 1950] loss: 9.333992004394531\n",
      "[step: 1951] loss: 9.753368377685547\n",
      "[step: 1952] loss: 8.938244819641113\n",
      "[step: 1953] loss: 9.109804153442383\n",
      "[step: 1954] loss: 9.540815353393555\n",
      "[step: 1955] loss: 8.971933364868164\n",
      "[step: 1956] loss: 8.978315353393555\n",
      "[step: 1957] loss: 9.339141845703125\n",
      "[step: 1958] loss: 8.927165985107422\n",
      "[step: 1959] loss: 8.916191101074219\n",
      "[step: 1960] loss: 9.194154739379883\n",
      "[step: 1961] loss: 8.894197463989258\n",
      "[step: 1962] loss: 8.856599807739258\n",
      "[step: 1963] loss: 9.064008712768555\n",
      "[step: 1964] loss: 8.87794017791748\n",
      "[step: 1965] loss: 8.799238204956055\n",
      "[step: 1966] loss: 8.958080291748047\n",
      "[step: 1967] loss: 8.856058120727539\n",
      "[step: 1968] loss: 8.76119327545166\n",
      "[step: 1969] loss: 8.879643440246582\n",
      "[step: 1970] loss: 8.840917587280273\n",
      "[step: 1971] loss: 8.740772247314453\n",
      "[step: 1972] loss: 8.800385475158691\n",
      "[step: 1973] loss: 8.818557739257812\n",
      "[step: 1974] loss: 8.736236572265625\n",
      "[step: 1975] loss: 8.74346923828125\n",
      "[step: 1976] loss: 8.78687858581543\n",
      "[step: 1977] loss: 8.740608215332031\n",
      "[step: 1978] loss: 8.707719802856445\n",
      "[step: 1979] loss: 8.74113941192627\n",
      "[step: 1980] loss: 8.742258071899414\n",
      "[step: 1981] loss: 8.698234558105469\n",
      "[step: 1982] loss: 8.696331024169922\n",
      "[step: 1983] loss: 8.719728469848633\n",
      "[step: 1984] loss: 8.701850891113281\n",
      "[step: 1985] loss: 8.675224304199219\n",
      "[step: 1986] loss: 8.681838989257812\n",
      "[step: 1987] loss: 8.692306518554688\n",
      "[step: 1988] loss: 8.676105499267578\n",
      "[step: 1989] loss: 8.658002853393555\n",
      "[step: 1990] loss: 8.661859512329102\n",
      "[step: 1991] loss: 8.667949676513672\n",
      "[step: 1992] loss: 8.656797409057617\n",
      "[step: 1993] loss: 8.64226245880127\n",
      "[step: 1994] loss: 8.641161918640137\n",
      "[step: 1995] loss: 8.64515209197998\n",
      "[step: 1996] loss: 8.64022445678711\n",
      "[step: 1997] loss: 8.628764152526855\n",
      "[step: 1998] loss: 8.62238883972168\n",
      "[step: 1999] loss: 8.623340606689453\n",
      "[step: 2000] loss: 8.622739791870117\n",
      "[step: 2001] loss: 8.616121292114258\n",
      "[step: 2002] loss: 8.607972145080566\n",
      "[step: 2003] loss: 8.603700637817383\n",
      "[step: 2004] loss: 8.603069305419922\n",
      "[step: 2005] loss: 8.601175308227539\n",
      "[step: 2006] loss: 8.595916748046875\n",
      "[step: 2007] loss: 8.58948802947998\n",
      "[step: 2008] loss: 8.584844589233398\n",
      "[step: 2009] loss: 8.582616806030273\n",
      "[step: 2010] loss: 8.58064079284668\n",
      "[step: 2011] loss: 8.577064514160156\n",
      "[step: 2012] loss: 8.572050094604492\n",
      "[step: 2013] loss: 8.567012786865234\n",
      "[step: 2014] loss: 8.563170433044434\n",
      "[step: 2015] loss: 8.560371398925781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2016] loss: 8.5576753616333\n",
      "[step: 2017] loss: 8.55436897277832\n",
      "[step: 2018] loss: 8.550276756286621\n",
      "[step: 2019] loss: 8.545916557312012\n",
      "[step: 2020] loss: 8.541828155517578\n",
      "[step: 2021] loss: 8.538265228271484\n",
      "[step: 2022] loss: 8.53512954711914\n",
      "[step: 2023] loss: 8.532060623168945\n",
      "[step: 2024] loss: 8.52879810333252\n",
      "[step: 2025] loss: 8.525249481201172\n",
      "[step: 2026] loss: 8.521462440490723\n",
      "[step: 2027] loss: 8.51761531829834\n",
      "[step: 2028] loss: 8.513829231262207\n",
      "[step: 2029] loss: 8.510193824768066\n",
      "[step: 2030] loss: 8.506723403930664\n",
      "[step: 2031] loss: 8.503360748291016\n",
      "[step: 2032] loss: 8.500056266784668\n",
      "[step: 2033] loss: 8.496759414672852\n",
      "[step: 2034] loss: 8.493443489074707\n",
      "[step: 2035] loss: 8.49009895324707\n",
      "[step: 2036] loss: 8.486723899841309\n",
      "[step: 2037] loss: 8.483317375183105\n",
      "[step: 2038] loss: 8.479928970336914\n",
      "[step: 2039] loss: 8.476552963256836\n",
      "[step: 2040] loss: 8.473221778869629\n",
      "[step: 2041] loss: 8.469939231872559\n",
      "[step: 2042] loss: 8.466773986816406\n",
      "[step: 2043] loss: 8.463769912719727\n",
      "[step: 2044] loss: 8.461030960083008\n",
      "[step: 2045] loss: 8.45872688293457\n",
      "[step: 2046] loss: 8.457162857055664\n",
      "[step: 2047] loss: 8.456888198852539\n",
      "[step: 2048] loss: 8.458948135375977\n",
      "[step: 2049] loss: 8.465194702148438\n",
      "[step: 2050] loss: 8.479440689086914\n",
      "[step: 2051] loss: 8.508415222167969\n",
      "[step: 2052] loss: 8.56664752960205\n",
      "[step: 2053] loss: 8.678434371948242\n",
      "[step: 2054] loss: 8.897512435913086\n",
      "[step: 2055] loss: 9.292118072509766\n",
      "[step: 2056] loss: 9.9993314743042\n",
      "[step: 2057] loss: 10.943206787109375\n",
      "[step: 2058] loss: 11.980390548706055\n",
      "[step: 2059] loss: 11.708965301513672\n",
      "[step: 2060] loss: 10.292007446289062\n",
      "[step: 2061] loss: 8.645549774169922\n",
      "[step: 2062] loss: 8.801300048828125\n",
      "[step: 2063] loss: 10.069647789001465\n",
      "[step: 2064] loss: 10.286083221435547\n",
      "[step: 2065] loss: 9.191468238830566\n",
      "[step: 2066] loss: 8.451391220092773\n",
      "[step: 2067] loss: 9.105754852294922\n",
      "[step: 2068] loss: 9.495002746582031\n",
      "[step: 2069] loss: 8.750129699707031\n",
      "[step: 2070] loss: 8.521739959716797\n",
      "[step: 2071] loss: 9.044350624084473\n",
      "[step: 2072] loss: 8.955717086791992\n",
      "[step: 2073] loss: 8.479082107543945\n",
      "[step: 2074] loss: 8.60776138305664\n",
      "[step: 2075] loss: 8.8621244430542\n",
      "[step: 2076] loss: 8.586478233337402\n",
      "[step: 2077] loss: 8.449174880981445\n",
      "[step: 2078] loss: 8.6964111328125\n",
      "[step: 2079] loss: 8.666406631469727\n",
      "[step: 2080] loss: 8.43018913269043\n",
      "[step: 2081] loss: 8.510440826416016\n",
      "[step: 2082] loss: 8.633306503295898\n",
      "[step: 2083] loss: 8.497194290161133\n",
      "[step: 2084] loss: 8.40158462524414\n",
      "[step: 2085] loss: 8.515227317810059\n",
      "[step: 2086] loss: 8.534311294555664\n",
      "[step: 2087] loss: 8.403850555419922\n",
      "[step: 2088] loss: 8.40226936340332\n",
      "[step: 2089] loss: 8.48478889465332\n",
      "[step: 2090] loss: 8.460822105407715\n",
      "[step: 2091] loss: 8.373214721679688\n",
      "[step: 2092] loss: 8.382984161376953\n",
      "[step: 2093] loss: 8.438535690307617\n",
      "[step: 2094] loss: 8.40945053100586\n",
      "[step: 2095] loss: 8.352151870727539\n",
      "[step: 2096] loss: 8.35603141784668\n",
      "[step: 2097] loss: 8.391396522521973\n",
      "[step: 2098] loss: 8.380456924438477\n",
      "[step: 2099] loss: 8.339704513549805\n",
      "[step: 2100] loss: 8.329526901245117\n",
      "[step: 2101] loss: 8.349234580993652\n",
      "[step: 2102] loss: 8.355260848999023\n",
      "[step: 2103] loss: 8.331993103027344\n",
      "[step: 2104] loss: 8.311195373535156\n",
      "[step: 2105] loss: 8.31476879119873\n",
      "[step: 2106] loss: 8.325654983520508\n",
      "[step: 2107] loss: 8.323049545288086\n",
      "[step: 2108] loss: 8.305416107177734\n",
      "[step: 2109] loss: 8.292505264282227\n",
      "[step: 2110] loss: 8.293624877929688\n",
      "[step: 2111] loss: 8.298904418945312\n",
      "[step: 2112] loss: 8.297935485839844\n",
      "[step: 2113] loss: 8.28759765625\n",
      "[step: 2114] loss: 8.276344299316406\n",
      "[step: 2115] loss: 8.271896362304688\n",
      "[step: 2116] loss: 8.272704124450684\n",
      "[step: 2117] loss: 8.27404499053955\n",
      "[step: 2118] loss: 8.270694732666016\n",
      "[step: 2119] loss: 8.263412475585938\n",
      "[step: 2120] loss: 8.256035804748535\n",
      "[step: 2121] loss: 8.250831604003906\n",
      "[step: 2122] loss: 8.24891471862793\n",
      "[step: 2123] loss: 8.248263359069824\n",
      "[step: 2124] loss: 8.24659252166748\n",
      "[step: 2125] loss: 8.243170738220215\n",
      "[step: 2126] loss: 8.238033294677734\n",
      "[step: 2127] loss: 8.232637405395508\n",
      "[step: 2128] loss: 8.227954864501953\n",
      "[step: 2129] loss: 8.224319458007812\n",
      "[step: 2130] loss: 8.221773147583008\n",
      "[step: 2131] loss: 8.219596862792969\n",
      "[step: 2132] loss: 8.217369079589844\n",
      "[step: 2133] loss: 8.214733123779297\n",
      "[step: 2134] loss: 8.211463928222656\n",
      "[step: 2135] loss: 8.207866668701172\n",
      "[step: 2136] loss: 8.204024314880371\n",
      "[step: 2137] loss: 8.200109481811523\n",
      "[step: 2138] loss: 8.19630241394043\n",
      "[step: 2139] loss: 8.192590713500977\n",
      "[step: 2140] loss: 8.189064025878906\n",
      "[step: 2141] loss: 8.185663223266602\n",
      "[step: 2142] loss: 8.182357788085938\n",
      "[step: 2143] loss: 8.179152488708496\n",
      "[step: 2144] loss: 8.175996780395508\n",
      "[step: 2145] loss: 8.172883987426758\n",
      "[step: 2146] loss: 8.16982364654541\n",
      "[step: 2147] loss: 8.166820526123047\n",
      "[step: 2148] loss: 8.163910865783691\n",
      "[step: 2149] loss: 8.161151885986328\n",
      "[step: 2150] loss: 8.15864372253418\n",
      "[step: 2151] loss: 8.156597137451172\n",
      "[step: 2152] loss: 8.155399322509766\n",
      "[step: 2153] loss: 8.155731201171875\n",
      "[step: 2154] loss: 8.159027099609375\n",
      "[step: 2155] loss: 8.168096542358398\n",
      "[step: 2156] loss: 8.188459396362305\n",
      "[step: 2157] loss: 8.231674194335938\n",
      "[step: 2158] loss: 8.320655822753906\n",
      "[step: 2159] loss: 8.502257347106934\n",
      "[step: 2160] loss: 8.860960006713867\n",
      "[step: 2161] loss: 9.542643547058105\n",
      "[step: 2162] loss: 10.635540962219238\n",
      "[step: 2163] loss: 12.013254165649414\n",
      "[step: 2164] loss: 12.306451797485352\n",
      "[step: 2165] loss: 10.961109161376953\n",
      "[step: 2166] loss: 8.611564636230469\n",
      "[step: 2167] loss: 8.501227378845215\n",
      "[step: 2168] loss: 10.113147735595703\n",
      "[step: 2169] loss: 10.442354202270508\n",
      "[step: 2170] loss: 8.982425689697266\n",
      "[step: 2171] loss: 8.202262878417969\n",
      "[step: 2172] loss: 9.179299354553223\n",
      "[step: 2173] loss: 9.386962890625\n",
      "[step: 2174] loss: 8.348363876342773\n",
      "[step: 2175] loss: 8.470376968383789\n",
      "[step: 2176] loss: 9.052001953125\n",
      "[step: 2177] loss: 8.503233909606934\n",
      "[step: 2178] loss: 8.22940444946289\n",
      "[step: 2179] loss: 8.698951721191406\n",
      "[step: 2180] loss: 8.564719200134277\n",
      "[step: 2181] loss: 8.1773042678833\n",
      "[step: 2182] loss: 8.49399471282959\n",
      "[step: 2183] loss: 8.530406951904297\n",
      "[step: 2184] loss: 8.180737495422363\n",
      "[step: 2185] loss: 8.30547046661377\n",
      "[step: 2186] loss: 8.467790603637695\n",
      "[step: 2187] loss: 8.227009773254395\n",
      "[step: 2188] loss: 8.16963005065918\n",
      "[step: 2189] loss: 8.373479843139648\n",
      "[step: 2190] loss: 8.27160930633545\n",
      "[step: 2191] loss: 8.123079299926758\n",
      "[step: 2192] loss: 8.241117477416992\n",
      "[step: 2193] loss: 8.279712677001953\n",
      "[step: 2194] loss: 8.147842407226562\n",
      "[step: 2195] loss: 8.118144989013672\n",
      "[step: 2196] loss: 8.227362632751465\n",
      "[step: 2197] loss: 8.18100357055664\n",
      "[step: 2198] loss: 8.085372924804688\n",
      "[step: 2199] loss: 8.13861083984375\n",
      "[step: 2200] loss: 8.177080154418945\n",
      "[step: 2201] loss: 8.121994018554688\n",
      "[step: 2202] loss: 8.067937850952148\n",
      "[step: 2203] loss: 8.115090370178223\n",
      "[step: 2204] loss: 8.137594223022461\n",
      "[step: 2205] loss: 8.080209732055664\n",
      "[step: 2206] loss: 8.056446075439453\n",
      "[step: 2207] loss: 8.083314895629883\n",
      "[step: 2208] loss: 8.098979949951172\n",
      "[step: 2209] loss: 8.059335708618164\n",
      "[step: 2210] loss: 8.036870956420898\n",
      "[step: 2211] loss: 8.053271293640137\n",
      "[step: 2212] loss: 8.064666748046875\n",
      "[step: 2213] loss: 8.047972679138184\n",
      "[step: 2214] loss: 8.022579193115234\n",
      "[step: 2215] loss: 8.02469539642334\n",
      "[step: 2216] loss: 8.035749435424805\n",
      "[step: 2217] loss: 8.033622741699219\n",
      "[step: 2218] loss: 8.017951965332031\n",
      "[step: 2219] loss: 8.004364967346191\n",
      "[step: 2220] loss: 8.008100509643555\n",
      "[step: 2221] loss: 8.013154983520508\n",
      "[step: 2222] loss: 8.009714126586914\n",
      "[step: 2223] loss: 7.998291969299316\n",
      "[step: 2224] loss: 7.988541126251221\n",
      "[step: 2225] loss: 7.9888505935668945\n",
      "[step: 2226] loss: 7.990697383880615\n",
      "[step: 2227] loss: 7.989505767822266\n",
      "[step: 2228] loss: 7.982913494110107\n",
      "[step: 2229] loss: 7.974461555480957\n",
      "[step: 2230] loss: 7.970635414123535\n",
      "[step: 2231] loss: 7.969546318054199\n",
      "[step: 2232] loss: 7.969656944274902\n",
      "[step: 2233] loss: 7.967333793640137\n",
      "[step: 2234] loss: 7.962006568908691\n",
      "[step: 2235] loss: 7.956658363342285\n",
      "[step: 2236] loss: 7.952064514160156\n",
      "[step: 2237] loss: 7.949765205383301\n",
      "[step: 2238] loss: 7.948506832122803\n",
      "[step: 2239] loss: 7.946552276611328\n",
      "[step: 2240] loss: 7.943849086761475\n",
      "[step: 2241] loss: 7.93983268737793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2242] loss: 7.935646057128906\n",
      "[step: 2243] loss: 7.9318742752075195\n",
      "[step: 2244] loss: 7.928600311279297\n",
      "[step: 2245] loss: 7.926220417022705\n",
      "[step: 2246] loss: 7.923948764801025\n",
      "[step: 2247] loss: 7.921652793884277\n",
      "[step: 2248] loss: 7.919172763824463\n",
      "[step: 2249] loss: 7.916183948516846\n",
      "[step: 2250] loss: 7.913066387176514\n",
      "[step: 2251] loss: 7.909768104553223\n",
      "[step: 2252] loss: 7.9064412117004395\n",
      "[step: 2253] loss: 7.903295516967773\n",
      "[step: 2254] loss: 7.900169372558594\n",
      "[step: 2255] loss: 7.897236347198486\n",
      "[step: 2256] loss: 7.8944172859191895\n",
      "[step: 2257] loss: 7.891630172729492\n",
      "[step: 2258] loss: 7.888969421386719\n",
      "[step: 2259] loss: 7.886300086975098\n",
      "[step: 2260] loss: 7.883695125579834\n",
      "[step: 2261] loss: 7.881171226501465\n",
      "[step: 2262] loss: 7.8786821365356445\n",
      "[step: 2263] loss: 7.876348495483398\n",
      "[step: 2264] loss: 7.8742146492004395\n",
      "[step: 2265] loss: 7.872400283813477\n",
      "[step: 2266] loss: 7.8711724281311035\n",
      "[step: 2267] loss: 7.870889186859131\n",
      "[step: 2268] loss: 7.872256278991699\n",
      "[step: 2269] loss: 7.876526832580566\n",
      "[step: 2270] loss: 7.886074066162109\n",
      "[step: 2271] loss: 7.905126571655273\n",
      "[step: 2272] loss: 7.942032337188721\n",
      "[step: 2273] loss: 8.011777877807617\n",
      "[step: 2274] loss: 8.14356803894043\n",
      "[step: 2275] loss: 8.383392333984375\n",
      "[step: 2276] loss: 8.811899185180664\n",
      "[step: 2277] loss: 9.490116119384766\n",
      "[step: 2278] loss: 10.371018409729004\n",
      "[step: 2279] loss: 11.056174278259277\n",
      "[step: 2280] loss: 10.635214805603027\n",
      "[step: 2281] loss: 9.237482070922852\n",
      "[step: 2282] loss: 7.979177474975586\n",
      "[step: 2283] loss: 8.1802396774292\n",
      "[step: 2284] loss: 9.219747543334961\n",
      "[step: 2285] loss: 9.549018859863281\n",
      "[step: 2286] loss: 8.666478157043457\n",
      "[step: 2287] loss: 7.886948108673096\n",
      "[step: 2288] loss: 8.282081604003906\n",
      "[step: 2289] loss: 8.796911239624023\n",
      "[step: 2290] loss: 8.387537002563477\n",
      "[step: 2291] loss: 7.896103858947754\n",
      "[step: 2292] loss: 8.161561012268066\n",
      "[step: 2293] loss: 8.487091064453125\n",
      "[step: 2294] loss: 8.161516189575195\n",
      "[step: 2295] loss: 7.857853412628174\n",
      "[step: 2296] loss: 8.102434158325195\n",
      "[step: 2297] loss: 8.228240966796875\n",
      "[step: 2298] loss: 7.9595417976379395\n",
      "[step: 2299] loss: 7.862469673156738\n",
      "[step: 2300] loss: 8.05418872833252\n",
      "[step: 2301] loss: 8.089128494262695\n",
      "[step: 2302] loss: 7.875992774963379\n",
      "[step: 2303] loss: 7.854623317718506\n",
      "[step: 2304] loss: 8.000009536743164\n",
      "[step: 2305] loss: 7.982404708862305\n",
      "[step: 2306] loss: 7.846095561981201\n",
      "[step: 2307] loss: 7.830726146697998\n",
      "[step: 2308] loss: 7.926316261291504\n",
      "[step: 2309] loss: 7.924557685852051\n",
      "[step: 2310] loss: 7.830178260803223\n",
      "[step: 2311] loss: 7.800481796264648\n",
      "[step: 2312] loss: 7.852555274963379\n",
      "[step: 2313] loss: 7.872827529907227\n",
      "[step: 2314] loss: 7.824572563171387\n",
      "[step: 2315] loss: 7.781733512878418\n",
      "[step: 2316] loss: 7.798328399658203\n",
      "[step: 2317] loss: 7.829350471496582\n",
      "[step: 2318] loss: 7.820072174072266\n",
      "[step: 2319] loss: 7.782959938049316\n",
      "[step: 2320] loss: 7.765766620635986\n",
      "[step: 2321] loss: 7.779895305633545\n",
      "[step: 2322] loss: 7.794726848602295\n",
      "[step: 2323] loss: 7.785290241241455\n",
      "[step: 2324] loss: 7.762754440307617\n",
      "[step: 2325] loss: 7.749648094177246\n",
      "[step: 2326] loss: 7.753686904907227\n",
      "[step: 2327] loss: 7.76316499710083\n",
      "[step: 2328] loss: 7.762675762176514\n",
      "[step: 2329] loss: 7.750657558441162\n",
      "[step: 2330] loss: 7.737544536590576\n",
      "[step: 2331] loss: 7.732242584228516\n",
      "[step: 2332] loss: 7.734870910644531\n",
      "[step: 2333] loss: 7.738483428955078\n",
      "[step: 2334] loss: 7.737308025360107\n",
      "[step: 2335] loss: 7.730616092681885\n",
      "[step: 2336] loss: 7.721999168395996\n",
      "[step: 2337] loss: 7.715570449829102\n",
      "[step: 2338] loss: 7.713106155395508\n",
      "[step: 2339] loss: 7.7134480476379395\n",
      "[step: 2340] loss: 7.713896751403809\n",
      "[step: 2341] loss: 7.712470054626465\n",
      "[step: 2342] loss: 7.708604335784912\n",
      "[step: 2343] loss: 7.703372001647949\n",
      "[step: 2344] loss: 7.698150634765625\n",
      "[step: 2345] loss: 7.693853378295898\n",
      "[step: 2346] loss: 7.69099235534668\n",
      "[step: 2347] loss: 7.689258575439453\n",
      "[step: 2348] loss: 7.6879987716674805\n",
      "[step: 2349] loss: 7.686694145202637\n",
      "[step: 2350] loss: 7.684905052185059\n",
      "[step: 2351] loss: 7.682618618011475\n",
      "[step: 2352] loss: 7.6798295974731445\n",
      "[step: 2353] loss: 7.676618576049805\n",
      "[step: 2354] loss: 7.673323631286621\n",
      "[step: 2355] loss: 7.669997215270996\n",
      "[step: 2356] loss: 7.66679048538208\n",
      "[step: 2357] loss: 7.663714408874512\n",
      "[step: 2358] loss: 7.660731792449951\n",
      "[step: 2359] loss: 7.657909393310547\n",
      "[step: 2360] loss: 7.6551642417907715\n",
      "[step: 2361] loss: 7.652476787567139\n",
      "[step: 2362] loss: 7.649869441986084\n",
      "[step: 2363] loss: 7.647309303283691\n",
      "[step: 2364] loss: 7.64479923248291\n",
      "[step: 2365] loss: 7.642358303070068\n",
      "[step: 2366] loss: 7.640009880065918\n",
      "[step: 2367] loss: 7.637814521789551\n",
      "[step: 2368] loss: 7.635870933532715\n",
      "[step: 2369] loss: 7.634371757507324\n",
      "[step: 2370] loss: 7.633698463439941\n",
      "[step: 2371] loss: 7.634571075439453\n",
      "[step: 2372] loss: 7.638388633728027\n",
      "[step: 2373] loss: 7.647860527038574\n",
      "[step: 2374] loss: 7.668722629547119\n",
      "[step: 2375] loss: 7.711692810058594\n",
      "[step: 2376] loss: 7.8007988929748535\n",
      "[step: 2377] loss: 7.976343154907227\n",
      "[step: 2378] loss: 8.33139705657959\n",
      "[step: 2379] loss: 8.967588424682617\n",
      "[step: 2380] loss: 10.053207397460938\n",
      "[step: 2381] loss: 11.272014617919922\n",
      "[step: 2382] loss: 11.717262268066406\n",
      "[step: 2383] loss: 10.428546905517578\n",
      "[step: 2384] loss: 8.171613693237305\n",
      "[step: 2385] loss: 7.870667457580566\n",
      "[step: 2386] loss: 9.31820011138916\n",
      "[step: 2387] loss: 9.971695899963379\n",
      "[step: 2388] loss: 8.734613418579102\n",
      "[step: 2389] loss: 7.670159339904785\n",
      "[step: 2390] loss: 8.388331413269043\n",
      "[step: 2391] loss: 8.940338134765625\n",
      "[step: 2392] loss: 8.090217590332031\n",
      "[step: 2393] loss: 7.766382217407227\n",
      "[step: 2394] loss: 8.399361610412598\n",
      "[step: 2395] loss: 8.305440902709961\n",
      "[step: 2396] loss: 7.698762893676758\n",
      "[step: 2397] loss: 7.933713912963867\n",
      "[step: 2398] loss: 8.2269287109375\n",
      "[step: 2399] loss: 7.800236701965332\n",
      "[step: 2400] loss: 7.733732223510742\n",
      "[step: 2401] loss: 8.080724716186523\n",
      "[step: 2402] loss: 7.908668518066406\n",
      "[step: 2403] loss: 7.644112586975098\n",
      "[step: 2404] loss: 7.871662139892578\n",
      "[step: 2405] loss: 7.918974876403809\n",
      "[step: 2406] loss: 7.6842498779296875\n",
      "[step: 2407] loss: 7.68082857131958\n",
      "[step: 2408] loss: 7.849393844604492\n",
      "[step: 2409] loss: 7.767279148101807\n",
      "[step: 2410] loss: 7.597461700439453\n",
      "[step: 2411] loss: 7.733396530151367\n",
      "[step: 2412] loss: 7.798891067504883\n",
      "[step: 2413] loss: 7.654828071594238\n",
      "[step: 2414] loss: 7.610450267791748\n",
      "[step: 2415] loss: 7.703902244567871\n",
      "[step: 2416] loss: 7.722496509552002\n",
      "[step: 2417] loss: 7.595306396484375\n",
      "[step: 2418] loss: 7.596218585968018\n",
      "[step: 2419] loss: 7.669244766235352\n",
      "[step: 2420] loss: 7.648796558380127\n",
      "[step: 2421] loss: 7.57999324798584\n",
      "[step: 2422] loss: 7.569343566894531\n",
      "[step: 2423] loss: 7.622037887573242\n",
      "[step: 2424] loss: 7.60823917388916\n",
      "[step: 2425] loss: 7.560176849365234\n",
      "[step: 2426] loss: 7.551539897918701\n",
      "[step: 2427] loss: 7.576263427734375\n",
      "[step: 2428] loss: 7.586113452911377\n",
      "[step: 2429] loss: 7.555727005004883\n",
      "[step: 2430] loss: 7.533933639526367\n",
      "[step: 2431] loss: 7.542736053466797\n",
      "[step: 2432] loss: 7.556680679321289\n",
      "[step: 2433] loss: 7.551623344421387\n",
      "[step: 2434] loss: 7.528142929077148\n",
      "[step: 2435] loss: 7.519784927368164\n",
      "[step: 2436] loss: 7.527353286743164\n",
      "[step: 2437] loss: 7.533444404602051\n",
      "[step: 2438] loss: 7.52724027633667\n",
      "[step: 2439] loss: 7.51231575012207\n",
      "[step: 2440] loss: 7.505468368530273\n",
      "[step: 2441] loss: 7.507859706878662\n",
      "[step: 2442] loss: 7.511670112609863\n",
      "[step: 2443] loss: 7.509706497192383\n",
      "[step: 2444] loss: 7.500211238861084\n",
      "[step: 2445] loss: 7.492629528045654\n",
      "[step: 2446] loss: 7.49032735824585\n",
      "[step: 2447] loss: 7.491586208343506\n",
      "[step: 2448] loss: 7.492283344268799\n",
      "[step: 2449] loss: 7.488539695739746\n",
      "[step: 2450] loss: 7.482607364654541\n",
      "[step: 2451] loss: 7.477294921875\n",
      "[step: 2452] loss: 7.474376201629639\n",
      "[step: 2453] loss: 7.4739885330200195\n",
      "[step: 2454] loss: 7.473403453826904\n",
      "[step: 2455] loss: 7.471436500549316\n",
      "[step: 2456] loss: 7.467925071716309\n",
      "[step: 2457] loss: 7.463468551635742\n",
      "[step: 2458] loss: 7.459802150726318\n",
      "[step: 2459] loss: 7.457190990447998\n",
      "[step: 2460] loss: 7.455402374267578\n",
      "[step: 2461] loss: 7.454103946685791\n",
      "[step: 2462] loss: 7.452335357666016\n",
      "[step: 2463] loss: 7.449985504150391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2464] loss: 7.447147846221924\n",
      "[step: 2465] loss: 7.443893909454346\n",
      "[step: 2466] loss: 7.440863132476807\n",
      "[step: 2467] loss: 7.438051700592041\n",
      "[step: 2468] loss: 7.435528755187988\n",
      "[step: 2469] loss: 7.433407306671143\n",
      "[step: 2470] loss: 7.431330680847168\n",
      "[step: 2471] loss: 7.429346561431885\n",
      "[step: 2472] loss: 7.427329063415527\n",
      "[step: 2473] loss: 7.425131797790527\n",
      "[step: 2474] loss: 7.422915935516357\n",
      "[step: 2475] loss: 7.420592308044434\n",
      "[step: 2476] loss: 7.418181896209717\n",
      "[step: 2477] loss: 7.415806770324707\n",
      "[step: 2478] loss: 7.413393974304199\n",
      "[step: 2479] loss: 7.411016941070557\n",
      "[step: 2480] loss: 7.408687591552734\n",
      "[step: 2481] loss: 7.406394958496094\n",
      "[step: 2482] loss: 7.404199123382568\n",
      "[step: 2483] loss: 7.402109146118164\n",
      "[step: 2484] loss: 7.400157451629639\n",
      "[step: 2485] loss: 7.3984527587890625\n",
      "[step: 2486] loss: 7.397097587585449\n",
      "[step: 2487] loss: 7.3963446617126465\n",
      "[step: 2488] loss: 7.396570205688477\n",
      "[step: 2489] loss: 7.398486614227295\n",
      "[step: 2490] loss: 7.403241157531738\n",
      "[step: 2491] loss: 7.413347244262695\n",
      "[step: 2492] loss: 7.432362079620361\n",
      "[step: 2493] loss: 7.469053745269775\n",
      "[step: 2494] loss: 7.534792423248291\n",
      "[step: 2495] loss: 7.6594624519348145\n",
      "[step: 2496] loss: 7.8716888427734375\n",
      "[step: 2497] loss: 8.26103687286377\n",
      "[step: 2498] loss: 8.81380844116211\n",
      "[step: 2499] loss: 9.605400085449219\n",
      "[step: 2500] loss: 10.111204147338867\n",
      "[step: 2501] loss: 9.925080299377441\n",
      "[step: 2502] loss: 8.788326263427734\n",
      "[step: 2503] loss: 7.611652374267578\n",
      "[step: 2504] loss: 7.494169235229492\n",
      "[step: 2505] loss: 8.256994247436523\n",
      "[step: 2506] loss: 8.869171142578125\n",
      "[step: 2507] loss: 8.479715347290039\n",
      "[step: 2508] loss: 7.649246692657471\n",
      "[step: 2509] loss: 7.45521879196167\n",
      "[step: 2510] loss: 7.966228008270264\n",
      "[step: 2511] loss: 8.165351867675781\n",
      "[step: 2512] loss: 7.689985275268555\n",
      "[step: 2513] loss: 7.408196926116943\n",
      "[step: 2514] loss: 7.742423057556152\n",
      "[step: 2515] loss: 7.969761848449707\n",
      "[step: 2516] loss: 7.643067836761475\n",
      "[step: 2517] loss: 7.403146743774414\n",
      "[step: 2518] loss: 7.569264888763428\n",
      "[step: 2519] loss: 7.7475714683532715\n",
      "[step: 2520] loss: 7.559597969055176\n",
      "[step: 2521] loss: 7.370893955230713\n",
      "[step: 2522] loss: 7.460155010223389\n",
      "[step: 2523] loss: 7.574414253234863\n",
      "[step: 2524] loss: 7.51140022277832\n",
      "[step: 2525] loss: 7.371970176696777\n",
      "[step: 2526] loss: 7.375074863433838\n",
      "[step: 2527] loss: 7.472652435302734\n",
      "[step: 2528] loss: 7.468775272369385\n",
      "[step: 2529] loss: 7.377832412719727\n",
      "[step: 2530] loss: 7.3374104499816895\n",
      "[step: 2531] loss: 7.3819427490234375\n",
      "[step: 2532] loss: 7.424435615539551\n",
      "[step: 2533] loss: 7.387805938720703\n",
      "[step: 2534] loss: 7.332708835601807\n",
      "[step: 2535] loss: 7.325789451599121\n",
      "[step: 2536] loss: 7.357505798339844\n",
      "[step: 2537] loss: 7.377072811126709\n",
      "[step: 2538] loss: 7.352772235870361\n",
      "[step: 2539] loss: 7.316849708557129\n",
      "[step: 2540] loss: 7.307626724243164\n",
      "[step: 2541] loss: 7.323334693908691\n",
      "[step: 2542] loss: 7.338149070739746\n",
      "[step: 2543] loss: 7.330449104309082\n",
      "[step: 2544] loss: 7.30958890914917\n",
      "[step: 2545] loss: 7.293887615203857\n",
      "[step: 2546] loss: 7.292691707611084\n",
      "[step: 2547] loss: 7.301560878753662\n",
      "[step: 2548] loss: 7.306896209716797\n",
      "[step: 2549] loss: 7.302057266235352\n",
      "[step: 2550] loss: 7.290089130401611\n",
      "[step: 2551] loss: 7.27931022644043\n",
      "[step: 2552] loss: 7.275360107421875\n",
      "[step: 2553] loss: 7.277141571044922\n",
      "[step: 2554] loss: 7.280590057373047\n",
      "[step: 2555] loss: 7.280917644500732\n",
      "[step: 2556] loss: 7.277134895324707\n",
      "[step: 2557] loss: 7.270510196685791\n",
      "[step: 2558] loss: 7.2633514404296875\n",
      "[step: 2559] loss: 7.258565902709961\n",
      "[step: 2560] loss: 7.2564921379089355\n",
      "[step: 2561] loss: 7.256305694580078\n",
      "[step: 2562] loss: 7.256685256958008\n",
      "[step: 2563] loss: 7.25614595413208\n",
      "[step: 2564] loss: 7.254321098327637\n",
      "[step: 2565] loss: 7.2511796951293945\n",
      "[step: 2566] loss: 7.247162342071533\n",
      "[step: 2567] loss: 7.243044853210449\n",
      "[step: 2568] loss: 7.23913049697876\n",
      "[step: 2569] loss: 7.235811233520508\n",
      "[step: 2570] loss: 7.233064651489258\n",
      "[step: 2571] loss: 7.230856895446777\n",
      "[step: 2572] loss: 7.229036331176758\n",
      "[step: 2573] loss: 7.227423191070557\n",
      "[step: 2574] loss: 7.22597074508667\n",
      "[step: 2575] loss: 7.2245965003967285\n",
      "[step: 2576] loss: 7.223392486572266\n",
      "[step: 2577] loss: 7.222301483154297\n",
      "[step: 2578] loss: 7.22150993347168\n",
      "[step: 2579] loss: 7.221199989318848\n",
      "[step: 2580] loss: 7.221678733825684\n",
      "[step: 2581] loss: 7.2232985496521\n",
      "[step: 2582] loss: 7.226987838745117\n",
      "[step: 2583] loss: 7.233588218688965\n",
      "[step: 2584] loss: 7.245885848999023\n",
      "[step: 2585] loss: 7.26621675491333\n",
      "[step: 2586] loss: 7.302348613739014\n",
      "[step: 2587] loss: 7.359633445739746\n",
      "[step: 2588] loss: 7.461525917053223\n",
      "[step: 2589] loss: 7.614373683929443\n",
      "[step: 2590] loss: 7.8794145584106445\n",
      "[step: 2591] loss: 8.213659286499023\n",
      "[step: 2592] loss: 8.695653915405273\n",
      "[step: 2593] loss: 9.002304077148438\n",
      "[step: 2594] loss: 9.077493667602539\n",
      "[step: 2595] loss: 8.541932106018066\n",
      "[step: 2596] loss: 7.750932216644287\n",
      "[step: 2597] loss: 7.2361907958984375\n",
      "[step: 2598] loss: 7.301210880279541\n",
      "[step: 2599] loss: 7.73628044128418\n",
      "[step: 2600] loss: 8.070802688598633\n",
      "[step: 2601] loss: 8.019147872924805\n",
      "[step: 2602] loss: 7.528700351715088\n",
      "[step: 2603] loss: 7.217133045196533\n",
      "[step: 2604] loss: 7.339212417602539\n",
      "[step: 2605] loss: 7.611426830291748\n",
      "[step: 2606] loss: 7.640297889709473\n",
      "[step: 2607] loss: 7.386816501617432\n",
      "[step: 2608] loss: 7.193785667419434\n",
      "[step: 2609] loss: 7.252187728881836\n",
      "[step: 2610] loss: 7.414870738983154\n",
      "[step: 2611] loss: 7.454231262207031\n",
      "[step: 2612] loss: 7.294987201690674\n",
      "[step: 2613] loss: 7.174959182739258\n",
      "[step: 2614] loss: 7.213230133056641\n",
      "[step: 2615] loss: 7.311491012573242\n",
      "[step: 2616] loss: 7.344912528991699\n",
      "[step: 2617] loss: 7.2666778564453125\n",
      "[step: 2618] loss: 7.174507141113281\n",
      "[step: 2619] loss: 7.161283493041992\n",
      "[step: 2620] loss: 7.212152481079102\n",
      "[step: 2621] loss: 7.260914325714111\n",
      "[step: 2622] loss: 7.246227264404297\n",
      "[step: 2623] loss: 7.1912431716918945\n",
      "[step: 2624] loss: 7.145998477935791\n",
      "[step: 2625] loss: 7.141645431518555\n",
      "[step: 2626] loss: 7.169692039489746\n",
      "[step: 2627] loss: 7.193941116333008\n",
      "[step: 2628] loss: 7.193249702453613\n",
      "[step: 2629] loss: 7.169363498687744\n",
      "[step: 2630] loss: 7.140751838684082\n",
      "[step: 2631] loss: 7.123658180236816\n",
      "[step: 2632] loss: 7.122979164123535\n",
      "[step: 2633] loss: 7.134506702423096\n",
      "[step: 2634] loss: 7.145554542541504\n",
      "[step: 2635] loss: 7.149004936218262\n",
      "[step: 2636] loss: 7.142803192138672\n",
      "[step: 2637] loss: 7.129720687866211\n",
      "[step: 2638] loss: 7.116026401519775\n",
      "[step: 2639] loss: 7.105889320373535\n",
      "[step: 2640] loss: 7.100849151611328\n",
      "[step: 2641] loss: 7.100834369659424\n",
      "[step: 2642] loss: 7.103325366973877\n",
      "[step: 2643] loss: 7.106386184692383\n",
      "[step: 2644] loss: 7.108588218688965\n",
      "[step: 2645] loss: 7.109064102172852\n",
      "[step: 2646] loss: 7.107648849487305\n",
      "[step: 2647] loss: 7.104412078857422\n",
      "[step: 2648] loss: 7.100629806518555\n",
      "[step: 2649] loss: 7.095885276794434\n",
      "[step: 2650] loss: 7.091320991516113\n",
      "[step: 2651] loss: 7.0867204666137695\n",
      "[step: 2652] loss: 7.082581520080566\n",
      "[step: 2653] loss: 7.079029083251953\n",
      "[step: 2654] loss: 7.075984001159668\n",
      "[step: 2655] loss: 7.073385715484619\n",
      "[step: 2656] loss: 7.071285247802734\n",
      "[step: 2657] loss: 7.069721221923828\n",
      "[step: 2658] loss: 7.068853378295898\n",
      "[step: 2659] loss: 7.068970203399658\n",
      "[step: 2660] loss: 7.070818901062012\n",
      "[step: 2661] loss: 7.0750226974487305\n",
      "[step: 2662] loss: 7.084085464477539\n",
      "[step: 2663] loss: 7.100513935089111\n",
      "[step: 2664] loss: 7.131748676300049\n",
      "[step: 2665] loss: 7.184822082519531\n",
      "[step: 2666] loss: 7.285294055938721\n",
      "[step: 2667] loss: 7.4467244148254395\n",
      "[step: 2668] loss: 7.749629020690918\n",
      "[step: 2669] loss: 8.163005828857422\n",
      "[step: 2670] loss: 8.82190227508545\n",
      "[step: 2671] loss: 9.28356647491455\n",
      "[step: 2672] loss: 9.4617338180542\n",
      "[step: 2673] loss: 8.732874870300293\n",
      "[step: 2674] loss: 7.633214950561523\n",
      "[step: 2675] loss: 7.061501979827881\n",
      "[step: 2676] loss: 7.360248565673828\n",
      "[step: 2677] loss: 8.026872634887695\n",
      "[step: 2678] loss: 8.274528503417969\n",
      "[step: 2679] loss: 7.8867082595825195\n",
      "[step: 2680] loss: 7.190844535827637\n",
      "[step: 2681] loss: 7.169219493865967\n",
      "[step: 2682] loss: 7.617994785308838\n",
      "[step: 2683] loss: 7.676039695739746\n",
      "[step: 2684] loss: 7.292754173278809\n",
      "[step: 2685] loss: 7.053679466247559\n",
      "[step: 2686] loss: 7.260918140411377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2687] loss: 7.466897010803223\n",
      "[step: 2688] loss: 7.336639404296875\n",
      "[step: 2689] loss: 7.089424133300781\n",
      "[step: 2690] loss: 7.081241607666016\n",
      "[step: 2691] loss: 7.268787384033203\n",
      "[step: 2692] loss: 7.325329303741455\n",
      "[step: 2693] loss: 7.142992973327637\n",
      "[step: 2694] loss: 7.023172855377197\n",
      "[step: 2695] loss: 7.102306842803955\n",
      "[step: 2696] loss: 7.202939987182617\n",
      "[step: 2697] loss: 7.175998687744141\n",
      "[step: 2698] loss: 7.063117027282715\n",
      "[step: 2699] loss: 7.007543563842773\n",
      "[step: 2700] loss: 7.070548057556152\n",
      "[step: 2701] loss: 7.132439613342285\n",
      "[step: 2702] loss: 7.115124702453613\n",
      "[step: 2703] loss: 7.042553424835205\n",
      "[step: 2704] loss: 6.994167327880859\n",
      "[step: 2705] loss: 7.0189409255981445\n",
      "[step: 2706] loss: 7.061092853546143\n",
      "[step: 2707] loss: 7.073561668395996\n",
      "[step: 2708] loss: 7.038883209228516\n",
      "[step: 2709] loss: 6.9928436279296875\n",
      "[step: 2710] loss: 6.980216979980469\n",
      "[step: 2711] loss: 6.997317314147949\n",
      "[step: 2712] loss: 7.02092170715332\n",
      "[step: 2713] loss: 7.0220489501953125\n",
      "[step: 2714] loss: 7.002485275268555\n",
      "[step: 2715] loss: 6.97825288772583\n",
      "[step: 2716] loss: 6.9630327224731445\n",
      "[step: 2717] loss: 6.966109275817871\n",
      "[step: 2718] loss: 6.9754438400268555\n",
      "[step: 2719] loss: 6.982419013977051\n",
      "[step: 2720] loss: 6.980759620666504\n",
      "[step: 2721] loss: 6.969845294952393\n",
      "[step: 2722] loss: 6.957748889923096\n",
      "[step: 2723] loss: 6.948240280151367\n",
      "[step: 2724] loss: 6.944770812988281\n",
      "[step: 2725] loss: 6.945833206176758\n",
      "[step: 2726] loss: 6.949129104614258\n",
      "[step: 2727] loss: 6.9512858390808105\n",
      "[step: 2728] loss: 6.949993133544922\n",
      "[step: 2729] loss: 6.946340560913086\n",
      "[step: 2730] loss: 6.9403486251831055\n",
      "[step: 2731] loss: 6.934268951416016\n",
      "[step: 2732] loss: 6.928992748260498\n",
      "[step: 2733] loss: 6.924572944641113\n",
      "[step: 2734] loss: 6.921819686889648\n",
      "[step: 2735] loss: 6.919964790344238\n",
      "[step: 2736] loss: 6.919012069702148\n",
      "[step: 2737] loss: 6.918262958526611\n",
      "[step: 2738] loss: 6.917845726013184\n",
      "[step: 2739] loss: 6.917459487915039\n",
      "[step: 2740] loss: 6.917031288146973\n",
      "[step: 2741] loss: 6.9171142578125\n",
      "[step: 2742] loss: 6.917186260223389\n",
      "[step: 2743] loss: 6.918218612670898\n",
      "[step: 2744] loss: 6.920143127441406\n",
      "[step: 2745] loss: 6.924147605895996\n",
      "[step: 2746] loss: 6.930542945861816\n",
      "[step: 2747] loss: 6.942044258117676\n",
      "[step: 2748] loss: 6.959451675415039\n",
      "[step: 2749] loss: 6.990485191345215\n",
      "[step: 2750] loss: 7.035892486572266\n",
      "[step: 2751] loss: 7.1149821281433105\n",
      "[step: 2752] loss: 7.22451114654541\n",
      "[step: 2753] loss: 7.415279388427734\n",
      "[step: 2754] loss: 7.6385393142700195\n",
      "[step: 2755] loss: 7.988219261169434\n",
      "[step: 2756] loss: 8.216124534606934\n",
      "[step: 2757] loss: 8.3983154296875\n",
      "[step: 2758] loss: 8.148193359375\n",
      "[step: 2759] loss: 7.671365737915039\n",
      "[step: 2760] loss: 7.135403156280518\n",
      "[step: 2761] loss: 6.884696960449219\n",
      "[step: 2762] loss: 6.987762928009033\n",
      "[step: 2763] loss: 7.279634475708008\n",
      "[step: 2764] loss: 7.528074741363525\n",
      "[step: 2765] loss: 7.460203170776367\n",
      "[step: 2766] loss: 7.196691036224365\n",
      "[step: 2767] loss: 6.92805814743042\n",
      "[step: 2768] loss: 6.9065260887146\n",
      "[step: 2769] loss: 7.085379123687744\n",
      "[step: 2770] loss: 7.228665828704834\n",
      "[step: 2771] loss: 7.199015140533447\n",
      "[step: 2772] loss: 7.023680686950684\n",
      "[step: 2773] loss: 6.884597301483154\n",
      "[step: 2774] loss: 6.87808895111084\n",
      "[step: 2775] loss: 6.970947265625\n",
      "[step: 2776] loss: 7.0521345138549805\n",
      "[step: 2777] loss: 7.025531768798828\n",
      "[step: 2778] loss: 6.939785957336426\n",
      "[step: 2779] loss: 6.861181735992432\n",
      "[step: 2780] loss: 6.848210334777832\n",
      "[step: 2781] loss: 6.890964508056641\n",
      "[step: 2782] loss: 6.937413215637207\n",
      "[step: 2783] loss: 6.949512004852295\n",
      "[step: 2784] loss: 6.918475151062012\n",
      "[step: 2785] loss: 6.872407913208008\n",
      "[step: 2786] loss: 6.836028099060059\n",
      "[step: 2787] loss: 6.828197479248047\n",
      "[step: 2788] loss: 6.844891548156738\n",
      "[step: 2789] loss: 6.867879390716553\n",
      "[step: 2790] loss: 6.8836236000061035\n",
      "[step: 2791] loss: 6.881494522094727\n",
      "[step: 2792] loss: 6.8663763999938965\n",
      "[step: 2793] loss: 6.843947410583496\n",
      "[step: 2794] loss: 6.823983669281006\n",
      "[step: 2795] loss: 6.810707092285156\n",
      "[step: 2796] loss: 6.806272506713867\n",
      "[step: 2797] loss: 6.809304237365723\n",
      "[step: 2798] loss: 6.816069602966309\n",
      "[step: 2799] loss: 6.823829174041748\n",
      "[step: 2800] loss: 6.829951286315918\n",
      "[step: 2801] loss: 6.8343505859375\n",
      "[step: 2802] loss: 6.835163593292236\n",
      "[step: 2803] loss: 6.833865165710449\n",
      "[step: 2804] loss: 6.829193592071533\n",
      "[step: 2805] loss: 6.8244476318359375\n",
      "[step: 2806] loss: 6.818799018859863\n",
      "[step: 2807] loss: 6.814696788787842\n",
      "[step: 2808] loss: 6.811030387878418\n",
      "[step: 2809] loss: 6.809178352355957\n",
      "[step: 2810] loss: 6.808170318603516\n",
      "[step: 2811] loss: 6.80955696105957\n",
      "[step: 2812] loss: 6.812190532684326\n",
      "[step: 2813] loss: 6.819029808044434\n",
      "[step: 2814] loss: 6.829645156860352\n",
      "[step: 2815] loss: 6.849660873413086\n",
      "[step: 2816] loss: 6.879010200500488\n",
      "[step: 2817] loss: 6.931672096252441\n",
      "[step: 2818] loss: 7.0038557052612305\n",
      "[step: 2819] loss: 7.131011486053467\n",
      "[step: 2820] loss: 7.286190986633301\n",
      "[step: 2821] loss: 7.541333198547363\n",
      "[step: 2822] loss: 7.769443511962891\n",
      "[step: 2823] loss: 8.062644958496094\n",
      "[step: 2824] loss: 8.08065414428711\n",
      "[step: 2825] loss: 7.936188697814941\n",
      "[step: 2826] loss: 7.4733357429504395\n",
      "[step: 2827] loss: 7.010952472686768\n",
      "[step: 2828] loss: 6.76436710357666\n",
      "[step: 2829] loss: 6.820022106170654\n",
      "[step: 2830] loss: 7.059689521789551\n",
      "[step: 2831] loss: 7.271059036254883\n",
      "[step: 2832] loss: 7.325855255126953\n",
      "[step: 2833] loss: 7.101199150085449\n",
      "[step: 2834] loss: 6.847405433654785\n",
      "[step: 2835] loss: 6.750051021575928\n",
      "[step: 2836] loss: 6.851240158081055\n",
      "[step: 2837] loss: 7.018434047698975\n",
      "[step: 2838] loss: 7.074308395385742\n",
      "[step: 2839] loss: 6.992398738861084\n",
      "[step: 2840] loss: 6.834902286529541\n",
      "[step: 2841] loss: 6.739123344421387\n",
      "[step: 2842] loss: 6.753717422485352\n",
      "[step: 2843] loss: 6.835885047912598\n",
      "[step: 2844] loss: 6.904616355895996\n",
      "[step: 2845] loss: 6.890066146850586\n",
      "[step: 2846] loss: 6.825595855712891\n",
      "[step: 2847] loss: 6.750250816345215\n",
      "[step: 2848] loss: 6.714565277099609\n",
      "[step: 2849] loss: 6.726078987121582\n",
      "[step: 2850] loss: 6.764344692230225\n",
      "[step: 2851] loss: 6.799932956695557\n",
      "[step: 2852] loss: 6.805895805358887\n",
      "[step: 2853] loss: 6.7861528396606445\n",
      "[step: 2854] loss: 6.74918794631958\n",
      "[step: 2855] loss: 6.716625213623047\n",
      "[step: 2856] loss: 6.696413040161133\n",
      "[step: 2857] loss: 6.693397045135498\n",
      "[step: 2858] loss: 6.7037835121154785\n",
      "[step: 2859] loss: 6.719099998474121\n",
      "[step: 2860] loss: 6.733327865600586\n",
      "[step: 2861] loss: 6.740714073181152\n",
      "[step: 2862] loss: 6.741854190826416\n",
      "[step: 2863] loss: 6.734066009521484\n",
      "[step: 2864] loss: 6.722748756408691\n",
      "[step: 2865] loss: 6.708131790161133\n",
      "[step: 2866] loss: 6.69550895690918\n",
      "[step: 2867] loss: 6.684349536895752\n",
      "[step: 2868] loss: 6.675318717956543\n",
      "[step: 2869] loss: 6.668539524078369\n",
      "[step: 2870] loss: 6.663848876953125\n",
      "[step: 2871] loss: 6.660308837890625\n",
      "[step: 2872] loss: 6.657705307006836\n",
      "[step: 2873] loss: 6.656092643737793\n",
      "[step: 2874] loss: 6.655106067657471\n",
      "[step: 2875] loss: 6.6546173095703125\n",
      "[step: 2876] loss: 6.654946327209473\n",
      "[step: 2877] loss: 6.656618118286133\n",
      "[step: 2878] loss: 6.659965515136719\n",
      "[step: 2879] loss: 6.666459083557129\n",
      "[step: 2880] loss: 6.677818775177002\n",
      "[step: 2881] loss: 6.699552536010742\n",
      "[step: 2882] loss: 6.735860347747803\n",
      "[step: 2883] loss: 6.805655002593994\n",
      "[step: 2884] loss: 6.916722774505615\n",
      "[step: 2885] loss: 7.12965202331543\n",
      "[step: 2886] loss: 7.424570560455322\n",
      "[step: 2887] loss: 7.950179100036621\n",
      "[step: 2888] loss: 8.41617488861084\n",
      "[step: 2889] loss: 8.935224533081055\n",
      "[step: 2890] loss: 8.689493179321289\n",
      "[step: 2891] loss: 7.897104263305664\n",
      "[step: 2892] loss: 6.977400779724121\n",
      "[step: 2893] loss: 6.643445014953613\n",
      "[step: 2894] loss: 7.0251312255859375\n",
      "[step: 2895] loss: 7.606241226196289\n",
      "[step: 2896] loss: 7.788285255432129\n",
      "[step: 2897] loss: 7.275243282318115\n",
      "[step: 2898] loss: 6.746866226196289\n",
      "[step: 2899] loss: 6.740679740905762\n",
      "[step: 2900] loss: 7.114795684814453\n",
      "[step: 2901] loss: 7.263696670532227\n",
      "[step: 2902] loss: 6.910312175750732\n",
      "[step: 2903] loss: 6.6461968421936035\n",
      "[step: 2904] loss: 6.749300479888916\n",
      "[step: 2905] loss: 6.96330451965332\n",
      "[step: 2906] loss: 6.957401275634766\n",
      "[step: 2907] loss: 6.724379539489746\n",
      "[step: 2908] loss: 6.6236162185668945\n",
      "[step: 2909] loss: 6.746921539306641\n",
      "[step: 2910] loss: 6.8581223487854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2911] loss: 6.799383163452148\n",
      "[step: 2912] loss: 6.651198387145996\n",
      "[step: 2913] loss: 6.609700679779053\n",
      "[step: 2914] loss: 6.68154239654541\n",
      "[step: 2915] loss: 6.747464179992676\n",
      "[step: 2916] loss: 6.724977970123291\n",
      "[step: 2917] loss: 6.633468151092529\n",
      "[step: 2918] loss: 6.589055061340332\n",
      "[step: 2919] loss: 6.616653919219971\n",
      "[step: 2920] loss: 6.667494297027588\n",
      "[step: 2921] loss: 6.680230140686035\n",
      "[step: 2922] loss: 6.6382036209106445\n",
      "[step: 2923] loss: 6.5906548500061035\n",
      "[step: 2924] loss: 6.573103904724121\n",
      "[step: 2925] loss: 6.590724468231201\n",
      "[step: 2926] loss: 6.616101264953613\n",
      "[step: 2927] loss: 6.6235785484313965\n",
      "[step: 2928] loss: 6.607850551605225\n",
      "[step: 2929] loss: 6.580330848693848\n",
      "[step: 2930] loss: 6.560245513916016\n",
      "[step: 2931] loss: 6.556646823883057\n",
      "[step: 2932] loss: 6.566197395324707\n",
      "[step: 2933] loss: 6.577898979187012\n",
      "[step: 2934] loss: 6.5813493728637695\n",
      "[step: 2935] loss: 6.574755668640137\n",
      "[step: 2936] loss: 6.561327934265137\n",
      "[step: 2937] loss: 6.54787540435791\n",
      "[step: 2938] loss: 6.539178848266602\n",
      "[step: 2939] loss: 6.536858081817627\n",
      "[step: 2940] loss: 6.539242744445801\n",
      "[step: 2941] loss: 6.543026924133301\n",
      "[step: 2942] loss: 6.545591354370117\n",
      "[step: 2943] loss: 6.545413494110107\n",
      "[step: 2944] loss: 6.542258262634277\n",
      "[step: 2945] loss: 6.536518096923828\n",
      "[step: 2946] loss: 6.530305862426758\n",
      "[step: 2947] loss: 6.5242815017700195\n",
      "[step: 2948] loss: 6.518944263458252\n",
      "[step: 2949] loss: 6.5146002769470215\n",
      "[step: 2950] loss: 6.511396884918213\n",
      "[step: 2951] loss: 6.508970260620117\n",
      "[step: 2952] loss: 6.507150650024414\n",
      "[step: 2953] loss: 6.505712985992432\n",
      "[step: 2954] loss: 6.504752159118652\n",
      "[step: 2955] loss: 6.504143714904785\n",
      "[step: 2956] loss: 6.503961086273193\n",
      "[step: 2957] loss: 6.50459098815918\n",
      "[step: 2958] loss: 6.50629186630249\n",
      "[step: 2959] loss: 6.509981155395508\n",
      "[step: 2960] loss: 6.516263008117676\n",
      "[step: 2961] loss: 6.528143405914307\n",
      "[step: 2962] loss: 6.547072887420654\n",
      "[step: 2963] loss: 6.582547187805176\n",
      "[step: 2964] loss: 6.636744499206543\n",
      "[step: 2965] loss: 6.738765716552734\n",
      "[step: 2966] loss: 6.881225109100342\n",
      "[step: 2967] loss: 7.145347595214844\n",
      "[step: 2968] loss: 7.439312934875488\n",
      "[step: 2969] loss: 7.918941497802734\n",
      "[step: 2970] loss: 8.136308670043945\n",
      "[step: 2971] loss: 8.225875854492188\n",
      "[step: 2972] loss: 7.714134216308594\n",
      "[step: 2973] loss: 7.011754989624023\n",
      "[step: 2974] loss: 6.534570217132568\n",
      "[step: 2975] loss: 6.546617031097412\n",
      "[step: 2976] loss: 6.912752151489258\n",
      "[step: 2977] loss: 7.250828742980957\n",
      "[step: 2978] loss: 7.272948265075684\n",
      "[step: 2979] loss: 6.882803440093994\n",
      "[step: 2980] loss: 6.538765907287598\n",
      "[step: 2981] loss: 6.537575721740723\n",
      "[step: 2982] loss: 6.792093753814697\n",
      "[step: 2983] loss: 6.9658942222595215\n",
      "[step: 2984] loss: 6.805088996887207\n",
      "[step: 2985] loss: 6.563244819641113\n",
      "[step: 2986] loss: 6.464193344116211\n",
      "[step: 2987] loss: 6.568719863891602\n",
      "[step: 2988] loss: 6.703279495239258\n",
      "[step: 2989] loss: 6.676774978637695\n",
      "[step: 2990] loss: 6.54831600189209\n",
      "[step: 2991] loss: 6.453399181365967\n",
      "[step: 2992] loss: 6.4742560386657715\n",
      "[step: 2993] loss: 6.560584545135498\n",
      "[step: 2994] loss: 6.6012678146362305\n",
      "[step: 2995] loss: 6.5615339279174805\n",
      "[step: 2996] loss: 6.481104373931885\n",
      "[step: 2997] loss: 6.435513496398926\n",
      "[step: 2998] loss: 6.44717264175415\n",
      "[step: 2999] loss: 6.490825653076172\n",
      "[step: 3000] loss: 6.524901390075684\n",
      "[step: 3001] loss: 6.515341758728027\n",
      "[step: 3002] loss: 6.479634761810303\n",
      "[step: 3003] loss: 6.436314582824707\n",
      "[step: 3004] loss: 6.415241241455078\n",
      "[step: 3005] loss: 6.421938419342041\n",
      "[step: 3006] loss: 6.442156791687012\n",
      "[step: 3007] loss: 6.460324287414551\n",
      "[step: 3008] loss: 6.464020252227783\n",
      "[step: 3009] loss: 6.453123092651367\n",
      "[step: 3010] loss: 6.43104887008667\n",
      "[step: 3011] loss: 6.410616874694824\n",
      "[step: 3012] loss: 6.397761344909668\n",
      "[step: 3013] loss: 6.394125938415527\n",
      "[step: 3014] loss: 6.397856712341309\n",
      "[step: 3015] loss: 6.404924392700195\n",
      "[step: 3016] loss: 6.411687850952148\n",
      "[step: 3017] loss: 6.414329528808594\n",
      "[step: 3018] loss: 6.413457870483398\n",
      "[step: 3019] loss: 6.408276557922363\n",
      "[step: 3020] loss: 6.401625633239746\n",
      "[step: 3021] loss: 6.393404960632324\n",
      "[step: 3022] loss: 6.385387897491455\n",
      "[step: 3023] loss: 6.378280162811279\n",
      "[step: 3024] loss: 6.372586250305176\n",
      "[step: 3025] loss: 6.3678436279296875\n",
      "[step: 3026] loss: 6.364234924316406\n",
      "[step: 3027] loss: 6.361508369445801\n",
      "[step: 3028] loss: 6.359294891357422\n",
      "[step: 3029] loss: 6.357481956481934\n",
      "[step: 3030] loss: 6.3560404777526855\n",
      "[step: 3031] loss: 6.355037689208984\n",
      "[step: 3032] loss: 6.354582786560059\n",
      "[step: 3033] loss: 6.3549299240112305\n",
      "[step: 3034] loss: 6.356610298156738\n",
      "[step: 3035] loss: 6.360960006713867\n",
      "[step: 3036] loss: 6.369255065917969\n",
      "[step: 3037] loss: 6.385912895202637\n",
      "[step: 3038] loss: 6.4141950607299805\n",
      "[step: 3039] loss: 6.469338417053223\n",
      "[step: 3040] loss: 6.558128356933594\n",
      "[step: 3041] loss: 6.734445095062256\n",
      "[step: 3042] loss: 6.989066123962402\n",
      "[step: 3043] loss: 7.4798078536987305\n",
      "[step: 3044] loss: 7.952392578125\n",
      "[step: 3045] loss: 8.613624572753906\n",
      "[step: 3046] loss: 8.533496856689453\n",
      "[step: 3047] loss: 7.910238265991211\n",
      "[step: 3048] loss: 6.956084728240967\n",
      "[step: 3049] loss: 6.378355503082275\n",
      "[step: 3050] loss: 6.58135986328125\n",
      "[step: 3051] loss: 7.195254325866699\n",
      "[step: 3052] loss: 7.514065265655518\n",
      "[step: 3053] loss: 7.160608291625977\n",
      "[step: 3054] loss: 6.556369781494141\n",
      "[step: 3055] loss: 6.403634548187256\n",
      "[step: 3056] loss: 6.76619815826416\n",
      "[step: 3057] loss: 6.9941911697387695\n",
      "[step: 3058] loss: 6.718853950500488\n",
      "[step: 3059] loss: 6.403496742248535\n",
      "[step: 3060] loss: 6.391651153564453\n",
      "[step: 3061] loss: 6.609203815460205\n",
      "[step: 3062] loss: 6.700905799865723\n",
      "[step: 3063] loss: 6.479373931884766\n",
      "[step: 3064] loss: 6.323633670806885\n",
      "[step: 3065] loss: 6.403720855712891\n",
      "[step: 3066] loss: 6.535919666290283\n",
      "[step: 3067] loss: 6.535521507263184\n",
      "[step: 3068] loss: 6.381903171539307\n",
      "[step: 3069] loss: 6.304635524749756\n",
      "[step: 3070] loss: 6.360154628753662\n",
      "[step: 3071] loss: 6.434289932250977\n",
      "[step: 3072] loss: 6.4296345710754395\n",
      "[step: 3073] loss: 6.347135543823242\n",
      "[step: 3074] loss: 6.291502952575684\n",
      "[step: 3075] loss: 6.3067474365234375\n",
      "[step: 3076] loss: 6.356280326843262\n",
      "[step: 3077] loss: 6.377119064331055\n",
      "[step: 3078] loss: 6.344083786010742\n",
      "[step: 3079] loss: 6.295417785644531\n",
      "[step: 3080] loss: 6.273436546325684\n",
      "[step: 3081] loss: 6.286367416381836\n",
      "[step: 3082] loss: 6.311201095581055\n",
      "[step: 3083] loss: 6.321144104003906\n",
      "[step: 3084] loss: 6.3095502853393555\n",
      "[step: 3085] loss: 6.281903266906738\n",
      "[step: 3086] loss: 6.2605485916137695\n",
      "[step: 3087] loss: 6.256038665771484\n",
      "[step: 3088] loss: 6.264937400817871\n",
      "[step: 3089] loss: 6.276350021362305\n",
      "[step: 3090] loss: 6.278478145599365\n",
      "[step: 3091] loss: 6.271707057952881\n",
      "[step: 3092] loss: 6.258514881134033\n",
      "[step: 3093] loss: 6.245904922485352\n",
      "[step: 3094] loss: 6.238128185272217\n",
      "[step: 3095] loss: 6.2367753982543945\n",
      "[step: 3096] loss: 6.23970365524292\n",
      "[step: 3097] loss: 6.243128776550293\n",
      "[step: 3098] loss: 6.244532585144043\n",
      "[step: 3099] loss: 6.242560863494873\n",
      "[step: 3100] loss: 6.238344192504883\n",
      "[step: 3101] loss: 6.2322845458984375\n",
      "[step: 3102] loss: 6.226113319396973\n",
      "[step: 3103] loss: 6.220287322998047\n",
      "[step: 3104] loss: 6.215799331665039\n",
      "[step: 3105] loss: 6.212594509124756\n",
      "[step: 3106] loss: 6.210369110107422\n",
      "[step: 3107] loss: 6.208847999572754\n",
      "[step: 3108] loss: 6.207879066467285\n",
      "[step: 3109] loss: 6.207335472106934\n",
      "[step: 3110] loss: 6.207055568695068\n",
      "[step: 3111] loss: 6.207125663757324\n",
      "[step: 3112] loss: 6.207617282867432\n",
      "[step: 3113] loss: 6.209183692932129\n",
      "[step: 3114] loss: 6.211702346801758\n",
      "[step: 3115] loss: 6.216809272766113\n",
      "[step: 3116] loss: 6.224462985992432\n",
      "[step: 3117] loss: 6.238936424255371\n",
      "[step: 3118] loss: 6.260267734527588\n",
      "[step: 3119] loss: 6.299258232116699\n",
      "[step: 3120] loss: 6.353791236877441\n",
      "[step: 3121] loss: 6.4540114402771\n",
      "[step: 3122] loss: 6.580133438110352\n",
      "[step: 3123] loss: 6.812799453735352\n",
      "[step: 3124] loss: 7.033266067504883\n",
      "[step: 3125] loss: 7.392503261566162\n",
      "[step: 3126] loss: 7.50203800201416\n",
      "[step: 3127] loss: 7.530605792999268\n",
      "[step: 3128] loss: 7.154150009155273\n",
      "[step: 3129] loss: 6.652776718139648\n",
      "[step: 3130] loss: 6.270273208618164\n",
      "[step: 3131] loss: 6.1839823722839355\n",
      "[step: 3132] loss: 6.3726935386657715\n",
      "[step: 3133] loss: 6.638693809509277\n",
      "[step: 3134] loss: 6.761134147644043\n",
      "[step: 3135] loss: 6.6199049949646\n",
      "[step: 3136] loss: 6.356978416442871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3137] loss: 6.180057525634766\n",
      "[step: 3138] loss: 6.226798057556152\n",
      "[step: 3139] loss: 6.397858619689941\n",
      "[step: 3140] loss: 6.504621505737305\n",
      "[step: 3141] loss: 6.48654842376709\n",
      "[step: 3142] loss: 6.317477703094482\n",
      "[step: 3143] loss: 6.181260585784912\n",
      "[step: 3144] loss: 6.159537315368652\n",
      "[step: 3145] loss: 6.239967346191406\n",
      "[step: 3146] loss: 6.332814693450928\n",
      "[step: 3147] loss: 6.334859848022461\n",
      "[step: 3148] loss: 6.277188301086426\n",
      "[step: 3149] loss: 6.189323425292969\n",
      "[step: 3150] loss: 6.137978553771973\n",
      "[step: 3151] loss: 6.141343116760254\n",
      "[step: 3152] loss: 6.180416584014893\n",
      "[step: 3153] loss: 6.221680641174316\n",
      "[step: 3154] loss: 6.231409072875977\n",
      "[step: 3155] loss: 6.211373805999756\n",
      "[step: 3156] loss: 6.171955585479736\n",
      "[step: 3157] loss: 6.135988235473633\n",
      "[step: 3158] loss: 6.114799499511719\n",
      "[step: 3159] loss: 6.113479137420654\n",
      "[step: 3160] loss: 6.125983238220215\n",
      "[step: 3161] loss: 6.1421918869018555\n",
      "[step: 3162] loss: 6.155951499938965\n",
      "[step: 3163] loss: 6.159097671508789\n",
      "[step: 3164] loss: 6.155193328857422\n",
      "[step: 3165] loss: 6.14085578918457\n",
      "[step: 3166] loss: 6.124931335449219\n",
      "[step: 3167] loss: 6.1093220710754395\n",
      "[step: 3168] loss: 6.096841335296631\n",
      "[step: 3169] loss: 6.08830451965332\n",
      "[step: 3170] loss: 6.084340572357178\n",
      "[step: 3171] loss: 6.08412504196167\n",
      "[step: 3172] loss: 6.085899353027344\n",
      "[step: 3173] loss: 6.088801383972168\n",
      "[step: 3174] loss: 6.0925774574279785\n",
      "[step: 3175] loss: 6.097384452819824\n",
      "[step: 3176] loss: 6.102536678314209\n",
      "[step: 3177] loss: 6.110250473022461\n",
      "[step: 3178] loss: 6.118657112121582\n",
      "[step: 3179] loss: 6.133003234863281\n",
      "[step: 3180] loss: 6.150518417358398\n",
      "[step: 3181] loss: 6.1816487312316895\n",
      "[step: 3182] loss: 6.221217155456543\n",
      "[step: 3183] loss: 6.289661884307861\n",
      "[step: 3184] loss: 6.370595455169678\n",
      "[step: 3185] loss: 6.510851860046387\n",
      "[step: 3186] loss: 6.645411968231201\n",
      "[step: 3187] loss: 6.866861343383789\n",
      "[step: 3188] loss: 6.983891010284424\n",
      "[step: 3189] loss: 7.116292953491211\n",
      "[step: 3190] loss: 7.001004219055176\n",
      "[step: 3191] loss: 6.775146961212158\n",
      "[step: 3192] loss: 6.442384719848633\n",
      "[step: 3193] loss: 6.163384437561035\n",
      "[step: 3194] loss: 6.0464277267456055\n",
      "[step: 3195] loss: 6.100834369659424\n",
      "[step: 3196] loss: 6.252243995666504\n",
      "[step: 3197] loss: 6.386612892150879\n",
      "[step: 3198] loss: 6.4160590171813965\n",
      "[step: 3199] loss: 6.309512138366699\n",
      "[step: 3200] loss: 6.158072471618652\n",
      "[step: 3201] loss: 6.048796653747559\n",
      "[step: 3202] loss: 6.042033672332764\n",
      "[step: 3203] loss: 6.111075401306152\n",
      "[step: 3204] loss: 6.194561958312988\n",
      "[step: 3205] loss: 6.249933242797852\n",
      "[step: 3206] loss: 6.224817752838135\n",
      "[step: 3207] loss: 6.160616874694824\n",
      "[step: 3208] loss: 6.069223880767822\n",
      "[step: 3209] loss: 6.016356468200684\n",
      "[step: 3210] loss: 6.0174946784973145\n",
      "[step: 3211] loss: 6.055917263031006\n",
      "[step: 3212] loss: 6.1077985763549805\n",
      "[step: 3213] loss: 6.142879009246826\n",
      "[step: 3214] loss: 6.16346549987793\n",
      "[step: 3215] loss: 6.144830226898193\n",
      "[step: 3216] loss: 6.10623836517334\n",
      "[step: 3217] loss: 6.056671619415283\n",
      "[step: 3218] loss: 6.017313480377197\n",
      "[step: 3219] loss: 5.994511604309082\n",
      "[step: 3220] loss: 5.987422943115234\n",
      "[step: 3221] loss: 5.992319583892822\n",
      "[step: 3222] loss: 6.005184173583984\n",
      "[step: 3223] loss: 6.021144866943359\n",
      "[step: 3224] loss: 6.035285949707031\n",
      "[step: 3225] loss: 6.048968315124512\n",
      "[step: 3226] loss: 6.057730674743652\n",
      "[step: 3227] loss: 6.070163726806641\n",
      "[step: 3228] loss: 6.0756096839904785\n",
      "[step: 3229] loss: 6.087096214294434\n",
      "[step: 3230] loss: 6.091861724853516\n",
      "[step: 3231] loss: 6.102529048919678\n",
      "[step: 3232] loss: 6.106679916381836\n",
      "[step: 3233] loss: 6.116786479949951\n",
      "[step: 3234] loss: 6.123315334320068\n",
      "[step: 3235] loss: 6.141021251678467\n",
      "[step: 3236] loss: 6.154002666473389\n",
      "[step: 3237] loss: 6.183282852172852\n",
      "[step: 3238] loss: 6.198678016662598\n",
      "[step: 3239] loss: 6.230921745300293\n",
      "[step: 3240] loss: 6.23907470703125\n",
      "[step: 3241] loss: 6.261739730834961\n",
      "[step: 3242] loss: 6.259285926818848\n",
      "[step: 3243] loss: 6.265165328979492\n",
      "[step: 3244] loss: 6.245416641235352\n",
      "[step: 3245] loss: 6.225669860839844\n",
      "[step: 3246] loss: 6.18072509765625\n",
      "[step: 3247] loss: 6.135573387145996\n",
      "[step: 3248] loss: 6.077448844909668\n",
      "[step: 3249] loss: 6.028037071228027\n",
      "[step: 3250] loss: 5.983039855957031\n",
      "[step: 3251] loss: 5.951194763183594\n",
      "[step: 3252] loss: 5.930013656616211\n",
      "[step: 3253] loss: 5.9192304611206055\n",
      "[step: 3254] loss: 5.916626930236816\n",
      "[step: 3255] loss: 5.920070648193359\n",
      "[step: 3256] loss: 5.927905559539795\n",
      "[step: 3257] loss: 5.938770294189453\n",
      "[step: 3258] loss: 5.953762054443359\n",
      "[step: 3259] loss: 5.971839904785156\n",
      "[step: 3260] loss: 5.999095916748047\n",
      "[step: 3261] loss: 6.032602787017822\n",
      "[step: 3262] loss: 6.087743759155273\n",
      "[step: 3263] loss: 6.15411376953125\n",
      "[step: 3264] loss: 6.267322063446045\n",
      "[step: 3265] loss: 6.390615940093994\n",
      "[step: 3266] loss: 6.59600830078125\n",
      "[step: 3267] loss: 6.760648727416992\n",
      "[step: 3268] loss: 6.98247766494751\n",
      "[step: 3269] loss: 7.005004405975342\n",
      "[step: 3270] loss: 6.9191789627075195\n",
      "[step: 3271] loss: 6.620224952697754\n",
      "[step: 3272] loss: 6.251442909240723\n",
      "[step: 3273] loss: 5.981863975524902\n",
      "[step: 3274] loss: 5.894318103790283\n",
      "[step: 3275] loss: 5.988522052764893\n",
      "[step: 3276] loss: 6.163427829742432\n",
      "[step: 3277] loss: 6.287637710571289\n",
      "[step: 3278] loss: 6.267404079437256\n",
      "[step: 3279] loss: 6.110402584075928\n",
      "[step: 3280] loss: 5.94162654876709\n",
      "[step: 3281] loss: 5.873937606811523\n",
      "[step: 3282] loss: 5.924228668212891\n",
      "[step: 3283] loss: 6.024786949157715\n",
      "[step: 3284] loss: 6.0923848152160645\n",
      "[step: 3285] loss: 6.087035179138184\n",
      "[step: 3286] loss: 6.027284622192383\n",
      "[step: 3287] loss: 5.935652256011963\n",
      "[step: 3288] loss: 5.875927925109863\n",
      "[step: 3289] loss: 5.864152431488037\n",
      "[step: 3290] loss: 5.894023895263672\n",
      "[step: 3291] loss: 5.9406328201293945\n",
      "[step: 3292] loss: 5.979397296905518\n",
      "[step: 3293] loss: 6.009637832641602\n",
      "[step: 3294] loss: 5.991223335266113\n",
      "[step: 3295] loss: 5.95777702331543\n",
      "[step: 3296] loss: 5.89894962310791\n",
      "[step: 3297] loss: 5.857155799865723\n",
      "[step: 3298] loss: 5.836923599243164\n",
      "[step: 3299] loss: 5.834894180297852\n",
      "[step: 3300] loss: 5.844095230102539\n",
      "[step: 3301] loss: 5.858930587768555\n",
      "[step: 3302] loss: 5.878041744232178\n",
      "[step: 3303] loss: 5.891735076904297\n",
      "[step: 3304] loss: 5.904599189758301\n",
      "[step: 3305] loss: 5.911490440368652\n",
      "[step: 3306] loss: 5.917332172393799\n",
      "[step: 3307] loss: 5.918222904205322\n",
      "[step: 3308] loss: 5.920299053192139\n",
      "[step: 3309] loss: 5.918769836425781\n",
      "[step: 3310] loss: 5.922238349914551\n",
      "[step: 3311] loss: 5.917265892028809\n",
      "[step: 3312] loss: 5.919172763824463\n",
      "[step: 3313] loss: 5.9139084815979\n",
      "[step: 3314] loss: 5.919422149658203\n",
      "[step: 3315] loss: 5.921641826629639\n",
      "[step: 3316] loss: 5.935586452484131\n",
      "[step: 3317] loss: 5.948084831237793\n",
      "[step: 3318] loss: 5.971412658691406\n",
      "[step: 3319] loss: 5.9926371574401855\n",
      "[step: 3320] loss: 6.028602600097656\n",
      "[step: 3321] loss: 6.062633514404297\n",
      "[step: 3322] loss: 6.122977256774902\n",
      "[step: 3323] loss: 6.169740676879883\n",
      "[step: 3324] loss: 6.24830436706543\n",
      "[step: 3325] loss: 6.274998664855957\n",
      "[step: 3326] loss: 6.313747406005859\n",
      "[step: 3327] loss: 6.271521091461182\n",
      "[step: 3328] loss: 6.215063095092773\n",
      "[step: 3329] loss: 6.110593795776367\n",
      "[step: 3330] loss: 5.997504711151123\n",
      "[step: 3331] loss: 5.891549110412598\n",
      "[step: 3332] loss: 5.8105268478393555\n",
      "[step: 3333] loss: 5.767411231994629\n",
      "[step: 3334] loss: 5.7621846199035645\n",
      "[step: 3335] loss: 5.785422325134277\n",
      "[step: 3336] loss: 5.822687149047852\n",
      "[step: 3337] loss: 5.859531402587891\n",
      "[step: 3338] loss: 5.887418746948242\n",
      "[step: 3339] loss: 5.904850959777832\n",
      "[step: 3340] loss: 5.906001091003418\n",
      "[step: 3341] loss: 5.904788017272949\n",
      "[step: 3342] loss: 5.884831428527832\n",
      "[step: 3343] loss: 5.86922550201416\n",
      "[step: 3344] loss: 5.839916229248047\n",
      "[step: 3345] loss: 5.817956447601318\n",
      "[step: 3346] loss: 5.7941789627075195\n",
      "[step: 3347] loss: 5.777580738067627\n",
      "[step: 3348] loss: 5.76431941986084\n",
      "[step: 3349] loss: 5.7546467781066895\n",
      "[step: 3350] loss: 5.746976852416992\n",
      "[step: 3351] loss: 5.740589141845703\n",
      "[step: 3352] loss: 5.735644340515137\n",
      "[step: 3353] loss: 5.7325239181518555\n",
      "[step: 3354] loss: 5.731418609619141\n",
      "[step: 3355] loss: 5.733179092407227\n",
      "[step: 3356] loss: 5.737409591674805\n",
      "[step: 3357] loss: 5.747147560119629\n",
      "[step: 3358] loss: 5.762241363525391\n",
      "[step: 3359] loss: 5.792741775512695\n",
      "[step: 3360] loss: 5.839540481567383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3361] loss: 5.932329177856445\n",
      "[step: 3362] loss: 6.069672584533691\n",
      "[step: 3363] loss: 6.334272384643555\n",
      "[step: 3364] loss: 6.668107986450195\n",
      "[step: 3365] loss: 7.233139991760254\n",
      "[step: 3366] loss: 7.637401580810547\n",
      "[step: 3367] loss: 7.934178352355957\n",
      "[step: 3368] loss: 7.586886882781982\n",
      "[step: 3369] loss: 6.731876850128174\n",
      "[step: 3370] loss: 6.054058074951172\n",
      "[step: 3371] loss: 5.839573383331299\n",
      "[step: 3372] loss: 6.182507038116455\n",
      "[step: 3373] loss: 6.59547758102417\n",
      "[step: 3374] loss: 6.638362884521484\n",
      "[step: 3375] loss: 6.237454891204834\n",
      "[step: 3376] loss: 5.801514148712158\n",
      "[step: 3377] loss: 5.860677242279053\n",
      "[step: 3378] loss: 6.182397842407227\n",
      "[step: 3379] loss: 6.263089179992676\n",
      "[step: 3380] loss: 5.995612144470215\n",
      "[step: 3381] loss: 5.716494083404541\n",
      "[step: 3382] loss: 5.765983581542969\n",
      "[step: 3383] loss: 5.985863208770752\n",
      "[step: 3384] loss: 6.006443500518799\n",
      "[step: 3385] loss: 5.823089599609375\n",
      "[step: 3386] loss: 5.687531471252441\n",
      "[step: 3387] loss: 5.737149715423584\n",
      "[step: 3388] loss: 5.851991653442383\n",
      "[step: 3389] loss: 5.862147808074951\n",
      "[step: 3390] loss: 5.778338432312012\n",
      "[step: 3391] loss: 5.714519500732422\n",
      "[step: 3392] loss: 5.7067766189575195\n",
      "[step: 3393] loss: 5.7269206047058105\n",
      "[step: 3394] loss: 5.7368364334106445\n",
      "[step: 3395] loss: 5.741503715515137\n",
      "[step: 3396] loss: 5.748260498046875\n",
      "[step: 3397] loss: 5.719353675842285\n",
      "[step: 3398] loss: 5.677376747131348\n",
      "[step: 3399] loss: 5.6473822593688965\n",
      "[step: 3400] loss: 5.660463333129883\n",
      "[step: 3401] loss: 5.6959147453308105\n",
      "[step: 3402] loss: 5.703060150146484\n",
      "[step: 3403] loss: 5.684791564941406\n",
      "[step: 3404] loss: 5.652892589569092\n",
      "[step: 3405] loss: 5.6389875411987305\n",
      "[step: 3406] loss: 5.639983177185059\n",
      "[step: 3407] loss: 5.639697074890137\n",
      "[step: 3408] loss: 5.6364264488220215\n",
      "[step: 3409] loss: 5.636622428894043\n",
      "[step: 3410] loss: 5.642746925354004\n",
      "[step: 3411] loss: 5.644856929779053\n",
      "[step: 3412] loss: 5.635822772979736\n",
      "[step: 3413] loss: 5.620810508728027\n",
      "[step: 3414] loss: 5.609336853027344\n",
      "[step: 3415] loss: 5.6059980392456055\n",
      "[step: 3416] loss: 5.606633186340332\n",
      "[step: 3417] loss: 5.6057586669921875\n",
      "[step: 3418] loss: 5.603531360626221\n",
      "[step: 3419] loss: 5.602954387664795\n",
      "[step: 3420] loss: 5.60506534576416\n",
      "[step: 3421] loss: 5.607953071594238\n",
      "[step: 3422] loss: 5.607099533081055\n",
      "[step: 3423] loss: 5.604903221130371\n",
      "[step: 3424] loss: 5.6020307540893555\n",
      "[step: 3425] loss: 5.601855754852295\n",
      "[step: 3426] loss: 5.603026866912842\n",
      "[step: 3427] loss: 5.604855060577393\n",
      "[step: 3428] loss: 5.606819152832031\n",
      "[step: 3429] loss: 5.611457824707031\n",
      "[step: 3430] loss: 5.61955451965332\n",
      "[step: 3431] loss: 5.636749744415283\n",
      "[step: 3432] loss: 5.658721446990967\n",
      "[step: 3433] loss: 5.701238632202148\n",
      "[step: 3434] loss: 5.75218391418457\n",
      "[step: 3435] loss: 5.851902961730957\n",
      "[step: 3436] loss: 5.96428108215332\n",
      "[step: 3437] loss: 6.169558525085449\n",
      "[step: 3438] loss: 6.3455915451049805\n",
      "[step: 3439] loss: 6.619313716888428\n",
      "[step: 3440] loss: 6.678818702697754\n",
      "[step: 3441] loss: 6.68893575668335\n",
      "[step: 3442] loss: 6.381775856018066\n",
      "[step: 3443] loss: 6.004398345947266\n",
      "[step: 3444] loss: 5.684399604797363\n",
      "[step: 3445] loss: 5.559719085693359\n",
      "[step: 3446] loss: 5.643929481506348\n",
      "[step: 3447] loss: 5.831626892089844\n",
      "[step: 3448] loss: 5.9878716468811035\n",
      "[step: 3449] loss: 5.979910850524902\n",
      "[step: 3450] loss: 5.81983757019043\n",
      "[step: 3451] loss: 5.625423431396484\n",
      "[step: 3452] loss: 5.540110111236572\n",
      "[step: 3453] loss: 5.593165874481201\n",
      "[step: 3454] loss: 5.7089385986328125\n",
      "[step: 3455] loss: 5.7931623458862305\n",
      "[step: 3456] loss: 5.784613132476807\n",
      "[step: 3457] loss: 5.709066867828369\n",
      "[step: 3458] loss: 5.596312046051025\n",
      "[step: 3459] loss: 5.529995441436768\n",
      "[step: 3460] loss: 5.534721374511719\n",
      "[step: 3461] loss: 5.590518951416016\n",
      "[step: 3462] loss: 5.657686233520508\n",
      "[step: 3463] loss: 5.691832542419434\n",
      "[step: 3464] loss: 5.698959827423096\n",
      "[step: 3465] loss: 5.645974159240723\n",
      "[step: 3466] loss: 5.581854343414307\n",
      "[step: 3467] loss: 5.5261969566345215\n",
      "[step: 3468] loss: 5.504435062408447\n",
      "[step: 3469] loss: 5.512066841125488\n",
      "[step: 3470] loss: 5.535608291625977\n",
      "[step: 3471] loss: 5.567705154418945\n",
      "[step: 3472] loss: 5.592962265014648\n",
      "[step: 3473] loss: 5.607464790344238\n",
      "[step: 3474] loss: 5.5981268882751465\n",
      "[step: 3475] loss: 5.578085422515869\n",
      "[step: 3476] loss: 5.551324844360352\n",
      "[step: 3477] loss: 5.526844501495361\n",
      "[step: 3478] loss: 5.50532865524292\n",
      "[step: 3479] loss: 5.490434646606445\n",
      "[step: 3480] loss: 5.480656623840332\n",
      "[step: 3481] loss: 5.4749016761779785\n",
      "[step: 3482] loss: 5.472377777099609\n",
      "[step: 3483] loss: 5.473260402679443\n",
      "[step: 3484] loss: 5.477014064788818\n",
      "[step: 3485] loss: 5.482449531555176\n",
      "[step: 3486] loss: 5.4902825355529785\n",
      "[step: 3487] loss: 5.499571800231934\n",
      "[step: 3488] loss: 5.513833999633789\n",
      "[step: 3489] loss: 5.531028747558594\n",
      "[step: 3490] loss: 5.560279846191406\n",
      "[step: 3491] loss: 5.599831581115723\n",
      "[step: 3492] loss: 5.669976711273193\n",
      "[step: 3493] loss: 5.762704372406006\n",
      "[step: 3494] loss: 5.926471710205078\n",
      "[step: 3495] loss: 6.101673126220703\n",
      "[step: 3496] loss: 6.387631416320801\n",
      "[step: 3497] loss: 6.55470085144043\n",
      "[step: 3498] loss: 6.746952056884766\n",
      "[step: 3499] loss: 6.615375995635986\n",
      "[step: 3500] loss: 6.332744598388672\n",
      "[step: 3501] loss: 5.925664901733398\n",
      "[step: 3502] loss: 5.581624507904053\n",
      "[step: 3503] loss: 5.45906925201416\n",
      "[step: 3504] loss: 5.554508209228516\n",
      "[step: 3505] loss: 5.763609886169434\n",
      "[step: 3506] loss: 5.909889221191406\n",
      "[step: 3507] loss: 5.872596740722656\n",
      "[step: 3508] loss: 5.686182498931885\n",
      "[step: 3509] loss: 5.490981101989746\n",
      "[step: 3510] loss: 5.433778762817383\n",
      "[step: 3511] loss: 5.517822742462158\n",
      "[step: 3512] loss: 5.638963222503662\n",
      "[step: 3513] loss: 5.697090148925781\n",
      "[step: 3514] loss: 5.6559906005859375\n",
      "[step: 3515] loss: 5.547640323638916\n",
      "[step: 3516] loss: 5.4562273025512695\n",
      "[step: 3517] loss: 5.423779010772705\n",
      "[step: 3518] loss: 5.454107284545898\n",
      "[step: 3519] loss: 5.509511947631836\n",
      "[step: 3520] loss: 5.554060935974121\n",
      "[step: 3521] loss: 5.57918643951416\n",
      "[step: 3522] loss: 5.5541462898254395\n",
      "[step: 3523] loss: 5.5117573738098145\n",
      "[step: 3524] loss: 5.4459228515625\n",
      "[step: 3525] loss: 5.403913497924805\n",
      "[step: 3526] loss: 5.39537239074707\n",
      "[step: 3527] loss: 5.4131364822387695\n",
      "[step: 3528] loss: 5.443511962890625\n",
      "[step: 3529] loss: 5.468306541442871\n",
      "[step: 3530] loss: 5.4875688552856445\n",
      "[step: 3531] loss: 5.481502056121826\n",
      "[step: 3532] loss: 5.467979431152344\n",
      "[step: 3533] loss: 5.446468353271484\n",
      "[step: 3534] loss: 5.424917221069336\n",
      "[step: 3535] loss: 5.403572082519531\n",
      "[step: 3536] loss: 5.38506555557251\n",
      "[step: 3537] loss: 5.373215675354004\n",
      "[step: 3538] loss: 5.368288040161133\n",
      "[step: 3539] loss: 5.367918491363525\n",
      "[step: 3540] loss: 5.369905471801758\n",
      "[step: 3541] loss: 5.3729448318481445\n",
      "[step: 3542] loss: 5.376903533935547\n",
      "[step: 3543] loss: 5.382694244384766\n",
      "[step: 3544] loss: 5.391164779663086\n",
      "[step: 3545] loss: 5.404778003692627\n",
      "[step: 3546] loss: 5.419923305511475\n",
      "[step: 3547] loss: 5.444927215576172\n",
      "[step: 3548] loss: 5.474271774291992\n",
      "[step: 3549] loss: 5.530030250549316\n",
      "[step: 3550] loss: 5.600772857666016\n",
      "[step: 3551] loss: 5.7292799949646\n",
      "[step: 3552] loss: 5.87655782699585\n",
      "[step: 3553] loss: 6.116765975952148\n",
      "[step: 3554] loss: 6.30916690826416\n",
      "[step: 3555] loss: 6.552779197692871\n",
      "[step: 3556] loss: 6.551790714263916\n",
      "[step: 3557] loss: 6.424607753753662\n",
      "[step: 3558] loss: 6.055609703063965\n",
      "[step: 3559] loss: 5.65162467956543\n",
      "[step: 3560] loss: 5.394576072692871\n",
      "[step: 3561] loss: 5.363040924072266\n",
      "[step: 3562] loss: 5.517314434051514\n",
      "[step: 3563] loss: 5.709100723266602\n",
      "[step: 3564] loss: 5.799552917480469\n",
      "[step: 3565] loss: 5.707619667053223\n",
      "[step: 3566] loss: 5.503570556640625\n",
      "[step: 3567] loss: 5.350009918212891\n",
      "[step: 3568] loss: 5.339139938354492\n",
      "[step: 3569] loss: 5.44118070602417\n",
      "[step: 3570] loss: 5.552877426147461\n",
      "[step: 3571] loss: 5.588789939880371\n",
      "[step: 3572] loss: 5.525700569152832\n",
      "[step: 3573] loss: 5.4175615310668945\n",
      "[step: 3574] loss: 5.330467224121094\n",
      "[step: 3575] loss: 5.313935279846191\n",
      "[step: 3576] loss: 5.355513095855713\n",
      "[step: 3577] loss: 5.413900375366211\n",
      "[step: 3578] loss: 5.456230163574219\n",
      "[step: 3579] loss: 5.461857795715332\n",
      "[step: 3580] loss: 5.445018291473389\n",
      "[step: 3581] loss: 5.389803886413574\n",
      "[step: 3582] loss: 5.33890438079834\n",
      "[step: 3583] loss: 5.301360130310059\n",
      "[step: 3584] loss: 5.290111541748047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3585] loss: 5.299986839294434\n",
      "[step: 3586] loss: 5.32275390625\n",
      "[step: 3587] loss: 5.351037979125977\n",
      "[step: 3588] loss: 5.365216255187988\n",
      "[step: 3589] loss: 5.370031356811523\n",
      "[step: 3590] loss: 5.354934215545654\n",
      "[step: 3591] loss: 5.33983850479126\n",
      "[step: 3592] loss: 5.321610927581787\n",
      "[step: 3593] loss: 5.305471420288086\n",
      "[step: 3594] loss: 5.29062032699585\n",
      "[step: 3595] loss: 5.277942657470703\n",
      "[step: 3596] loss: 5.267940044403076\n",
      "[step: 3597] loss: 5.260961532592773\n",
      "[step: 3598] loss: 5.257537841796875\n",
      "[step: 3599] loss: 5.25703239440918\n",
      "[step: 3600] loss: 5.257950782775879\n",
      "[step: 3601] loss: 5.259486198425293\n",
      "[step: 3602] loss: 5.2618937492370605\n",
      "[step: 3603] loss: 5.265926837921143\n",
      "[step: 3604] loss: 5.272823333740234\n",
      "[step: 3605] loss: 5.283698081970215\n",
      "[step: 3606] loss: 5.303749084472656\n",
      "[step: 3607] loss: 5.333512306213379\n",
      "[step: 3608] loss: 5.389763832092285\n",
      "[step: 3609] loss: 5.468442916870117\n",
      "[step: 3610] loss: 5.619105815887451\n",
      "[step: 3611] loss: 5.805967807769775\n",
      "[step: 3612] loss: 6.148037910461426\n",
      "[step: 3613] loss: 6.461949348449707\n",
      "[step: 3614] loss: 6.914203643798828\n",
      "[step: 3615] loss: 6.999299049377441\n",
      "[step: 3616] loss: 6.816310405731201\n",
      "[step: 3617] loss: 6.258488178253174\n",
      "[step: 3618] loss: 5.610361099243164\n",
      "[step: 3619] loss: 5.315269470214844\n",
      "[step: 3620] loss: 5.423476219177246\n",
      "[step: 3621] loss: 5.7788190841674805\n",
      "[step: 3622] loss: 5.99766206741333\n",
      "[step: 3623] loss: 5.84876012802124\n",
      "[step: 3624] loss: 5.499302864074707\n",
      "[step: 3625] loss: 5.2671942710876465\n",
      "[step: 3626] loss: 5.36686897277832\n",
      "[step: 3627] loss: 5.604361534118652\n",
      "[step: 3628] loss: 5.682246685028076\n",
      "[step: 3629] loss: 5.519467830657959\n",
      "[step: 3630] loss: 5.287855625152588\n",
      "[step: 3631] loss: 5.227681636810303\n",
      "[step: 3632] loss: 5.345559120178223\n",
      "[step: 3633] loss: 5.463269233703613\n",
      "[step: 3634] loss: 5.454823970794678\n",
      "[step: 3635] loss: 5.340784072875977\n",
      "[step: 3636] loss: 5.238933086395264\n",
      "[step: 3637] loss: 5.22959566116333\n",
      "[step: 3638] loss: 5.275571823120117\n",
      "[step: 3639] loss: 5.315024375915527\n",
      "[step: 3640] loss: 5.3179121017456055\n",
      "[step: 3641] loss: 5.291238784790039\n",
      "[step: 3642] loss: 5.267251491546631\n",
      "[step: 3643] loss: 5.235846519470215\n",
      "[step: 3644] loss: 5.211063385009766\n",
      "[step: 3645] loss: 5.200483322143555\n",
      "[step: 3646] loss: 5.215282917022705\n",
      "[step: 3647] loss: 5.242049217224121\n",
      "[step: 3648] loss: 5.248597145080566\n",
      "[step: 3649] loss: 5.236723899841309\n",
      "[step: 3650] loss: 5.203503608703613\n",
      "[step: 3651] loss: 5.179600238800049\n",
      "[step: 3652] loss: 5.174242973327637\n",
      "[step: 3653] loss: 5.180837631225586\n",
      "[step: 3654] loss: 5.187743663787842\n",
      "[step: 3655] loss: 5.187414169311523\n",
      "[step: 3656] loss: 5.186094284057617\n",
      "[step: 3657] loss: 5.186110973358154\n",
      "[step: 3658] loss: 5.187129974365234\n",
      "[step: 3659] loss: 5.183687686920166\n",
      "[step: 3660] loss: 5.173923492431641\n",
      "[step: 3661] loss: 5.1619720458984375\n",
      "[step: 3662] loss: 5.152692794799805\n",
      "[step: 3663] loss: 5.148934841156006\n",
      "[step: 3664] loss: 5.148340225219727\n",
      "[step: 3665] loss: 5.147170066833496\n",
      "[step: 3666] loss: 5.144553184509277\n",
      "[step: 3667] loss: 5.141660213470459\n",
      "[step: 3668] loss: 5.140681266784668\n",
      "[step: 3669] loss: 5.1423420906066895\n",
      "[step: 3670] loss: 5.145115375518799\n",
      "[step: 3671] loss: 5.148699760437012\n",
      "[step: 3672] loss: 5.152082920074463\n",
      "[step: 3673] loss: 5.159210205078125\n",
      "[step: 3674] loss: 5.170950889587402\n",
      "[step: 3675] loss: 5.194086074829102\n",
      "[step: 3676] loss: 5.229397773742676\n",
      "[step: 3677] loss: 5.2918500900268555\n",
      "[step: 3678] loss: 5.382847785949707\n",
      "[step: 3679] loss: 5.548149585723877\n",
      "[step: 3680] loss: 5.753654479980469\n",
      "[step: 3681] loss: 6.120733737945557\n",
      "[step: 3682] loss: 6.389096260070801\n",
      "[step: 3683] loss: 6.7645039558410645\n",
      "[step: 3684] loss: 6.632778167724609\n",
      "[step: 3685] loss: 6.2919487953186035\n",
      "[step: 3686] loss: 5.741579532623291\n",
      "[step: 3687] loss: 5.2956156730651855\n",
      "[step: 3688] loss: 5.194902420043945\n",
      "[step: 3689] loss: 5.377991199493408\n",
      "[step: 3690] loss: 5.6798529624938965\n",
      "[step: 3691] loss: 5.805938720703125\n",
      "[step: 3692] loss: 5.608611106872559\n",
      "[step: 3693] loss: 5.298526763916016\n",
      "[step: 3694] loss: 5.1291608810424805\n",
      "[step: 3695] loss: 5.2348527908325195\n",
      "[step: 3696] loss: 5.44862699508667\n",
      "[step: 3697] loss: 5.519135475158691\n",
      "[step: 3698] loss: 5.394743919372559\n",
      "[step: 3699] loss: 5.193134784698486\n",
      "[step: 3700] loss: 5.105712890625\n",
      "[step: 3701] loss: 5.171204566955566\n",
      "[step: 3702] loss: 5.279184818267822\n",
      "[step: 3703] loss: 5.324464797973633\n",
      "[step: 3704] loss: 5.282467842102051\n",
      "[step: 3705] loss: 5.195683479309082\n",
      "[step: 3706] loss: 5.13611364364624\n",
      "[step: 3707] loss: 5.108055114746094\n",
      "[step: 3708] loss: 5.120637893676758\n",
      "[step: 3709] loss: 5.160887718200684\n",
      "[step: 3710] loss: 5.197330474853516\n",
      "[step: 3711] loss: 5.21526575088501\n",
      "[step: 3712] loss: 5.17450475692749\n",
      "[step: 3713] loss: 5.120917797088623\n",
      "[step: 3714] loss: 5.076440811157227\n",
      "[step: 3715] loss: 5.070320129394531\n",
      "[step: 3716] loss: 5.091823577880859\n",
      "[step: 3717] loss: 5.113424301147461\n",
      "[step: 3718] loss: 5.127416610717773\n",
      "[step: 3719] loss: 5.125288009643555\n",
      "[step: 3720] loss: 5.118416786193848\n",
      "[step: 3721] loss: 5.1031646728515625\n",
      "[step: 3722] loss: 5.081248760223389\n",
      "[step: 3723] loss: 5.060166358947754\n",
      "[step: 3724] loss: 5.04770565032959\n",
      "[step: 3725] loss: 5.048518657684326\n",
      "[step: 3726] loss: 5.058408737182617\n",
      "[step: 3727] loss: 5.068506240844727\n",
      "[step: 3728] loss: 5.074540138244629\n",
      "[step: 3729] loss: 5.076458930969238\n",
      "[step: 3730] loss: 5.076448917388916\n",
      "[step: 3731] loss: 5.077683925628662\n",
      "[step: 3732] loss: 5.076107978820801\n",
      "[step: 3733] loss: 5.074404716491699\n",
      "[step: 3734] loss: 5.068202018737793\n",
      "[step: 3735] loss: 5.064621925354004\n",
      "[step: 3736] loss: 5.062361240386963\n",
      "[step: 3737] loss: 5.063004970550537\n",
      "[step: 3738] loss: 5.064517021179199\n",
      "[step: 3739] loss: 5.067693710327148\n",
      "[step: 3740] loss: 5.073514938354492\n",
      "[step: 3741] loss: 5.0863037109375\n",
      "[step: 3742] loss: 5.105576992034912\n",
      "[step: 3743] loss: 5.141486167907715\n",
      "[step: 3744] loss: 5.185615539550781\n",
      "[step: 3745] loss: 5.265735626220703\n",
      "[step: 3746] loss: 5.359349250793457\n",
      "[step: 3747] loss: 5.524909019470215\n",
      "[step: 3748] loss: 5.694218158721924\n",
      "[step: 3749] loss: 5.950660705566406\n",
      "[step: 3750] loss: 6.106048583984375\n",
      "[step: 3751] loss: 6.232468128204346\n",
      "[step: 3752] loss: 6.07594108581543\n",
      "[step: 3753] loss: 5.767206192016602\n",
      "[step: 3754] loss: 5.3658599853515625\n",
      "[step: 3755] loss: 5.076726913452148\n",
      "[step: 3756] loss: 5.011049747467041\n",
      "[step: 3757] loss: 5.139488220214844\n",
      "[step: 3758] loss: 5.339387893676758\n",
      "[step: 3759] loss: 5.4534759521484375\n",
      "[step: 3760] loss: 5.403868198394775\n",
      "[step: 3761] loss: 5.219186782836914\n",
      "[step: 3762] loss: 5.0463361740112305\n",
      "[step: 3763] loss: 4.993087291717529\n",
      "[step: 3764] loss: 5.061398983001709\n",
      "[step: 3765] loss: 5.171348571777344\n",
      "[step: 3766] loss: 5.237825393676758\n",
      "[step: 3767] loss: 5.227270126342773\n",
      "[step: 3768] loss: 5.136639595031738\n",
      "[step: 3769] loss: 5.039610862731934\n",
      "[step: 3770] loss: 4.982725143432617\n",
      "[step: 3771] loss: 4.988656044006348\n",
      "[step: 3772] loss: 5.0355963706970215\n",
      "[step: 3773] loss: 5.089631080627441\n",
      "[step: 3774] loss: 5.131481170654297\n",
      "[step: 3775] loss: 5.1289777755737305\n",
      "[step: 3776] loss: 5.100057601928711\n",
      "[step: 3777] loss: 5.041050910949707\n",
      "[step: 3778] loss: 4.992490768432617\n",
      "[step: 3779] loss: 4.964487075805664\n",
      "[step: 3780] loss: 4.959249496459961\n",
      "[step: 3781] loss: 4.970922946929932\n",
      "[step: 3782] loss: 4.991313934326172\n",
      "[step: 3783] loss: 5.013189792633057\n",
      "[step: 3784] loss: 5.0246076583862305\n",
      "[step: 3785] loss: 5.0288848876953125\n",
      "[step: 3786] loss: 5.022879600524902\n",
      "[step: 3787] loss: 5.013229846954346\n",
      "[step: 3788] loss: 4.9985198974609375\n",
      "[step: 3789] loss: 4.983593463897705\n",
      "[step: 3790] loss: 4.9689812660217285\n",
      "[step: 3791] loss: 4.956866264343262\n",
      "[step: 3792] loss: 4.946540832519531\n",
      "[step: 3793] loss: 4.939023017883301\n",
      "[step: 3794] loss: 4.933701992034912\n",
      "[step: 3795] loss: 4.929871082305908\n",
      "[step: 3796] loss: 4.927011013031006\n",
      "[step: 3797] loss: 4.925201416015625\n",
      "[step: 3798] loss: 4.924220561981201\n",
      "[step: 3799] loss: 4.923687934875488\n",
      "[step: 3800] loss: 4.923759460449219\n",
      "[step: 3801] loss: 4.924870491027832\n",
      "[step: 3802] loss: 4.927824020385742\n",
      "[step: 3803] loss: 4.933537483215332\n",
      "[step: 3804] loss: 4.944999694824219\n",
      "[step: 3805] loss: 4.965183258056641\n",
      "[step: 3806] loss: 5.0034942626953125\n",
      "[step: 3807] loss: 5.068437576293945\n",
      "[step: 3808] loss: 5.195158958435059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3809] loss: 5.398564338684082\n",
      "[step: 3810] loss: 5.796850204467773\n",
      "[step: 3811] loss: 6.3000288009643555\n",
      "[step: 3812] loss: 7.128273963928223\n",
      "[step: 3813] loss: 7.496750831604004\n",
      "[step: 3814] loss: 7.475667953491211\n",
      "[step: 3815] loss: 6.573634147644043\n",
      "[step: 3816] loss: 5.473781585693359\n",
      "[step: 3817] loss: 5.122608184814453\n",
      "[step: 3818] loss: 5.472061634063721\n",
      "[step: 3819] loss: 6.088422775268555\n",
      "[step: 3820] loss: 6.150615692138672\n",
      "[step: 3821] loss: 5.525528430938721\n",
      "[step: 3822] loss: 5.076384544372559\n",
      "[step: 3823] loss: 5.211991310119629\n",
      "[step: 3824] loss: 5.595396518707275\n",
      "[step: 3825] loss: 5.577386856079102\n",
      "[step: 3826] loss: 5.141669273376465\n",
      "[step: 3827] loss: 4.94948673248291\n",
      "[step: 3828] loss: 5.169322490692139\n",
      "[step: 3829] loss: 5.351823329925537\n",
      "[step: 3830] loss: 5.194630146026611\n",
      "[step: 3831] loss: 4.937530994415283\n",
      "[step: 3832] loss: 4.96572208404541\n",
      "[step: 3833] loss: 5.160178184509277\n",
      "[step: 3834] loss: 5.178927421569824\n",
      "[step: 3835] loss: 5.013195991516113\n",
      "[step: 3836] loss: 4.902913570404053\n",
      "[step: 3837] loss: 4.963648796081543\n",
      "[step: 3838] loss: 5.050637245178223\n",
      "[step: 3839] loss: 5.026861190795898\n",
      "[step: 3840] loss: 4.956287384033203\n",
      "[step: 3841] loss: 4.92832088470459\n",
      "[step: 3842] loss: 4.929586410522461\n",
      "[step: 3843] loss: 4.933545112609863\n",
      "[step: 3844] loss: 4.934240341186523\n",
      "[step: 3845] loss: 4.936861038208008\n",
      "[step: 3846] loss: 4.930426597595215\n",
      "[step: 3847] loss: 4.8953142166137695\n",
      "[step: 3848] loss: 4.872567176818848\n",
      "[step: 3849] loss: 4.885205268859863\n",
      "[step: 3850] loss: 4.9091339111328125\n",
      "[step: 3851] loss: 4.910747051239014\n",
      "[step: 3852] loss: 4.87895393371582\n",
      "[step: 3853] loss: 4.855990409851074\n",
      "[step: 3854] loss: 4.8600172996521\n",
      "[step: 3855] loss: 4.8732008934021\n",
      "[step: 3856] loss: 4.87518310546875\n",
      "[step: 3857] loss: 4.864945411682129\n",
      "[step: 3858] loss: 4.858323097229004\n",
      "[step: 3859] loss: 4.856110572814941\n",
      "[step: 3860] loss: 4.8506622314453125\n",
      "[step: 3861] loss: 4.842429161071777\n",
      "[step: 3862] loss: 4.838959693908691\n",
      "[step: 3863] loss: 4.843183517456055\n",
      "[step: 3864] loss: 4.847984313964844\n",
      "[step: 3865] loss: 4.845939636230469\n",
      "[step: 3866] loss: 4.838610649108887\n",
      "[step: 3867] loss: 4.832144737243652\n",
      "[step: 3868] loss: 4.829170227050781\n",
      "[step: 3869] loss: 4.827162742614746\n",
      "[step: 3870] loss: 4.823551177978516\n",
      "[step: 3871] loss: 4.8200459480285645\n",
      "[step: 3872] loss: 4.818960666656494\n",
      "[step: 3873] loss: 4.819967269897461\n",
      "[step: 3874] loss: 4.820590019226074\n",
      "[step: 3875] loss: 4.818969249725342\n",
      "[step: 3876] loss: 4.816595077514648\n",
      "[step: 3877] loss: 4.814823150634766\n",
      "[step: 3878] loss: 4.8140082359313965\n",
      "[step: 3879] loss: 4.812925338745117\n",
      "[step: 3880] loss: 4.8109002113342285\n",
      "[step: 3881] loss: 4.808553695678711\n",
      "[step: 3882] loss: 4.8069329261779785\n",
      "[step: 3883] loss: 4.806147575378418\n",
      "[step: 3884] loss: 4.806103706359863\n",
      "[step: 3885] loss: 4.806036949157715\n",
      "[step: 3886] loss: 4.807012557983398\n",
      "[step: 3887] loss: 4.809377193450928\n",
      "[step: 3888] loss: 4.815150260925293\n",
      "[step: 3889] loss: 4.824624061584473\n",
      "[step: 3890] loss: 4.841293811798096\n",
      "[step: 3891] loss: 4.8670573234558105\n",
      "[step: 3892] loss: 4.9135918617248535\n",
      "[step: 3893] loss: 4.983072757720947\n",
      "[step: 3894] loss: 5.112703323364258\n",
      "[step: 3895] loss: 5.281469345092773\n",
      "[step: 3896] loss: 5.5919976234436035\n",
      "[step: 3897] loss: 5.883116722106934\n",
      "[step: 3898] loss: 6.333425521850586\n",
      "[step: 3899] loss: 6.423081398010254\n",
      "[step: 3900] loss: 6.343827247619629\n",
      "[step: 3901] loss: 5.800930023193359\n",
      "[step: 3902] loss: 5.178276062011719\n",
      "[step: 3903] loss: 4.831549167633057\n",
      "[step: 3904] loss: 4.890439987182617\n",
      "[step: 3905] loss: 5.221667289733887\n",
      "[step: 3906] loss: 5.490044116973877\n",
      "[step: 3907] loss: 5.439653396606445\n",
      "[step: 3908] loss: 5.114343643188477\n",
      "[step: 3909] loss: 4.821584224700928\n",
      "[step: 3910] loss: 4.823228359222412\n",
      "[step: 3911] loss: 5.03924036026001\n",
      "[step: 3912] loss: 5.198850631713867\n",
      "[step: 3913] loss: 5.145273685455322\n",
      "[step: 3914] loss: 4.953254699707031\n",
      "[step: 3915] loss: 4.796826362609863\n",
      "[step: 3916] loss: 4.800191879272461\n",
      "[step: 3917] loss: 4.909296989440918\n",
      "[step: 3918] loss: 4.9991254806518555\n",
      "[step: 3919] loss: 5.0026936531066895\n",
      "[step: 3920] loss: 4.908603668212891\n",
      "[step: 3921] loss: 4.813701629638672\n",
      "[step: 3922] loss: 4.762661933898926\n",
      "[step: 3923] loss: 4.783731460571289\n",
      "[step: 3924] loss: 4.845876216888428\n",
      "[step: 3925] loss: 4.8870954513549805\n",
      "[step: 3926] loss: 4.889023780822754\n",
      "[step: 3927] loss: 4.832553863525391\n",
      "[step: 3928] loss: 4.773571491241455\n",
      "[step: 3929] loss: 4.741798400878906\n",
      "[step: 3930] loss: 4.7501020431518555\n",
      "[step: 3931] loss: 4.778835773468018\n",
      "[step: 3932] loss: 4.799755573272705\n",
      "[step: 3933] loss: 4.807706832885742\n",
      "[step: 3934] loss: 4.794804573059082\n",
      "[step: 3935] loss: 4.771429061889648\n",
      "[step: 3936] loss: 4.746440410614014\n",
      "[step: 3937] loss: 4.72931432723999\n",
      "[step: 3938] loss: 4.724753379821777\n",
      "[step: 3939] loss: 4.732195854187012\n",
      "[step: 3940] loss: 4.745794296264648\n",
      "[step: 3941] loss: 4.7562031745910645\n",
      "[step: 3942] loss: 4.7583208084106445\n",
      "[step: 3943] loss: 4.753348350524902\n",
      "[step: 3944] loss: 4.7451324462890625\n",
      "[step: 3945] loss: 4.734494209289551\n",
      "[step: 3946] loss: 4.724890232086182\n",
      "[step: 3947] loss: 4.715935707092285\n",
      "[step: 3948] loss: 4.708542823791504\n",
      "[step: 3949] loss: 4.703444480895996\n",
      "[step: 3950] loss: 4.701397895812988\n",
      "[step: 3951] loss: 4.701152801513672\n",
      "[step: 3952] loss: 4.701369762420654\n",
      "[step: 3953] loss: 4.702064514160156\n",
      "[step: 3954] loss: 4.703400611877441\n",
      "[step: 3955] loss: 4.705979347229004\n",
      "[step: 3956] loss: 4.710154056549072\n",
      "[step: 3957] loss: 4.716915130615234\n",
      "[step: 3958] loss: 4.726644515991211\n",
      "[step: 3959] loss: 4.742193222045898\n",
      "[step: 3960] loss: 4.7652435302734375\n",
      "[step: 3961] loss: 4.804616451263428\n",
      "[step: 3962] loss: 4.860524654388428\n",
      "[step: 3963] loss: 4.960053443908691\n",
      "[step: 3964] loss: 5.091192245483398\n",
      "[step: 3965] loss: 5.321804046630859\n",
      "[step: 3966] loss: 5.567810535430908\n",
      "[step: 3967] loss: 5.9397687911987305\n",
      "[step: 3968] loss: 6.1362409591674805\n",
      "[step: 3969] loss: 6.2453203201293945\n",
      "[step: 3970] loss: 5.939266204833984\n",
      "[step: 3971] loss: 5.417983531951904\n",
      "[step: 3972] loss: 4.919344902038574\n",
      "[step: 3973] loss: 4.696246147155762\n",
      "[step: 3974] loss: 4.818130016326904\n",
      "[step: 3975] loss: 5.10686731338501\n",
      "[step: 3976] loss: 5.302667140960693\n",
      "[step: 3977] loss: 5.228087425231934\n",
      "[step: 3978] loss: 4.943960189819336\n",
      "[step: 3979] loss: 4.709982872009277\n",
      "[step: 3980] loss: 4.703180313110352\n",
      "[step: 3981] loss: 4.868003845214844\n",
      "[step: 3982] loss: 5.016490936279297\n",
      "[step: 3983] loss: 5.014974594116211\n",
      "[step: 3984] loss: 4.8732757568359375\n",
      "[step: 3985] loss: 4.724719047546387\n",
      "[step: 3986] loss: 4.670787334442139\n",
      "[step: 3987] loss: 4.727509498596191\n",
      "[step: 3988] loss: 4.820334434509277\n",
      "[step: 3989] loss: 4.869838714599609\n",
      "[step: 3990] loss: 4.858089447021484\n",
      "[step: 3991] loss: 4.781135559082031\n",
      "[step: 3992] loss: 4.705604553222656\n",
      "[step: 3993] loss: 4.6587371826171875\n",
      "[step: 3994] loss: 4.66485071182251\n",
      "[step: 3995] loss: 4.709087371826172\n",
      "[step: 3996] loss: 4.753798961639404\n",
      "[step: 3997] loss: 4.777838230133057\n",
      "[step: 3998] loss: 4.750988006591797\n",
      "[step: 3999] loss: 4.706094741821289\n",
      "[step: 4000] loss: 4.658631801605225\n",
      "[step: 4001] loss: 4.636955261230469\n",
      "[step: 4002] loss: 4.6432366371154785\n",
      "[step: 4003] loss: 4.6638617515563965\n",
      "[step: 4004] loss: 4.685633659362793\n",
      "[step: 4005] loss: 4.695122718811035\n",
      "[step: 4006] loss: 4.696023464202881\n",
      "[step: 4007] loss: 4.684277057647705\n",
      "[step: 4008] loss: 4.665051460266113\n",
      "[step: 4009] loss: 4.643884658813477\n",
      "[step: 4010] loss: 4.626922607421875\n",
      "[step: 4011] loss: 4.617575168609619\n",
      "[step: 4012] loss: 4.616080284118652\n",
      "[step: 4013] loss: 4.621024131774902\n",
      "[step: 4014] loss: 4.628687858581543\n",
      "[step: 4015] loss: 4.635231018066406\n",
      "[step: 4016] loss: 4.640597820281982\n",
      "[step: 4017] loss: 4.646644115447998\n",
      "[step: 4018] loss: 4.6528143882751465\n",
      "[step: 4019] loss: 4.661380290985107\n",
      "[step: 4020] loss: 4.668832778930664\n",
      "[step: 4021] loss: 4.680682182312012\n",
      "[step: 4022] loss: 4.6911468505859375\n",
      "[step: 4023] loss: 4.709994316101074\n",
      "[step: 4024] loss: 4.731993675231934\n",
      "[step: 4025] loss: 4.7693190574646\n",
      "[step: 4026] loss: 4.815366268157959\n",
      "[step: 4027] loss: 4.888111114501953\n",
      "[step: 4028] loss: 4.971087455749512\n",
      "[step: 4029] loss: 5.0948004722595215\n",
      "[step: 4030] loss: 5.201001167297363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4031] loss: 5.342181205749512\n",
      "[step: 4032] loss: 5.388460159301758\n",
      "[step: 4033] loss: 5.4145612716674805\n",
      "[step: 4034] loss: 5.289811611175537\n",
      "[step: 4035] loss: 5.107169151306152\n",
      "[step: 4036] loss: 4.877860069274902\n",
      "[step: 4037] loss: 4.6884260177612305\n",
      "[step: 4038] loss: 4.593500137329102\n",
      "[step: 4039] loss: 4.6014862060546875\n",
      "[step: 4040] loss: 4.682831764221191\n",
      "[step: 4041] loss: 4.782694339752197\n",
      "[step: 4042] loss: 4.851218223571777\n",
      "[step: 4043] loss: 4.853170394897461\n",
      "[step: 4044] loss: 4.793800354003906\n",
      "[step: 4045] loss: 4.702622413635254\n",
      "[step: 4046] loss: 4.6227707862854\n",
      "[step: 4047] loss: 4.577000617980957\n",
      "[step: 4048] loss: 4.5708112716674805\n",
      "[step: 4049] loss: 4.593403339385986\n",
      "[step: 4050] loss: 4.629742622375488\n",
      "[step: 4051] loss: 4.665634632110596\n",
      "[step: 4052] loss: 4.690116882324219\n",
      "[step: 4053] loss: 4.704756736755371\n",
      "[step: 4054] loss: 4.699331283569336\n",
      "[step: 4055] loss: 4.689333915710449\n",
      "[step: 4056] loss: 4.6631879806518555\n",
      "[step: 4057] loss: 4.638372421264648\n",
      "[step: 4058] loss: 4.609782695770264\n",
      "[step: 4059] loss: 4.586983680725098\n",
      "[step: 4060] loss: 4.568890571594238\n",
      "[step: 4061] loss: 4.556307315826416\n",
      "[step: 4062] loss: 4.548010349273682\n",
      "[step: 4063] loss: 4.542840957641602\n",
      "[step: 4064] loss: 4.540229797363281\n",
      "[step: 4065] loss: 4.539825439453125\n",
      "[step: 4066] loss: 4.541425704956055\n",
      "[step: 4067] loss: 4.544831275939941\n",
      "[step: 4068] loss: 4.550041198730469\n",
      "[step: 4069] loss: 4.557407379150391\n",
      "[step: 4070] loss: 4.568143844604492\n",
      "[step: 4071] loss: 4.584136486053467\n",
      "[step: 4072] loss: 4.61000919342041\n",
      "[step: 4073] loss: 4.649128437042236\n",
      "[step: 4074] loss: 4.716332912445068\n",
      "[step: 4075] loss: 4.814131736755371\n",
      "[step: 4076] loss: 4.987143516540527\n",
      "[step: 4077] loss: 5.214052677154541\n",
      "[step: 4078] loss: 5.594521522521973\n",
      "[step: 4079] loss: 5.955288410186768\n",
      "[step: 4080] loss: 6.384445667266846\n",
      "[step: 4081] loss: 6.4090895652771\n",
      "[step: 4082] loss: 6.0652923583984375\n",
      "[step: 4083] loss: 5.399989128112793\n",
      "[step: 4084] loss: 4.769923210144043\n",
      "[step: 4085] loss: 4.594121932983398\n",
      "[step: 4086] loss: 4.840853691101074\n",
      "[step: 4087] loss: 5.224764347076416\n",
      "[step: 4088] loss: 5.334877967834473\n",
      "[step: 4089] loss: 5.049445152282715\n",
      "[step: 4090] loss: 4.685527801513672\n",
      "[step: 4091] loss: 4.568376541137695\n",
      "[step: 4092] loss: 4.7619428634643555\n",
      "[step: 4093] loss: 4.97968864440918\n",
      "[step: 4094] loss: 4.973091125488281\n",
      "[step: 4095] loss: 4.758852958679199\n",
      "[step: 4096] loss: 4.55350399017334\n",
      "[step: 4097] loss: 4.549560546875\n",
      "[step: 4098] loss: 4.693994522094727\n",
      "[step: 4099] loss: 4.800191879272461\n",
      "[step: 4100] loss: 4.755840301513672\n",
      "[step: 4101] loss: 4.616220474243164\n",
      "[step: 4102] loss: 4.515435218811035\n",
      "[step: 4103] loss: 4.524504661560059\n",
      "[step: 4104] loss: 4.599653720855713\n",
      "[step: 4105] loss: 4.649423599243164\n",
      "[step: 4106] loss: 4.628856658935547\n",
      "[step: 4107] loss: 4.570519924163818\n",
      "[step: 4108] loss: 4.528529167175293\n",
      "[step: 4109] loss: 4.516476631164551\n",
      "[step: 4110] loss: 4.528435707092285\n",
      "[step: 4111] loss: 4.539361476898193\n",
      "[step: 4112] loss: 4.5412373542785645\n",
      "[step: 4113] loss: 4.5387349128723145\n",
      "[step: 4114] loss: 4.532057762145996\n",
      "[step: 4115] loss: 4.523792266845703\n",
      "[step: 4116] loss: 4.505392074584961\n",
      "[step: 4117] loss: 4.489010334014893\n",
      "[step: 4118] loss: 4.481454372406006\n",
      "[step: 4119] loss: 4.486787796020508\n",
      "[step: 4120] loss: 4.500537872314453\n",
      "[step: 4121] loss: 4.508664608001709\n",
      "[step: 4122] loss: 4.506290912628174\n",
      "[step: 4123] loss: 4.4904398918151855\n",
      "[step: 4124] loss: 4.474806785583496\n",
      "[step: 4125] loss: 4.464804172515869\n",
      "[step: 4126] loss: 4.462577819824219\n",
      "[step: 4127] loss: 4.4646196365356445\n",
      "[step: 4128] loss: 4.465387344360352\n",
      "[step: 4129] loss: 4.463398456573486\n",
      "[step: 4130] loss: 4.460446357727051\n",
      "[step: 4131] loss: 4.459867477416992\n",
      "[step: 4132] loss: 4.4617919921875\n",
      "[step: 4133] loss: 4.465394973754883\n",
      "[step: 4134] loss: 4.468648433685303\n",
      "[step: 4135] loss: 4.4699554443359375\n",
      "[step: 4136] loss: 4.4704389572143555\n",
      "[step: 4137] loss: 4.472160339355469\n",
      "[step: 4138] loss: 4.476469039916992\n",
      "[step: 4139] loss: 4.4861555099487305\n",
      "[step: 4140] loss: 4.499037265777588\n",
      "[step: 4141] loss: 4.520981788635254\n",
      "[step: 4142] loss: 4.546739101409912\n",
      "[step: 4143] loss: 4.593405723571777\n",
      "[step: 4144] loss: 4.651793003082275\n",
      "[step: 4145] loss: 4.7571282386779785\n",
      "[step: 4146] loss: 4.883483409881592\n",
      "[step: 4147] loss: 5.0907979011535645\n",
      "[step: 4148] loss: 5.292212963104248\n",
      "[step: 4149] loss: 5.562058448791504\n",
      "[step: 4150] loss: 5.680678844451904\n",
      "[step: 4151] loss: 5.704831123352051\n",
      "[step: 4152] loss: 5.4331536293029785\n",
      "[step: 4153] loss: 5.024173736572266\n",
      "[step: 4154] loss: 4.636363983154297\n",
      "[step: 4155] loss: 4.445818901062012\n",
      "[step: 4156] loss: 4.501828670501709\n",
      "[step: 4157] loss: 4.700047492980957\n",
      "[step: 4158] loss: 4.885921955108643\n",
      "[step: 4159] loss: 4.90776252746582\n",
      "[step: 4160] loss: 4.75595760345459\n",
      "[step: 4161] loss: 4.548022270202637\n",
      "[step: 4162] loss: 4.427029132843018\n",
      "[step: 4163] loss: 4.452540397644043\n",
      "[step: 4164] loss: 4.566953659057617\n",
      "[step: 4165] loss: 4.672782897949219\n",
      "[step: 4166] loss: 4.692998886108398\n",
      "[step: 4167] loss: 4.618605613708496\n",
      "[step: 4168] loss: 4.501880168914795\n",
      "[step: 4169] loss: 4.419503211975098\n",
      "[step: 4170] loss: 4.40948486328125\n",
      "[step: 4171] loss: 4.456063747406006\n",
      "[step: 4172] loss: 4.515743255615234\n",
      "[step: 4173] loss: 4.553073883056641\n",
      "[step: 4174] loss: 4.553272247314453\n",
      "[step: 4175] loss: 4.515836715698242\n",
      "[step: 4176] loss: 4.469046592712402\n",
      "[step: 4177] loss: 4.426904678344727\n",
      "[step: 4178] loss: 4.406278610229492\n",
      "[step: 4179] loss: 4.40190315246582\n",
      "[step: 4180] loss: 4.409181594848633\n",
      "[step: 4181] loss: 4.423161506652832\n",
      "[step: 4182] loss: 4.438347816467285\n",
      "[step: 4183] loss: 4.450189113616943\n",
      "[step: 4184] loss: 4.450544357299805\n",
      "[step: 4185] loss: 4.446506500244141\n",
      "[step: 4186] loss: 4.430840492248535\n",
      "[step: 4187] loss: 4.413961410522461\n",
      "[step: 4188] loss: 4.395485877990723\n",
      "[step: 4189] loss: 4.382165908813477\n",
      "[step: 4190] loss: 4.373913764953613\n",
      "[step: 4191] loss: 4.370240688323975\n",
      "[step: 4192] loss: 4.369557857513428\n",
      "[step: 4193] loss: 4.369833469390869\n",
      "[step: 4194] loss: 4.370156288146973\n",
      "[step: 4195] loss: 4.370586395263672\n",
      "[step: 4196] loss: 4.372710704803467\n",
      "[step: 4197] loss: 4.376877784729004\n",
      "[step: 4198] loss: 4.384600639343262\n",
      "[step: 4199] loss: 4.396404266357422\n",
      "[step: 4200] loss: 4.415149688720703\n",
      "[step: 4201] loss: 4.443352699279785\n",
      "[step: 4202] loss: 4.489611625671387\n",
      "[step: 4203] loss: 4.560050010681152\n",
      "[step: 4204] loss: 4.680305004119873\n",
      "[step: 4205] loss: 4.847987174987793\n",
      "[step: 4206] loss: 5.132424354553223\n",
      "[step: 4207] loss: 5.444976806640625\n",
      "[step: 4208] loss: 5.903584957122803\n",
      "[step: 4209] loss: 6.109940528869629\n",
      "[step: 4210] loss: 6.149646282196045\n",
      "[step: 4211] loss: 5.6856536865234375\n",
      "[step: 4212] loss: 5.0155439376831055\n",
      "[step: 4213] loss: 4.559072494506836\n",
      "[step: 4214] loss: 4.484783172607422\n",
      "[step: 4215] loss: 4.753911972045898\n",
      "[step: 4216] loss: 5.0146918296813965\n",
      "[step: 4217] loss: 5.058434009552002\n",
      "[step: 4218] loss: 4.832415580749512\n",
      "[step: 4219] loss: 4.53501033782959\n",
      "[step: 4220] loss: 4.46029806137085\n",
      "[step: 4221] loss: 4.5816264152526855\n",
      "[step: 4222] loss: 4.749536991119385\n",
      "[step: 4223] loss: 4.763980865478516\n",
      "[step: 4224] loss: 4.610908508300781\n",
      "[step: 4225] loss: 4.449131965637207\n",
      "[step: 4226] loss: 4.389313697814941\n",
      "[step: 4227] loss: 4.471410751342773\n",
      "[step: 4228] loss: 4.579035758972168\n",
      "[step: 4229] loss: 4.594132423400879\n",
      "[step: 4230] loss: 4.507582664489746\n",
      "[step: 4231] loss: 4.378803730010986\n",
      "[step: 4232] loss: 4.329415321350098\n",
      "[step: 4233] loss: 4.383143901824951\n",
      "[step: 4234] loss: 4.459070682525635\n",
      "[step: 4235] loss: 4.481647491455078\n",
      "[step: 4236] loss: 4.431667327880859\n",
      "[step: 4237] loss: 4.36077880859375\n",
      "[step: 4238] loss: 4.322486877441406\n",
      "[step: 4239] loss: 4.333857536315918\n",
      "[step: 4240] loss: 4.366294860839844\n",
      "[step: 4241] loss: 4.382291793823242\n",
      "[step: 4242] loss: 4.376855373382568\n",
      "[step: 4243] loss: 4.359134674072266\n",
      "[step: 4244] loss: 4.3400068283081055\n",
      "[step: 4245] loss: 4.328153610229492\n",
      "[step: 4246] loss: 4.3199968338012695\n",
      "[step: 4247] loss: 4.314802646636963\n",
      "[step: 4248] loss: 4.312981605529785\n",
      "[step: 4249] loss: 4.318296432495117\n",
      "[step: 4250] loss: 4.327576637268066\n",
      "[step: 4251] loss: 4.331453323364258\n",
      "[step: 4252] loss: 4.328695297241211\n",
      "[step: 4253] loss: 4.315840721130371\n",
      "[step: 4254] loss: 4.3016252517700195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4255] loss: 4.290241241455078\n",
      "[step: 4256] loss: 4.285667419433594\n",
      "[step: 4257] loss: 4.285741329193115\n",
      "[step: 4258] loss: 4.286257743835449\n",
      "[step: 4259] loss: 4.285861015319824\n",
      "[step: 4260] loss: 4.284262657165527\n",
      "[step: 4261] loss: 4.283307075500488\n",
      "[step: 4262] loss: 4.28453254699707\n",
      "[step: 4263] loss: 4.2880754470825195\n",
      "[step: 4264] loss: 4.292625427246094\n",
      "[step: 4265] loss: 4.297562599182129\n",
      "[step: 4266] loss: 4.303186416625977\n",
      "[step: 4267] loss: 4.311002731323242\n",
      "[step: 4268] loss: 4.323338031768799\n",
      "[step: 4269] loss: 4.34451961517334\n",
      "[step: 4270] loss: 4.374163627624512\n",
      "[step: 4271] loss: 4.424570560455322\n",
      "[step: 4272] loss: 4.490260124206543\n",
      "[step: 4273] loss: 4.605570316314697\n",
      "[step: 4274] loss: 4.745794296264648\n",
      "[step: 4275] loss: 4.9795613288879395\n",
      "[step: 4276] loss: 5.2083845138549805\n",
      "[step: 4277] loss: 5.514245986938477\n",
      "[step: 4278] loss: 5.6613922119140625\n",
      "[step: 4279] loss: 5.673291206359863\n",
      "[step: 4280] loss: 5.382884979248047\n",
      "[step: 4281] loss: 4.899731159210205\n",
      "[step: 4282] loss: 4.466514587402344\n",
      "[step: 4283] loss: 4.2732110023498535\n",
      "[step: 4284] loss: 4.374073505401611\n",
      "[step: 4285] loss: 4.621738433837891\n",
      "[step: 4286] loss: 4.804731369018555\n",
      "[step: 4287] loss: 4.775081634521484\n",
      "[step: 4288] loss: 4.548952102661133\n",
      "[step: 4289] loss: 4.32173490524292\n",
      "[step: 4290] loss: 4.2518815994262695\n",
      "[step: 4291] loss: 4.351208686828613\n",
      "[step: 4292] loss: 4.500278472900391\n",
      "[step: 4293] loss: 4.569862365722656\n",
      "[step: 4294] loss: 4.513477802276611\n",
      "[step: 4295] loss: 4.378796577453613\n",
      "[step: 4296] loss: 4.266026496887207\n",
      "[step: 4297] loss: 4.244408130645752\n",
      "[step: 4298] loss: 4.300365447998047\n",
      "[step: 4299] loss: 4.3739213943481445\n",
      "[step: 4300] loss: 4.409276962280273\n",
      "[step: 4301] loss: 4.391557693481445\n",
      "[step: 4302] loss: 4.344120025634766\n",
      "[step: 4303] loss: 4.287325382232666\n",
      "[step: 4304] loss: 4.254535675048828\n",
      "[step: 4305] loss: 4.246494770050049\n",
      "[step: 4306] loss: 4.256893157958984\n",
      "[step: 4307] loss: 4.2719035148620605\n",
      "[step: 4308] loss: 4.2857818603515625\n",
      "[step: 4309] loss: 4.295851707458496\n",
      "[step: 4310] loss: 4.290619850158691\n",
      "[step: 4311] loss: 4.277018070220947\n",
      "[step: 4312] loss: 4.251341819763184\n",
      "[step: 4313] loss: 4.228740215301514\n",
      "[step: 4314] loss: 4.212136745452881\n",
      "[step: 4315] loss: 4.206555366516113\n",
      "[step: 4316] loss: 4.212009906768799\n",
      "[step: 4317] loss: 4.223189830780029\n",
      "[step: 4318] loss: 4.234152317047119\n",
      "[step: 4319] loss: 4.238770484924316\n",
      "[step: 4320] loss: 4.2407331466674805\n",
      "[step: 4321] loss: 4.237899303436279\n",
      "[step: 4322] loss: 4.236532211303711\n",
      "[step: 4323] loss: 4.235836029052734\n",
      "[step: 4324] loss: 4.2376790046691895\n",
      "[step: 4325] loss: 4.239692687988281\n",
      "[step: 4326] loss: 4.241474151611328\n",
      "[step: 4327] loss: 4.24370002746582\n",
      "[step: 4328] loss: 4.247906684875488\n",
      "[step: 4329] loss: 4.25571346282959\n",
      "[step: 4330] loss: 4.271327018737793\n",
      "[step: 4331] loss: 4.294130802154541\n",
      "[step: 4332] loss: 4.333430767059326\n",
      "[step: 4333] loss: 4.382236003875732\n",
      "[step: 4334] loss: 4.465152740478516\n",
      "[step: 4335] loss: 4.563148498535156\n",
      "[step: 4336] loss: 4.727576732635498\n",
      "[step: 4337] loss: 4.901680946350098\n",
      "[step: 4338] loss: 5.151680946350098\n",
      "[step: 4339] loss: 5.330941200256348\n",
      "[step: 4340] loss: 5.459738254547119\n",
      "[step: 4341] loss: 5.37205696105957\n",
      "[step: 4342] loss: 5.07792854309082\n",
      "[step: 4343] loss: 4.68302059173584\n",
      "[step: 4344] loss: 4.328457832336426\n",
      "[step: 4345] loss: 4.185634136199951\n",
      "[step: 4346] loss: 4.265621185302734\n",
      "[step: 4347] loss: 4.464506149291992\n",
      "[step: 4348] loss: 4.622227668762207\n",
      "[step: 4349] loss: 4.620856285095215\n",
      "[step: 4350] loss: 4.473595142364502\n",
      "[step: 4351] loss: 4.278115272521973\n",
      "[step: 4352] loss: 4.16977071762085\n",
      "[step: 4353] loss: 4.190446853637695\n",
      "[step: 4354] loss: 4.291468620300293\n",
      "[step: 4355] loss: 4.3888773918151855\n",
      "[step: 4356] loss: 4.414651870727539\n",
      "[step: 4357] loss: 4.360214710235596\n",
      "[step: 4358] loss: 4.265070915222168\n",
      "[step: 4359] loss: 4.186209201812744\n",
      "[step: 4360] loss: 4.1612725257873535\n",
      "[step: 4361] loss: 4.181888103485107\n",
      "[step: 4362] loss: 4.219921112060547\n",
      "[step: 4363] loss: 4.246549606323242\n",
      "[step: 4364] loss: 4.253938674926758\n",
      "[step: 4365] loss: 4.243719577789307\n",
      "[step: 4366] loss: 4.221364974975586\n",
      "[step: 4367] loss: 4.203946113586426\n",
      "[step: 4368] loss: 4.186432838439941\n",
      "[step: 4369] loss: 4.173673629760742\n",
      "[step: 4370] loss: 4.15870475769043\n",
      "[step: 4371] loss: 4.147712230682373\n",
      "[step: 4372] loss: 4.142853736877441\n",
      "[step: 4373] loss: 4.146215915679932\n",
      "[step: 4374] loss: 4.156461715698242\n",
      "[step: 4375] loss: 4.167874336242676\n",
      "[step: 4376] loss: 4.1784515380859375\n",
      "[step: 4377] loss: 4.179133415222168\n",
      "[step: 4378] loss: 4.176826477050781\n",
      "[step: 4379] loss: 4.16745662689209\n",
      "[step: 4380] loss: 4.160661697387695\n",
      "[step: 4381] loss: 4.155507564544678\n",
      "[step: 4382] loss: 4.155862331390381\n",
      "[step: 4383] loss: 4.159543037414551\n",
      "[step: 4384] loss: 4.1653008460998535\n",
      "[step: 4385] loss: 4.172890663146973\n",
      "[step: 4386] loss: 4.182141304016113\n",
      "[step: 4387] loss: 4.196282386779785\n",
      "[step: 4388] loss: 4.219308853149414\n",
      "[step: 4389] loss: 4.255335330963135\n",
      "[step: 4390] loss: 4.317937850952148\n",
      "[step: 4391] loss: 4.403630256652832\n",
      "[step: 4392] loss: 4.550506591796875\n",
      "[step: 4393] loss: 4.7184343338012695\n",
      "[step: 4394] loss: 4.987553596496582\n",
      "[step: 4395] loss: 5.20527458190918\n",
      "[step: 4396] loss: 5.468576908111572\n",
      "[step: 4397] loss: 5.5072431564331055\n",
      "[step: 4398] loss: 5.367866039276123\n",
      "[step: 4399] loss: 5.018325328826904\n",
      "[step: 4400] loss: 4.550563812255859\n",
      "[step: 4401] loss: 4.235743522644043\n",
      "[step: 4402] loss: 4.159826278686523\n",
      "[step: 4403] loss: 4.314228057861328\n",
      "[step: 4404] loss: 4.527398109436035\n",
      "[step: 4405] loss: 4.629295349121094\n",
      "[step: 4406] loss: 4.547496795654297\n",
      "[step: 4407] loss: 4.319559097290039\n",
      "[step: 4408] loss: 4.145472526550293\n",
      "[step: 4409] loss: 4.119109630584717\n",
      "[step: 4410] loss: 4.225300312042236\n",
      "[step: 4411] loss: 4.353656768798828\n",
      "[step: 4412] loss: 4.396113395690918\n",
      "[step: 4413] loss: 4.3299055099487305\n",
      "[step: 4414] loss: 4.195662498474121\n",
      "[step: 4415] loss: 4.097721099853516\n",
      "[step: 4416] loss: 4.087047576904297\n",
      "[step: 4417] loss: 4.146839618682861\n",
      "[step: 4418] loss: 4.2180986404418945\n",
      "[step: 4419] loss: 4.243440628051758\n",
      "[step: 4420] loss: 4.217751502990723\n",
      "[step: 4421] loss: 4.163301467895508\n",
      "[step: 4422] loss: 4.113386154174805\n",
      "[step: 4423] loss: 4.092596530914307\n",
      "[step: 4424] loss: 4.096066951751709\n",
      "[step: 4425] loss: 4.109328269958496\n",
      "[step: 4426] loss: 4.115347862243652\n",
      "[step: 4427] loss: 4.115998268127441\n",
      "[step: 4428] loss: 4.11419153213501\n",
      "[step: 4429] loss: 4.111806869506836\n",
      "[step: 4430] loss: 4.1116533279418945\n",
      "[step: 4431] loss: 4.1055192947387695\n",
      "[step: 4432] loss: 4.096549987792969\n",
      "[step: 4433] loss: 4.0788350105285645\n",
      "[step: 4434] loss: 4.061637878417969\n",
      "[step: 4435] loss: 4.049603462219238\n",
      "[step: 4436] loss: 4.046391487121582\n",
      "[step: 4437] loss: 4.050661087036133\n",
      "[step: 4438] loss: 4.058182716369629\n",
      "[step: 4439] loss: 4.065269947052002\n",
      "[step: 4440] loss: 4.0678558349609375\n",
      "[step: 4441] loss: 4.069012641906738\n",
      "[step: 4442] loss: 4.0688157081604\n",
      "[step: 4443] loss: 4.072986125946045\n",
      "[step: 4444] loss: 4.081252574920654\n",
      "[step: 4445] loss: 4.097012519836426\n",
      "[step: 4446] loss: 4.119724750518799\n",
      "[step: 4447] loss: 4.153209686279297\n",
      "[step: 4448] loss: 4.200868606567383\n",
      "[step: 4449] loss: 4.274419784545898\n",
      "[step: 4450] loss: 4.377702713012695\n",
      "[step: 4451] loss: 4.541280746459961\n",
      "[step: 4452] loss: 4.736440658569336\n",
      "[step: 4453] loss: 5.030134201049805\n",
      "[step: 4454] loss: 5.253576278686523\n",
      "[step: 4455] loss: 5.490453720092773\n",
      "[step: 4456] loss: 5.397252082824707\n",
      "[step: 4457] loss: 5.1118035316467285\n",
      "[step: 4458] loss: 4.644893646240234\n",
      "[step: 4459] loss: 4.2377824783325195\n",
      "[step: 4460] loss: 4.086634635925293\n",
      "[step: 4461] loss: 4.17918062210083\n",
      "[step: 4462] loss: 4.400406360626221\n",
      "[step: 4463] loss: 4.537919044494629\n",
      "[step: 4464] loss: 4.51128625869751\n",
      "[step: 4465] loss: 4.345059394836426\n",
      "[step: 4466] loss: 4.150269508361816\n",
      "[step: 4467] loss: 4.0732855796813965\n",
      "[step: 4468] loss: 4.117441654205322\n",
      "[step: 4469] loss: 4.227481365203857\n",
      "[step: 4470] loss: 4.302923679351807\n",
      "[step: 4471] loss: 4.288365364074707\n",
      "[step: 4472] loss: 4.199769020080566\n",
      "[step: 4473] loss: 4.082451343536377\n",
      "[step: 4474] loss: 4.021986961364746\n",
      "[step: 4475] loss: 4.040253639221191\n",
      "[step: 4476] loss: 4.110675811767578\n",
      "[step: 4477] loss: 4.174201011657715\n",
      "[step: 4478] loss: 4.17927885055542\n",
      "[step: 4479] loss: 4.131958961486816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4480] loss: 4.058135986328125\n",
      "[step: 4481] loss: 4.005033493041992\n",
      "[step: 4482] loss: 3.994797706604004\n",
      "[step: 4483] loss: 4.01978874206543\n",
      "[step: 4484] loss: 4.054147720336914\n",
      "[step: 4485] loss: 4.073375701904297\n",
      "[step: 4486] loss: 4.074589729309082\n",
      "[step: 4487] loss: 4.058395862579346\n",
      "[step: 4488] loss: 4.034991264343262\n",
      "[step: 4489] loss: 4.015837669372559\n",
      "[step: 4490] loss: 4.0022125244140625\n",
      "[step: 4491] loss: 3.9939427375793457\n",
      "[step: 4492] loss: 3.9873974323272705\n",
      "[step: 4493] loss: 3.9842312335968018\n",
      "[step: 4494] loss: 3.9846112728118896\n",
      "[step: 4495] loss: 3.989259719848633\n",
      "[step: 4496] loss: 3.997817277908325\n",
      "[step: 4497] loss: 4.0069074630737305\n",
      "[step: 4498] loss: 4.016059875488281\n",
      "[step: 4499] loss: 4.020197868347168\n",
      "[step: 4500] loss: 4.024784088134766\n",
      "[step: 4501] loss: 4.025883674621582\n",
      "[step: 4502] loss: 4.031238555908203\n",
      "[step: 4503] loss: 4.0380449295043945\n",
      "[step: 4504] loss: 4.052249908447266\n",
      "[step: 4505] loss: 4.072329998016357\n",
      "[step: 4506] loss: 4.102846145629883\n",
      "[step: 4507] loss: 4.145227432250977\n",
      "[step: 4508] loss: 4.206634521484375\n",
      "[step: 4509] loss: 4.288129806518555\n",
      "[step: 4510] loss: 4.404923439025879\n",
      "[step: 4511] loss: 4.541049957275391\n",
      "[step: 4512] loss: 4.728483200073242\n",
      "[step: 4513] loss: 4.885272979736328\n",
      "[step: 4514] loss: 5.046647071838379\n",
      "[step: 4515] loss: 5.034223556518555\n",
      "[step: 4516] loss: 4.900203227996826\n",
      "[step: 4517] loss: 4.596285343170166\n",
      "[step: 4518] loss: 4.2626729011535645\n",
      "[step: 4519] loss: 4.031943321228027\n",
      "[step: 4520] loss: 3.9653608798980713\n",
      "[step: 4521] loss: 4.048810958862305\n",
      "[step: 4522] loss: 4.193020820617676\n",
      "[step: 4523] loss: 4.310515880584717\n",
      "[step: 4524] loss: 4.319723129272461\n",
      "[step: 4525] loss: 4.224617958068848\n",
      "[step: 4526] loss: 4.0916595458984375\n",
      "[step: 4527] loss: 3.982625961303711\n",
      "[step: 4528] loss: 3.945863723754883\n",
      "[step: 4529] loss: 3.976391077041626\n",
      "[step: 4530] loss: 4.043980121612549\n",
      "[step: 4531] loss: 4.103643417358398\n",
      "[step: 4532] loss: 4.124433517456055\n",
      "[step: 4533] loss: 4.0991902351379395\n",
      "[step: 4534] loss: 4.03875732421875\n",
      "[step: 4535] loss: 3.9759206771850586\n",
      "[step: 4536] loss: 3.932054281234741\n",
      "[step: 4537] loss: 3.9183077812194824\n",
      "[step: 4538] loss: 3.930835723876953\n",
      "[step: 4539] loss: 3.9570364952087402\n",
      "[step: 4540] loss: 3.9835205078125\n",
      "[step: 4541] loss: 3.9988014698028564\n",
      "[step: 4542] loss: 4.002114295959473\n",
      "[step: 4543] loss: 3.9946486949920654\n",
      "[step: 4544] loss: 3.9806690216064453\n",
      "[step: 4545] loss: 3.9656620025634766\n",
      "[step: 4546] loss: 3.9498114585876465\n",
      "[step: 4547] loss: 3.9370498657226562\n",
      "[step: 4548] loss: 3.9248275756835938\n",
      "[step: 4549] loss: 3.9151766300201416\n",
      "[step: 4550] loss: 3.906578540802002\n",
      "[step: 4551] loss: 3.8999624252319336\n",
      "[step: 4552] loss: 3.895238161087036\n",
      "[step: 4553] loss: 3.8927011489868164\n",
      "[step: 4554] loss: 3.8921828269958496\n",
      "[step: 4555] loss: 3.8934006690979004\n",
      "[step: 4556] loss: 3.896206855773926\n",
      "[step: 4557] loss: 3.9003942012786865\n",
      "[step: 4558] loss: 3.90696382522583\n",
      "[step: 4559] loss: 3.9162917137145996\n",
      "[step: 4560] loss: 3.932582378387451\n",
      "[step: 4561] loss: 3.957663059234619\n",
      "[step: 4562] loss: 4.003564834594727\n",
      "[step: 4563] loss: 4.076318740844727\n",
      "[step: 4564] loss: 4.209848403930664\n",
      "[step: 4565] loss: 4.418710231781006\n",
      "[step: 4566] loss: 4.7868218421936035\n",
      "[step: 4567] loss: 5.2946343421936035\n",
      "[step: 4568] loss: 6.014145374298096\n",
      "[step: 4569] loss: 6.588066101074219\n",
      "[step: 4570] loss: 6.687020301818848\n",
      "[step: 4571] loss: 5.948437690734863\n",
      "[step: 4572] loss: 4.669804573059082\n",
      "[step: 4573] loss: 3.99019718170166\n",
      "[step: 4574] loss: 4.289920806884766\n",
      "[step: 4575] loss: 5.030281066894531\n",
      "[step: 4576] loss: 5.186945915222168\n",
      "[step: 4577] loss: 4.586361885070801\n",
      "[step: 4578] loss: 4.010109901428223\n",
      "[step: 4579] loss: 4.132823944091797\n",
      "[step: 4580] loss: 4.625163555145264\n",
      "[step: 4581] loss: 4.626303195953369\n",
      "[step: 4582] loss: 4.195153713226318\n",
      "[step: 4583] loss: 3.9152603149414062\n",
      "[step: 4584] loss: 4.090362548828125\n",
      "[step: 4585] loss: 4.361319541931152\n",
      "[step: 4586] loss: 4.257946491241455\n",
      "[step: 4587] loss: 3.974548578262329\n",
      "[step: 4588] loss: 3.889934539794922\n",
      "[step: 4589] loss: 4.05757474899292\n",
      "[step: 4590] loss: 4.187863349914551\n",
      "[step: 4591] loss: 4.078112602233887\n",
      "[step: 4592] loss: 3.9104058742523193\n",
      "[step: 4593] loss: 3.891031265258789\n",
      "[step: 4594] loss: 3.998054265975952\n",
      "[step: 4595] loss: 4.050826072692871\n",
      "[step: 4596] loss: 3.980642557144165\n",
      "[step: 4597] loss: 3.8948798179626465\n",
      "[step: 4598] loss: 3.8864755630493164\n",
      "[step: 4599] loss: 3.9418885707855225\n",
      "[step: 4600] loss: 3.964148759841919\n",
      "[step: 4601] loss: 3.9236037731170654\n",
      "[step: 4602] loss: 3.875528335571289\n",
      "[step: 4603] loss: 3.869655132293701\n",
      "[step: 4604] loss: 3.899674892425537\n",
      "[step: 4605] loss: 3.9144179821014404\n",
      "[step: 4606] loss: 3.8956832885742188\n",
      "[step: 4607] loss: 3.8625190258026123\n",
      "[step: 4608] loss: 3.8466849327087402\n",
      "[step: 4609] loss: 3.858092784881592\n",
      "[step: 4610] loss: 3.873971462249756\n",
      "[step: 4611] loss: 3.874778985977173\n",
      "[step: 4612] loss: 3.8576712608337402\n",
      "[step: 4613] loss: 3.8384668827056885\n",
      "[step: 4614] loss: 3.831847906112671\n",
      "[step: 4615] loss: 3.8374829292297363\n",
      "[step: 4616] loss: 3.8458399772644043\n",
      "[step: 4617] loss: 3.846611261367798\n",
      "[step: 4618] loss: 3.8385701179504395\n",
      "[step: 4619] loss: 3.827324628829956\n",
      "[step: 4620] loss: 3.8195645809173584\n",
      "[step: 4621] loss: 3.818045139312744\n",
      "[step: 4622] loss: 3.821024179458618\n",
      "[step: 4623] loss: 3.823723077774048\n",
      "[step: 4624] loss: 3.822972297668457\n",
      "[step: 4625] loss: 3.819075584411621\n",
      "[step: 4626] loss: 3.813262462615967\n",
      "[step: 4627] loss: 3.808202028274536\n",
      "[step: 4628] loss: 3.805072784423828\n",
      "[step: 4629] loss: 3.803917407989502\n",
      "[step: 4630] loss: 3.8037452697753906\n",
      "[step: 4631] loss: 3.80329966545105\n",
      "[step: 4632] loss: 3.802140712738037\n",
      "[step: 4633] loss: 3.7999205589294434\n",
      "[step: 4634] loss: 3.7972331047058105\n",
      "[step: 4635] loss: 3.794534683227539\n",
      "[step: 4636] loss: 3.7923085689544678\n",
      "[step: 4637] loss: 3.7906813621520996\n",
      "[step: 4638] loss: 3.789496898651123\n",
      "[step: 4639] loss: 3.7886064052581787\n",
      "[step: 4640] loss: 3.7875895500183105\n",
      "[step: 4641] loss: 3.786421060562134\n",
      "[step: 4642] loss: 3.7848684787750244\n",
      "[step: 4643] loss: 3.783107280731201\n",
      "[step: 4644] loss: 3.781186819076538\n",
      "[step: 4645] loss: 3.7794346809387207\n",
      "[step: 4646] loss: 3.777939558029175\n",
      "[step: 4647] loss: 3.7770915031433105\n",
      "[step: 4648] loss: 3.7770590782165527\n",
      "[step: 4649] loss: 3.778423309326172\n",
      "[step: 4650] loss: 3.7818551063537598\n",
      "[step: 4651] loss: 3.788869857788086\n",
      "[step: 4652] loss: 3.8018686771392822\n",
      "[step: 4653] loss: 3.825714111328125\n",
      "[step: 4654] loss: 3.8686254024505615\n",
      "[step: 4655] loss: 3.9470465183258057\n",
      "[step: 4656] loss: 4.086904525756836\n",
      "[step: 4657] loss: 4.340950012207031\n",
      "[step: 4658] loss: 4.767322540283203\n",
      "[step: 4659] loss: 5.470197677612305\n",
      "[step: 4660] loss: 6.3638105392456055\n",
      "[step: 4661] loss: 7.214374542236328\n",
      "[step: 4662] loss: 7.186368942260742\n",
      "[step: 4663] loss: 5.879188060760498\n",
      "[step: 4664] loss: 4.323843955993652\n",
      "[step: 4665] loss: 3.9204421043395996\n",
      "[step: 4666] loss: 4.825271129608154\n",
      "[step: 4667] loss: 5.521777153015137\n",
      "[step: 4668] loss: 5.058045387268066\n",
      "[step: 4669] loss: 4.051560401916504\n",
      "[step: 4670] loss: 3.9923477172851562\n",
      "[step: 4671] loss: 4.697805404663086\n",
      "[step: 4672] loss: 4.706109046936035\n",
      "[step: 4673] loss: 4.075475692749023\n",
      "[step: 4674] loss: 3.812898635864258\n",
      "[step: 4675] loss: 4.200204849243164\n",
      "[step: 4676] loss: 4.42462158203125\n",
      "[step: 4677] loss: 4.040768623352051\n",
      "[step: 4678] loss: 3.788181781768799\n",
      "[step: 4679] loss: 4.000373363494873\n",
      "[step: 4680] loss: 4.190041542053223\n",
      "[step: 4681] loss: 4.027020454406738\n",
      "[step: 4682] loss: 3.808835506439209\n",
      "[step: 4683] loss: 3.876924753189087\n",
      "[step: 4684] loss: 4.018211364746094\n",
      "[step: 4685] loss: 3.9508590698242188\n",
      "[step: 4686] loss: 3.799802303314209\n",
      "[step: 4687] loss: 3.8130416870117188\n",
      "[step: 4688] loss: 3.9282567501068115\n",
      "[step: 4689] loss: 3.915769577026367\n",
      "[step: 4690] loss: 3.8003387451171875\n",
      "[step: 4691] loss: 3.7523622512817383\n",
      "[step: 4692] loss: 3.818521499633789\n",
      "[step: 4693] loss: 3.8682985305786133\n",
      "[step: 4694] loss: 3.8192386627197266\n",
      "[step: 4695] loss: 3.7551321983337402\n",
      "[step: 4696] loss: 3.754906177520752\n",
      "[step: 4697] loss: 3.7963483333587646\n",
      "[step: 4698] loss: 3.8045907020568848\n",
      "[step: 4699] loss: 3.766414165496826\n",
      "[step: 4700] loss: 3.7363052368164062\n",
      "[step: 4701] loss: 3.745776414871216\n",
      "[step: 4702] loss: 3.770437240600586\n",
      "[step: 4703] loss: 3.7683987617492676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4704] loss: 3.7414426803588867\n",
      "[step: 4705] loss: 3.721864700317383\n",
      "[step: 4706] loss: 3.7259902954101562\n",
      "[step: 4707] loss: 3.7415666580200195\n",
      "[step: 4708] loss: 3.745509147644043\n",
      "[step: 4709] loss: 3.7324483394622803\n",
      "[step: 4710] loss: 3.716397762298584\n",
      "[step: 4711] loss: 3.7107198238372803\n",
      "[step: 4712] loss: 3.715763807296753\n",
      "[step: 4713] loss: 3.7216598987579346\n",
      "[step: 4714] loss: 3.7199769020080566\n",
      "[step: 4715] loss: 3.711991786956787\n",
      "[step: 4716] loss: 3.703798532485962\n",
      "[step: 4717] loss: 3.700672149658203\n",
      "[step: 4718] loss: 3.7024037837982178\n",
      "[step: 4719] loss: 3.7043538093566895\n",
      "[step: 4720] loss: 3.703413963317871\n",
      "[step: 4721] loss: 3.6988954544067383\n",
      "[step: 4722] loss: 3.6931581497192383\n",
      "[step: 4723] loss: 3.688866138458252\n",
      "[step: 4724] loss: 3.6872496604919434\n",
      "[step: 4725] loss: 3.6877079010009766\n",
      "[step: 4726] loss: 3.6882827281951904\n",
      "[step: 4727] loss: 3.687711000442505\n",
      "[step: 4728] loss: 3.685472249984741\n",
      "[step: 4729] loss: 3.68204665184021\n",
      "[step: 4730] loss: 3.678356885910034\n",
      "[step: 4731] loss: 3.675262928009033\n",
      "[step: 4732] loss: 3.6731669902801514\n",
      "[step: 4733] loss: 3.6719353199005127\n",
      "[step: 4734] loss: 3.6711950302124023\n",
      "[step: 4735] loss: 3.670550584793091\n",
      "[step: 4736] loss: 3.669630527496338\n",
      "[step: 4737] loss: 3.668318271636963\n",
      "[step: 4738] loss: 3.6666455268859863\n",
      "[step: 4739] loss: 3.6646430492401123\n",
      "[step: 4740] loss: 3.6625304222106934\n",
      "[step: 4741] loss: 3.660353660583496\n",
      "[step: 4742] loss: 3.65824031829834\n",
      "[step: 4743] loss: 3.6562206745147705\n",
      "[step: 4744] loss: 3.654317855834961\n",
      "[step: 4745] loss: 3.6525180339813232\n",
      "[step: 4746] loss: 3.650792121887207\n",
      "[step: 4747] loss: 3.649152994155884\n",
      "[step: 4748] loss: 3.647555351257324\n",
      "[step: 4749] loss: 3.645996332168579\n",
      "[step: 4750] loss: 3.6444735527038574\n",
      "[step: 4751] loss: 3.6429929733276367\n",
      "[step: 4752] loss: 3.6415648460388184\n",
      "[step: 4753] loss: 3.6402063369750977\n",
      "[step: 4754] loss: 3.6389756202697754\n",
      "[step: 4755] loss: 3.6379621028900146\n",
      "[step: 4756] loss: 3.637320041656494\n",
      "[step: 4757] loss: 3.637346029281616\n",
      "[step: 4758] loss: 3.638608455657959\n",
      "[step: 4759] loss: 3.642141103744507\n",
      "[step: 4760] loss: 3.650041103363037\n",
      "[step: 4761] loss: 3.6662228107452393\n",
      "[step: 4762] loss: 3.6988754272460938\n",
      "[step: 4763] loss: 3.7633297443389893\n",
      "[step: 4764] loss: 3.89241623878479\n",
      "[step: 4765] loss: 4.1431989669799805\n",
      "[step: 4766] loss: 4.630455017089844\n",
      "[step: 4767] loss: 5.489135265350342\n",
      "[step: 4768] loss: 6.8331828117370605\n",
      "[step: 4769] loss: 8.266738891601562\n",
      "[step: 4770] loss: 8.456473350524902\n",
      "[step: 4771] loss: 6.659067153930664\n",
      "[step: 4772] loss: 4.174612045288086\n",
      "[step: 4773] loss: 4.141762733459473\n",
      "[step: 4774] loss: 5.807250499725342\n",
      "[step: 4775] loss: 6.179876327514648\n",
      "[step: 4776] loss: 4.491710662841797\n",
      "[step: 4777] loss: 3.8143036365509033\n",
      "[step: 4778] loss: 4.879350662231445\n",
      "[step: 4779] loss: 4.977317810058594\n",
      "[step: 4780] loss: 3.9028594493865967\n",
      "[step: 4781] loss: 3.921886920928955\n",
      "[step: 4782] loss: 4.651663780212402\n",
      "[step: 4783] loss: 4.3558173179626465\n",
      "[step: 4784] loss: 3.7102303504943848\n",
      "[step: 4785] loss: 4.056547164916992\n",
      "[step: 4786] loss: 4.346104621887207\n",
      "[step: 4787] loss: 3.886643886566162\n",
      "[step: 4788] loss: 3.7152326107025146\n",
      "[step: 4789] loss: 4.080025672912598\n",
      "[step: 4790] loss: 4.041882514953613\n",
      "[step: 4791] loss: 3.6885170936584473\n",
      "[step: 4792] loss: 3.8011374473571777\n",
      "[step: 4793] loss: 4.000241279602051\n",
      "[step: 4794] loss: 3.792877674102783\n",
      "[step: 4795] loss: 3.657538414001465\n",
      "[step: 4796] loss: 3.8401436805725098\n",
      "[step: 4797] loss: 3.8493449687957764\n",
      "[step: 4798] loss: 3.677622079849243\n",
      "[step: 4799] loss: 3.6759843826293945\n",
      "[step: 4800] loss: 3.7992420196533203\n",
      "[step: 4801] loss: 3.7461910247802734\n",
      "[step: 4802] loss: 3.634082794189453\n",
      "[step: 4803] loss: 3.6800038814544678\n",
      "[step: 4804] loss: 3.744266986846924\n",
      "[step: 4805] loss: 3.6812243461608887\n",
      "[step: 4806] loss: 3.6192121505737305\n",
      "[step: 4807] loss: 3.667360544204712\n",
      "[step: 4808] loss: 3.694326639175415\n",
      "[step: 4809] loss: 3.6452319622039795\n",
      "[step: 4810] loss: 3.6101813316345215\n",
      "[step: 4811] loss: 3.6448206901550293\n",
      "[step: 4812] loss: 3.6617937088012695\n",
      "[step: 4813] loss: 3.6254825592041016\n",
      "[step: 4814] loss: 3.602485418319702\n",
      "[step: 4815] loss: 3.6218528747558594\n",
      "[step: 4816] loss: 3.635812282562256\n",
      "[step: 4817] loss: 3.61287784576416\n",
      "[step: 4818] loss: 3.593583345413208\n",
      "[step: 4819] loss: 3.6013612747192383\n",
      "[step: 4820] loss: 3.614737033843994\n",
      "[step: 4821] loss: 3.6041483879089355\n",
      "[step: 4822] loss: 3.5872273445129395\n",
      "[step: 4823] loss: 3.5856387615203857\n",
      "[step: 4824] loss: 3.5947680473327637\n",
      "[step: 4825] loss: 3.595083475112915\n",
      "[step: 4826] loss: 3.5834808349609375\n",
      "[step: 4827] loss: 3.5755157470703125\n",
      "[step: 4828] loss: 3.5771546363830566\n",
      "[step: 4829] loss: 3.5817582607269287\n",
      "[step: 4830] loss: 3.5786542892456055\n",
      "[step: 4831] loss: 3.5707244873046875\n",
      "[step: 4832] loss: 3.565786123275757\n",
      "[step: 4833] loss: 3.566951036453247\n",
      "[step: 4834] loss: 3.5688705444335938\n",
      "[step: 4835] loss: 3.566356897354126\n",
      "[step: 4836] loss: 3.5610549449920654\n",
      "[step: 4837] loss: 3.556971549987793\n",
      "[step: 4838] loss: 3.556483507156372\n",
      "[step: 4839] loss: 3.557025909423828\n",
      "[step: 4840] loss: 3.556065082550049\n",
      "[step: 4841] loss: 3.552703857421875\n",
      "[step: 4842] loss: 3.549025058746338\n",
      "[step: 4843] loss: 3.546886444091797\n",
      "[step: 4844] loss: 3.546232223510742\n",
      "[step: 4845] loss: 3.5458526611328125\n",
      "[step: 4846] loss: 3.5443263053894043\n",
      "[step: 4847] loss: 3.541872024536133\n",
      "[step: 4848] loss: 3.539215564727783\n",
      "[step: 4849] loss: 3.5373458862304688\n",
      "[step: 4850] loss: 3.5363402366638184\n",
      "[step: 4851] loss: 3.535705804824829\n",
      "[step: 4852] loss: 3.5349693298339844\n",
      "[step: 4853] loss: 3.5338356494903564\n",
      "[step: 4854] loss: 3.5327985286712646\n",
      "[step: 4855] loss: 3.5323076248168945\n",
      "[step: 4856] loss: 3.533210277557373\n",
      "[step: 4857] loss: 3.535754919052124\n",
      "[step: 4858] loss: 3.54121732711792\n",
      "[step: 4859] loss: 3.5499517917633057\n",
      "[step: 4860] loss: 3.5658743381500244\n",
      "[step: 4861] loss: 3.5890884399414062\n",
      "[step: 4862] loss: 3.6317548751831055\n",
      "[step: 4863] loss: 3.6864066123962402\n",
      "[step: 4864] loss: 3.783151149749756\n",
      "[step: 4865] loss: 3.870149612426758\n",
      "[step: 4866] loss: 4.004728317260742\n",
      "[step: 4867] loss: 4.00980281829834\n",
      "[step: 4868] loss: 3.992075204849243\n",
      "[step: 4869] loss: 3.8009543418884277\n",
      "[step: 4870] loss: 3.6396217346191406\n",
      "[step: 4871] loss: 3.554748058319092\n",
      "[step: 4872] loss: 3.5829412937164307\n",
      "[step: 4873] loss: 3.6618869304656982\n",
      "[step: 4874] loss: 3.6850485801696777\n",
      "[step: 4875] loss: 3.642958641052246\n",
      "[step: 4876] loss: 3.5508627891540527\n",
      "[step: 4877] loss: 3.5031838417053223\n",
      "[step: 4878] loss: 3.5282227993011475\n",
      "[step: 4879] loss: 3.585658550262451\n",
      "[step: 4880] loss: 3.620009422302246\n",
      "[step: 4881] loss: 3.5936379432678223\n",
      "[step: 4882] loss: 3.5511441230773926\n",
      "[step: 4883] loss: 3.532047986984253\n",
      "[step: 4884] loss: 3.5525529384613037\n",
      "[step: 4885] loss: 3.5877270698547363\n",
      "[step: 4886] loss: 3.5949935913085938\n",
      "[step: 4887] loss: 3.5842537879943848\n",
      "[step: 4888] loss: 3.5670881271362305\n",
      "[step: 4889] loss: 3.5771093368530273\n",
      "[step: 4890] loss: 3.615443706512451\n",
      "[step: 4891] loss: 3.6599555015563965\n",
      "[step: 4892] loss: 3.7101755142211914\n",
      "[step: 4893] loss: 3.7577192783355713\n",
      "[step: 4894] loss: 3.8423523902893066\n",
      "[step: 4895] loss: 3.9685285091400146\n",
      "[step: 4896] loss: 4.127770900726318\n",
      "[step: 4897] loss: 4.3240861892700195\n",
      "[step: 4898] loss: 4.463410377502441\n",
      "[step: 4899] loss: 4.563143730163574\n",
      "[step: 4900] loss: 4.527828693389893\n",
      "[step: 4901] loss: 4.335500717163086\n",
      "[step: 4902] loss: 4.047276973724365\n",
      "[step: 4903] loss: 3.7197136878967285\n",
      "[step: 4904] loss: 3.5117130279541016\n",
      "[step: 4905] loss: 3.4793386459350586\n",
      "[step: 4906] loss: 3.591013193130493\n",
      "[step: 4907] loss: 3.7499125003814697\n",
      "[step: 4908] loss: 3.8412673473358154\n",
      "[step: 4909] loss: 3.82930326461792\n",
      "[step: 4910] loss: 3.719144821166992\n",
      "[step: 4911] loss: 3.5837061405181885\n",
      "[step: 4912] loss: 3.492835521697998\n",
      "[step: 4913] loss: 3.4711384773254395\n",
      "[step: 4914] loss: 3.507004499435425\n",
      "[step: 4915] loss: 3.565568208694458\n",
      "[step: 4916] loss: 3.6148180961608887\n",
      "[step: 4917] loss: 3.6325254440307617\n",
      "[step: 4918] loss: 3.604434013366699\n",
      "[step: 4919] loss: 3.5534868240356445\n",
      "[step: 4920] loss: 3.4943675994873047\n",
      "[step: 4921] loss: 3.455928325653076\n",
      "[step: 4922] loss: 3.446308135986328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4923] loss: 3.462095022201538\n",
      "[step: 4924] loss: 3.4904847145080566\n",
      "[step: 4925] loss: 3.5142030715942383\n",
      "[step: 4926] loss: 3.526292324066162\n",
      "[step: 4927] loss: 3.520589828491211\n",
      "[step: 4928] loss: 3.506511688232422\n",
      "[step: 4929] loss: 3.487525224685669\n",
      "[step: 4930] loss: 3.4687752723693848\n",
      "[step: 4931] loss: 3.452486515045166\n",
      "[step: 4932] loss: 3.439462184906006\n",
      "[step: 4933] loss: 3.431008815765381\n",
      "[step: 4934] loss: 3.4275145530700684\n",
      "[step: 4935] loss: 3.428633213043213\n",
      "[step: 4936] loss: 3.433084487915039\n",
      "[step: 4937] loss: 3.439220428466797\n",
      "[step: 4938] loss: 3.446082353591919\n",
      "[step: 4939] loss: 3.4532718658447266\n",
      "[step: 4940] loss: 3.4619083404541016\n",
      "[step: 4941] loss: 3.4732744693756104\n",
      "[step: 4942] loss: 3.488642692565918\n",
      "[step: 4943] loss: 3.511094093322754\n",
      "[step: 4944] loss: 3.5412774085998535\n",
      "[step: 4945] loss: 3.5874905586242676\n",
      "[step: 4946] loss: 3.6510839462280273\n",
      "[step: 4947] loss: 3.7488293647766113\n",
      "[step: 4948] loss: 3.8821425437927246\n",
      "[step: 4949] loss: 4.072463035583496\n",
      "[step: 4950] loss: 4.306517601013184\n",
      "[step: 4951] loss: 4.5758819580078125\n",
      "[step: 4952] loss: 4.796899318695068\n",
      "[step: 4953] loss: 4.871477127075195\n",
      "[step: 4954] loss: 4.682369709014893\n",
      "[step: 4955] loss: 4.250129699707031\n",
      "[step: 4956] loss: 3.7570409774780273\n",
      "[step: 4957] loss: 3.452791690826416\n",
      "[step: 4958] loss: 3.4499945640563965\n",
      "[step: 4959] loss: 3.664956569671631\n",
      "[step: 4960] loss: 3.900085926055908\n",
      "[step: 4961] loss: 3.9756596088409424\n",
      "[step: 4962] loss: 3.8367435932159424\n",
      "[step: 4963] loss: 3.5897977352142334\n",
      "[step: 4964] loss: 3.42269229888916\n",
      "[step: 4965] loss: 3.4270176887512207\n",
      "[step: 4966] loss: 3.5523641109466553\n",
      "[step: 4967] loss: 3.6796154975891113\n",
      "[step: 4968] loss: 3.712118625640869\n",
      "[step: 4969] loss: 3.6335010528564453\n",
      "[step: 4970] loss: 3.499033212661743\n",
      "[step: 4971] loss: 3.4087953567504883\n",
      "[step: 4972] loss: 3.4064974784851074\n",
      "[step: 4973] loss: 3.4669125080108643\n",
      "[step: 4974] loss: 3.5315356254577637\n",
      "[step: 4975] loss: 3.5546929836273193\n",
      "[step: 4976] loss: 3.526087999343872\n",
      "[step: 4977] loss: 3.463576078414917\n",
      "[step: 4978] loss: 3.4112682342529297\n",
      "[step: 4979] loss: 3.3898134231567383\n",
      "[step: 4980] loss: 3.4001359939575195\n",
      "[step: 4981] loss: 3.4270577430725098\n",
      "[step: 4982] loss: 3.45232892036438\n",
      "[step: 4983] loss: 3.462293863296509\n",
      "[step: 4984] loss: 3.451411485671997\n",
      "[step: 4985] loss: 3.4295310974121094\n",
      "[step: 4986] loss: 3.4020793437957764\n",
      "[step: 4987] loss: 3.3805971145629883\n",
      "[step: 4988] loss: 3.3694281578063965\n",
      "[step: 4989] loss: 3.369311571121216\n",
      "[step: 4990] loss: 3.376936674118042\n",
      "[step: 4991] loss: 3.3881187438964844\n",
      "[step: 4992] loss: 3.3991260528564453\n",
      "[step: 4993] loss: 3.405285358428955\n",
      "[step: 4994] loss: 3.4074912071228027\n",
      "[step: 4995] loss: 3.4041686058044434\n",
      "[step: 4996] loss: 3.3989620208740234\n",
      "[step: 4997] loss: 3.39201021194458\n",
      "[step: 4998] loss: 3.3861470222473145\n",
      "[step: 4999] loss: 3.381296157836914\n",
      "[step: 5000] loss: 3.3774399757385254\n",
      "[step: 5001] loss: 3.374610662460327\n",
      "[step: 5002] loss: 3.3723549842834473\n",
      "[step: 5003] loss: 3.3711347579956055\n",
      "[step: 5004] loss: 3.371302604675293\n",
      "[step: 5005] loss: 3.374114513397217\n",
      "[step: 5006] loss: 3.380537509918213\n",
      "[step: 5007] loss: 3.3923892974853516\n",
      "[step: 5008] loss: 3.4130043983459473\n",
      "[step: 5009] loss: 3.4461770057678223\n",
      "[step: 5010] loss: 3.5018978118896484\n",
      "[step: 5011] loss: 3.5910086631774902\n",
      "[step: 5012] loss: 3.741544723510742\n",
      "[step: 5013] loss: 3.9764480590820312\n",
      "[step: 5014] loss: 4.344542503356934\n",
      "[step: 5015] loss: 4.837787628173828\n",
      "[step: 5016] loss: 5.401045799255371\n",
      "[step: 5017] loss: 5.779651641845703\n",
      "[step: 5018] loss: 5.599179267883301\n",
      "[step: 5019] loss: 4.7986249923706055\n",
      "[step: 5020] loss: 3.794304847717285\n",
      "[step: 5021] loss: 3.36086368560791\n",
      "[step: 5022] loss: 3.679137706756592\n",
      "[step: 5023] loss: 4.253177642822266\n",
      "[step: 5024] loss: 4.442392349243164\n",
      "[step: 5025] loss: 4.019330024719238\n",
      "[step: 5026] loss: 3.475313186645508\n",
      "[step: 5027] loss: 3.3948051929473877\n",
      "[step: 5028] loss: 3.743647575378418\n",
      "[step: 5029] loss: 4.007317543029785\n",
      "[step: 5030] loss: 3.8557307720184326\n",
      "[step: 5031] loss: 3.5000383853912354\n",
      "[step: 5032] loss: 3.3540549278259277\n",
      "[step: 5033] loss: 3.5081980228424072\n",
      "[step: 5034] loss: 3.690101146697998\n",
      "[step: 5035] loss: 3.6392834186553955\n",
      "[step: 5036] loss: 3.4454667568206787\n",
      "[step: 5037] loss: 3.3565239906311035\n",
      "[step: 5038] loss: 3.4387166500091553\n",
      "[step: 5039] loss: 3.536830425262451\n",
      "[step: 5040] loss: 3.4962706565856934\n",
      "[step: 5041] loss: 3.385012149810791\n",
      "[step: 5042] loss: 3.3343594074249268\n",
      "[step: 5043] loss: 3.385876178741455\n",
      "[step: 5044] loss: 3.455975294113159\n",
      "[step: 5045] loss: 3.4413790702819824\n",
      "[step: 5046] loss: 3.3687124252319336\n",
      "[step: 5047] loss: 3.3151416778564453\n",
      "[step: 5048] loss: 3.3289310932159424\n",
      "[step: 5049] loss: 3.376025438308716\n",
      "[step: 5050] loss: 3.3920412063598633\n",
      "[step: 5051] loss: 3.3638477325439453\n",
      "[step: 5052] loss: 3.319742202758789\n",
      "[step: 5053] loss: 3.3026223182678223\n",
      "[step: 5054] loss: 3.3195605278015137\n",
      "[step: 5055] loss: 3.342749834060669\n",
      "[step: 5056] loss: 3.3452911376953125\n",
      "[step: 5057] loss: 3.3247764110565186\n",
      "[step: 5058] loss: 3.301109790802002\n",
      "[step: 5059] loss: 3.291720151901245\n",
      "[step: 5060] loss: 3.2995119094848633\n",
      "[step: 5061] loss: 3.311978340148926\n",
      "[step: 5062] loss: 3.315916061401367\n",
      "[step: 5063] loss: 3.3082258701324463\n",
      "[step: 5064] loss: 3.2946653366088867\n",
      "[step: 5065] loss: 3.284708023071289\n",
      "[step: 5066] loss: 3.2820513248443604\n",
      "[step: 5067] loss: 3.2854747772216797\n",
      "[step: 5068] loss: 3.2897632122039795\n",
      "[step: 5069] loss: 3.2899725437164307\n",
      "[step: 5070] loss: 3.2860426902770996\n",
      "[step: 5071] loss: 3.2796378135681152\n",
      "[step: 5072] loss: 3.273972511291504\n",
      "[step: 5073] loss: 3.2710466384887695\n",
      "[step: 5074] loss: 3.2708747386932373\n",
      "[step: 5075] loss: 3.2720203399658203\n",
      "[step: 5076] loss: 3.2727129459381104\n",
      "[step: 5077] loss: 3.2719736099243164\n",
      "[step: 5078] loss: 3.269228458404541\n",
      "[step: 5079] loss: 3.2654006481170654\n",
      "[step: 5080] loss: 3.261298894882202\n",
      "[step: 5081] loss: 3.2580204010009766\n",
      "[step: 5082] loss: 3.2557852268218994\n",
      "[step: 5083] loss: 3.2547683715820312\n",
      "[step: 5084] loss: 3.254667043685913\n",
      "[step: 5085] loss: 3.254951000213623\n",
      "[step: 5086] loss: 3.2555553913116455\n",
      "[step: 5087] loss: 3.2564597129821777\n",
      "[step: 5088] loss: 3.258234739303589\n",
      "[step: 5089] loss: 3.2617897987365723\n",
      "[step: 5090] loss: 3.269038438796997\n",
      "[step: 5091] loss: 3.283172607421875\n",
      "[step: 5092] loss: 3.309391975402832\n",
      "[step: 5093] loss: 3.3593809604644775\n",
      "[step: 5094] loss: 3.4476633071899414\n",
      "[step: 5095] loss: 3.617739677429199\n",
      "[step: 5096] loss: 3.8966145515441895\n",
      "[step: 5097] loss: 4.4172515869140625\n",
      "[step: 5098] loss: 5.093067646026611\n",
      "[step: 5099] loss: 6.08332633972168\n",
      "[step: 5100] loss: 6.545536041259766\n",
      "[step: 5101] loss: 6.290456771850586\n",
      "[step: 5102] loss: 4.993289947509766\n",
      "[step: 5103] loss: 3.6894893646240234\n",
      "[step: 5104] loss: 3.579552173614502\n",
      "[step: 5105] loss: 4.327118873596191\n",
      "[step: 5106] loss: 4.889073848724365\n",
      "[step: 5107] loss: 4.372032642364502\n",
      "[step: 5108] loss: 3.6166205406188965\n",
      "[step: 5109] loss: 3.626276969909668\n",
      "[step: 5110] loss: 4.033222675323486\n",
      "[step: 5111] loss: 4.089040756225586\n",
      "[step: 5112] loss: 3.621732234954834\n",
      "[step: 5113] loss: 3.4405481815338135\n",
      "[step: 5114] loss: 3.698976516723633\n",
      "[step: 5115] loss: 3.7501912117004395\n",
      "[step: 5116] loss: 3.5280966758728027\n",
      "[step: 5117] loss: 3.364065170288086\n",
      "[step: 5118] loss: 3.492365837097168\n",
      "[step: 5119] loss: 3.6328282356262207\n",
      "[step: 5120] loss: 3.4862194061279297\n",
      "[step: 5121] loss: 3.3350448608398438\n",
      "[step: 5122] loss: 3.370847463607788\n",
      "[step: 5123] loss: 3.46354079246521\n",
      "[step: 5124] loss: 3.4318490028381348\n",
      "[step: 5125] loss: 3.316009521484375\n",
      "[step: 5126] loss: 3.3069992065429688\n",
      "[step: 5127] loss: 3.3717055320739746\n",
      "[step: 5128] loss: 3.35343599319458\n",
      "[step: 5129] loss: 3.282306671142578\n",
      "[step: 5130] loss: 3.2718138694763184\n",
      "[step: 5131] loss: 3.313473701477051\n",
      "[step: 5132] loss: 3.313082218170166\n",
      "[step: 5133] loss: 3.2560081481933594\n",
      "[step: 5134] loss: 3.2391409873962402\n",
      "[step: 5135] loss: 3.277470588684082\n",
      "[step: 5136] loss: 3.285637140274048\n",
      "[step: 5137] loss: 3.2458362579345703\n",
      "[step: 5138] loss: 3.2150917053222656\n",
      "[step: 5139] loss: 3.2328615188598633\n",
      "[step: 5140] loss: 3.259502649307251\n",
      "[step: 5141] loss: 3.2436609268188477\n",
      "[step: 5142] loss: 3.212174415588379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5143] loss: 3.2065067291259766\n",
      "[step: 5144] loss: 3.2234742641448975\n",
      "[step: 5145] loss: 3.228945255279541\n",
      "[step: 5146] loss: 3.21181058883667\n",
      "[step: 5147] loss: 3.1999917030334473\n",
      "[step: 5148] loss: 3.2047033309936523\n",
      "[step: 5149] loss: 3.2098846435546875\n",
      "[step: 5150] loss: 3.202885627746582\n",
      "[step: 5151] loss: 3.191518783569336\n",
      "[step: 5152] loss: 3.190631151199341\n",
      "[step: 5153] loss: 3.1968441009521484\n",
      "[step: 5154] loss: 3.1975231170654297\n",
      "[step: 5155] loss: 3.1895713806152344\n",
      "[step: 5156] loss: 3.1817102432250977\n",
      "[step: 5157] loss: 3.1813411712646484\n",
      "[step: 5158] loss: 3.183711290359497\n",
      "[step: 5159] loss: 3.1821837425231934\n",
      "[step: 5160] loss: 3.1771633625030518\n",
      "[step: 5161] loss: 3.1739745140075684\n",
      "[step: 5162] loss: 3.17461895942688\n",
      "[step: 5163] loss: 3.1754307746887207\n",
      "[step: 5164] loss: 3.1732330322265625\n",
      "[step: 5165] loss: 3.168984889984131\n",
      "[step: 5166] loss: 3.1660332679748535\n",
      "[step: 5167] loss: 3.1654067039489746\n",
      "[step: 5168] loss: 3.165010452270508\n",
      "[step: 5169] loss: 3.1629321575164795\n",
      "[step: 5170] loss: 3.1597912311553955\n",
      "[step: 5171] loss: 3.1575450897216797\n",
      "[step: 5172] loss: 3.1566362380981445\n",
      "[step: 5173] loss: 3.1559956073760986\n",
      "[step: 5174] loss: 3.154456615447998\n",
      "[step: 5175] loss: 3.1521620750427246\n",
      "[step: 5176] loss: 3.150179862976074\n",
      "[step: 5177] loss: 3.148982286453247\n",
      "[step: 5178] loss: 3.1481597423553467\n",
      "[step: 5179] loss: 3.14693021774292\n",
      "[step: 5180] loss: 3.1452443599700928\n",
      "[step: 5181] loss: 3.1436383724212646\n",
      "[step: 5182] loss: 3.1426026821136475\n",
      "[step: 5183] loss: 3.142247200012207\n",
      "[step: 5184] loss: 3.1424341201782227\n",
      "[step: 5185] loss: 3.143542766571045\n",
      "[step: 5186] loss: 3.146817207336426\n",
      "[step: 5187] loss: 3.1549127101898193\n",
      "[step: 5188] loss: 3.172776460647583\n",
      "[step: 5189] loss: 3.2098135948181152\n",
      "[step: 5190] loss: 3.2874999046325684\n",
      "[step: 5191] loss: 3.445977210998535\n",
      "[step: 5192] loss: 3.7783102989196777\n",
      "[step: 5193] loss: 4.4246296882629395\n",
      "[step: 5194] loss: 5.639892578125\n",
      "[step: 5195] loss: 7.3341522216796875\n",
      "[step: 5196] loss: 8.917482376098633\n",
      "[step: 5197] loss: 7.917622089385986\n",
      "[step: 5198] loss: 4.928142547607422\n",
      "[step: 5199] loss: 3.304537296295166\n",
      "[step: 5200] loss: 4.8375244140625\n",
      "[step: 5201] loss: 6.4210710525512695\n",
      "[step: 5202] loss: 4.941731929779053\n",
      "[step: 5203] loss: 3.3904824256896973\n",
      "[step: 5204] loss: 4.303149700164795\n",
      "[step: 5205] loss: 4.854992866516113\n",
      "[step: 5206] loss: 3.686234712600708\n",
      "[step: 5207] loss: 3.4167442321777344\n",
      "[step: 5208] loss: 4.272982597351074\n",
      "[step: 5209] loss: 4.119265079498291\n",
      "[step: 5210] loss: 3.276512861251831\n",
      "[step: 5211] loss: 3.724053144454956\n",
      "[step: 5212] loss: 4.061049461364746\n",
      "[step: 5213] loss: 3.43225359916687\n",
      "[step: 5214] loss: 3.410123825073242\n",
      "[step: 5215] loss: 3.8894004821777344\n",
      "[step: 5216] loss: 3.449942111968994\n",
      "[step: 5217] loss: 3.2863669395446777\n",
      "[step: 5218] loss: 3.6246581077575684\n",
      "[step: 5219] loss: 3.4361538887023926\n",
      "[step: 5220] loss: 3.2050697803497314\n",
      "[step: 5221] loss: 3.419834613800049\n",
      "[step: 5222] loss: 3.4164044857025146\n",
      "[step: 5223] loss: 3.184065580368042\n",
      "[step: 5224] loss: 3.27227783203125\n",
      "[step: 5225] loss: 3.349602699279785\n",
      "[step: 5226] loss: 3.2157325744628906\n",
      "[step: 5227] loss: 3.180525779724121\n",
      "[step: 5228] loss: 3.2885260581970215\n",
      "[step: 5229] loss: 3.226654529571533\n",
      "[step: 5230] loss: 3.1572957038879395\n",
      "[step: 5231] loss: 3.221132755279541\n",
      "[step: 5232] loss: 3.2331223487854004\n",
      "[step: 5233] loss: 3.1509411334991455\n",
      "[step: 5234] loss: 3.1689000129699707\n",
      "[step: 5235] loss: 3.2122206687927246\n",
      "[step: 5236] loss: 3.1632423400878906\n",
      "[step: 5237] loss: 3.1374659538269043\n",
      "[step: 5238] loss: 3.1743762493133545\n",
      "[step: 5239] loss: 3.168844223022461\n",
      "[step: 5240] loss: 3.1295909881591797\n",
      "[step: 5241] loss: 3.1414709091186523\n",
      "[step: 5242] loss: 3.156341552734375\n",
      "[step: 5243] loss: 3.134990930557251\n",
      "[step: 5244] loss: 3.1191089153289795\n",
      "[step: 5245] loss: 3.1368792057037354\n",
      "[step: 5246] loss: 3.1351280212402344\n",
      "[step: 5247] loss: 3.1151843070983887\n",
      "[step: 5248] loss: 3.1143815517425537\n",
      "[step: 5249] loss: 3.125603675842285\n",
      "[step: 5250] loss: 3.1175882816314697\n",
      "[step: 5251] loss: 3.103846788406372\n",
      "[step: 5252] loss: 3.1074934005737305\n",
      "[step: 5253] loss: 3.112180233001709\n",
      "[step: 5254] loss: 3.1046700477600098\n",
      "[step: 5255] loss: 3.095893621444702\n",
      "[step: 5256] loss: 3.0993030071258545\n",
      "[step: 5257] loss: 3.10089373588562\n",
      "[step: 5258] loss: 3.0946218967437744\n",
      "[step: 5259] loss: 3.088899612426758\n",
      "[step: 5260] loss: 3.090724468231201\n",
      "[step: 5261] loss: 3.0913097858428955\n",
      "[step: 5262] loss: 3.086231231689453\n",
      "[step: 5263] loss: 3.082092761993408\n",
      "[step: 5264] loss: 3.0823819637298584\n",
      "[step: 5265] loss: 3.082782030105591\n",
      "[step: 5266] loss: 3.078939437866211\n",
      "[step: 5267] loss: 3.0755655765533447\n",
      "[step: 5268] loss: 3.074829339981079\n",
      "[step: 5269] loss: 3.0747690200805664\n",
      "[step: 5270] loss: 3.072206974029541\n",
      "[step: 5271] loss: 3.069157123565674\n",
      "[step: 5272] loss: 3.0679140090942383\n",
      "[step: 5273] loss: 3.067422389984131\n",
      "[step: 5274] loss: 3.0656814575195312\n",
      "[step: 5275] loss: 3.0630016326904297\n",
      "[step: 5276] loss: 3.0613441467285156\n",
      "[step: 5277] loss: 3.0604915618896484\n",
      "[step: 5278] loss: 3.059314250946045\n",
      "[step: 5279] loss: 3.05716872215271\n",
      "[step: 5280] loss: 3.055209159851074\n",
      "[step: 5281] loss: 3.053956985473633\n",
      "[step: 5282] loss: 3.052879810333252\n",
      "[step: 5283] loss: 3.0512993335723877\n",
      "[step: 5284] loss: 3.0493712425231934\n",
      "[step: 5285] loss: 3.0478391647338867\n",
      "[step: 5286] loss: 3.0466437339782715\n",
      "[step: 5287] loss: 3.0453720092773438\n",
      "[step: 5288] loss: 3.043712615966797\n",
      "[step: 5289] loss: 3.0420594215393066\n",
      "[step: 5290] loss: 3.0406484603881836\n",
      "[step: 5291] loss: 3.039388656616211\n",
      "[step: 5292] loss: 3.038007974624634\n",
      "[step: 5293] loss: 3.0364418029785156\n",
      "[step: 5294] loss: 3.034931182861328\n",
      "[step: 5295] loss: 3.0335514545440674\n",
      "[step: 5296] loss: 3.0322625637054443\n",
      "[step: 5297] loss: 3.030871868133545\n",
      "[step: 5298] loss: 3.0293827056884766\n",
      "[step: 5299] loss: 3.02793025970459\n",
      "[step: 5300] loss: 3.026561737060547\n",
      "[step: 5301] loss: 3.0252485275268555\n",
      "[step: 5302] loss: 3.023871898651123\n",
      "[step: 5303] loss: 3.0224485397338867\n",
      "[step: 5304] loss: 3.021024703979492\n",
      "[step: 5305] loss: 3.0196616649627686\n",
      "[step: 5306] loss: 3.0183348655700684\n",
      "[step: 5307] loss: 3.016979455947876\n",
      "[step: 5308] loss: 3.0155975818634033\n",
      "[step: 5309] loss: 3.014204740524292\n",
      "[step: 5310] loss: 3.0128417015075684\n",
      "[step: 5311] loss: 3.0115065574645996\n",
      "[step: 5312] loss: 3.010176658630371\n",
      "[step: 5313] loss: 3.0088281631469727\n",
      "[step: 5314] loss: 3.007464647293091\n",
      "[step: 5315] loss: 3.006122589111328\n",
      "[step: 5316] loss: 3.004805088043213\n",
      "[step: 5317] loss: 3.003511905670166\n",
      "[step: 5318] loss: 3.002232074737549\n",
      "[step: 5319] loss: 3.0010006427764893\n",
      "[step: 5320] loss: 2.999847888946533\n",
      "[step: 5321] loss: 2.9988770484924316\n",
      "[step: 5322] loss: 2.998246669769287\n",
      "[step: 5323] loss: 2.9982635974884033\n",
      "[step: 5324] loss: 2.9995951652526855\n",
      "[step: 5325] loss: 3.0035500526428223\n",
      "[step: 5326] loss: 3.013061285018921\n",
      "[step: 5327] loss: 3.033870220184326\n",
      "[step: 5328] loss: 3.0795891284942627\n",
      "[step: 5329] loss: 3.1754989624023438\n",
      "[step: 5330] loss: 3.38594651222229\n",
      "[step: 5331] loss: 3.80670166015625\n",
      "[step: 5332] loss: 4.69196891784668\n",
      "[step: 5333] loss: 6.092813014984131\n",
      "[step: 5334] loss: 8.149340629577637\n",
      "[step: 5335] loss: 8.715598106384277\n",
      "[step: 5336] loss: 6.807909965515137\n",
      "[step: 5337] loss: 3.7560782432556152\n",
      "[step: 5338] loss: 3.548621654510498\n",
      "[step: 5339] loss: 5.74354887008667\n",
      "[step: 5340] loss: 6.109333038330078\n",
      "[step: 5341] loss: 3.9749064445495605\n",
      "[step: 5342] loss: 3.3091163635253906\n",
      "[step: 5343] loss: 4.64876651763916\n",
      "[step: 5344] loss: 4.547499179840088\n",
      "[step: 5345] loss: 3.1894617080688477\n",
      "[step: 5346] loss: 3.7329628467559814\n",
      "[step: 5347] loss: 4.54669189453125\n",
      "[step: 5348] loss: 3.674020767211914\n",
      "[step: 5349] loss: 3.1513924598693848\n",
      "[step: 5350] loss: 3.989602565765381\n",
      "[step: 5351] loss: 3.8268916606903076\n",
      "[step: 5352] loss: 3.138185977935791\n",
      "[step: 5353] loss: 3.438129186630249\n",
      "[step: 5354] loss: 3.7734436988830566\n",
      "[step: 5355] loss: 3.311546802520752\n",
      "[step: 5356] loss: 3.1574530601501465\n",
      "[step: 5357] loss: 3.560295581817627\n",
      "[step: 5358] loss: 3.35718035697937\n",
      "[step: 5359] loss: 3.134204387664795\n",
      "[step: 5360] loss: 3.3033971786499023\n",
      "[step: 5361] loss: 3.355738401412964\n",
      "[step: 5362] loss: 3.104989528656006\n",
      "[step: 5363] loss: 3.1688661575317383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5364] loss: 3.247365951538086\n",
      "[step: 5365] loss: 3.1216938495635986\n",
      "[step: 5366] loss: 3.0659494400024414\n",
      "[step: 5367] loss: 3.1674580574035645\n",
      "[step: 5368] loss: 3.1334261894226074\n",
      "[step: 5369] loss: 3.0318875312805176\n",
      "[step: 5370] loss: 3.08792781829834\n",
      "[step: 5371] loss: 3.123372793197632\n",
      "[step: 5372] loss: 3.0441105365753174\n",
      "[step: 5373] loss: 3.015049934387207\n",
      "[step: 5374] loss: 3.0824222564697266\n",
      "[step: 5375] loss: 3.0571646690368652\n",
      "[step: 5376] loss: 3.0030155181884766\n",
      "[step: 5377] loss: 3.0160512924194336\n",
      "[step: 5378] loss: 3.050844669342041\n",
      "[step: 5379] loss: 3.016045570373535\n",
      "[step: 5380] loss: 2.987435817718506\n",
      "[step: 5381] loss: 3.0090408325195312\n",
      "[step: 5382] loss: 3.0216264724731445\n",
      "[step: 5383] loss: 2.99198317527771\n",
      "[step: 5384] loss: 2.978372097015381\n",
      "[step: 5385] loss: 2.9967145919799805\n",
      "[step: 5386] loss: 2.997549057006836\n",
      "[step: 5387] loss: 2.9783384799957275\n",
      "[step: 5388] loss: 2.9699552059173584\n",
      "[step: 5389] loss: 2.9821524620056152\n",
      "[step: 5390] loss: 2.981977701187134\n",
      "[step: 5391] loss: 2.9683940410614014\n",
      "[step: 5392] loss: 2.961653470993042\n",
      "[step: 5393] loss: 2.968381643295288\n",
      "[step: 5394] loss: 2.9694294929504395\n",
      "[step: 5395] loss: 2.9602911472320557\n",
      "[step: 5396] loss: 2.9539875984191895\n",
      "[step: 5397] loss: 2.956416606903076\n",
      "[step: 5398] loss: 2.9588069915771484\n",
      "[step: 5399] loss: 2.9532999992370605\n",
      "[step: 5400] loss: 2.9474761486053467\n",
      "[step: 5401] loss: 2.946671485900879\n",
      "[step: 5402] loss: 2.9485244750976562\n",
      "[step: 5403] loss: 2.946394920349121\n",
      "[step: 5404] loss: 2.9418039321899414\n",
      "[step: 5405] loss: 2.9390993118286133\n",
      "[step: 5406] loss: 2.939133882522583\n",
      "[step: 5407] loss: 2.9387757778167725\n",
      "[step: 5408] loss: 2.936063289642334\n",
      "[step: 5409] loss: 2.933178424835205\n",
      "[step: 5410] loss: 2.9315543174743652\n",
      "[step: 5411] loss: 2.9309892654418945\n",
      "[step: 5412] loss: 2.929438591003418\n",
      "[step: 5413] loss: 2.927281141281128\n",
      "[step: 5414] loss: 2.925422430038452\n",
      "[step: 5415] loss: 2.9242103099823\n",
      "[step: 5416] loss: 2.922933578491211\n",
      "[step: 5417] loss: 2.9210991859436035\n",
      "[step: 5418] loss: 2.919290065765381\n",
      "[step: 5419] loss: 2.9178645610809326\n",
      "[step: 5420] loss: 2.916804313659668\n",
      "[step: 5421] loss: 2.915395736694336\n",
      "[step: 5422] loss: 2.913602828979492\n",
      "[step: 5423] loss: 2.9118025302886963\n",
      "[step: 5424] loss: 2.91044282913208\n",
      "[step: 5425] loss: 2.9093596935272217\n",
      "[step: 5426] loss: 2.908083200454712\n",
      "[step: 5427] loss: 2.9065208435058594\n",
      "[step: 5428] loss: 2.904860496520996\n",
      "[step: 5429] loss: 2.9034171104431152\n",
      "[step: 5430] loss: 2.9021353721618652\n",
      "[step: 5431] loss: 2.900860071182251\n",
      "[step: 5432] loss: 2.8994414806365967\n",
      "[step: 5433] loss: 2.897965431213379\n",
      "[step: 5434] loss: 2.896584987640381\n",
      "[step: 5435] loss: 2.895299196243286\n",
      "[step: 5436] loss: 2.89401912689209\n",
      "[step: 5437] loss: 2.892641544342041\n",
      "[step: 5438] loss: 2.891209125518799\n",
      "[step: 5439] loss: 2.8897953033447266\n",
      "[step: 5440] loss: 2.8884711265563965\n",
      "[step: 5441] loss: 2.8871848583221436\n",
      "[step: 5442] loss: 2.8858742713928223\n",
      "[step: 5443] loss: 2.8845365047454834\n",
      "[step: 5444] loss: 2.8831818103790283\n",
      "[step: 5445] loss: 2.881864309310913\n",
      "[step: 5446] loss: 2.880575656890869\n",
      "[step: 5447] loss: 2.8792924880981445\n",
      "[step: 5448] loss: 2.8779916763305664\n",
      "[step: 5449] loss: 2.876676082611084\n",
      "[step: 5450] loss: 2.8753583431243896\n",
      "[step: 5451] loss: 2.8740620613098145\n",
      "[step: 5452] loss: 2.8727951049804688\n",
      "[step: 5453] loss: 2.871528148651123\n",
      "[step: 5454] loss: 2.8702688217163086\n",
      "[step: 5455] loss: 2.869016170501709\n",
      "[step: 5456] loss: 2.8677964210510254\n",
      "[step: 5457] loss: 2.8666296005249023\n",
      "[step: 5458] loss: 2.8655409812927246\n",
      "[step: 5459] loss: 2.864589214324951\n",
      "[step: 5460] loss: 2.8638551235198975\n",
      "[step: 5461] loss: 2.8635048866271973\n",
      "[step: 5462] loss: 2.8638672828674316\n",
      "[step: 5463] loss: 2.865530014038086\n",
      "[step: 5464] loss: 2.869664192199707\n",
      "[step: 5465] loss: 2.8784306049346924\n",
      "[step: 5466] loss: 2.8962926864624023\n",
      "[step: 5467] loss: 2.9316649436950684\n",
      "[step: 5468] loss: 3.002458095550537\n",
      "[step: 5469] loss: 3.1410112380981445\n",
      "[step: 5470] loss: 3.4168410301208496\n",
      "[step: 5471] loss: 3.9288196563720703\n",
      "[step: 5472] loss: 4.853789806365967\n",
      "[step: 5473] loss: 6.147769927978516\n",
      "[step: 5474] loss: 7.429609298706055\n",
      "[step: 5475] loss: 7.221571922302246\n",
      "[step: 5476] loss: 5.06648063659668\n",
      "[step: 5477] loss: 3.110811710357666\n",
      "[step: 5478] loss: 3.486471176147461\n",
      "[step: 5479] loss: 5.049820899963379\n",
      "[step: 5480] loss: 5.219412803649902\n",
      "[step: 5481] loss: 3.6980485916137695\n",
      "[step: 5482] loss: 3.041935443878174\n",
      "[step: 5483] loss: 3.9695143699645996\n",
      "[step: 5484] loss: 4.227969646453857\n",
      "[step: 5485] loss: 3.2817094326019287\n",
      "[step: 5486] loss: 3.0258970260620117\n",
      "[step: 5487] loss: 3.6516852378845215\n",
      "[step: 5488] loss: 3.730466365814209\n",
      "[step: 5489] loss: 3.0639164447784424\n",
      "[step: 5490] loss: 3.083594560623169\n",
      "[step: 5491] loss: 3.5116915702819824\n",
      "[step: 5492] loss: 3.292832374572754\n",
      "[step: 5493] loss: 2.936734676361084\n",
      "[step: 5494] loss: 3.164916753768921\n",
      "[step: 5495] loss: 3.288224458694458\n",
      "[step: 5496] loss: 3.0094754695892334\n",
      "[step: 5497] loss: 2.941516637802124\n",
      "[step: 5498] loss: 3.161071300506592\n",
      "[step: 5499] loss: 3.141655683517456\n",
      "[step: 5500] loss: 2.899728775024414\n",
      "[step: 5501] loss: 2.9752707481384277\n",
      "[step: 5502] loss: 3.111421585083008\n",
      "[step: 5503] loss: 2.997631549835205\n",
      "[step: 5504] loss: 2.875059127807617\n",
      "[step: 5505] loss: 2.9806385040283203\n",
      "[step: 5506] loss: 3.018537759780884\n",
      "[step: 5507] loss: 2.9098196029663086\n",
      "[step: 5508] loss: 2.878452777862549\n",
      "[step: 5509] loss: 2.9566760063171387\n",
      "[step: 5510] loss: 2.9596633911132812\n",
      "[step: 5511] loss: 2.8671822547912598\n",
      "[step: 5512] loss: 2.877206325531006\n",
      "[step: 5513] loss: 2.9292821884155273\n",
      "[step: 5514] loss: 2.913205146789551\n",
      "[step: 5515] loss: 2.8529393672943115\n",
      "[step: 5516] loss: 2.8660085201263428\n",
      "[step: 5517] loss: 2.8984756469726562\n",
      "[step: 5518] loss: 2.8793468475341797\n",
      "[step: 5519] loss: 2.845362424850464\n",
      "[step: 5520] loss: 2.8526976108551025\n",
      "[step: 5521] loss: 2.8750648498535156\n",
      "[step: 5522] loss: 2.857382297515869\n",
      "[step: 5523] loss: 2.8369140625\n",
      "[step: 5524] loss: 2.8391292095184326\n",
      "[step: 5525] loss: 2.8542070388793945\n",
      "[step: 5526] loss: 2.8457489013671875\n",
      "[step: 5527] loss: 2.828627824783325\n",
      "[step: 5528] loss: 2.826690196990967\n",
      "[step: 5529] loss: 2.835834503173828\n",
      "[step: 5530] loss: 2.836040496826172\n",
      "[step: 5531] loss: 2.8234920501708984\n",
      "[step: 5532] loss: 2.8173937797546387\n",
      "[step: 5533] loss: 2.8205080032348633\n",
      "[step: 5534] loss: 2.824631690979004\n",
      "[step: 5535] loss: 2.8190863132476807\n",
      "[step: 5536] loss: 2.8114428520202637\n",
      "[step: 5537] loss: 2.809978485107422\n",
      "[step: 5538] loss: 2.8125405311584473\n",
      "[step: 5539] loss: 2.81270694732666\n",
      "[step: 5540] loss: 2.807278633117676\n",
      "[step: 5541] loss: 2.8030219078063965\n",
      "[step: 5542] loss: 2.80218505859375\n",
      "[step: 5543] loss: 2.8035356998443604\n",
      "[step: 5544] loss: 2.802335500717163\n",
      "[step: 5545] loss: 2.7986092567443848\n",
      "[step: 5546] loss: 2.795546293258667\n",
      "[step: 5547] loss: 2.7945497035980225\n",
      "[step: 5548] loss: 2.794740915298462\n",
      "[step: 5549] loss: 2.793394088745117\n",
      "[step: 5550] loss: 2.7909021377563477\n",
      "[step: 5551] loss: 2.78837513923645\n",
      "[step: 5552] loss: 2.7871625423431396\n",
      "[step: 5553] loss: 2.786677837371826\n",
      "[step: 5554] loss: 2.7855896949768066\n",
      "[step: 5555] loss: 2.7836971282958984\n",
      "[step: 5556] loss: 2.781456232070923\n",
      "[step: 5557] loss: 2.7799429893493652\n",
      "[step: 5558] loss: 2.7789688110351562\n",
      "[step: 5559] loss: 2.7780840396881104\n",
      "[step: 5560] loss: 2.7767207622528076\n",
      "[step: 5561] loss: 2.774937868118286\n",
      "[step: 5562] loss: 2.773256778717041\n",
      "[step: 5563] loss: 2.7719006538391113\n",
      "[step: 5564] loss: 2.7708804607391357\n",
      "[step: 5565] loss: 2.769747257232666\n",
      "[step: 5566] loss: 2.7683866024017334\n",
      "[step: 5567] loss: 2.7668423652648926\n",
      "[step: 5568] loss: 2.7653326988220215\n",
      "[step: 5569] loss: 2.764038562774658\n",
      "[step: 5570] loss: 2.76285982131958\n",
      "[step: 5571] loss: 2.7616982460021973\n",
      "[step: 5572] loss: 2.7603888511657715\n",
      "[step: 5573] loss: 2.758995532989502\n",
      "[step: 5574] loss: 2.757596492767334\n",
      "[step: 5575] loss: 2.7562756538391113\n",
      "[step: 5576] loss: 2.755061626434326\n",
      "[step: 5577] loss: 2.7538583278656006\n",
      "[step: 5578] loss: 2.7526416778564453\n",
      "[step: 5579] loss: 2.751354932785034\n",
      "[step: 5580] loss: 2.7500524520874023\n",
      "[step: 5581] loss: 2.748777389526367\n",
      "[step: 5582] loss: 2.7475690841674805\n",
      "[step: 5583] loss: 2.746450901031494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5584] loss: 2.7454123497009277\n",
      "[step: 5585] loss: 2.744518756866455\n",
      "[step: 5586] loss: 2.7438673973083496\n",
      "[step: 5587] loss: 2.7437567710876465\n",
      "[step: 5588] loss: 2.7446932792663574\n",
      "[step: 5589] loss: 2.7477877140045166\n",
      "[step: 5590] loss: 2.7551279067993164\n",
      "[step: 5591] loss: 2.7714247703552246\n",
      "[step: 5592] loss: 2.8055472373962402\n",
      "[step: 5593] loss: 2.878962516784668\n",
      "[step: 5594] loss: 3.0287411212921143\n",
      "[step: 5595] loss: 3.352041721343994\n",
      "[step: 5596] loss: 3.9590096473693848\n",
      "[step: 5597] loss: 5.1618123054504395\n",
      "[step: 5598] loss: 6.678656578063965\n",
      "[step: 5599] loss: 8.252018928527832\n",
      "[step: 5600] loss: 7.260385513305664\n",
      "[step: 5601] loss: 4.4339518547058105\n",
      "[step: 5602] loss: 2.897216320037842\n",
      "[step: 5603] loss: 4.216176986694336\n",
      "[step: 5604] loss: 5.889219284057617\n",
      "[step: 5605] loss: 4.791347026824951\n",
      "[step: 5606] loss: 3.0354220867156982\n",
      "[step: 5607] loss: 3.4663681983947754\n",
      "[step: 5608] loss: 4.446897506713867\n",
      "[step: 5609] loss: 3.6540839672088623\n",
      "[step: 5610] loss: 2.848360776901245\n",
      "[step: 5611] loss: 3.5475575923919678\n",
      "[step: 5612] loss: 4.000135898590088\n",
      "[step: 5613] loss: 3.1313343048095703\n",
      "[step: 5614] loss: 2.94637393951416\n",
      "[step: 5615] loss: 3.544616460800171\n",
      "[step: 5616] loss: 3.4256227016448975\n",
      "[step: 5617] loss: 2.8512041568756104\n",
      "[step: 5618] loss: 3.101590871810913\n",
      "[step: 5619] loss: 3.3477559089660645\n",
      "[step: 5620] loss: 3.0229883193969727\n",
      "[step: 5621] loss: 2.874210834503174\n",
      "[step: 5622] loss: 3.1818044185638428\n",
      "[step: 5623] loss: 3.0867199897766113\n",
      "[step: 5624] loss: 2.8181509971618652\n",
      "[step: 5625] loss: 2.960747718811035\n",
      "[step: 5626] loss: 3.0702452659606934\n",
      "[step: 5627] loss: 2.900493621826172\n",
      "[step: 5628] loss: 2.782552719116211\n",
      "[step: 5629] loss: 2.9690513610839844\n",
      "[step: 5630] loss: 2.9674625396728516\n",
      "[step: 5631] loss: 2.8015079498291016\n",
      "[step: 5632] loss: 2.8058156967163086\n",
      "[step: 5633] loss: 2.9263815879821777\n",
      "[step: 5634] loss: 2.871053695678711\n",
      "[step: 5635] loss: 2.7561826705932617\n",
      "[step: 5636] loss: 2.8181488513946533\n",
      "[step: 5637] loss: 2.864041328430176\n",
      "[step: 5638] loss: 2.8071482181549072\n",
      "[step: 5639] loss: 2.742410182952881\n",
      "[step: 5640] loss: 2.802640676498413\n",
      "[step: 5641] loss: 2.8236098289489746\n",
      "[step: 5642] loss: 2.765244483947754\n",
      "[step: 5643] loss: 2.739309072494507\n",
      "[step: 5644] loss: 2.778487205505371\n",
      "[step: 5645] loss: 2.792945623397827\n",
      "[step: 5646] loss: 2.7435953617095947\n",
      "[step: 5647] loss: 2.730957508087158\n",
      "[step: 5648] loss: 2.753089427947998\n",
      "[step: 5649] loss: 2.7654430866241455\n",
      "[step: 5650] loss: 2.7338969707489014\n",
      "[step: 5651] loss: 2.7202811241149902\n",
      "[step: 5652] loss: 2.7340102195739746\n",
      "[step: 5653] loss: 2.7420670986175537\n",
      "[step: 5654] loss: 2.7285866737365723\n",
      "[step: 5655] loss: 2.7115366458892822\n",
      "[step: 5656] loss: 2.7167134284973145\n",
      "[step: 5657] loss: 2.723432779312134\n",
      "[step: 5658] loss: 2.720496654510498\n",
      "[step: 5659] loss: 2.707364559173584\n",
      "[step: 5660] loss: 2.7033486366271973\n",
      "[step: 5661] loss: 2.7081665992736816\n",
      "[step: 5662] loss: 2.710052967071533\n",
      "[step: 5663] loss: 2.7045187950134277\n",
      "[step: 5664] loss: 2.696104049682617\n",
      "[step: 5665] loss: 2.695737838745117\n",
      "[step: 5666] loss: 2.6979053020477295\n",
      "[step: 5667] loss: 2.6982617378234863\n",
      "[step: 5668] loss: 2.693185329437256\n",
      "[step: 5669] loss: 2.688260316848755\n",
      "[step: 5670] loss: 2.6871042251586914\n",
      "[step: 5671] loss: 2.6878976821899414\n",
      "[step: 5672] loss: 2.6880578994750977\n",
      "[step: 5673] loss: 2.684710741043091\n",
      "[step: 5674] loss: 2.6812026500701904\n",
      "[step: 5675] loss: 2.6789798736572266\n",
      "[step: 5676] loss: 2.678840160369873\n",
      "[step: 5677] loss: 2.678708076477051\n",
      "[step: 5678] loss: 2.6770241260528564\n",
      "[step: 5679] loss: 2.674531936645508\n",
      "[step: 5680] loss: 2.6719582080841064\n",
      "[step: 5681] loss: 2.670788526535034\n",
      "[step: 5682] loss: 2.6700901985168457\n",
      "[step: 5683] loss: 2.669283151626587\n",
      "[step: 5684] loss: 2.667712688446045\n",
      "[step: 5685] loss: 2.6656494140625\n",
      "[step: 5686] loss: 2.6638970375061035\n",
      "[step: 5687] loss: 2.6625139713287354\n",
      "[step: 5688] loss: 2.6616413593292236\n",
      "[step: 5689] loss: 2.660569906234741\n",
      "[step: 5690] loss: 2.6592202186584473\n",
      "[step: 5691] loss: 2.6575841903686523\n",
      "[step: 5692] loss: 2.6559531688690186\n",
      "[step: 5693] loss: 2.6546247005462646\n",
      "[step: 5694] loss: 2.653452157974243\n",
      "[step: 5695] loss: 2.6524221897125244\n",
      "[step: 5696] loss: 2.651217460632324\n",
      "[step: 5697] loss: 2.6498525142669678\n",
      "[step: 5698] loss: 2.6483969688415527\n",
      "[step: 5699] loss: 2.6469597816467285\n",
      "[step: 5700] loss: 2.6456823348999023\n",
      "[step: 5701] loss: 2.644481658935547\n",
      "[step: 5702] loss: 2.6433610916137695\n",
      "[step: 5703] loss: 2.6421689987182617\n",
      "[step: 5704] loss: 2.6409053802490234\n",
      "[step: 5705] loss: 2.6396007537841797\n",
      "[step: 5706] loss: 2.638267755508423\n",
      "[step: 5707] loss: 2.6370036602020264\n",
      "[step: 5708] loss: 2.635772466659546\n",
      "[step: 5709] loss: 2.6345973014831543\n",
      "[step: 5710] loss: 2.633427619934082\n",
      "[step: 5711] loss: 2.6322388648986816\n",
      "[step: 5712] loss: 2.631031036376953\n",
      "[step: 5713] loss: 2.6298041343688965\n",
      "[step: 5714] loss: 2.6285879611968994\n",
      "[step: 5715] loss: 2.6274051666259766\n",
      "[step: 5716] loss: 2.626288414001465\n",
      "[step: 5717] loss: 2.625253677368164\n",
      "[step: 5718] loss: 2.6243460178375244\n",
      "[step: 5719] loss: 2.623673439025879\n",
      "[step: 5720] loss: 2.623410224914551\n",
      "[step: 5721] loss: 2.6239705085754395\n",
      "[step: 5722] loss: 2.6261184215545654\n",
      "[step: 5723] loss: 2.6315155029296875\n",
      "[step: 5724] loss: 2.6433637142181396\n",
      "[step: 5725] loss: 2.6688718795776367\n",
      "[step: 5726] loss: 2.721874952316284\n",
      "[step: 5727] loss: 2.8347623348236084\n",
      "[step: 5728] loss: 3.0648298263549805\n",
      "[step: 5729] loss: 3.547684669494629\n",
      "[step: 5730] loss: 4.4308624267578125\n",
      "[step: 5731] loss: 5.981531143188477\n",
      "[step: 5732] loss: 7.563092231750488\n",
      "[step: 5733] loss: 8.00156021118164\n",
      "[step: 5734] loss: 5.610263347625732\n",
      "[step: 5735] loss: 3.0054826736450195\n",
      "[step: 5736] loss: 3.426118850708008\n",
      "[step: 5737] loss: 5.461174964904785\n",
      "[step: 5738] loss: 5.423783302307129\n",
      "[step: 5739] loss: 3.3283441066741943\n",
      "[step: 5740] loss: 3.049530029296875\n",
      "[step: 5741] loss: 4.2755632400512695\n",
      "[step: 5742] loss: 4.030145168304443\n",
      "[step: 5743] loss: 2.7986814975738525\n",
      "[step: 5744] loss: 3.325584888458252\n",
      "[step: 5745] loss: 4.075854301452637\n",
      "[step: 5746] loss: 3.2032313346862793\n",
      "[step: 5747] loss: 2.7988247871398926\n",
      "[step: 5748] loss: 3.496401309967041\n",
      "[step: 5749] loss: 3.348324775695801\n",
      "[step: 5750] loss: 2.708564519882202\n",
      "[step: 5751] loss: 3.1065893173217773\n",
      "[step: 5752] loss: 3.270676612854004\n",
      "[step: 5753] loss: 2.8417887687683105\n",
      "[step: 5754] loss: 2.8063857555389404\n",
      "[step: 5755] loss: 3.1681647300720215\n",
      "[step: 5756] loss: 2.9220545291900635\n",
      "[step: 5757] loss: 2.6923699378967285\n",
      "[step: 5758] loss: 2.985095977783203\n",
      "[step: 5759] loss: 2.9581339359283447\n",
      "[step: 5760] loss: 2.7096033096313477\n",
      "[step: 5761] loss: 2.779418706893921\n",
      "[step: 5762] loss: 2.9406228065490723\n",
      "[step: 5763] loss: 2.721957206726074\n",
      "[step: 5764] loss: 2.690086603164673\n",
      "[step: 5765] loss: 2.835695743560791\n",
      "[step: 5766] loss: 2.765397787094116\n",
      "[step: 5767] loss: 2.657351493835449\n",
      "[step: 5768] loss: 2.720822334289551\n",
      "[step: 5769] loss: 2.765045166015625\n",
      "[step: 5770] loss: 2.6569509506225586\n",
      "[step: 5771] loss: 2.6572279930114746\n",
      "[step: 5772] loss: 2.7121729850769043\n",
      "[step: 5773] loss: 2.6861050128936768\n",
      "[step: 5774] loss: 2.6290860176086426\n",
      "[step: 5775] loss: 2.6593360900878906\n",
      "[step: 5776] loss: 2.683526039123535\n",
      "[step: 5777] loss: 2.636219024658203\n",
      "[step: 5778] loss: 2.624119758605957\n",
      "[step: 5779] loss: 2.650258779525757\n",
      "[step: 5780] loss: 2.6498546600341797\n",
      "[step: 5781] loss: 2.615173816680908\n",
      "[step: 5782] loss: 2.6172375679016113\n",
      "[step: 5783] loss: 2.6343016624450684\n",
      "[step: 5784] loss: 2.6254329681396484\n",
      "[step: 5785] loss: 2.6056857109069824\n",
      "[step: 5786] loss: 2.6073694229125977\n",
      "[step: 5787] loss: 2.619588851928711\n",
      "[step: 5788] loss: 2.6100268363952637\n",
      "[step: 5789] loss: 2.5975430011749268\n",
      "[step: 5790] loss: 2.597813129425049\n",
      "[step: 5791] loss: 2.605536937713623\n",
      "[step: 5792] loss: 2.6004440784454346\n",
      "[step: 5793] loss: 2.5898914337158203\n",
      "[step: 5794] loss: 2.589244842529297\n",
      "[step: 5795] loss: 2.593336582183838\n",
      "[step: 5796] loss: 2.5921897888183594\n",
      "[step: 5797] loss: 2.583858013153076\n",
      "[step: 5798] loss: 2.58123779296875\n",
      "[step: 5799] loss: 2.5832362174987793\n",
      "[step: 5800] loss: 2.583700180053711\n",
      "[step: 5801] loss: 2.579052686691284\n",
      "[step: 5802] loss: 2.5745677947998047\n",
      "[step: 5803] loss: 2.574439525604248\n",
      "[step: 5804] loss: 2.5749874114990234\n",
      "[step: 5805] loss: 2.5736682415008545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5806] loss: 2.569728136062622\n",
      "[step: 5807] loss: 2.5673036575317383\n",
      "[step: 5808] loss: 2.5669374465942383\n",
      "[step: 5809] loss: 2.566810369491577\n",
      "[step: 5810] loss: 2.5651049613952637\n",
      "[step: 5811] loss: 2.562152862548828\n",
      "[step: 5812] loss: 2.560300827026367\n",
      "[step: 5813] loss: 2.559459924697876\n",
      "[step: 5814] loss: 2.559006690979004\n",
      "[step: 5815] loss: 2.5575428009033203\n",
      "[step: 5816] loss: 2.555419445037842\n",
      "[step: 5817] loss: 2.553548812866211\n",
      "[step: 5818] loss: 2.5523414611816406\n",
      "[step: 5819] loss: 2.5516443252563477\n",
      "[step: 5820] loss: 2.5505149364471436\n",
      "[step: 5821] loss: 2.5489864349365234\n",
      "[step: 5822] loss: 2.5472164154052734\n",
      "[step: 5823] loss: 2.5457468032836914\n",
      "[step: 5824] loss: 2.544646978378296\n",
      "[step: 5825] loss: 2.5436336994171143\n",
      "[step: 5826] loss: 2.5425095558166504\n",
      "[step: 5827] loss: 2.541071653366089\n",
      "[step: 5828] loss: 2.5396013259887695\n",
      "[step: 5829] loss: 2.5382089614868164\n",
      "[step: 5830] loss: 2.537024736404419\n",
      "[step: 5831] loss: 2.5359416007995605\n",
      "[step: 5832] loss: 2.5348145961761475\n",
      "[step: 5833] loss: 2.533595561981201\n",
      "[step: 5834] loss: 2.5322518348693848\n",
      "[step: 5835] loss: 2.5309290885925293\n",
      "[step: 5836] loss: 2.5296630859375\n",
      "[step: 5837] loss: 2.5284814834594727\n",
      "[step: 5838] loss: 2.527360677719116\n",
      "[step: 5839] loss: 2.526215076446533\n",
      "[step: 5840] loss: 2.525050163269043\n",
      "[step: 5841] loss: 2.52382230758667\n",
      "[step: 5842] loss: 2.5225887298583984\n",
      "[step: 5843] loss: 2.521376609802246\n",
      "[step: 5844] loss: 2.520200490951538\n",
      "[step: 5845] loss: 2.5190765857696533\n",
      "[step: 5846] loss: 2.5179848670959473\n",
      "[step: 5847] loss: 2.516946792602539\n",
      "[step: 5848] loss: 2.5159566402435303\n",
      "[step: 5849] loss: 2.5150814056396484\n",
      "[step: 5850] loss: 2.5144190788269043\n",
      "[step: 5851] loss: 2.514163017272949\n",
      "[step: 5852] loss: 2.5147249698638916\n",
      "[step: 5853] loss: 2.516742706298828\n",
      "[step: 5854] loss: 2.5217556953430176\n",
      "[step: 5855] loss: 2.5322232246398926\n",
      "[step: 5856] loss: 2.5543627738952637\n",
      "[step: 5857] loss: 2.597245693206787\n",
      "[step: 5858] loss: 2.68770694732666\n",
      "[step: 5859] loss: 2.854016065597534\n",
      "[step: 5860] loss: 3.2070140838623047\n",
      "[step: 5861] loss: 3.7564074993133545\n",
      "[step: 5862] loss: 4.817441940307617\n",
      "[step: 5863] loss: 5.652509689331055\n",
      "[step: 5864] loss: 6.4875640869140625\n",
      "[step: 5865] loss: 5.40938663482666\n",
      "[step: 5866] loss: 3.7853434085845947\n",
      "[step: 5867] loss: 2.8730435371398926\n",
      "[step: 5868] loss: 3.291349411010742\n",
      "[step: 5869] loss: 4.2993879318237305\n",
      "[step: 5870] loss: 4.194760799407959\n",
      "[step: 5871] loss: 3.483064889907837\n",
      "[step: 5872] loss: 3.037886142730713\n",
      "[step: 5873] loss: 3.151845932006836\n",
      "[step: 5874] loss: 3.4736175537109375\n",
      "[step: 5875] loss: 3.3017990589141846\n",
      "[step: 5876] loss: 3.06453013420105\n",
      "[step: 5877] loss: 2.9837851524353027\n",
      "[step: 5878] loss: 2.944624185562134\n",
      "[step: 5879] loss: 3.028351306915283\n",
      "[step: 5880] loss: 2.9987354278564453\n",
      "[step: 5881] loss: 2.8803446292877197\n",
      "[step: 5882] loss: 2.7740678787231445\n",
      "[step: 5883] loss: 2.8459720611572266\n",
      "[step: 5884] loss: 2.940591812133789\n",
      "[step: 5885] loss: 2.7863519191741943\n",
      "[step: 5886] loss: 2.62003231048584\n",
      "[step: 5887] loss: 2.6907966136932373\n",
      "[step: 5888] loss: 2.8556950092315674\n",
      "[step: 5889] loss: 2.7480552196502686\n",
      "[step: 5890] loss: 2.5486724376678467\n",
      "[step: 5891] loss: 2.577658176422119\n",
      "[step: 5892] loss: 2.714895725250244\n",
      "[step: 5893] loss: 2.6974871158599854\n",
      "[step: 5894] loss: 2.5442700386047363\n",
      "[step: 5895] loss: 2.5282466411590576\n",
      "[step: 5896] loss: 2.614716053009033\n",
      "[step: 5897] loss: 2.6182923316955566\n",
      "[step: 5898] loss: 2.5532853603363037\n",
      "[step: 5899] loss: 2.5265324115753174\n",
      "[step: 5900] loss: 2.5495266914367676\n",
      "[step: 5901] loss: 2.548506259918213\n",
      "[step: 5902] loss: 2.5351881980895996\n",
      "[step: 5903] loss: 2.536184072494507\n",
      "[step: 5904] loss: 2.5302202701568604\n",
      "[step: 5905] loss: 2.5078439712524414\n",
      "[step: 5906] loss: 2.5002002716064453\n",
      "[step: 5907] loss: 2.520754814147949\n",
      "[step: 5908] loss: 2.5248067378997803\n",
      "[step: 5909] loss: 2.5009145736694336\n",
      "[step: 5910] loss: 2.483166456222534\n",
      "[step: 5911] loss: 2.491492748260498\n",
      "[step: 5912] loss: 2.50459361076355\n",
      "[step: 5913] loss: 2.497243642807007\n",
      "[step: 5914] loss: 2.4855728149414062\n",
      "[step: 5915] loss: 2.483088970184326\n",
      "[step: 5916] loss: 2.4834210872650146\n",
      "[step: 5917] loss: 2.47932767868042\n",
      "[step: 5918] loss: 2.4756200313568115\n",
      "[step: 5919] loss: 2.4791579246520996\n",
      "[step: 5920] loss: 2.481013298034668\n",
      "[step: 5921] loss: 2.47501540184021\n",
      "[step: 5922] loss: 2.466933250427246\n",
      "[step: 5923] loss: 2.464951992034912\n",
      "[step: 5924] loss: 2.4672622680664062\n",
      "[step: 5925] loss: 2.466869354248047\n",
      "[step: 5926] loss: 2.4644718170166016\n",
      "[step: 5927] loss: 2.4632925987243652\n",
      "[step: 5928] loss: 2.463212490081787\n",
      "[step: 5929] loss: 2.460841655731201\n",
      "[step: 5930] loss: 2.45670223236084\n",
      "[step: 5931] loss: 2.4539740085601807\n",
      "[step: 5932] loss: 2.4534480571746826\n",
      "[step: 5933] loss: 2.45334529876709\n",
      "[step: 5934] loss: 2.451900005340576\n",
      "[step: 5935] loss: 2.4504191875457764\n",
      "[step: 5936] loss: 2.449936866760254\n",
      "[step: 5937] loss: 2.4497570991516113\n",
      "[step: 5938] loss: 2.4485626220703125\n",
      "[step: 5939] loss: 2.446560859680176\n",
      "[step: 5940] loss: 2.445040702819824\n",
      "[step: 5941] loss: 2.4440436363220215\n",
      "[step: 5942] loss: 2.4428772926330566\n",
      "[step: 5943] loss: 2.4411768913269043\n",
      "[step: 5944] loss: 2.439534902572632\n",
      "[step: 5945] loss: 2.4384169578552246\n",
      "[step: 5946] loss: 2.4375505447387695\n",
      "[step: 5947] loss: 2.4365241527557373\n",
      "[step: 5948] loss: 2.4353365898132324\n",
      "[step: 5949] loss: 2.4344849586486816\n",
      "[step: 5950] loss: 2.4341466426849365\n",
      "[step: 5951] loss: 2.4342808723449707\n",
      "[step: 5952] loss: 2.43491530418396\n",
      "[step: 5953] loss: 2.4367618560791016\n",
      "[step: 5954] loss: 2.4409384727478027\n",
      "[step: 5955] loss: 2.449296474456787\n",
      "[step: 5956] loss: 2.464775562286377\n",
      "[step: 5957] loss: 2.4937849044799805\n",
      "[step: 5958] loss: 2.547201156616211\n",
      "[step: 5959] loss: 2.649610996246338\n",
      "[step: 5960] loss: 2.8343286514282227\n",
      "[step: 5961] loss: 3.184685230255127\n",
      "[step: 5962] loss: 3.752730369567871\n",
      "[step: 5963] loss: 4.688854694366455\n",
      "[step: 5964] loss: 5.638205528259277\n",
      "[step: 5965] loss: 6.219459533691406\n",
      "[step: 5966] loss: 5.247328281402588\n",
      "[step: 5967] loss: 3.4513931274414062\n",
      "[step: 5968] loss: 2.504910469055176\n",
      "[step: 5969] loss: 3.1582300662994385\n",
      "[step: 5970] loss: 4.256722927093506\n",
      "[step: 5971] loss: 4.219615459442139\n",
      "[step: 5972] loss: 3.1820125579833984\n",
      "[step: 5973] loss: 2.5454092025756836\n",
      "[step: 5974] loss: 3.0447940826416016\n",
      "[step: 5975] loss: 3.574209690093994\n",
      "[step: 5976] loss: 3.1815319061279297\n",
      "[step: 5977] loss: 2.598167657852173\n",
      "[step: 5978] loss: 2.630342483520508\n",
      "[step: 5979] loss: 3.008256435394287\n",
      "[step: 5980] loss: 3.0044703483581543\n",
      "[step: 5981] loss: 2.631948947906494\n",
      "[step: 5982] loss: 2.549542188644409\n",
      "[step: 5983] loss: 2.740340232849121\n",
      "[step: 5984] loss: 2.7963345050811768\n",
      "[step: 5985] loss: 2.657242774963379\n",
      "[step: 5986] loss: 2.535964250564575\n",
      "[step: 5987] loss: 2.572838068008423\n",
      "[step: 5988] loss: 2.6398215293884277\n",
      "[step: 5989] loss: 2.6166322231292725\n",
      "[step: 5990] loss: 2.5398941040039062\n",
      "[step: 5991] loss: 2.486055612564087\n",
      "[step: 5992] loss: 2.5204968452453613\n",
      "[step: 5993] loss: 2.5750815868377686\n",
      "[step: 5994] loss: 2.5345218181610107\n",
      "[step: 5995] loss: 2.454937696456909\n",
      "[step: 5996] loss: 2.451791286468506\n",
      "[step: 5997] loss: 2.512383460998535\n",
      "[step: 5998] loss: 2.524622917175293\n",
      "[step: 5999] loss: 2.4567718505859375\n",
      "[step: 6000] loss: 2.4215869903564453\n",
      "[step: 6001] loss: 2.4555258750915527\n",
      "[step: 6002] loss: 2.4822707176208496\n",
      "[step: 6003] loss: 2.457963466644287\n",
      "[step: 6004] loss: 2.4229557514190674\n",
      "[step: 6005] loss: 2.423927068710327\n",
      "[step: 6006] loss: 2.440457820892334\n",
      "[step: 6007] loss: 2.4389848709106445\n",
      "[step: 6008] loss: 2.426664113998413\n",
      "[step: 6009] loss: 2.4206576347351074\n",
      "[step: 6010] loss: 2.418468952178955\n",
      "[step: 6011] loss: 2.413015127182007\n",
      "[step: 6012] loss: 2.4105358123779297\n",
      "[step: 6013] loss: 2.4156270027160645\n",
      "[step: 6014] loss: 2.4172823429107666\n",
      "[step: 6015] loss: 2.4072277545928955\n",
      "[step: 6016] loss: 2.3961093425750732\n",
      "[step: 6017] loss: 2.3960936069488525\n",
      "[step: 6018] loss: 2.402780771255493\n",
      "[step: 6019] loss: 2.404240369796753\n",
      "[step: 6020] loss: 2.398632764816284\n",
      "[step: 6021] loss: 2.39310622215271\n",
      "[step: 6022] loss: 2.39072585105896\n",
      "[step: 6023] loss: 2.388894557952881\n",
      "[step: 6024] loss: 2.386597156524658\n",
      "[step: 6025] loss: 2.386021137237549\n",
      "[step: 6026] loss: 2.387350559234619\n",
      "[step: 6027] loss: 2.387537956237793\n",
      "[step: 6028] loss: 2.3844189643859863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6029] loss: 2.380095958709717\n",
      "[step: 6030] loss: 2.3774778842926025\n",
      "[step: 6031] loss: 2.376605987548828\n",
      "[step: 6032] loss: 2.375624179840088\n",
      "[step: 6033] loss: 2.3740949630737305\n",
      "[step: 6034] loss: 2.3732218742370605\n",
      "[step: 6035] loss: 2.373263359069824\n",
      "[step: 6036] loss: 2.372978925704956\n",
      "[step: 6037] loss: 2.3716163635253906\n",
      "[step: 6038] loss: 2.369724988937378\n",
      "[step: 6039] loss: 2.3681631088256836\n",
      "[step: 6040] loss: 2.366950511932373\n",
      "[step: 6041] loss: 2.3655028343200684\n",
      "[step: 6042] loss: 2.363693952560425\n",
      "[step: 6043] loss: 2.361961841583252\n",
      "[step: 6044] loss: 2.360678195953369\n",
      "[step: 6045] loss: 2.3596291542053223\n",
      "[step: 6046] loss: 2.3583998680114746\n",
      "[step: 6047] loss: 2.357003927230835\n",
      "[step: 6048] loss: 2.355727195739746\n",
      "[step: 6049] loss: 2.3547041416168213\n",
      "[step: 6050] loss: 2.3537564277648926\n",
      "[step: 6051] loss: 2.352705717086792\n",
      "[step: 6052] loss: 2.3516225814819336\n",
      "[step: 6053] loss: 2.3507080078125\n",
      "[step: 6054] loss: 2.3501157760620117\n",
      "[step: 6055] loss: 2.3498752117156982\n",
      "[step: 6056] loss: 2.3501601219177246\n",
      "[step: 6057] loss: 2.3514933586120605\n",
      "[step: 6058] loss: 2.3550124168395996\n",
      "[step: 6059] loss: 2.362661838531494\n",
      "[step: 6060] loss: 2.378549098968506\n",
      "[step: 6061] loss: 2.4100918769836426\n",
      "[step: 6062] loss: 2.4747352600097656\n",
      "[step: 6063] loss: 2.601938247680664\n",
      "[step: 6064] loss: 2.863847494125366\n",
      "[step: 6065] loss: 3.3518526554107666\n",
      "[step: 6066] loss: 4.284390449523926\n",
      "[step: 6067] loss: 5.575387954711914\n",
      "[step: 6068] loss: 7.0269775390625\n",
      "[step: 6069] loss: 6.754364013671875\n",
      "[step: 6070] loss: 4.582339286804199\n",
      "[step: 6071] loss: 2.5973997116088867\n",
      "[step: 6072] loss: 3.078749656677246\n",
      "[step: 6073] loss: 4.723776340484619\n",
      "[step: 6074] loss: 4.796308517456055\n",
      "[step: 6075] loss: 3.222846031188965\n",
      "[step: 6076] loss: 2.5937442779541016\n",
      "[step: 6077] loss: 3.525423049926758\n",
      "[step: 6078] loss: 3.769406795501709\n",
      "[step: 6079] loss: 2.8088786602020264\n",
      "[step: 6080] loss: 2.6151626110076904\n",
      "[step: 6081] loss: 3.1645703315734863\n",
      "[step: 6082] loss: 3.1707351207733154\n",
      "[step: 6083] loss: 2.6436946392059326\n",
      "[step: 6084] loss: 2.6902995109558105\n",
      "[step: 6085] loss: 2.9392948150634766\n",
      "[step: 6086] loss: 2.740752935409546\n",
      "[step: 6087] loss: 2.5833380222320557\n",
      "[step: 6088] loss: 2.727480888366699\n",
      "[step: 6089] loss: 2.6561779975891113\n",
      "[step: 6090] loss: 2.497020959854126\n",
      "[step: 6091] loss: 2.6274642944335938\n",
      "[step: 6092] loss: 2.642291307449341\n",
      "[step: 6093] loss: 2.4699249267578125\n",
      "[step: 6094] loss: 2.457601547241211\n",
      "[step: 6095] loss: 2.6057074069976807\n",
      "[step: 6096] loss: 2.529599189758301\n",
      "[step: 6097] loss: 2.3796539306640625\n",
      "[step: 6098] loss: 2.47212815284729\n",
      "[step: 6099] loss: 2.5436196327209473\n",
      "[step: 6100] loss: 2.429595470428467\n",
      "[step: 6101] loss: 2.368957042694092\n",
      "[step: 6102] loss: 2.4700560569763184\n",
      "[step: 6103] loss: 2.4587392807006836\n",
      "[step: 6104] loss: 2.372032880783081\n",
      "[step: 6105] loss: 2.3875534534454346\n",
      "[step: 6106] loss: 2.4353044033050537\n",
      "[step: 6107] loss: 2.3957924842834473\n",
      "[step: 6108] loss: 2.3513693809509277\n",
      "[step: 6109] loss: 2.393636703491211\n",
      "[step: 6110] loss: 2.3994109630584717\n",
      "[step: 6111] loss: 2.3567051887512207\n",
      "[step: 6112] loss: 2.347551107406616\n",
      "[step: 6113] loss: 2.3810064792633057\n",
      "[step: 6114] loss: 2.3707056045532227\n",
      "[step: 6115] loss: 2.3360447883605957\n",
      "[step: 6116] loss: 2.3442296981811523\n",
      "[step: 6117] loss: 2.3605384826660156\n",
      "[step: 6118] loss: 2.3493237495422363\n",
      "[step: 6119] loss: 2.3289074897766113\n",
      "[step: 6120] loss: 2.3382296562194824\n",
      "[step: 6121] loss: 2.3434243202209473\n",
      "[step: 6122] loss: 2.3308868408203125\n",
      "[step: 6123] loss: 2.324152946472168\n",
      "[step: 6124] loss: 2.3308181762695312\n",
      "[step: 6125] loss: 2.3315296173095703\n",
      "[step: 6126] loss: 2.3185534477233887\n",
      "[step: 6127] loss: 2.3163580894470215\n",
      "[step: 6128] loss: 2.321941375732422\n",
      "[step: 6129] loss: 2.321657657623291\n",
      "[step: 6130] loss: 2.312671184539795\n",
      "[step: 6131] loss: 2.3097758293151855\n",
      "[step: 6132] loss: 2.313387870788574\n",
      "[step: 6133] loss: 2.3114686012268066\n",
      "[step: 6134] loss: 2.3064749240875244\n",
      "[step: 6135] loss: 2.304549217224121\n",
      "[step: 6136] loss: 2.3067233562469482\n",
      "[step: 6137] loss: 2.3049609661102295\n",
      "[step: 6138] loss: 2.3005077838897705\n",
      "[step: 6139] loss: 2.2987799644470215\n",
      "[step: 6140] loss: 2.2993805408477783\n",
      "[step: 6141] loss: 2.2987823486328125\n",
      "[step: 6142] loss: 2.295701026916504\n",
      "[step: 6143] loss: 2.2941455841064453\n",
      "[step: 6144] loss: 2.294001817703247\n",
      "[step: 6145] loss: 2.2931244373321533\n",
      "[step: 6146] loss: 2.290754795074463\n",
      "[step: 6147] loss: 2.2887327671051025\n",
      "[step: 6148] loss: 2.288351535797119\n",
      "[step: 6149] loss: 2.2876663208007812\n",
      "[step: 6150] loss: 2.2860941886901855\n",
      "[step: 6151] loss: 2.2843449115753174\n",
      "[step: 6152] loss: 2.2835350036621094\n",
      "[step: 6153] loss: 2.2829737663269043\n",
      "[step: 6154] loss: 2.281646966934204\n",
      "[step: 6155] loss: 2.2800965309143066\n",
      "[step: 6156] loss: 2.2788939476013184\n",
      "[step: 6157] loss: 2.278134346008301\n",
      "[step: 6158] loss: 2.2770659923553467\n",
      "[step: 6159] loss: 2.2756495475769043\n",
      "[step: 6160] loss: 2.2743980884552\n",
      "[step: 6161] loss: 2.273435354232788\n",
      "[step: 6162] loss: 2.272543430328369\n",
      "[step: 6163] loss: 2.2713546752929688\n",
      "[step: 6164] loss: 2.270115375518799\n",
      "[step: 6165] loss: 2.269063949584961\n",
      "[step: 6166] loss: 2.2681546211242676\n",
      "[step: 6167] loss: 2.2671756744384766\n",
      "[step: 6168] loss: 2.266052722930908\n",
      "[step: 6169] loss: 2.2650258541107178\n",
      "[step: 6170] loss: 2.2641687393188477\n",
      "[step: 6171] loss: 2.2634520530700684\n",
      "[step: 6172] loss: 2.2628297805786133\n",
      "[step: 6173] loss: 2.262465000152588\n",
      "[step: 6174] loss: 2.2627601623535156\n",
      "[step: 6175] loss: 2.264207363128662\n",
      "[step: 6176] loss: 2.2678170204162598\n",
      "[step: 6177] loss: 2.2754056453704834\n",
      "[step: 6178] loss: 2.2912697792053223\n",
      "[step: 6179] loss: 2.32296085357666\n",
      "[step: 6180] loss: 2.388491630554199\n",
      "[step: 6181] loss: 2.5168659687042236\n",
      "[step: 6182] loss: 2.783019542694092\n",
      "[step: 6183] loss: 3.270781993865967\n",
      "[step: 6184] loss: 4.218608379364014\n",
      "[step: 6185] loss: 5.460651397705078\n",
      "[step: 6186] loss: 6.9135003089904785\n",
      "[step: 6187] loss: 6.503052711486816\n",
      "[step: 6188] loss: 4.400433540344238\n",
      "[step: 6189] loss: 2.490584373474121\n",
      "[step: 6190] loss: 2.9652860164642334\n",
      "[step: 6191] loss: 4.610620498657227\n",
      "[step: 6192] loss: 4.700776100158691\n",
      "[step: 6193] loss: 3.1825685501098633\n",
      "[step: 6194] loss: 2.4534831047058105\n",
      "[step: 6195] loss: 3.3300137519836426\n",
      "[step: 6196] loss: 3.7292299270629883\n",
      "[step: 6197] loss: 2.791593551635742\n",
      "[step: 6198] loss: 2.43681263923645\n",
      "[step: 6199] loss: 2.976341485977173\n",
      "[step: 6200] loss: 3.1510462760925293\n",
      "[step: 6201] loss: 2.626486301422119\n",
      "[step: 6202] loss: 2.483901023864746\n",
      "[step: 6203] loss: 2.781557083129883\n",
      "[step: 6204] loss: 2.7843761444091797\n",
      "[step: 6205] loss: 2.5127904415130615\n",
      "[step: 6206] loss: 2.5013234615325928\n",
      "[step: 6207] loss: 2.558879852294922\n",
      "[step: 6208] loss: 2.5162458419799805\n",
      "[step: 6209] loss: 2.499512195587158\n",
      "[step: 6210] loss: 2.45910906791687\n",
      "[step: 6211] loss: 2.411406993865967\n",
      "[step: 6212] loss: 2.432316780090332\n",
      "[step: 6213] loss: 2.469390869140625\n",
      "[step: 6214] loss: 2.386233329772949\n",
      "[step: 6215] loss: 2.3174009323120117\n",
      "[step: 6216] loss: 2.389336347579956\n",
      "[step: 6217] loss: 2.4250640869140625\n",
      "[step: 6218] loss: 2.325282335281372\n",
      "[step: 6219] loss: 2.2831921577453613\n",
      "[step: 6220] loss: 2.3663883209228516\n",
      "[step: 6221] loss: 2.367164134979248\n",
      "[step: 6222] loss: 2.286957263946533\n",
      "[step: 6223] loss: 2.2784831523895264\n",
      "[step: 6224] loss: 2.3306353092193604\n",
      "[step: 6225] loss: 2.3198156356811523\n",
      "[step: 6226] loss: 2.269315719604492\n",
      "[step: 6227] loss: 2.2800962924957275\n",
      "[step: 6228] loss: 2.299783945083618\n",
      "[step: 6229] loss: 2.2801623344421387\n",
      "[step: 6230] loss: 2.2613449096679688\n",
      "[step: 6231] loss: 2.276435136795044\n",
      "[step: 6232] loss: 2.2764058113098145\n",
      "[step: 6233] loss: 2.2533822059631348\n",
      "[step: 6234] loss: 2.2526397705078125\n",
      "[step: 6235] loss: 2.267273187637329\n",
      "[step: 6236] loss: 2.262331008911133\n",
      "[step: 6237] loss: 2.2414257526397705\n",
      "[step: 6238] loss: 2.2423741817474365\n",
      "[step: 6239] loss: 2.2532100677490234\n",
      "[step: 6240] loss: 2.249324321746826\n",
      "[step: 6241] loss: 2.2378835678100586\n",
      "[step: 6242] loss: 2.236417531967163\n",
      "[step: 6243] loss: 2.2399985790252686\n",
      "[step: 6244] loss: 2.2350800037384033\n",
      "[step: 6245] loss: 2.2307751178741455\n",
      "[step: 6246] loss: 2.2324752807617188\n",
      "[step: 6247] loss: 2.23341703414917\n",
      "[step: 6248] loss: 2.227346897125244\n",
      "[step: 6249] loss: 2.222283363342285\n",
      "[step: 6250] loss: 2.2239508628845215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6251] loss: 2.2252025604248047\n",
      "[step: 6252] loss: 2.222400188446045\n",
      "[step: 6253] loss: 2.218785047531128\n",
      "[step: 6254] loss: 2.218393325805664\n",
      "[step: 6255] loss: 2.2179667949676514\n",
      "[step: 6256] loss: 2.215139865875244\n",
      "[step: 6257] loss: 2.212785243988037\n",
      "[step: 6258] loss: 2.212601661682129\n",
      "[step: 6259] loss: 2.2129149436950684\n",
      "[step: 6260] loss: 2.2111620903015137\n",
      "[step: 6261] loss: 2.20883846282959\n",
      "[step: 6262] loss: 2.207716464996338\n",
      "[step: 6263] loss: 2.207033157348633\n",
      "[step: 6264] loss: 2.205510377883911\n",
      "[step: 6265] loss: 2.2034945487976074\n",
      "[step: 6266] loss: 2.2024998664855957\n",
      "[step: 6267] loss: 2.2020750045776367\n",
      "[step: 6268] loss: 2.2011454105377197\n",
      "[step: 6269] loss: 2.1996891498565674\n",
      "[step: 6270] loss: 2.198509454727173\n",
      "[step: 6271] loss: 2.197840690612793\n",
      "[step: 6272] loss: 2.1969196796417236\n",
      "[step: 6273] loss: 2.1955933570861816\n",
      "[step: 6274] loss: 2.1942787170410156\n",
      "[step: 6275] loss: 2.1933088302612305\n",
      "[step: 6276] loss: 2.1924400329589844\n",
      "[step: 6277] loss: 2.1912875175476074\n",
      "[step: 6278] loss: 2.1900711059570312\n",
      "[step: 6279] loss: 2.1890358924865723\n",
      "[step: 6280] loss: 2.188204288482666\n",
      "[step: 6281] loss: 2.187321186065674\n",
      "[step: 6282] loss: 2.1863369941711426\n",
      "[step: 6283] loss: 2.185490608215332\n",
      "[step: 6284] loss: 2.18491530418396\n",
      "[step: 6285] loss: 2.184617757797241\n",
      "[step: 6286] loss: 2.184622049331665\n",
      "[step: 6287] loss: 2.185272216796875\n",
      "[step: 6288] loss: 2.1872751712799072\n",
      "[step: 6289] loss: 2.1915619373321533\n",
      "[step: 6290] loss: 2.2000675201416016\n",
      "[step: 6291] loss: 2.2160868644714355\n",
      "[step: 6292] loss: 2.247098445892334\n",
      "[step: 6293] loss: 2.304725170135498\n",
      "[step: 6294] loss: 2.418111562728882\n",
      "[step: 6295] loss: 2.623116970062256\n",
      "[step: 6296] loss: 3.022507667541504\n",
      "[step: 6297] loss: 3.657320499420166\n",
      "[step: 6298] loss: 4.719546794891357\n",
      "[step: 6299] loss: 5.640769958496094\n",
      "[step: 6300] loss: 6.063016891479492\n",
      "[step: 6301] loss: 4.671678066253662\n",
      "[step: 6302] loss: 2.839552402496338\n",
      "[step: 6303] loss: 2.3058981895446777\n",
      "[step: 6304] loss: 3.296560764312744\n",
      "[step: 6305] loss: 4.293311595916748\n",
      "[step: 6306] loss: 3.786900520324707\n",
      "[step: 6307] loss: 2.6585745811462402\n",
      "[step: 6308] loss: 2.370149850845337\n",
      "[step: 6309] loss: 3.0518479347229004\n",
      "[step: 6310] loss: 3.3515713214874268\n",
      "[step: 6311] loss: 2.741037368774414\n",
      "[step: 6312] loss: 2.299999952316284\n",
      "[step: 6313] loss: 2.51375675201416\n",
      "[step: 6314] loss: 2.821916103363037\n",
      "[step: 6315] loss: 2.673102378845215\n",
      "[step: 6316] loss: 2.3314192295074463\n",
      "[step: 6317] loss: 2.3593437671661377\n",
      "[step: 6318] loss: 2.566229820251465\n",
      "[step: 6319] loss: 2.5499954223632812\n",
      "[step: 6320] loss: 2.3674633502960205\n",
      "[step: 6321] loss: 2.2711386680603027\n",
      "[step: 6322] loss: 2.3627567291259766\n",
      "[step: 6323] loss: 2.453787088394165\n",
      "[step: 6324] loss: 2.3617987632751465\n",
      "[step: 6325] loss: 2.2367374897003174\n",
      "[step: 6326] loss: 2.2461180686950684\n",
      "[step: 6327] loss: 2.3412909507751465\n",
      "[step: 6328] loss: 2.346245288848877\n",
      "[step: 6329] loss: 2.2317728996276855\n",
      "[step: 6330] loss: 2.1931142807006836\n",
      "[step: 6331] loss: 2.2661361694335938\n",
      "[step: 6332] loss: 2.298929452896118\n",
      "[step: 6333] loss: 2.2381410598754883\n",
      "[step: 6334] loss: 2.183572769165039\n",
      "[step: 6335] loss: 2.20694899559021\n",
      "[step: 6336] loss: 2.2422847747802734\n",
      "[step: 6337] loss: 2.2256155014038086\n",
      "[step: 6338] loss: 2.1951236724853516\n",
      "[step: 6339] loss: 2.187246322631836\n",
      "[step: 6340] loss: 2.192232131958008\n",
      "[step: 6341] loss: 2.1949687004089355\n",
      "[step: 6342] loss: 2.195324420928955\n",
      "[step: 6343] loss: 2.1910390853881836\n",
      "[step: 6344] loss: 2.1769633293151855\n",
      "[step: 6345] loss: 2.1657180786132812\n",
      "[step: 6346] loss: 2.1709742546081543\n",
      "[step: 6347] loss: 2.1826865673065186\n",
      "[step: 6348] loss: 2.1805496215820312\n",
      "[step: 6349] loss: 2.1652684211730957\n",
      "[step: 6350] loss: 2.1553432941436768\n",
      "[step: 6351] loss: 2.1581029891967773\n",
      "[step: 6352] loss: 2.163644313812256\n",
      "[step: 6353] loss: 2.163534641265869\n",
      "[step: 6354] loss: 2.1599225997924805\n",
      "[step: 6355] loss: 2.156200408935547\n",
      "[step: 6356] loss: 2.151482105255127\n",
      "[step: 6357] loss: 2.1468191146850586\n",
      "[step: 6358] loss: 2.1458029747009277\n",
      "[step: 6359] loss: 2.148658275604248\n",
      "[step: 6360] loss: 2.1505072116851807\n",
      "[step: 6361] loss: 2.148118019104004\n",
      "[step: 6362] loss: 2.1439332962036133\n",
      "[step: 6363] loss: 2.140871524810791\n",
      "[step: 6364] loss: 2.138975143432617\n",
      "[step: 6365] loss: 2.1370885372161865\n",
      "[step: 6366] loss: 2.13570499420166\n",
      "[step: 6367] loss: 2.1356430053710938\n",
      "[step: 6368] loss: 2.1362695693969727\n",
      "[step: 6369] loss: 2.136040210723877\n",
      "[step: 6370] loss: 2.134509325027466\n",
      "[step: 6371] loss: 2.132646322250366\n",
      "[step: 6372] loss: 2.131110191345215\n",
      "[step: 6373] loss: 2.1295576095581055\n",
      "[step: 6374] loss: 2.127572536468506\n",
      "[step: 6375] loss: 2.1256420612335205\n",
      "[step: 6376] loss: 2.1242971420288086\n",
      "[step: 6377] loss: 2.1233482360839844\n",
      "[step: 6378] loss: 2.1223297119140625\n",
      "[step: 6379] loss: 2.121180534362793\n",
      "[step: 6380] loss: 2.1201977729797363\n",
      "[step: 6381] loss: 2.119508743286133\n",
      "[step: 6382] loss: 2.118934154510498\n",
      "[step: 6383] loss: 2.1182870864868164\n",
      "[step: 6384] loss: 2.1177010536193848\n",
      "[step: 6385] loss: 2.1174542903900146\n",
      "[step: 6386] loss: 2.1177337169647217\n",
      "[step: 6387] loss: 2.1186165809631348\n",
      "[step: 6388] loss: 2.1205291748046875\n",
      "[step: 6389] loss: 2.124288558959961\n",
      "[step: 6390] loss: 2.1315393447875977\n",
      "[step: 6391] loss: 2.144557476043701\n",
      "[step: 6392] loss: 2.1684317588806152\n",
      "[step: 6393] loss: 2.2107129096984863\n",
      "[step: 6394] loss: 2.290137767791748\n",
      "[step: 6395] loss: 2.4290809631347656\n",
      "[step: 6396] loss: 2.690922260284424\n",
      "[step: 6397] loss: 3.1103882789611816\n",
      "[step: 6398] loss: 3.83453369140625\n",
      "[step: 6399] loss: 4.639718532562256\n",
      "[step: 6400] loss: 5.441457748413086\n",
      "[step: 6401] loss: 5.084290027618408\n",
      "[step: 6402] loss: 3.7846453189849854\n",
      "[step: 6403] loss: 2.417842388153076\n",
      "[step: 6404] loss: 2.295119047164917\n",
      "[step: 6405] loss: 3.1784138679504395\n",
      "[step: 6406] loss: 3.8205695152282715\n",
      "[step: 6407] loss: 3.474383592605591\n",
      "[step: 6408] loss: 2.511178493499756\n",
      "[step: 6409] loss: 2.2465107440948486\n",
      "[step: 6410] loss: 2.764355182647705\n",
      "[step: 6411] loss: 3.099216938018799\n",
      "[step: 6412] loss: 2.7770614624023438\n",
      "[step: 6413] loss: 2.2806835174560547\n",
      "[step: 6414] loss: 2.2668402194976807\n",
      "[step: 6415] loss: 2.5507564544677734\n",
      "[step: 6416] loss: 2.5872671604156494\n",
      "[step: 6417] loss: 2.3674533367156982\n",
      "[step: 6418] loss: 2.207151412963867\n",
      "[step: 6419] loss: 2.2802581787109375\n",
      "[step: 6420] loss: 2.4168033599853516\n",
      "[step: 6421] loss: 2.3817338943481445\n",
      "[step: 6422] loss: 2.2423455715179443\n",
      "[step: 6423] loss: 2.1651573181152344\n",
      "[step: 6424] loss: 2.240163564682007\n",
      "[step: 6425] loss: 2.3379569053649902\n",
      "[step: 6426] loss: 2.279120922088623\n",
      "[step: 6427] loss: 2.159727096557617\n",
      "[step: 6428] loss: 2.129383087158203\n",
      "[step: 6429] loss: 2.2119429111480713\n",
      "[step: 6430] loss: 2.2574799060821533\n",
      "[step: 6431] loss: 2.1884307861328125\n",
      "[step: 6432] loss: 2.118067741394043\n",
      "[step: 6433] loss: 2.1285135746002197\n",
      "[step: 6434] loss: 2.1788156032562256\n",
      "[step: 6435] loss: 2.1854045391082764\n",
      "[step: 6436] loss: 2.145509719848633\n",
      "[step: 6437] loss: 2.116222381591797\n",
      "[step: 6438] loss: 2.121863603591919\n",
      "[step: 6439] loss: 2.133758068084717\n",
      "[step: 6440] loss: 2.1360394954681396\n",
      "[step: 6441] loss: 2.1317334175109863\n",
      "[step: 6442] loss: 2.122601270675659\n",
      "[step: 6443] loss: 2.1082539558410645\n",
      "[step: 6444] loss: 2.0974178314208984\n",
      "[step: 6445] loss: 2.105266571044922\n",
      "[step: 6446] loss: 2.118338108062744\n",
      "[step: 6447] loss: 2.1183199882507324\n",
      "[step: 6448] loss: 2.10245418548584\n",
      "[step: 6449] loss: 2.0884766578674316\n",
      "[step: 6450] loss: 2.0874996185302734\n",
      "[step: 6451] loss: 2.0927679538726807\n",
      "[step: 6452] loss: 2.0963821411132812\n",
      "[step: 6453] loss: 2.095064878463745\n",
      "[step: 6454] loss: 2.0930685997009277\n",
      "[step: 6455] loss: 2.089017868041992\n",
      "[step: 6456] loss: 2.0829262733459473\n",
      "[step: 6457] loss: 2.076894998550415\n",
      "[step: 6458] loss: 2.0752527713775635\n",
      "[step: 6459] loss: 2.0779175758361816\n",
      "[step: 6460] loss: 2.0800914764404297\n",
      "[step: 6461] loss: 2.079909563064575\n",
      "[step: 6462] loss: 2.077932357788086\n",
      "[step: 6463] loss: 2.0763840675354004\n",
      "[step: 6464] loss: 2.074352979660034\n",
      "[step: 6465] loss: 2.0713894367218018\n",
      "[step: 6466] loss: 2.0678648948669434\n",
      "[step: 6467] loss: 2.065316915512085\n",
      "[step: 6468] loss: 2.064218759536743\n",
      "[step: 6469] loss: 2.0636420249938965\n",
      "[step: 6470] loss: 2.062893867492676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6471] loss: 2.06197452545166\n",
      "[step: 6472] loss: 2.061736822128296\n",
      "[step: 6473] loss: 2.0619611740112305\n",
      "[step: 6474] loss: 2.0622398853302\n",
      "[step: 6475] loss: 2.0622496604919434\n",
      "[step: 6476] loss: 2.062642812728882\n",
      "[step: 6477] loss: 2.0638437271118164\n",
      "[step: 6478] loss: 2.06620717048645\n",
      "[step: 6479] loss: 2.0699291229248047\n",
      "[step: 6480] loss: 2.0760202407836914\n",
      "[step: 6481] loss: 2.0860977172851562\n",
      "[step: 6482] loss: 2.103682279586792\n",
      "[step: 6483] loss: 2.132351875305176\n",
      "[step: 6484] loss: 2.1821420192718506\n",
      "[step: 6485] loss: 2.262603282928467\n",
      "[step: 6486] loss: 2.4063472747802734\n",
      "[step: 6487] loss: 2.631046772003174\n",
      "[step: 6488] loss: 3.0181386470794678\n",
      "[step: 6489] loss: 3.516686201095581\n",
      "[step: 6490] loss: 4.195655822753906\n",
      "[step: 6491] loss: 4.53470516204834\n",
      "[step: 6492] loss: 4.418744087219238\n",
      "[step: 6493] loss: 3.4332046508789062\n",
      "[step: 6494] loss: 2.4294321537017822\n",
      "[step: 6495] loss: 2.0989389419555664\n",
      "[step: 6496] loss: 2.5308377742767334\n",
      "[step: 6497] loss: 3.149664878845215\n",
      "[step: 6498] loss: 3.2484099864959717\n",
      "[step: 6499] loss: 2.800631523132324\n",
      "[step: 6500] loss: 2.213564395904541\n",
      "[step: 6501] loss: 2.15584659576416\n",
      "[step: 6502] loss: 2.5488271713256836\n",
      "[step: 6503] loss: 2.813370943069458\n",
      "[step: 6504] loss: 2.666107654571533\n",
      "[step: 6505] loss: 2.2648487091064453\n",
      "[step: 6506] loss: 2.0952539443969727\n",
      "[step: 6507] loss: 2.248920440673828\n",
      "[step: 6508] loss: 2.438622236251831\n",
      "[step: 6509] loss: 2.4263863563537598\n",
      "[step: 6510] loss: 2.2124414443969727\n",
      "[step: 6511] loss: 2.0749967098236084\n",
      "[step: 6512] loss: 2.1298446655273438\n",
      "[step: 6513] loss: 2.254537582397461\n",
      "[step: 6514] loss: 2.2809629440307617\n",
      "[step: 6515] loss: 2.1658129692077637\n",
      "[step: 6516] loss: 2.0654520988464355\n",
      "[step: 6517] loss: 2.07249116897583\n",
      "[step: 6518] loss: 2.1472339630126953\n",
      "[step: 6519] loss: 2.1903486251831055\n",
      "[step: 6520] loss: 2.1424965858459473\n",
      "[step: 6521] loss: 2.070727586746216\n",
      "[step: 6522] loss: 2.046724319458008\n",
      "[step: 6523] loss: 2.079174518585205\n",
      "[step: 6524] loss: 2.1180520057678223\n",
      "[step: 6525] loss: 2.116605758666992\n",
      "[step: 6526] loss: 2.086489677429199\n",
      "[step: 6527] loss: 2.0538740158081055\n",
      "[step: 6528] loss: 2.0424702167510986\n",
      "[step: 6529] loss: 2.0528929233551025\n",
      "[step: 6530] loss: 2.069380760192871\n",
      "[step: 6531] loss: 2.0773892402648926\n",
      "[step: 6532] loss: 2.070303440093994\n",
      "[step: 6533] loss: 2.0532071590423584\n",
      "[step: 6534] loss: 2.0353424549102783\n",
      "[step: 6535] loss: 2.027972459793091\n",
      "[step: 6536] loss: 2.033950090408325\n",
      "[step: 6537] loss: 2.0452544689178467\n",
      "[step: 6538] loss: 2.0518369674682617\n",
      "[step: 6539] loss: 2.0498504638671875\n",
      "[step: 6540] loss: 2.0416440963745117\n",
      "[step: 6541] loss: 2.0314831733703613\n",
      "[step: 6542] loss: 2.0236306190490723\n",
      "[step: 6543] loss: 2.0195162296295166\n",
      "[step: 6544] loss: 2.018751621246338\n",
      "[step: 6545] loss: 2.0206446647644043\n",
      "[step: 6546] loss: 2.0243492126464844\n",
      "[step: 6547] loss: 2.0277957916259766\n",
      "[step: 6548] loss: 2.029085636138916\n",
      "[step: 6549] loss: 2.02840518951416\n",
      "[step: 6550] loss: 2.0263240337371826\n",
      "[step: 6551] loss: 2.023843288421631\n",
      "[step: 6552] loss: 2.021149158477783\n",
      "[step: 6553] loss: 2.0184664726257324\n",
      "[step: 6554] loss: 2.0156095027923584\n",
      "[step: 6555] loss: 2.012956142425537\n",
      "[step: 6556] loss: 2.0108988285064697\n",
      "[step: 6557] loss: 2.0095059871673584\n",
      "[step: 6558] loss: 2.008403778076172\n",
      "[step: 6559] loss: 2.0076327323913574\n",
      "[step: 6560] loss: 2.00732684135437\n",
      "[step: 6561] loss: 2.007852554321289\n",
      "[step: 6562] loss: 2.0095503330230713\n",
      "[step: 6563] loss: 2.0132627487182617\n",
      "[step: 6564] loss: 2.019984722137451\n",
      "[step: 6565] loss: 2.032255172729492\n",
      "[step: 6566] loss: 2.0537242889404297\n",
      "[step: 6567] loss: 2.0930819511413574\n",
      "[step: 6568] loss: 2.160982131958008\n",
      "[step: 6569] loss: 2.2874889373779297\n",
      "[step: 6570] loss: 2.500274658203125\n",
      "[step: 6571] loss: 2.893054485321045\n",
      "[step: 6572] loss: 3.461127281188965\n",
      "[step: 6573] loss: 4.335524082183838\n",
      "[step: 6574] loss: 4.958161354064941\n",
      "[step: 6575] loss: 5.084394931793213\n",
      "[step: 6576] loss: 3.910712718963623\n",
      "[step: 6577] loss: 2.5270633697509766\n",
      "[step: 6578] loss: 2.0796618461608887\n",
      "[step: 6579] loss: 2.734610080718994\n",
      "[step: 6580] loss: 3.52595591545105\n",
      "[step: 6581] loss: 3.4220376014709473\n",
      "[step: 6582] loss: 2.671666383743286\n",
      "[step: 6583] loss: 2.1407103538513184\n",
      "[step: 6584] loss: 2.4066827297210693\n",
      "[step: 6585] loss: 2.858145236968994\n",
      "[step: 6586] loss: 2.78070068359375\n",
      "[step: 6587] loss: 2.3966031074523926\n",
      "[step: 6588] loss: 2.1892123222351074\n",
      "[step: 6589] loss: 2.3007397651672363\n",
      "[step: 6590] loss: 2.4103403091430664\n",
      "[step: 6591] loss: 2.3335204124450684\n",
      "[step: 6592] loss: 2.255765438079834\n",
      "[step: 6593] loss: 2.197614908218384\n",
      "[step: 6594] loss: 2.1684656143188477\n",
      "[step: 6595] loss: 2.189107894897461\n",
      "[step: 6596] loss: 2.2269811630249023\n",
      "[step: 6597] loss: 2.205057144165039\n",
      "[step: 6598] loss: 2.0947747230529785\n",
      "[step: 6599] loss: 2.062448024749756\n",
      "[step: 6600] loss: 2.1414318084716797\n",
      "[step: 6601] loss: 2.184858560562134\n",
      "[step: 6602] loss: 2.1077630519866943\n",
      "[step: 6603] loss: 2.0128796100616455\n",
      "[step: 6604] loss: 2.036306381225586\n",
      "[step: 6605] loss: 2.1137187480926514\n",
      "[step: 6606] loss: 2.1086585521698\n",
      "[step: 6607] loss: 2.0318775177001953\n",
      "[step: 6608] loss: 1.9996392726898193\n",
      "[step: 6609] loss: 2.0351033210754395\n",
      "[step: 6610] loss: 2.060896635055542\n",
      "[step: 6611] loss: 2.0329642295837402\n",
      "[step: 6612] loss: 2.0056474208831787\n",
      "[step: 6613] loss: 2.0128657817840576\n",
      "[step: 6614] loss: 2.0207221508026123\n",
      "[step: 6615] loss: 2.007312774658203\n",
      "[step: 6616] loss: 1.9923051595687866\n",
      "[step: 6617] loss: 2.0004401206970215\n",
      "[step: 6618] loss: 2.0115442276000977\n",
      "[step: 6619] loss: 2.002845525741577\n",
      "[step: 6620] loss: 1.98271644115448\n",
      "[step: 6621] loss: 1.9760724306106567\n",
      "[step: 6622] loss: 1.9862838983535767\n",
      "[step: 6623] loss: 1.9926440715789795\n",
      "[step: 6624] loss: 1.986825942993164\n",
      "[step: 6625] loss: 1.978158712387085\n",
      "[step: 6626] loss: 1.9772844314575195\n",
      "[step: 6627] loss: 1.9781465530395508\n",
      "[step: 6628] loss: 1.9741175174713135\n",
      "[step: 6629] loss: 1.9676692485809326\n",
      "[step: 6630] loss: 1.966325044631958\n",
      "[step: 6631] loss: 1.9701060056686401\n",
      "[step: 6632] loss: 1.9720299243927002\n",
      "[step: 6633] loss: 1.9693059921264648\n",
      "[step: 6634] loss: 1.964919090270996\n",
      "[step: 6635] loss: 1.963545560836792\n",
      "[step: 6636] loss: 1.963731050491333\n",
      "[step: 6637] loss: 1.9622881412506104\n",
      "[step: 6638] loss: 1.958555817604065\n",
      "[step: 6639] loss: 1.9555631875991821\n",
      "[step: 6640] loss: 1.9548592567443848\n",
      "[step: 6641] loss: 1.9550132751464844\n",
      "[step: 6642] loss: 1.954024314880371\n",
      "[step: 6643] loss: 1.9519933462142944\n",
      "[step: 6644] loss: 1.9506030082702637\n",
      "[step: 6645] loss: 1.9503991603851318\n",
      "[step: 6646] loss: 1.9506462812423706\n",
      "[step: 6647] loss: 1.950178623199463\n",
      "[step: 6648] loss: 1.9493823051452637\n",
      "[step: 6649] loss: 1.9492416381835938\n",
      "[step: 6650] loss: 1.9505525827407837\n",
      "[step: 6651] loss: 1.9531919956207275\n",
      "[step: 6652] loss: 1.9574809074401855\n",
      "[step: 6653] loss: 1.9648520946502686\n",
      "[step: 6654] loss: 1.9789059162139893\n",
      "[step: 6655] loss: 2.004462242126465\n",
      "[step: 6656] loss: 2.0524632930755615\n",
      "[step: 6657] loss: 2.1362643241882324\n",
      "[step: 6658] loss: 2.2958574295043945\n",
      "[step: 6659] loss: 2.5648703575134277\n",
      "[step: 6660] loss: 3.0674266815185547\n",
      "[step: 6661] loss: 3.7645020484924316\n",
      "[step: 6662] loss: 4.779073238372803\n",
      "[step: 6663] loss: 5.245397567749023\n",
      "[step: 6664] loss: 4.908979415893555\n",
      "[step: 6665] loss: 3.3111650943756104\n",
      "[step: 6666] loss: 2.1186699867248535\n",
      "[step: 6667] loss: 2.282010555267334\n",
      "[step: 6668] loss: 3.2574291229248047\n",
      "[step: 6669] loss: 3.7596969604492188\n",
      "[step: 6670] loss: 3.068068265914917\n",
      "[step: 6671] loss: 2.233752727508545\n",
      "[step: 6672] loss: 2.183262586593628\n",
      "[step: 6673] loss: 2.7323999404907227\n",
      "[step: 6674] loss: 2.9313063621520996\n",
      "[step: 6675] loss: 2.4911932945251465\n",
      "[step: 6676] loss: 2.1329126358032227\n",
      "[step: 6677] loss: 2.1853322982788086\n",
      "[step: 6678] loss: 2.405165910720825\n",
      "[step: 6679] loss: 2.409224271774292\n",
      "[step: 6680] loss: 2.2035229206085205\n",
      "[step: 6681] loss: 2.0959038734436035\n",
      "[step: 6682] loss: 2.1398587226867676\n",
      "[step: 6683] loss: 2.2248802185058594\n",
      "[step: 6684] loss: 2.2323946952819824\n",
      "[step: 6685] loss: 2.1106786727905273\n",
      "[step: 6686] loss: 2.0147221088409424\n",
      "[step: 6687] loss: 2.07708740234375\n",
      "[step: 6688] loss: 2.1655359268188477\n",
      "[step: 6689] loss: 2.1405444145202637\n",
      "[step: 6690] loss: 2.001277208328247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6691] loss: 1.9694585800170898\n",
      "[step: 6692] loss: 2.066420316696167\n",
      "[step: 6693] loss: 2.100432872772217\n",
      "[step: 6694] loss: 2.0277628898620605\n",
      "[step: 6695] loss: 1.9495220184326172\n",
      "[step: 6696] loss: 1.9791830778121948\n",
      "[step: 6697] loss: 2.0313897132873535\n",
      "[step: 6698] loss: 2.0188770294189453\n",
      "[step: 6699] loss: 1.972517967224121\n",
      "[step: 6700] loss: 1.9577361345291138\n",
      "[step: 6701] loss: 1.973646879196167\n",
      "[step: 6702] loss: 1.9733498096466064\n",
      "[step: 6703] loss: 1.9649816751480103\n",
      "[step: 6704] loss: 1.9647624492645264\n",
      "[step: 6705] loss: 1.967771291732788\n",
      "[step: 6706] loss: 1.9496827125549316\n",
      "[step: 6707] loss: 1.932361125946045\n",
      "[step: 6708] loss: 1.9384044408798218\n",
      "[step: 6709] loss: 1.955814242362976\n",
      "[step: 6710] loss: 1.956949234008789\n",
      "[step: 6711] loss: 1.93837571144104\n",
      "[step: 6712] loss: 1.9255452156066895\n",
      "[step: 6713] loss: 1.9254270792007446\n",
      "[step: 6714] loss: 1.9308116436004639\n",
      "[step: 6715] loss: 1.9297947883605957\n",
      "[step: 6716] loss: 1.9279553890228271\n",
      "[step: 6717] loss: 1.9286606311798096\n",
      "[step: 6718] loss: 1.9281740188598633\n",
      "[step: 6719] loss: 1.9216477870941162\n",
      "[step: 6720] loss: 1.9133968353271484\n",
      "[step: 6721] loss: 1.9111077785491943\n",
      "[step: 6722] loss: 1.9132966995239258\n",
      "[step: 6723] loss: 1.9153125286102295\n",
      "[step: 6724] loss: 1.9138455390930176\n",
      "[step: 6725] loss: 1.9126334190368652\n",
      "[step: 6726] loss: 1.9126250743865967\n",
      "[step: 6727] loss: 1.9126386642456055\n",
      "[step: 6728] loss: 1.9101333618164062\n",
      "[step: 6729] loss: 1.9062938690185547\n",
      "[step: 6730] loss: 1.903616189956665\n",
      "[step: 6731] loss: 1.9024336338043213\n",
      "[step: 6732] loss: 1.9014651775360107\n",
      "[step: 6733] loss: 1.899423360824585\n",
      "[step: 6734] loss: 1.897533893585205\n",
      "[step: 6735] loss: 1.8965657949447632\n",
      "[step: 6736] loss: 1.896471619606018\n",
      "[step: 6737] loss: 1.89595365524292\n",
      "[step: 6738] loss: 1.894972562789917\n",
      "[step: 6739] loss: 1.8941035270690918\n",
      "[step: 6740] loss: 1.8940043449401855\n",
      "[step: 6741] loss: 1.8945231437683105\n",
      "[step: 6742] loss: 1.8953094482421875\n",
      "[step: 6743] loss: 1.896672248840332\n",
      "[step: 6744] loss: 1.8996987342834473\n",
      "[step: 6745] loss: 1.9058358669281006\n",
      "[step: 6746] loss: 1.9171658754348755\n",
      "[step: 6747] loss: 1.9368153810501099\n",
      "[step: 6748] loss: 1.9732489585876465\n",
      "[step: 6749] loss: 2.0379271507263184\n",
      "[step: 6750] loss: 2.16176700592041\n",
      "[step: 6751] loss: 2.3747048377990723\n",
      "[step: 6752] loss: 2.7751779556274414\n",
      "[step: 6753] loss: 3.364443302154541\n",
      "[step: 6754] loss: 4.296994209289551\n",
      "[step: 6755] loss: 4.9710693359375\n",
      "[step: 6756] loss: 5.167353630065918\n",
      "[step: 6757] loss: 3.903913974761963\n",
      "[step: 6758] loss: 2.4245715141296387\n",
      "[step: 6759] loss: 1.982100486755371\n",
      "[step: 6760] loss: 2.7331624031066895\n",
      "[step: 6761] loss: 3.5842299461364746\n",
      "[step: 6762] loss: 3.373553991317749\n",
      "[step: 6763] loss: 2.5104451179504395\n",
      "[step: 6764] loss: 2.008850574493408\n",
      "[step: 6765] loss: 2.39915132522583\n",
      "[step: 6766] loss: 2.883730173110962\n",
      "[step: 6767] loss: 2.673799514770508\n",
      "[step: 6768] loss: 2.1830854415893555\n",
      "[step: 6769] loss: 2.0258383750915527\n",
      "[step: 6770] loss: 2.267796039581299\n",
      "[step: 6771] loss: 2.413947105407715\n",
      "[step: 6772] loss: 2.2152271270751953\n",
      "[step: 6773] loss: 2.0324440002441406\n",
      "[step: 6774] loss: 2.0394039154052734\n",
      "[step: 6775] loss: 2.1555376052856445\n",
      "[step: 6776] loss: 2.1988983154296875\n",
      "[step: 6777] loss: 2.0850701332092285\n",
      "[step: 6778] loss: 1.9676029682159424\n",
      "[step: 6779] loss: 1.9847980737686157\n",
      "[step: 6780] loss: 2.084972381591797\n",
      "[step: 6781] loss: 2.1094794273376465\n",
      "[step: 6782] loss: 1.9907433986663818\n",
      "[step: 6783] loss: 1.9063727855682373\n",
      "[step: 6784] loss: 1.962235450744629\n",
      "[step: 6785] loss: 2.035024642944336\n",
      "[step: 6786] loss: 2.0114283561706543\n",
      "[step: 6787] loss: 1.9179607629776\n",
      "[step: 6788] loss: 1.896662712097168\n",
      "[step: 6789] loss: 1.9500846862792969\n",
      "[step: 6790] loss: 1.97453773021698\n",
      "[step: 6791] loss: 1.9419605731964111\n",
      "[step: 6792] loss: 1.9002572298049927\n",
      "[step: 6793] loss: 1.903687596321106\n",
      "[step: 6794] loss: 1.9186451435089111\n",
      "[step: 6795] loss: 1.9177333116531372\n",
      "[step: 6796] loss: 1.9093877077102661\n",
      "[step: 6797] loss: 1.907285213470459\n",
      "[step: 6798] loss: 1.9009952545166016\n",
      "[step: 6799] loss: 1.885171890258789\n",
      "[step: 6800] loss: 1.88038170337677\n",
      "[step: 6801] loss: 1.8912765979766846\n",
      "[step: 6802] loss: 1.902137279510498\n",
      "[step: 6803] loss: 1.8938183784484863\n",
      "[step: 6804] loss: 1.87660551071167\n",
      "[step: 6805] loss: 1.8678797483444214\n",
      "[step: 6806] loss: 1.8718911409378052\n",
      "[step: 6807] loss: 1.8771482706069946\n",
      "[step: 6808] loss: 1.8765418529510498\n",
      "[step: 6809] loss: 1.8742293119430542\n",
      "[step: 6810] loss: 1.8730685710906982\n",
      "[step: 6811] loss: 1.8705148696899414\n",
      "[step: 6812] loss: 1.8640270233154297\n",
      "[step: 6813] loss: 1.8582961559295654\n",
      "[step: 6814] loss: 1.857222080230713\n",
      "[step: 6815] loss: 1.8600125312805176\n",
      "[step: 6816] loss: 1.8615843057632446\n",
      "[step: 6817] loss: 1.8607420921325684\n",
      "[step: 6818] loss: 1.8591285943984985\n",
      "[step: 6819] loss: 1.858490228652954\n",
      "[step: 6820] loss: 1.857792854309082\n",
      "[step: 6821] loss: 1.8553194999694824\n",
      "[step: 6822] loss: 1.8519165515899658\n",
      "[step: 6823] loss: 1.8492436408996582\n",
      "[step: 6824] loss: 1.8481216430664062\n",
      "[step: 6825] loss: 1.8471484184265137\n",
      "[step: 6826] loss: 1.8456292152404785\n",
      "[step: 6827] loss: 1.8439154624938965\n",
      "[step: 6828] loss: 1.8429720401763916\n",
      "[step: 6829] loss: 1.8426802158355713\n",
      "[step: 6830] loss: 1.842353105545044\n",
      "[step: 6831] loss: 1.841629147529602\n",
      "[step: 6832] loss: 1.8408852815628052\n",
      "[step: 6833] loss: 1.8407784700393677\n",
      "[step: 6834] loss: 1.8413968086242676\n",
      "[step: 6835] loss: 1.842645287513733\n",
      "[step: 6836] loss: 1.8447778224945068\n",
      "[step: 6837] loss: 1.8488858938217163\n",
      "[step: 6838] loss: 1.8569000959396362\n",
      "[step: 6839] loss: 1.87135910987854\n",
      "[step: 6840] loss: 1.898130178451538\n",
      "[step: 6841] loss: 1.945417046546936\n",
      "[step: 6842] loss: 2.035709857940674\n",
      "[step: 6843] loss: 2.1935136318206787\n",
      "[step: 6844] loss: 2.4955101013183594\n",
      "[step: 6845] loss: 2.9690353870391846\n",
      "[step: 6846] loss: 3.78192138671875\n",
      "[step: 6847] loss: 4.587215423583984\n",
      "[step: 6848] loss: 5.277955055236816\n",
      "[step: 6849] loss: 4.568240165710449\n",
      "[step: 6850] loss: 3.082794666290283\n",
      "[step: 6851] loss: 1.9720871448516846\n",
      "[step: 6852] loss: 2.2282633781433105\n",
      "[step: 6853] loss: 3.209815502166748\n",
      "[step: 6854] loss: 3.555032730102539\n",
      "[step: 6855] loss: 2.937162160873413\n",
      "[step: 6856] loss: 2.0796194076538086\n",
      "[step: 6857] loss: 2.104703903198242\n",
      "[step: 6858] loss: 2.671618938446045\n",
      "[step: 6859] loss: 2.776103973388672\n",
      "[step: 6860] loss: 2.349045753479004\n",
      "[step: 6861] loss: 1.998989462852478\n",
      "[step: 6862] loss: 2.108222246170044\n",
      "[step: 6863] loss: 2.320082187652588\n",
      "[step: 6864] loss: 2.240206718444824\n",
      "[step: 6865] loss: 2.067507743835449\n",
      "[step: 6866] loss: 1.9907855987548828\n",
      "[step: 6867] loss: 2.0479822158813477\n",
      "[step: 6868] loss: 2.1278772354125977\n",
      "[step: 6869] loss: 2.092402458190918\n",
      "[step: 6870] loss: 1.984083890914917\n",
      "[step: 6871] loss: 1.9106452465057373\n",
      "[step: 6872] loss: 1.9765950441360474\n",
      "[step: 6873] loss: 2.0645699501037598\n",
      "[step: 6874] loss: 2.007882833480835\n",
      "[step: 6875] loss: 1.885223150253296\n",
      "[step: 6876] loss: 1.8662655353546143\n",
      "[step: 6877] loss: 1.9541693925857544\n",
      "[step: 6878] loss: 1.9900531768798828\n",
      "[step: 6879] loss: 1.9092891216278076\n",
      "[step: 6880] loss: 1.8410069942474365\n",
      "[step: 6881] loss: 1.8728171586990356\n",
      "[step: 6882] loss: 1.9198967218399048\n",
      "[step: 6883] loss: 1.9092090129852295\n",
      "[step: 6884] loss: 1.8601603507995605\n",
      "[step: 6885] loss: 1.8491864204406738\n",
      "[step: 6886] loss: 1.866737723350525\n",
      "[step: 6887] loss: 1.8666898012161255\n",
      "[step: 6888] loss: 1.8564101457595825\n",
      "[step: 6889] loss: 1.8541282415390015\n",
      "[step: 6890] loss: 1.857869267463684\n",
      "[step: 6891] loss: 1.842969298362732\n",
      "[step: 6892] loss: 1.8276454210281372\n",
      "[step: 6893] loss: 1.8315184116363525\n",
      "[step: 6894] loss: 1.8470144271850586\n",
      "[step: 6895] loss: 1.8484845161437988\n",
      "[step: 6896] loss: 1.8317837715148926\n",
      "[step: 6897] loss: 1.8189067840576172\n",
      "[step: 6898] loss: 1.818993091583252\n",
      "[step: 6899] loss: 1.8249645233154297\n",
      "[step: 6900] loss: 1.8246337175369263\n",
      "[step: 6901] loss: 1.8218858242034912\n",
      "[step: 6902] loss: 1.8214820623397827\n",
      "[step: 6903] loss: 1.821377158164978\n",
      "[step: 6904] loss: 1.816330909729004\n",
      "[step: 6905] loss: 1.8088786602020264\n",
      "[step: 6906] loss: 1.8058643341064453\n",
      "[step: 6907] loss: 1.807814598083496\n",
      "[step: 6908] loss: 1.810034990310669\n",
      "[step: 6909] loss: 1.8091567754745483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6910] loss: 1.8074437379837036\n",
      "[step: 6911] loss: 1.8069055080413818\n",
      "[step: 6912] loss: 1.8071092367172241\n",
      "[step: 6913] loss: 1.8053632974624634\n",
      "[step: 6914] loss: 1.8019814491271973\n",
      "[step: 6915] loss: 1.7990458011627197\n",
      "[step: 6916] loss: 1.7978183031082153\n",
      "[step: 6917] loss: 1.7970846891403198\n",
      "[step: 6918] loss: 1.7954931259155273\n",
      "[step: 6919] loss: 1.7935646772384644\n",
      "[step: 6920] loss: 1.7923215627670288\n",
      "[step: 6921] loss: 1.7920401096343994\n",
      "[step: 6922] loss: 1.7916932106018066\n",
      "[step: 6923] loss: 1.790854573249817\n",
      "[step: 6924] loss: 1.789791464805603\n",
      "[step: 6925] loss: 1.789328694343567\n",
      "[step: 6926] loss: 1.789502739906311\n",
      "[step: 6927] loss: 1.7900077104568481\n",
      "[step: 6928] loss: 1.7907791137695312\n",
      "[step: 6929] loss: 1.7925403118133545\n",
      "[step: 6930] loss: 1.796568751335144\n",
      "[step: 6931] loss: 1.8041889667510986\n",
      "[step: 6932] loss: 1.8182995319366455\n",
      "[step: 6933] loss: 1.843259334564209\n",
      "[step: 6934] loss: 1.8908708095550537\n",
      "[step: 6935] loss: 1.9762648344039917\n",
      "[step: 6936] loss: 2.1419925689697266\n",
      "[step: 6937] loss: 2.4241714477539062\n",
      "[step: 6938] loss: 2.953575372695923\n",
      "[step: 6939] loss: 3.6784520149230957\n",
      "[step: 6940] loss: 4.7362847328186035\n",
      "[step: 6941] loss: 5.152889251708984\n",
      "[step: 6942] loss: 4.724480628967285\n",
      "[step: 6943] loss: 3.0438406467437744\n",
      "[step: 6944] loss: 1.9172532558441162\n",
      "[step: 6945] loss: 2.2200965881347656\n",
      "[step: 6946] loss: 3.2369775772094727\n",
      "[step: 6947] loss: 3.627222776412964\n",
      "[step: 6948] loss: 2.8014168739318848\n",
      "[step: 6949] loss: 2.0069522857666016\n",
      "[step: 6950] loss: 2.1053924560546875\n",
      "[step: 6951] loss: 2.663959264755249\n",
      "[step: 6952] loss: 2.732144832611084\n",
      "[step: 6953] loss: 2.230734348297119\n",
      "[step: 6954] loss: 1.951278567314148\n",
      "[step: 6955] loss: 2.0939011573791504\n",
      "[step: 6956] loss: 2.2763633728027344\n",
      "[step: 6957] loss: 2.2048699855804443\n",
      "[step: 6958] loss: 2.0052709579467773\n",
      "[step: 6959] loss: 1.947278618812561\n",
      "[step: 6960] loss: 2.0269505977630615\n",
      "[step: 6961] loss: 2.0838379859924316\n",
      "[step: 6962] loss: 2.050109386444092\n",
      "[step: 6963] loss: 1.918244481086731\n",
      "[step: 6964] loss: 1.8695420026779175\n",
      "[step: 6965] loss: 1.9694024324417114\n",
      "[step: 6966] loss: 2.019351005554199\n",
      "[step: 6967] loss: 1.940019130706787\n",
      "[step: 6968] loss: 1.8158035278320312\n",
      "[step: 6969] loss: 1.8477919101715088\n",
      "[step: 6970] loss: 1.9477579593658447\n",
      "[step: 6971] loss: 1.9297188520431519\n",
      "[step: 6972] loss: 1.830918550491333\n",
      "[step: 6973] loss: 1.7950406074523926\n",
      "[step: 6974] loss: 1.85597562789917\n",
      "[step: 6975] loss: 1.8863210678100586\n",
      "[step: 6976] loss: 1.8450649976730347\n",
      "[step: 6977] loss: 1.8018834590911865\n",
      "[step: 6978] loss: 1.8119158744812012\n",
      "[step: 6979] loss: 1.825969934463501\n",
      "[step: 6980] loss: 1.8187878131866455\n",
      "[step: 6981] loss: 1.809015154838562\n",
      "[step: 6982] loss: 1.811740756034851\n",
      "[step: 6983] loss: 1.807076334953308\n",
      "[step: 6984] loss: 1.7855664491653442\n",
      "[step: 6985] loss: 1.7813575267791748\n",
      "[step: 6986] loss: 1.7960381507873535\n",
      "[step: 6987] loss: 1.8074862957000732\n",
      "[step: 6988] loss: 1.7921581268310547\n",
      "[step: 6989] loss: 1.7732336521148682\n",
      "[step: 6990] loss: 1.7699165344238281\n",
      "[step: 6991] loss: 1.7775001525878906\n",
      "[step: 6992] loss: 1.7809219360351562\n",
      "[step: 6993] loss: 1.776487112045288\n",
      "[step: 6994] loss: 1.7745802402496338\n",
      "[step: 6995] loss: 1.7733335494995117\n",
      "[step: 6996] loss: 1.7695059776306152\n",
      "[step: 6997] loss: 1.7613098621368408\n",
      "[step: 6998] loss: 1.7581149339675903\n",
      "[step: 6999] loss: 1.7609703540802002\n",
      "[step: 7000] loss: 1.7644853591918945\n",
      "[step: 7001] loss: 1.7636669874191284\n",
      "[step: 7002] loss: 1.76054847240448\n",
      "[step: 7003] loss: 1.7590858936309814\n",
      "[step: 7004] loss: 1.7582553625106812\n",
      "[step: 7005] loss: 1.7561006546020508\n",
      "[step: 7006] loss: 1.752018928527832\n",
      "[step: 7007] loss: 1.7493321895599365\n",
      "[step: 7008] loss: 1.7486798763275146\n",
      "[step: 7009] loss: 1.74898099899292\n",
      "[step: 7010] loss: 1.7480952739715576\n",
      "[step: 7011] loss: 1.7468960285186768\n",
      "[step: 7012] loss: 1.7465757131576538\n",
      "[step: 7013] loss: 1.747190237045288\n",
      "[step: 7014] loss: 1.747605800628662\n",
      "[step: 7015] loss: 1.7472856044769287\n",
      "[step: 7016] loss: 1.7472012042999268\n",
      "[step: 7017] loss: 1.7479770183563232\n",
      "[step: 7018] loss: 1.7499336004257202\n",
      "[step: 7019] loss: 1.7524911165237427\n",
      "[step: 7020] loss: 1.7565104961395264\n",
      "[step: 7021] loss: 1.7631280422210693\n",
      "[step: 7022] loss: 1.7750582695007324\n",
      "[step: 7023] loss: 1.7941555976867676\n",
      "[step: 7024] loss: 1.826869249343872\n",
      "[step: 7025] loss: 1.87896728515625\n",
      "[step: 7026] loss: 1.9720771312713623\n",
      "[step: 7027] loss: 2.119171142578125\n",
      "[step: 7028] loss: 2.38006591796875\n",
      "[step: 7029] loss: 2.743260145187378\n",
      "[step: 7030] loss: 3.318453311920166\n",
      "[step: 7031] loss: 3.8122901916503906\n",
      "[step: 7032] loss: 4.217533588409424\n",
      "[step: 7033] loss: 3.779636859893799\n",
      "[step: 7034] loss: 2.889958381652832\n",
      "[step: 7035] loss: 1.99200439453125\n",
      "[step: 7036] loss: 1.8011666536331177\n",
      "[step: 7037] loss: 2.2589163780212402\n",
      "[step: 7038] loss: 2.781616687774658\n",
      "[step: 7039] loss: 2.8885655403137207\n",
      "[step: 7040] loss: 2.3755297660827637\n",
      "[step: 7041] loss: 1.8698737621307373\n",
      "[step: 7042] loss: 1.8406981229782104\n",
      "[step: 7043] loss: 2.2008466720581055\n",
      "[step: 7044] loss: 2.462447166442871\n",
      "[step: 7045] loss: 2.291989326477051\n",
      "[step: 7046] loss: 1.9565937519073486\n",
      "[step: 7047] loss: 1.7786048650741577\n",
      "[step: 7048] loss: 1.90804123878479\n",
      "[step: 7049] loss: 2.1151013374328613\n",
      "[step: 7050] loss: 2.1035757064819336\n",
      "[step: 7051] loss: 1.9225717782974243\n",
      "[step: 7052] loss: 1.766113042831421\n",
      "[step: 7053] loss: 1.7926390171051025\n",
      "[step: 7054] loss: 1.9185667037963867\n",
      "[step: 7055] loss: 1.9613839387893677\n",
      "[step: 7056] loss: 1.8863186836242676\n",
      "[step: 7057] loss: 1.7724347114562988\n",
      "[step: 7058] loss: 1.743150234222412\n",
      "[step: 7059] loss: 1.8019057512283325\n",
      "[step: 7060] loss: 1.8569459915161133\n",
      "[step: 7061] loss: 1.8480455875396729\n",
      "[step: 7062] loss: 1.7863552570343018\n",
      "[step: 7063] loss: 1.7409220933914185\n",
      "[step: 7064] loss: 1.7436115741729736\n",
      "[step: 7065] loss: 1.7751524448394775\n",
      "[step: 7066] loss: 1.7987308502197266\n",
      "[step: 7067] loss: 1.7912089824676514\n",
      "[step: 7068] loss: 1.764160394668579\n",
      "[step: 7069] loss: 1.7382553815841675\n",
      "[step: 7070] loss: 1.729339838027954\n",
      "[step: 7071] loss: 1.737349271774292\n",
      "[step: 7072] loss: 1.7526016235351562\n",
      "[step: 7073] loss: 1.7628005743026733\n",
      "[step: 7074] loss: 1.758169174194336\n",
      "[step: 7075] loss: 1.7423375844955444\n",
      "[step: 7076] loss: 1.7257592678070068\n",
      "[step: 7077] loss: 1.7176263332366943\n",
      "[step: 7078] loss: 1.7192028760910034\n",
      "[step: 7079] loss: 1.7262558937072754\n",
      "[step: 7080] loss: 1.7332353591918945\n",
      "[step: 7081] loss: 1.7360820770263672\n",
      "[step: 7082] loss: 1.7347207069396973\n",
      "[step: 7083] loss: 1.7300138473510742\n",
      "[step: 7084] loss: 1.7234597206115723\n",
      "[step: 7085] loss: 1.71622896194458\n",
      "[step: 7086] loss: 1.7106742858886719\n",
      "[step: 7087] loss: 1.7076987028121948\n",
      "[step: 7088] loss: 1.7069075107574463\n",
      "[step: 7089] loss: 1.7075295448303223\n",
      "[step: 7090] loss: 1.7087879180908203\n",
      "[step: 7091] loss: 1.7102805376052856\n",
      "[step: 7092] loss: 1.7120670080184937\n",
      "[step: 7093] loss: 1.7143634557724\n",
      "[step: 7094] loss: 1.716845154762268\n",
      "[step: 7095] loss: 1.7197346687316895\n",
      "[step: 7096] loss: 1.723268747329712\n",
      "[step: 7097] loss: 1.7286807298660278\n",
      "[step: 7098] loss: 1.7364081144332886\n",
      "[step: 7099] loss: 1.7487773895263672\n",
      "[step: 7100] loss: 1.7666536569595337\n",
      "[step: 7101] loss: 1.7959046363830566\n",
      "[step: 7102] loss: 1.8394043445587158\n",
      "[step: 7103] loss: 1.9132252931594849\n",
      "[step: 7104] loss: 2.021636962890625\n",
      "[step: 7105] loss: 2.2047481536865234\n",
      "[step: 7106] loss: 2.446976661682129\n",
      "[step: 7107] loss: 2.8220982551574707\n",
      "[step: 7108] loss: 3.1703732013702393\n",
      "[step: 7109] loss: 3.53948974609375\n",
      "[step: 7110] loss: 3.458932399749756\n",
      "[step: 7111] loss: 3.067068099975586\n",
      "[step: 7112] loss: 2.3368442058563232\n",
      "[step: 7113] loss: 1.8183820247650146\n",
      "[step: 7114] loss: 1.7497882843017578\n",
      "[step: 7115] loss: 2.0485219955444336\n",
      "[step: 7116] loss: 2.4147098064422607\n",
      "[step: 7117] loss: 2.5034842491149902\n",
      "[step: 7118] loss: 2.2927567958831787\n",
      "[step: 7119] loss: 1.9084258079528809\n",
      "[step: 7120] loss: 1.7247799634933472\n",
      "[step: 7121] loss: 1.8340697288513184\n",
      "[step: 7122] loss: 2.059192657470703\n",
      "[step: 7123] loss: 2.1853556632995605\n",
      "[step: 7124] loss: 2.0811400413513184\n",
      "[step: 7125] loss: 1.877744197845459\n",
      "[step: 7126] loss: 1.726832389831543\n",
      "[step: 7127] loss: 1.7544492483139038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7128] loss: 1.89227294921875\n",
      "[step: 7129] loss: 1.9833292961120605\n",
      "[step: 7130] loss: 1.966055989265442\n",
      "[step: 7131] loss: 1.8402851819992065\n",
      "[step: 7132] loss: 1.7270907163619995\n",
      "[step: 7133] loss: 1.7027928829193115\n",
      "[step: 7134] loss: 1.7683980464935303\n",
      "[step: 7135] loss: 1.850095510482788\n",
      "[step: 7136] loss: 1.8673902750015259\n",
      "[step: 7137] loss: 1.8242580890655518\n",
      "[step: 7138] loss: 1.7469338178634644\n",
      "[step: 7139] loss: 1.6970100402832031\n",
      "[step: 7140] loss: 1.7003501653671265\n",
      "[step: 7141] loss: 1.73819899559021\n",
      "[step: 7142] loss: 1.7749569416046143\n",
      "[step: 7143] loss: 1.7838636636734009\n",
      "[step: 7144] loss: 1.7685859203338623\n",
      "[step: 7145] loss: 1.735114574432373\n",
      "[step: 7146] loss: 1.703862190246582\n",
      "[step: 7147] loss: 1.6877562999725342\n",
      "[step: 7148] loss: 1.6884013414382935\n",
      "[step: 7149] loss: 1.7011234760284424\n",
      "[step: 7150] loss: 1.7189183235168457\n",
      "[step: 7151] loss: 1.7332020998001099\n",
      "[step: 7152] loss: 1.735278844833374\n",
      "[step: 7153] loss: 1.7268688678741455\n",
      "[step: 7154] loss: 1.7117897272109985\n",
      "[step: 7155] loss: 1.6962440013885498\n",
      "[step: 7156] loss: 1.6835863590240479\n",
      "[step: 7157] loss: 1.6761200428009033\n",
      "[step: 7158] loss: 1.672523021697998\n",
      "[step: 7159] loss: 1.6715055704116821\n",
      "[step: 7160] loss: 1.673121452331543\n",
      "[step: 7161] loss: 1.6769118309020996\n",
      "[step: 7162] loss: 1.6820638179779053\n",
      "[step: 7163] loss: 1.6882414817810059\n",
      "[step: 7164] loss: 1.6958177089691162\n",
      "[step: 7165] loss: 1.7039684057235718\n",
      "[step: 7166] loss: 1.7149302959442139\n",
      "[step: 7167] loss: 1.729848861694336\n",
      "[step: 7168] loss: 1.7532174587249756\n",
      "[step: 7169] loss: 1.7854053974151611\n",
      "[step: 7170] loss: 1.8369500637054443\n",
      "[step: 7171] loss: 1.9080054759979248\n",
      "[step: 7172] loss: 2.022304058074951\n",
      "[step: 7173] loss: 2.171863555908203\n",
      "[step: 7174] loss: 2.4081966876983643\n",
      "[step: 7175] loss: 2.66025972366333\n",
      "[step: 7176] loss: 2.99088978767395\n",
      "[step: 7177] loss: 3.1364798545837402\n",
      "[step: 7178] loss: 3.149796962738037\n",
      "[step: 7179] loss: 2.7558891773223877\n",
      "[step: 7180] loss: 2.2496392726898193\n",
      "[step: 7181] loss: 1.8133207559585571\n",
      "[step: 7182] loss: 1.6796311140060425\n",
      "[step: 7183] loss: 1.8302887678146362\n",
      "[step: 7184] loss: 2.0920121669769287\n",
      "[step: 7185] loss: 2.2812061309814453\n",
      "[step: 7186] loss: 2.225222110748291\n",
      "[step: 7187] loss: 2.0116868019104004\n",
      "[step: 7188] loss: 1.7722387313842773\n",
      "[step: 7189] loss: 1.685225486755371\n",
      "[step: 7190] loss: 1.7569488286972046\n",
      "[step: 7191] loss: 1.8891761302947998\n",
      "[step: 7192] loss: 1.9819647073745728\n",
      "[step: 7193] loss: 1.9540674686431885\n",
      "[step: 7194] loss: 1.850939154624939\n",
      "[step: 7195] loss: 1.7295727729797363\n",
      "[step: 7196] loss: 1.6785342693328857\n",
      "[step: 7197] loss: 1.7055366039276123\n",
      "[step: 7198] loss: 1.7722599506378174\n",
      "[step: 7199] loss: 1.8336701393127441\n",
      "[step: 7200] loss: 1.8409724235534668\n",
      "[step: 7201] loss: 1.8013465404510498\n",
      "[step: 7202] loss: 1.7269130945205688\n",
      "[step: 7203] loss: 1.6712088584899902\n",
      "[step: 7204] loss: 1.6555685997009277\n",
      "[step: 7205] loss: 1.6774570941925049\n",
      "[step: 7206] loss: 1.716018557548523\n",
      "[step: 7207] loss: 1.7433607578277588\n",
      "[step: 7208] loss: 1.749475121498108\n",
      "[step: 7209] loss: 1.7284760475158691\n",
      "[step: 7210] loss: 1.6999131441116333\n",
      "[step: 7211] loss: 1.674235224723816\n",
      "[step: 7212] loss: 1.6589797735214233\n",
      "[step: 7213] loss: 1.6526825428009033\n",
      "[step: 7214] loss: 1.6526044607162476\n",
      "[step: 7215] loss: 1.6568126678466797\n",
      "[step: 7216] loss: 1.6634479761123657\n",
      "[step: 7217] loss: 1.672201156616211\n",
      "[step: 7218] loss: 1.681886911392212\n",
      "[step: 7219] loss: 1.6916149854660034\n",
      "[step: 7220] loss: 1.6980082988739014\n",
      "[step: 7221] loss: 1.7026309967041016\n",
      "[step: 7222] loss: 1.705235242843628\n",
      "[step: 7223] loss: 1.709740161895752\n",
      "[step: 7224] loss: 1.715320110321045\n",
      "[step: 7225] loss: 1.726446270942688\n",
      "[step: 7226] loss: 1.7406049966812134\n",
      "[step: 7227] loss: 1.7636237144470215\n",
      "[step: 7228] loss: 1.7921228408813477\n",
      "[step: 7229] loss: 1.8391435146331787\n",
      "[step: 7230] loss: 1.899742603302002\n",
      "[step: 7231] loss: 1.9981305599212646\n",
      "[step: 7232] loss: 2.117497205734253\n",
      "[step: 7233] loss: 2.2983808517456055\n",
      "[step: 7234] loss: 2.475409507751465\n",
      "[step: 7235] loss: 2.6968069076538086\n",
      "[step: 7236] loss: 2.783217191696167\n",
      "[step: 7237] loss: 2.790844440460205\n",
      "[step: 7238] loss: 2.5337438583374023\n",
      "[step: 7239] loss: 2.1972475051879883\n",
      "[step: 7240] loss: 1.8522331714630127\n",
      "[step: 7241] loss: 1.670473575592041\n",
      "[step: 7242] loss: 1.6752376556396484\n",
      "[step: 7243] loss: 1.8082345724105835\n",
      "[step: 7244] loss: 1.974979043006897\n",
      "[step: 7245] loss: 2.064230442047119\n",
      "[step: 7246] loss: 2.0529568195343018\n",
      "[step: 7247] loss: 1.9213718175888062\n",
      "[step: 7248] loss: 1.778801679611206\n",
      "[step: 7249] loss: 1.676767349243164\n",
      "[step: 7250] loss: 1.6535290479660034\n",
      "[step: 7251] loss: 1.6930586099624634\n",
      "[step: 7252] loss: 1.7584813833236694\n",
      "[step: 7253] loss: 1.8147289752960205\n",
      "[step: 7254] loss: 1.8237639665603638\n",
      "[step: 7255] loss: 1.7974742650985718\n",
      "[step: 7256] loss: 1.7363578081130981\n",
      "[step: 7257] loss: 1.678739309310913\n",
      "[step: 7258] loss: 1.638092041015625\n",
      "[step: 7259] loss: 1.6268484592437744\n",
      "[step: 7260] loss: 1.641994833946228\n",
      "[step: 7261] loss: 1.6712254285812378\n",
      "[step: 7262] loss: 1.7016068696975708\n",
      "[step: 7263] loss: 1.7180140018463135\n",
      "[step: 7264] loss: 1.7223650217056274\n",
      "[step: 7265] loss: 1.7097117900848389\n",
      "[step: 7266] loss: 1.692654013633728\n",
      "[step: 7267] loss: 1.6728651523590088\n",
      "[step: 7268] loss: 1.6572418212890625\n",
      "[step: 7269] loss: 1.6449416875839233\n",
      "[step: 7270] loss: 1.6355602741241455\n",
      "[step: 7271] loss: 1.6272695064544678\n",
      "[step: 7272] loss: 1.6202564239501953\n",
      "[step: 7273] loss: 1.6148381233215332\n",
      "[step: 7274] loss: 1.6117585897445679\n",
      "[step: 7275] loss: 1.6111942529678345\n",
      "[step: 7276] loss: 1.6127843856811523\n",
      "[step: 7277] loss: 1.6159487962722778\n",
      "[step: 7278] loss: 1.6202327013015747\n",
      "[step: 7279] loss: 1.6260441541671753\n",
      "[step: 7280] loss: 1.6344878673553467\n",
      "[step: 7281] loss: 1.6490814685821533\n",
      "[step: 7282] loss: 1.6737630367279053\n",
      "[step: 7283] loss: 1.7186291217803955\n",
      "[step: 7284] loss: 1.7938363552093506\n",
      "[step: 7285] loss: 1.9326741695404053\n",
      "[step: 7286] loss: 2.1550850868225098\n",
      "[step: 7287] loss: 2.5607283115386963\n",
      "[step: 7288] loss: 3.105029344558716\n",
      "[step: 7289] loss: 3.9322972297668457\n",
      "[step: 7290] loss: 4.395679473876953\n",
      "[step: 7291] loss: 4.39440393447876\n",
      "[step: 7292] loss: 3.2257442474365234\n",
      "[step: 7293] loss: 2.0129613876342773\n",
      "[step: 7294] loss: 1.6949129104614258\n",
      "[step: 7295] loss: 2.3120689392089844\n",
      "[step: 7296] loss: 2.9973177909851074\n",
      "[step: 7297] loss: 2.8832602500915527\n",
      "[step: 7298] loss: 2.217974901199341\n",
      "[step: 7299] loss: 1.773247480392456\n",
      "[step: 7300] loss: 2.018582344055176\n",
      "[step: 7301] loss: 2.3841311931610107\n",
      "[step: 7302] loss: 2.238954782485962\n",
      "[step: 7303] loss: 1.875083565711975\n",
      "[step: 7304] loss: 1.8300817012786865\n",
      "[step: 7305] loss: 2.0455121994018555\n",
      "[step: 7306] loss: 2.048684597015381\n",
      "[step: 7307] loss: 1.7761147022247314\n",
      "[step: 7308] loss: 1.6701831817626953\n",
      "[step: 7309] loss: 1.8435617685317993\n",
      "[step: 7310] loss: 1.9600224494934082\n",
      "[step: 7311] loss: 1.8271079063415527\n",
      "[step: 7312] loss: 1.6439402103424072\n",
      "[step: 7313] loss: 1.6760196685791016\n",
      "[step: 7314] loss: 1.8123048543930054\n",
      "[step: 7315] loss: 1.8092968463897705\n",
      "[step: 7316] loss: 1.6871215105056763\n",
      "[step: 7317] loss: 1.6368529796600342\n",
      "[step: 7318] loss: 1.7011151313781738\n",
      "[step: 7319] loss: 1.7446601390838623\n",
      "[step: 7320] loss: 1.6867437362670898\n",
      "[step: 7321] loss: 1.6206947565078735\n",
      "[step: 7322] loss: 1.6349425315856934\n",
      "[step: 7323] loss: 1.6882402896881104\n",
      "[step: 7324] loss: 1.6940386295318604\n",
      "[step: 7325] loss: 1.6424732208251953\n",
      "[step: 7326] loss: 1.608238935470581\n",
      "[step: 7327] loss: 1.62404203414917\n",
      "[step: 7328] loss: 1.6512371301651\n",
      "[step: 7329] loss: 1.647797703742981\n",
      "[step: 7330] loss: 1.6222600936889648\n",
      "[step: 7331] loss: 1.609490156173706\n",
      "[step: 7332] loss: 1.617849588394165\n",
      "[step: 7333] loss: 1.627573013305664\n",
      "[step: 7334] loss: 1.6207358837127686\n",
      "[step: 7335] loss: 1.6035315990447998\n",
      "[step: 7336] loss: 1.5950672626495361\n",
      "[step: 7337] loss: 1.6021227836608887\n",
      "[step: 7338] loss: 1.6120201349258423\n",
      "[step: 7339] loss: 1.6112818717956543\n",
      "[step: 7340] loss: 1.6004571914672852\n",
      "[step: 7341] loss: 1.5903595685958862\n",
      "[step: 7342] loss: 1.5885237455368042\n",
      "[step: 7343] loss: 1.5930733680725098\n",
      "[step: 7344] loss: 1.5962209701538086\n",
      "[step: 7345] loss: 1.593109130859375\n",
      "[step: 7346] loss: 1.586479663848877\n",
      "[step: 7347] loss: 1.5819653272628784\n",
      "[step: 7348] loss: 1.5821537971496582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7349] loss: 1.584892749786377\n",
      "[step: 7350] loss: 1.586477279663086\n",
      "[step: 7351] loss: 1.5850026607513428\n",
      "[step: 7352] loss: 1.5816786289215088\n",
      "[step: 7353] loss: 1.5792734622955322\n",
      "[step: 7354] loss: 1.579190969467163\n",
      "[step: 7355] loss: 1.5806756019592285\n",
      "[step: 7356] loss: 1.582076072692871\n",
      "[step: 7357] loss: 1.5825074911117554\n",
      "[step: 7358] loss: 1.5821974277496338\n",
      "[step: 7359] loss: 1.5830779075622559\n",
      "[step: 7360] loss: 1.5866402387619019\n",
      "[step: 7361] loss: 1.5946005582809448\n",
      "[step: 7362] loss: 1.6081160306930542\n",
      "[step: 7363] loss: 1.6311063766479492\n",
      "[step: 7364] loss: 1.668062448501587\n",
      "[step: 7365] loss: 1.7344344854354858\n",
      "[step: 7366] loss: 1.843261480331421\n",
      "[step: 7367] loss: 2.046231269836426\n",
      "[step: 7368] loss: 2.3499979972839355\n",
      "[step: 7369] loss: 2.8888988494873047\n",
      "[step: 7370] loss: 3.4604923725128174\n",
      "[step: 7371] loss: 4.194884777069092\n",
      "[step: 7372] loss: 4.140374660491943\n",
      "[step: 7373] loss: 3.539165496826172\n",
      "[step: 7374] loss: 2.331112861633301\n",
      "[step: 7375] loss: 1.640621304512024\n",
      "[step: 7376] loss: 1.8428444862365723\n",
      "[step: 7377] loss: 2.5196423530578613\n",
      "[step: 7378] loss: 2.9653046131134033\n",
      "[step: 7379] loss: 2.5950818061828613\n",
      "[step: 7380] loss: 1.9182389974594116\n",
      "[step: 7381] loss: 1.6302132606506348\n",
      "[step: 7382] loss: 1.9635355472564697\n",
      "[step: 7383] loss: 2.3729946613311768\n",
      "[step: 7384] loss: 2.3001503944396973\n",
      "[step: 7385] loss: 1.9408313035964966\n",
      "[step: 7386] loss: 1.6927639245986938\n",
      "[step: 7387] loss: 1.7757606506347656\n",
      "[step: 7388] loss: 1.9427564144134521\n",
      "[step: 7389] loss: 1.9276481866836548\n",
      "[step: 7390] loss: 1.8080227375030518\n",
      "[step: 7391] loss: 1.7258436679840088\n",
      "[step: 7392] loss: 1.7347359657287598\n",
      "[step: 7393] loss: 1.7432184219360352\n",
      "[step: 7394] loss: 1.7164052724838257\n",
      "[step: 7395] loss: 1.7108747959136963\n",
      "[step: 7396] loss: 1.718348741531372\n",
      "[step: 7397] loss: 1.7058151960372925\n",
      "[step: 7398] loss: 1.658594012260437\n",
      "[step: 7399] loss: 1.6194456815719604\n",
      "[step: 7400] loss: 1.6364946365356445\n",
      "[step: 7401] loss: 1.676910161972046\n",
      "[step: 7402] loss: 1.6767191886901855\n",
      "[step: 7403] loss: 1.6240200996398926\n",
      "[step: 7404] loss: 1.5795577764511108\n",
      "[step: 7405] loss: 1.589108943939209\n",
      "[step: 7406] loss: 1.62650465965271\n",
      "[step: 7407] loss: 1.6388565301895142\n",
      "[step: 7408] loss: 1.6085290908813477\n",
      "[step: 7409] loss: 1.5716862678527832\n",
      "[step: 7410] loss: 1.565022349357605\n",
      "[step: 7411] loss: 1.5860651731491089\n",
      "[step: 7412] loss: 1.6023763418197632\n",
      "[step: 7413] loss: 1.5927705764770508\n",
      "[step: 7414] loss: 1.5703213214874268\n",
      "[step: 7415] loss: 1.5589576959609985\n",
      "[step: 7416] loss: 1.5652393102645874\n",
      "[step: 7417] loss: 1.5753165483474731\n",
      "[step: 7418] loss: 1.574932336807251\n",
      "[step: 7419] loss: 1.563866138458252\n",
      "[step: 7420] loss: 1.5537121295928955\n",
      "[step: 7421] loss: 1.5533392429351807\n",
      "[step: 7422] loss: 1.5598899126052856\n",
      "[step: 7423] loss: 1.563950777053833\n",
      "[step: 7424] loss: 1.5602318048477173\n",
      "[step: 7425] loss: 1.5525696277618408\n",
      "[step: 7426] loss: 1.547196626663208\n",
      "[step: 7427] loss: 1.5470644235610962\n",
      "[step: 7428] loss: 1.5496673583984375\n",
      "[step: 7429] loss: 1.5504465103149414\n",
      "[step: 7430] loss: 1.5476552248001099\n",
      "[step: 7431] loss: 1.5434961318969727\n",
      "[step: 7432] loss: 1.541031002998352\n",
      "[step: 7433] loss: 1.5413014888763428\n",
      "[step: 7434] loss: 1.5429035425186157\n",
      "[step: 7435] loss: 1.5435144901275635\n",
      "[step: 7436] loss: 1.5423076152801514\n",
      "[step: 7437] loss: 1.5402357578277588\n",
      "[step: 7438] loss: 1.5390710830688477\n",
      "[step: 7439] loss: 1.5396653413772583\n",
      "[step: 7440] loss: 1.5419645309448242\n",
      "[step: 7441] loss: 1.5453652143478394\n",
      "[step: 7442] loss: 1.5501655340194702\n",
      "[step: 7443] loss: 1.5578035116195679\n",
      "[step: 7444] loss: 1.5723254680633545\n",
      "[step: 7445] loss: 1.5985288619995117\n",
      "[step: 7446] loss: 1.648008108139038\n",
      "[step: 7447] loss: 1.7333990335464478\n",
      "[step: 7448] loss: 1.8961834907531738\n",
      "[step: 7449] loss: 2.1629714965820312\n",
      "[step: 7450] loss: 2.6615993976593018\n",
      "[step: 7451] loss: 3.3160111904144287\n",
      "[step: 7452] loss: 4.281792640686035\n",
      "[step: 7453] loss: 4.638257026672363\n",
      "[step: 7454] loss: 4.311683654785156\n",
      "[step: 7455] loss: 2.8121399879455566\n",
      "[step: 7456] loss: 1.7080978155136108\n",
      "[step: 7457] loss: 1.8235156536102295\n",
      "[step: 7458] loss: 2.689180612564087\n",
      "[step: 7459] loss: 3.1779990196228027\n",
      "[step: 7460] loss: 2.628443717956543\n",
      "[step: 7461] loss: 1.889822244644165\n",
      "[step: 7462] loss: 1.7818946838378906\n",
      "[step: 7463] loss: 2.204482078552246\n",
      "[step: 7464] loss: 2.3643081188201904\n",
      "[step: 7465] loss: 2.0475893020629883\n",
      "[step: 7466] loss: 1.8435945510864258\n",
      "[step: 7467] loss: 1.9072864055633545\n",
      "[step: 7468] loss: 1.932253122329712\n",
      "[step: 7469] loss: 1.7918510437011719\n",
      "[step: 7470] loss: 1.7516149282455444\n",
      "[step: 7471] loss: 1.8740344047546387\n",
      "[step: 7472] loss: 1.8706111907958984\n",
      "[step: 7473] loss: 1.6792893409729004\n",
      "[step: 7474] loss: 1.6078808307647705\n",
      "[step: 7475] loss: 1.7444573640823364\n",
      "[step: 7476] loss: 1.81333589553833\n",
      "[step: 7477] loss: 1.6860969066619873\n",
      "[step: 7478] loss: 1.5553492307662964\n",
      "[step: 7479] loss: 1.6111719608306885\n",
      "[step: 7480] loss: 1.7115943431854248\n",
      "[step: 7481] loss: 1.6713941097259521\n",
      "[step: 7482] loss: 1.5695586204528809\n",
      "[step: 7483] loss: 1.5621509552001953\n",
      "[step: 7484] loss: 1.6239724159240723\n",
      "[step: 7485] loss: 1.6310234069824219\n",
      "[step: 7486] loss: 1.573572039604187\n",
      "[step: 7487] loss: 1.5484727621078491\n",
      "[step: 7488] loss: 1.5820035934448242\n",
      "[step: 7489] loss: 1.5991928577423096\n",
      "[step: 7490] loss: 1.5684709548950195\n",
      "[step: 7491] loss: 1.534508228302002\n",
      "[step: 7492] loss: 1.5453969240188599\n",
      "[step: 7493] loss: 1.5724036693572998\n",
      "[step: 7494] loss: 1.56805419921875\n",
      "[step: 7495] loss: 1.5402644872665405\n",
      "[step: 7496] loss: 1.5270912647247314\n",
      "[step: 7497] loss: 1.5385520458221436\n",
      "[step: 7498] loss: 1.5488898754119873\n",
      "[step: 7499] loss: 1.5414657592773438\n",
      "[step: 7500] loss: 1.5289175510406494\n",
      "[step: 7501] loss: 1.5280665159225464\n",
      "[step: 7502] loss: 1.5339834690093994\n",
      "[step: 7503] loss: 1.5332012176513672\n",
      "[step: 7504] loss: 1.5235188007354736\n",
      "[step: 7505] loss: 1.516528844833374\n",
      "[step: 7506] loss: 1.5188896656036377\n",
      "[step: 7507] loss: 1.5246882438659668\n",
      "[step: 7508] loss: 1.5244849920272827\n",
      "[step: 7509] loss: 1.518542766571045\n",
      "[step: 7510] loss: 1.51365065574646\n",
      "[step: 7511] loss: 1.5138561725616455\n",
      "[step: 7512] loss: 1.5158896446228027\n",
      "[step: 7513] loss: 1.5147767066955566\n",
      "[step: 7514] loss: 1.5105775594711304\n",
      "[step: 7515] loss: 1.5070933103561401\n",
      "[step: 7516] loss: 1.5069103240966797\n",
      "[step: 7517] loss: 1.5081853866577148\n",
      "[step: 7518] loss: 1.5081310272216797\n",
      "[step: 7519] loss: 1.5060683488845825\n",
      "[step: 7520] loss: 1.5040068626403809\n",
      "[step: 7521] loss: 1.503482460975647\n",
      "[step: 7522] loss: 1.5042915344238281\n",
      "[step: 7523] loss: 1.5047276020050049\n",
      "[step: 7524] loss: 1.504106044769287\n",
      "[step: 7525] loss: 1.503218173980713\n",
      "[step: 7526] loss: 1.5034632682800293\n",
      "[step: 7527] loss: 1.5053561925888062\n",
      "[step: 7528] loss: 1.5087982416152954\n",
      "[step: 7529] loss: 1.5138089656829834\n",
      "[step: 7530] loss: 1.5221490859985352\n",
      "[step: 7531] loss: 1.5367026329040527\n",
      "[step: 7532] loss: 1.5638660192489624\n",
      "[step: 7533] loss: 1.610388159751892\n",
      "[step: 7534] loss: 1.6966357231140137\n",
      "[step: 7535] loss: 1.8392266035079956\n",
      "[step: 7536] loss: 2.1078357696533203\n",
      "[step: 7537] loss: 2.5102550983428955\n",
      "[step: 7538] loss: 3.20796275138855\n",
      "[step: 7539] loss: 3.880375623703003\n",
      "[step: 7540] loss: 4.556547164916992\n",
      "[step: 7541] loss: 4.064476490020752\n",
      "[step: 7542] loss: 2.9162750244140625\n",
      "[step: 7543] loss: 1.7608058452606201\n",
      "[step: 7544] loss: 1.6440966129302979\n",
      "[step: 7545] loss: 2.3657522201538086\n",
      "[step: 7546] loss: 2.951159954071045\n",
      "[step: 7547] loss: 2.8162031173706055\n",
      "[step: 7548] loss: 2.015669822692871\n",
      "[step: 7549] loss: 1.6147392988204956\n",
      "[step: 7550] loss: 1.9150831699371338\n",
      "[step: 7551] loss: 2.3144612312316895\n",
      "[step: 7552] loss: 2.277719020843506\n",
      "[step: 7553] loss: 1.8713643550872803\n",
      "[step: 7554] loss: 1.660150408744812\n",
      "[step: 7555] loss: 1.7468256950378418\n",
      "[step: 7556] loss: 1.8834006786346436\n",
      "[step: 7557] loss: 1.8922948837280273\n",
      "[step: 7558] loss: 1.751024842262268\n",
      "[step: 7559] loss: 1.6413967609405518\n",
      "[step: 7560] loss: 1.635725736618042\n",
      "[step: 7561] loss: 1.6998217105865479\n",
      "[step: 7562] loss: 1.7530412673950195\n",
      "[step: 7563] loss: 1.6843383312225342\n",
      "[step: 7564] loss: 1.5774428844451904\n",
      "[step: 7565] loss: 1.5502009391784668\n",
      "[step: 7566] loss: 1.6246602535247803\n",
      "[step: 7567] loss: 1.6851580142974854\n",
      "[step: 7568] loss: 1.6151186227798462\n",
      "[step: 7569] loss: 1.523651361465454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7570] loss: 1.518066644668579\n",
      "[step: 7571] loss: 1.5829274654388428\n",
      "[step: 7572] loss: 1.6130421161651611\n",
      "[step: 7573] loss: 1.562766194343567\n",
      "[step: 7574] loss: 1.5113003253936768\n",
      "[step: 7575] loss: 1.5111591815948486\n",
      "[step: 7576] loss: 1.5415518283843994\n",
      "[step: 7577] loss: 1.5495405197143555\n",
      "[step: 7578] loss: 1.5276024341583252\n",
      "[step: 7579] loss: 1.5125833749771118\n",
      "[step: 7580] loss: 1.5151029825210571\n",
      "[step: 7581] loss: 1.51556396484375\n",
      "[step: 7582] loss: 1.5050221681594849\n",
      "[step: 7583] loss: 1.4976418018341064\n",
      "[step: 7584] loss: 1.5048320293426514\n",
      "[step: 7585] loss: 1.5145519971847534\n",
      "[step: 7586] loss: 1.5108126401901245\n",
      "[step: 7587] loss: 1.4948925971984863\n",
      "[step: 7588] loss: 1.4835258722305298\n",
      "[step: 7589] loss: 1.4851958751678467\n",
      "[step: 7590] loss: 1.4921987056732178\n",
      "[step: 7591] loss: 1.4943082332611084\n",
      "[step: 7592] loss: 1.4908331632614136\n",
      "[step: 7593] loss: 1.488480806350708\n",
      "[step: 7594] loss: 1.488543152809143\n",
      "[step: 7595] loss: 1.4878321886062622\n",
      "[step: 7596] loss: 1.482954740524292\n",
      "[step: 7597] loss: 1.4767937660217285\n",
      "[step: 7598] loss: 1.4736956357955933\n",
      "[step: 7599] loss: 1.4744576215744019\n",
      "[step: 7600] loss: 1.4758894443511963\n",
      "[step: 7601] loss: 1.4753587245941162\n",
      "[step: 7602] loss: 1.4737581014633179\n",
      "[step: 7603] loss: 1.473388910293579\n",
      "[step: 7604] loss: 1.474766492843628\n",
      "[step: 7605] loss: 1.476263165473938\n",
      "[step: 7606] loss: 1.4767735004425049\n",
      "[step: 7607] loss: 1.4764288663864136\n",
      "[step: 7608] loss: 1.476994276046753\n",
      "[step: 7609] loss: 1.4792721271514893\n",
      "[step: 7610] loss: 1.4833500385284424\n",
      "[step: 7611] loss: 1.4886037111282349\n",
      "[step: 7612] loss: 1.4967074394226074\n",
      "[step: 7613] loss: 1.5093588829040527\n",
      "[step: 7614] loss: 1.5315327644348145\n",
      "[step: 7615] loss: 1.5663723945617676\n",
      "[step: 7616] loss: 1.6264653205871582\n",
      "[step: 7617] loss: 1.71883225440979\n",
      "[step: 7618] loss: 1.8810124397277832\n",
      "[step: 7619] loss: 2.1142420768737793\n",
      "[step: 7620] loss: 2.505685806274414\n",
      "[step: 7621] loss: 2.9335639476776123\n",
      "[step: 7622] loss: 3.4875659942626953\n",
      "[step: 7623] loss: 3.5928916931152344\n",
      "[step: 7624] loss: 3.337212562561035\n",
      "[step: 7625] loss: 2.453754425048828\n",
      "[step: 7626] loss: 1.7006527185440063\n",
      "[step: 7627] loss: 1.5078247785568237\n",
      "[step: 7628] loss: 1.862095594406128\n",
      "[step: 7629] loss: 2.343383312225342\n",
      "[step: 7630] loss: 2.442063093185425\n",
      "[step: 7631] loss: 2.134685754776001\n",
      "[step: 7632] loss: 1.6574177742004395\n",
      "[step: 7633] loss: 1.5221867561340332\n",
      "[step: 7634] loss: 1.7549681663513184\n",
      "[step: 7635] loss: 2.008727550506592\n",
      "[step: 7636] loss: 2.029125452041626\n",
      "[step: 7637] loss: 1.7806605100631714\n",
      "[step: 7638] loss: 1.5585263967514038\n",
      "[step: 7639] loss: 1.5368248224258423\n",
      "[step: 7640] loss: 1.6830370426177979\n",
      "[step: 7641] loss: 1.8064979314804077\n",
      "[step: 7642] loss: 1.7536581754684448\n",
      "[step: 7643] loss: 1.6155691146850586\n",
      "[step: 7644] loss: 1.5011377334594727\n",
      "[step: 7645] loss: 1.5107088088989258\n",
      "[step: 7646] loss: 1.607102632522583\n",
      "[step: 7647] loss: 1.6657629013061523\n",
      "[step: 7648] loss: 1.6343259811401367\n",
      "[step: 7649] loss: 1.533660650253296\n",
      "[step: 7650] loss: 1.4712119102478027\n",
      "[step: 7651] loss: 1.4848706722259521\n",
      "[step: 7652] loss: 1.541712760925293\n",
      "[step: 7653] loss: 1.5783910751342773\n",
      "[step: 7654] loss: 1.552344799041748\n",
      "[step: 7655] loss: 1.502968668937683\n",
      "[step: 7656] loss: 1.4706387519836426\n",
      "[step: 7657] loss: 1.4701248407363892\n",
      "[step: 7658] loss: 1.4880685806274414\n",
      "[step: 7659] loss: 1.5034575462341309\n",
      "[step: 7660] loss: 1.5072929859161377\n",
      "[step: 7661] loss: 1.4994192123413086\n",
      "[step: 7662] loss: 1.4869005680084229\n",
      "[step: 7663] loss: 1.4710383415222168\n",
      "[step: 7664] loss: 1.4577898979187012\n",
      "[step: 7665] loss: 1.4536850452423096\n",
      "[step: 7666] loss: 1.4602646827697754\n",
      "[step: 7667] loss: 1.4722461700439453\n",
      "[step: 7668] loss: 1.4810975790023804\n",
      "[step: 7669] loss: 1.4817724227905273\n",
      "[step: 7670] loss: 1.474004864692688\n",
      "[step: 7671] loss: 1.4643628597259521\n",
      "[step: 7672] loss: 1.4564945697784424\n",
      "[step: 7673] loss: 1.4508641958236694\n",
      "[step: 7674] loss: 1.4464004039764404\n",
      "[step: 7675] loss: 1.4431345462799072\n",
      "[step: 7676] loss: 1.4416011571884155\n",
      "[step: 7677] loss: 1.4425921440124512\n",
      "[step: 7678] loss: 1.445608377456665\n",
      "[step: 7679] loss: 1.4489719867706299\n",
      "[step: 7680] loss: 1.4519857168197632\n",
      "[step: 7681] loss: 1.4548375606536865\n",
      "[step: 7682] loss: 1.4590095281600952\n",
      "[step: 7683] loss: 1.4650311470031738\n",
      "[step: 7684] loss: 1.4743376970291138\n",
      "[step: 7685] loss: 1.4870073795318604\n",
      "[step: 7686] loss: 1.5066144466400146\n",
      "[step: 7687] loss: 1.5347856283187866\n",
      "[step: 7688] loss: 1.5813785791397095\n",
      "[step: 7689] loss: 1.648728370666504\n",
      "[step: 7690] loss: 1.761718988418579\n",
      "[step: 7691] loss: 1.9177405834197998\n",
      "[step: 7692] loss: 2.1768646240234375\n",
      "[step: 7693] loss: 2.4780449867248535\n",
      "[step: 7694] loss: 2.904717206954956\n",
      "[step: 7695] loss: 3.1346096992492676\n",
      "[step: 7696] loss: 3.2144150733947754\n",
      "[step: 7697] loss: 2.7378594875335693\n",
      "[step: 7698] loss: 2.097902297973633\n",
      "[step: 7699] loss: 1.5750155448913574\n",
      "[step: 7700] loss: 1.4798603057861328\n",
      "[step: 7701] loss: 1.7407714128494263\n",
      "[step: 7702] loss: 2.0651493072509766\n",
      "[step: 7703] loss: 2.210476875305176\n",
      "[step: 7704] loss: 2.0035436153411865\n",
      "[step: 7705] loss: 1.684767484664917\n",
      "[step: 7706] loss: 1.490368127822876\n",
      "[step: 7707] loss: 1.5449299812316895\n",
      "[step: 7708] loss: 1.7226911783218384\n",
      "[step: 7709] loss: 1.8315783739089966\n",
      "[step: 7710] loss: 1.805722951889038\n",
      "[step: 7711] loss: 1.640291690826416\n",
      "[step: 7712] loss: 1.5047513246536255\n",
      "[step: 7713] loss: 1.4827933311462402\n",
      "[step: 7714] loss: 1.5600378513336182\n",
      "[step: 7715] loss: 1.6530765295028687\n",
      "[step: 7716] loss: 1.6734648942947388\n",
      "[step: 7717] loss: 1.621349573135376\n",
      "[step: 7718] loss: 1.513988733291626\n",
      "[step: 7719] loss: 1.447583794593811\n",
      "[step: 7720] loss: 1.4569119215011597\n",
      "[step: 7721] loss: 1.516337513923645\n",
      "[step: 7722] loss: 1.5737380981445312\n",
      "[step: 7723] loss: 1.5732704401016235\n",
      "[step: 7724] loss: 1.5264921188354492\n",
      "[step: 7725] loss: 1.463486671447754\n",
      "[step: 7726] loss: 1.4332127571105957\n",
      "[step: 7727] loss: 1.4418237209320068\n",
      "[step: 7728] loss: 1.4694993495941162\n",
      "[step: 7729] loss: 1.4943935871124268\n",
      "[step: 7730] loss: 1.4976699352264404\n",
      "[step: 7731] loss: 1.4865477085113525\n",
      "[step: 7732] loss: 1.4699634313583374\n",
      "[step: 7733] loss: 1.4553993940353394\n",
      "[step: 7734] loss: 1.440964937210083\n",
      "[step: 7735] loss: 1.4300580024719238\n",
      "[step: 7736] loss: 1.4252705574035645\n",
      "[step: 7737] loss: 1.4274463653564453\n",
      "[step: 7738] loss: 1.436655044555664\n",
      "[step: 7739] loss: 1.4490467309951782\n",
      "[step: 7740] loss: 1.4587032794952393\n",
      "[step: 7741] loss: 1.4615983963012695\n",
      "[step: 7742] loss: 1.461267352104187\n",
      "[step: 7743] loss: 1.4586632251739502\n",
      "[step: 7744] loss: 1.457012414932251\n",
      "[step: 7745] loss: 1.4559615850448608\n",
      "[step: 7746] loss: 1.456001877784729\n",
      "[step: 7747] loss: 1.4544239044189453\n",
      "[step: 7748] loss: 1.4539281129837036\n",
      "[step: 7749] loss: 1.4549516439437866\n",
      "[step: 7750] loss: 1.4605913162231445\n",
      "[step: 7751] loss: 1.4705896377563477\n",
      "[step: 7752] loss: 1.4882545471191406\n",
      "[step: 7753] loss: 1.513235330581665\n",
      "[step: 7754] loss: 1.5546784400939941\n",
      "[step: 7755] loss: 1.6135306358337402\n",
      "[step: 7756] loss: 1.7119412422180176\n",
      "[step: 7757] loss: 1.8467521667480469\n",
      "[step: 7758] loss: 2.069410562515259\n",
      "[step: 7759] loss: 2.329735279083252\n",
      "[step: 7760] loss: 2.7039525508880615\n",
      "[step: 7761] loss: 2.9366440773010254\n",
      "[step: 7762] loss: 3.0800671577453613\n",
      "[step: 7763] loss: 2.745262384414673\n",
      "[step: 7764] loss: 2.220235824584961\n",
      "[step: 7765] loss: 1.6680209636688232\n",
      "[step: 7766] loss: 1.4313764572143555\n",
      "[step: 7767] loss: 1.5478382110595703\n",
      "[step: 7768] loss: 1.837709903717041\n",
      "[step: 7769] loss: 2.0695559978485107\n",
      "[step: 7770] loss: 2.024596691131592\n",
      "[step: 7771] loss: 1.7918658256530762\n",
      "[step: 7772] loss: 1.5317515134811401\n",
      "[step: 7773] loss: 1.4533958435058594\n",
      "[step: 7774] loss: 1.5480139255523682\n",
      "[step: 7775] loss: 1.68169367313385\n",
      "[step: 7776] loss: 1.7456152439117432\n",
      "[step: 7777] loss: 1.6808974742889404\n",
      "[step: 7778] loss: 1.5688347816467285\n",
      "[step: 7779] loss: 1.4766430854797363\n",
      "[step: 7780] loss: 1.4625558853149414\n",
      "[step: 7781] loss: 1.5044898986816406\n",
      "[step: 7782] loss: 1.5554986000061035\n",
      "[step: 7783] loss: 1.586970329284668\n",
      "[step: 7784] loss: 1.5676149129867554\n",
      "[step: 7785] loss: 1.520831823348999\n",
      "[step: 7786] loss: 1.4589825868606567\n",
      "[step: 7787] loss: 1.4226794242858887\n",
      "[step: 7788] loss: 1.4259591102600098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7789] loss: 1.459499478340149\n",
      "[step: 7790] loss: 1.4970128536224365\n",
      "[step: 7791] loss: 1.5061533451080322\n",
      "[step: 7792] loss: 1.4884058237075806\n",
      "[step: 7793] loss: 1.4473158121109009\n",
      "[step: 7794] loss: 1.4130181074142456\n",
      "[step: 7795] loss: 1.400752067565918\n",
      "[step: 7796] loss: 1.40925133228302\n",
      "[step: 7797] loss: 1.424894094467163\n",
      "[step: 7798] loss: 1.435417890548706\n",
      "[step: 7799] loss: 1.438955307006836\n",
      "[step: 7800] loss: 1.4350595474243164\n",
      "[step: 7801] loss: 1.431717872619629\n",
      "[step: 7802] loss: 1.4309053421020508\n",
      "[step: 7803] loss: 1.4311838150024414\n",
      "[step: 7804] loss: 1.4273134469985962\n",
      "[step: 7805] loss: 1.4202022552490234\n",
      "[step: 7806] loss: 1.4107586145401\n",
      "[step: 7807] loss: 1.4026094675064087\n",
      "[step: 7808] loss: 1.397636890411377\n",
      "[step: 7809] loss: 1.3963539600372314\n",
      "[step: 7810] loss: 1.3964755535125732\n",
      "[step: 7811] loss: 1.395966649055481\n",
      "[step: 7812] loss: 1.3939435482025146\n",
      "[step: 7813] loss: 1.3915010690689087\n",
      "[step: 7814] loss: 1.3896782398223877\n",
      "[step: 7815] loss: 1.389782428741455\n",
      "[step: 7816] loss: 1.3923242092132568\n",
      "[step: 7817] loss: 1.3975285291671753\n",
      "[step: 7818] loss: 1.4057002067565918\n",
      "[step: 7819] loss: 1.4198925495147705\n",
      "[step: 7820] loss: 1.444675087928772\n",
      "[step: 7821] loss: 1.4924473762512207\n",
      "[step: 7822] loss: 1.5780467987060547\n",
      "[step: 7823] loss: 1.7463380098342896\n",
      "[step: 7824] loss: 2.033918857574463\n",
      "[step: 7825] loss: 2.5861690044403076\n",
      "[step: 7826] loss: 3.33728289604187\n",
      "[step: 7827] loss: 4.4570465087890625\n",
      "[step: 7828] loss: 4.818692207336426\n",
      "[step: 7829] loss: 4.2975006103515625\n",
      "[step: 7830] loss: 2.538296937942505\n",
      "[step: 7831] loss: 1.4955811500549316\n",
      "[step: 7832] loss: 1.9367411136627197\n",
      "[step: 7833] loss: 2.942505359649658\n",
      "[step: 7834] loss: 3.1333515644073486\n",
      "[step: 7835] loss: 2.2253479957580566\n",
      "[step: 7836] loss: 1.6196964979171753\n",
      "[step: 7837] loss: 1.9357752799987793\n",
      "[step: 7838] loss: 2.319284439086914\n",
      "[step: 7839] loss: 2.052640914916992\n",
      "[step: 7840] loss: 1.6525920629501343\n",
      "[step: 7841] loss: 1.7772884368896484\n",
      "[step: 7842] loss: 1.999053955078125\n",
      "[step: 7843] loss: 1.7462915182113647\n",
      "[step: 7844] loss: 1.4865270853042603\n",
      "[step: 7845] loss: 1.6679253578186035\n",
      "[step: 7846] loss: 1.861086130142212\n",
      "[step: 7847] loss: 1.6850610971450806\n",
      "[step: 7848] loss: 1.4348057508468628\n",
      "[step: 7849] loss: 1.5309293270111084\n",
      "[step: 7850] loss: 1.7155855894088745\n",
      "[step: 7851] loss: 1.6138687133789062\n",
      "[step: 7852] loss: 1.4429105520248413\n",
      "[step: 7853] loss: 1.4741432666778564\n",
      "[step: 7854] loss: 1.5822875499725342\n",
      "[step: 7855] loss: 1.533822774887085\n",
      "[step: 7856] loss: 1.4248884916305542\n",
      "[step: 7857] loss: 1.4479873180389404\n",
      "[step: 7858] loss: 1.5276447534561157\n",
      "[step: 7859] loss: 1.4958701133728027\n",
      "[step: 7860] loss: 1.4144316911697388\n",
      "[step: 7861] loss: 1.408975601196289\n",
      "[step: 7862] loss: 1.4663262367248535\n",
      "[step: 7863] loss: 1.4762908220291138\n",
      "[step: 7864] loss: 1.425940752029419\n",
      "[step: 7865] loss: 1.3968865871429443\n",
      "[step: 7866] loss: 1.41877281665802\n",
      "[step: 7867] loss: 1.4374046325683594\n",
      "[step: 7868] loss: 1.4192910194396973\n",
      "[step: 7869] loss: 1.3961234092712402\n",
      "[step: 7870] loss: 1.4002548456192017\n",
      "[step: 7871] loss: 1.4149489402770996\n",
      "[step: 7872] loss: 1.409205436706543\n",
      "[step: 7873] loss: 1.3899303674697876\n",
      "[step: 7874] loss: 1.3814306259155273\n",
      "[step: 7875] loss: 1.3925836086273193\n",
      "[step: 7876] loss: 1.4015929698944092\n",
      "[step: 7877] loss: 1.3942068815231323\n",
      "[step: 7878] loss: 1.3790578842163086\n",
      "[step: 7879] loss: 1.3741583824157715\n",
      "[step: 7880] loss: 1.3804067373275757\n",
      "[step: 7881] loss: 1.3855476379394531\n",
      "[step: 7882] loss: 1.3814915418624878\n",
      "[step: 7883] loss: 1.373283863067627\n",
      "[step: 7884] loss: 1.369812250137329\n",
      "[step: 7885] loss: 1.3727778196334839\n",
      "[step: 7886] loss: 1.3761169910430908\n",
      "[step: 7887] loss: 1.373987078666687\n",
      "[step: 7888] loss: 1.3685288429260254\n",
      "[step: 7889] loss: 1.364667534828186\n",
      "[step: 7890] loss: 1.3650362491607666\n",
      "[step: 7891] loss: 1.3668540716171265\n",
      "[step: 7892] loss: 1.366858720779419\n",
      "[step: 7893] loss: 1.3643336296081543\n",
      "[step: 7894] loss: 1.3616217374801636\n",
      "[step: 7895] loss: 1.3606975078582764\n",
      "[step: 7896] loss: 1.3614814281463623\n",
      "[step: 7897] loss: 1.3620574474334717\n",
      "[step: 7898] loss: 1.3611509799957275\n",
      "[step: 7899] loss: 1.3591684103012085\n",
      "[step: 7900] loss: 1.357503890991211\n",
      "[step: 7901] loss: 1.3570213317871094\n",
      "[step: 7902] loss: 1.3573644161224365\n",
      "[step: 7903] loss: 1.3576703071594238\n",
      "[step: 7904] loss: 1.357365608215332\n",
      "[step: 7905] loss: 1.356842279434204\n",
      "[step: 7906] loss: 1.356846570968628\n",
      "[step: 7907] loss: 1.3581576347351074\n",
      "[step: 7908] loss: 1.3610193729400635\n",
      "[step: 7909] loss: 1.3659882545471191\n",
      "[step: 7910] loss: 1.3737874031066895\n",
      "[step: 7911] loss: 1.3877100944519043\n",
      "[step: 7912] loss: 1.411639928817749\n",
      "[step: 7913] loss: 1.456549882888794\n",
      "[step: 7914] loss: 1.5334627628326416\n",
      "[step: 7915] loss: 1.678844928741455\n",
      "[step: 7916] loss: 1.9145063161849976\n",
      "[step: 7917] loss: 2.3505983352661133\n",
      "[step: 7918] loss: 2.9238476753234863\n",
      "[step: 7919] loss: 3.8016462326049805\n",
      "[step: 7920] loss: 4.205731391906738\n",
      "[step: 7921] loss: 4.146825790405273\n",
      "[step: 7922] loss: 2.8631649017333984\n",
      "[step: 7923] loss: 1.6822690963745117\n",
      "[step: 7924] loss: 1.4497883319854736\n",
      "[step: 7925] loss: 2.119706392288208\n",
      "[step: 7926] loss: 2.8378124237060547\n",
      "[step: 7927] loss: 2.6743664741516113\n",
      "[step: 7928] loss: 1.9483054876327515\n",
      "[step: 7929] loss: 1.447981357574463\n",
      "[step: 7930] loss: 1.7238359451293945\n",
      "[step: 7931] loss: 2.2055110931396484\n",
      "[step: 7932] loss: 2.1472036838531494\n",
      "[step: 7933] loss: 1.744004249572754\n",
      "[step: 7934] loss: 1.4993164539337158\n",
      "[step: 7935] loss: 1.626359462738037\n",
      "[step: 7936] loss: 1.7801001071929932\n",
      "[step: 7937] loss: 1.7017223834991455\n",
      "[step: 7938] loss: 1.5742113590240479\n",
      "[step: 7939] loss: 1.5340495109558105\n",
      "[step: 7940] loss: 1.5552781820297241\n",
      "[step: 7941] loss: 1.5479389429092407\n",
      "[step: 7942] loss: 1.524237871170044\n",
      "[step: 7943] loss: 1.5369913578033447\n",
      "[step: 7944] loss: 1.5194754600524902\n",
      "[step: 7945] loss: 1.459291696548462\n",
      "[step: 7946] loss: 1.4293524026870728\n",
      "[step: 7947] loss: 1.4592951536178589\n",
      "[step: 7948] loss: 1.4966931343078613\n",
      "[step: 7949] loss: 1.466756820678711\n",
      "[step: 7950] loss: 1.3968987464904785\n",
      "[step: 7951] loss: 1.376131296157837\n",
      "[step: 7952] loss: 1.419450283050537\n",
      "[step: 7953] loss: 1.4527137279510498\n",
      "[step: 7954] loss: 1.420145034790039\n",
      "[step: 7955] loss: 1.3661552667617798\n",
      "[step: 7956] loss: 1.3580565452575684\n",
      "[step: 7957] loss: 1.3915718793869019\n",
      "[step: 7958] loss: 1.4113653898239136\n",
      "[step: 7959] loss: 1.389796495437622\n",
      "[step: 7960] loss: 1.3570562601089478\n",
      "[step: 7961] loss: 1.351145625114441\n",
      "[step: 7962] loss: 1.3698041439056396\n",
      "[step: 7963] loss: 1.3797500133514404\n",
      "[step: 7964] loss: 1.3678529262542725\n",
      "[step: 7965] loss: 1.350801944732666\n",
      "[step: 7966] loss: 1.3474607467651367\n",
      "[step: 7967] loss: 1.3568320274353027\n",
      "[step: 7968] loss: 1.3612637519836426\n",
      "[step: 7969] loss: 1.3531992435455322\n",
      "[step: 7970] loss: 1.3416756391525269\n",
      "[step: 7971] loss: 1.3389509916305542\n",
      "[step: 7972] loss: 1.3452281951904297\n",
      "[step: 7973] loss: 1.3499212265014648\n",
      "[step: 7974] loss: 1.3471285104751587\n",
      "[step: 7975] loss: 1.3399035930633545\n",
      "[step: 7976] loss: 1.3352210521697998\n",
      "[step: 7977] loss: 1.3359826803207397\n",
      "[step: 7978] loss: 1.3383431434631348\n",
      "[step: 7979] loss: 1.3374576568603516\n",
      "[step: 7980] loss: 1.3336313962936401\n",
      "[step: 7981] loss: 1.33034086227417\n",
      "[step: 7982] loss: 1.3301235437393188\n",
      "[step: 7983] loss: 1.3319445848464966\n",
      "[step: 7984] loss: 1.332820177078247\n",
      "[step: 7985] loss: 1.3315818309783936\n",
      "[step: 7986] loss: 1.329214334487915\n",
      "[step: 7987] loss: 1.327871322631836\n",
      "[step: 7988] loss: 1.3282883167266846\n",
      "[step: 7989] loss: 1.3294697999954224\n",
      "[step: 7990] loss: 1.3301358222961426\n",
      "[step: 7991] loss: 1.3301236629486084\n",
      "[step: 7992] loss: 1.3303346633911133\n",
      "[step: 7993] loss: 1.3323125839233398\n",
      "[step: 7994] loss: 1.336704134941101\n",
      "[step: 7995] loss: 1.3443617820739746\n",
      "[step: 7996] loss: 1.3560965061187744\n",
      "[step: 7997] loss: 1.376157283782959\n",
      "[step: 7998] loss: 1.4094882011413574\n",
      "[step: 7999] loss: 1.4709053039550781\n",
      "[step: 8000] loss: 1.5731197595596313\n",
      "[step: 8001] loss: 1.7622030973434448\n",
      "[step: 8002] loss: 2.0517430305480957\n",
      "[step: 8003] loss: 2.5624797344207764\n",
      "[step: 8004] loss: 3.1475322246551514\n",
      "[step: 8005] loss: 3.907870292663574\n",
      "[step: 8006] loss: 3.958615779876709\n",
      "[step: 8007] loss: 3.4165303707122803\n",
      "[step: 8008] loss: 2.164778232574463\n",
      "[step: 8009] loss: 1.414891004562378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8010] loss: 1.5993345975875854\n",
      "[step: 8011] loss: 2.2762093544006348\n",
      "[step: 8012] loss: 2.677652359008789\n",
      "[step: 8013] loss: 2.277801275253296\n",
      "[step: 8014] loss: 1.6527529954910278\n",
      "[step: 8015] loss: 1.4464771747589111\n",
      "[step: 8016] loss: 1.770843267440796\n",
      "[step: 8017] loss: 2.066136360168457\n",
      "[step: 8018] loss: 1.9171497821807861\n",
      "[step: 8019] loss: 1.6184544563293457\n",
      "[step: 8020] loss: 1.4813454151153564\n",
      "[step: 8021] loss: 1.5782321691513062\n",
      "[step: 8022] loss: 1.6739070415496826\n",
      "[step: 8023] loss: 1.6214276552200317\n",
      "[step: 8024] loss: 1.5425043106079102\n",
      "[step: 8025] loss: 1.4897702932357788\n",
      "[step: 8026] loss: 1.4700095653533936\n",
      "[step: 8027] loss: 1.4727065563201904\n",
      "[step: 8028] loss: 1.495768427848816\n",
      "[step: 8029] loss: 1.5120278596878052\n",
      "[step: 8030] loss: 1.4573386907577515\n",
      "[step: 8031] loss: 1.3873381614685059\n",
      "[step: 8032] loss: 1.379949688911438\n",
      "[step: 8033] loss: 1.4279968738555908\n",
      "[step: 8034] loss: 1.4614070653915405\n",
      "[step: 8035] loss: 1.4173928499221802\n",
      "[step: 8036] loss: 1.348577857017517\n",
      "[step: 8037] loss: 1.3367035388946533\n",
      "[step: 8038] loss: 1.3817408084869385\n",
      "[step: 8039] loss: 1.411319613456726\n",
      "[step: 8040] loss: 1.3807611465454102\n",
      "[step: 8041] loss: 1.334428071975708\n",
      "[step: 8042] loss: 1.3247215747833252\n",
      "[step: 8043] loss: 1.349845290184021\n",
      "[step: 8044] loss: 1.3669352531433105\n",
      "[step: 8045] loss: 1.3529247045516968\n",
      "[step: 8046] loss: 1.3296144008636475\n",
      "[step: 8047] loss: 1.3240069150924683\n",
      "[step: 8048] loss: 1.3347008228302002\n",
      "[step: 8049] loss: 1.3389689922332764\n",
      "[step: 8050] loss: 1.3293818235397339\n",
      "[step: 8051] loss: 1.3176689147949219\n",
      "[step: 8052] loss: 1.3171286582946777\n",
      "[step: 8053] loss: 1.3260345458984375\n",
      "[step: 8054] loss: 1.3297021389007568\n",
      "[step: 8055] loss: 1.3226596117019653\n",
      "[step: 8056] loss: 1.3121364116668701\n",
      "[step: 8057] loss: 1.3079183101654053\n",
      "[step: 8058] loss: 1.3108811378479004\n",
      "[step: 8059] loss: 1.3141593933105469\n",
      "[step: 8060] loss: 1.312911868095398\n",
      "[step: 8061] loss: 1.308652639389038\n",
      "[step: 8062] loss: 1.3062888383865356\n",
      "[step: 8063] loss: 1.3078513145446777\n",
      "[step: 8064] loss: 1.3102715015411377\n",
      "[step: 8065] loss: 1.309752106666565\n",
      "[step: 8066] loss: 1.3067258596420288\n",
      "[step: 8067] loss: 1.3035805225372314\n",
      "[step: 8068] loss: 1.3027108907699585\n",
      "[step: 8069] loss: 1.303555965423584\n",
      "[step: 8070] loss: 1.304038405418396\n",
      "[step: 8071] loss: 1.3030316829681396\n",
      "[step: 8072] loss: 1.3014001846313477\n",
      "[step: 8073] loss: 1.3006432056427002\n",
      "[step: 8074] loss: 1.301675796508789\n",
      "[step: 8075] loss: 1.3039944171905518\n",
      "[step: 8076] loss: 1.3072023391723633\n",
      "[step: 8077] loss: 1.311470627784729\n",
      "[step: 8078] loss: 1.319201946258545\n",
      "[step: 8079] loss: 1.3332056999206543\n",
      "[step: 8080] loss: 1.359336018562317\n",
      "[step: 8081] loss: 1.4036821126937866\n",
      "[step: 8082] loss: 1.484961748123169\n",
      "[step: 8083] loss: 1.6193492412567139\n",
      "[step: 8084] loss: 1.870227575302124\n",
      "[step: 8085] loss: 2.247669219970703\n",
      "[step: 8086] loss: 2.9001717567443848\n",
      "[step: 8087] loss: 3.546450138092041\n",
      "[step: 8088] loss: 4.223602771759033\n",
      "[step: 8089] loss: 3.8365256786346436\n",
      "[step: 8090] loss: 2.79744553565979\n",
      "[step: 8091] loss: 1.6257946491241455\n",
      "[step: 8092] loss: 1.3945242166519165\n",
      "[step: 8093] loss: 2.025954246520996\n",
      "[step: 8094] loss: 2.6289820671081543\n",
      "[step: 8095] loss: 2.5627055168151855\n",
      "[step: 8096] loss: 1.8285305500030518\n",
      "[step: 8097] loss: 1.412250280380249\n",
      "[step: 8098] loss: 1.6630797386169434\n",
      "[step: 8099] loss: 2.033792734146118\n",
      "[step: 8100] loss: 1.9950916767120361\n",
      "[step: 8101] loss: 1.6334868669509888\n",
      "[step: 8102] loss: 1.4678759574890137\n",
      "[step: 8103] loss: 1.5653289556503296\n",
      "[step: 8104] loss: 1.6564066410064697\n",
      "[step: 8105] loss: 1.6113351583480835\n",
      "[step: 8106] loss: 1.511631965637207\n",
      "[step: 8107] loss: 1.4863989353179932\n",
      "[step: 8108] loss: 1.4796395301818848\n",
      "[step: 8109] loss: 1.4507867097854614\n",
      "[step: 8110] loss: 1.4647866487503052\n",
      "[step: 8111] loss: 1.4888420104980469\n",
      "[step: 8112] loss: 1.4551773071289062\n",
      "[step: 8113] loss: 1.3760030269622803\n",
      "[step: 8114] loss: 1.353972315788269\n",
      "[step: 8115] loss: 1.4108887910842896\n",
      "[step: 8116] loss: 1.4417399168014526\n",
      "[step: 8117] loss: 1.394465684890747\n",
      "[step: 8118] loss: 1.3215162754058838\n",
      "[step: 8119] loss: 1.3175992965698242\n",
      "[step: 8120] loss: 1.3714882135391235\n",
      "[step: 8121] loss: 1.3895297050476074\n",
      "[step: 8122] loss: 1.3470182418823242\n",
      "[step: 8123] loss: 1.3016207218170166\n",
      "[step: 8124] loss: 1.308269739151001\n",
      "[step: 8125] loss: 1.3407915830612183\n",
      "[step: 8126] loss: 1.3451220989227295\n",
      "[step: 8127] loss: 1.3188098669052124\n",
      "[step: 8128] loss: 1.2977226972579956\n",
      "[step: 8129] loss: 1.30461585521698\n",
      "[step: 8130] loss: 1.3192594051361084\n",
      "[step: 8131] loss: 1.3152912855148315\n",
      "[step: 8132] loss: 1.298691749572754\n",
      "[step: 8133] loss: 1.2911033630371094\n",
      "[step: 8134] loss: 1.2985942363739014\n",
      "[step: 8135] loss: 1.3066892623901367\n",
      "[step: 8136] loss: 1.3022549152374268\n",
      "[step: 8137] loss: 1.28934907913208\n",
      "[step: 8138] loss: 1.2827386856079102\n",
      "[step: 8139] loss: 1.286534070968628\n",
      "[step: 8140] loss: 1.2922693490982056\n",
      "[step: 8141] loss: 1.2914412021636963\n",
      "[step: 8142] loss: 1.285617709159851\n",
      "[step: 8143] loss: 1.282084584236145\n",
      "[step: 8144] loss: 1.2831209897994995\n",
      "[step: 8145] loss: 1.285365343093872\n",
      "[step: 8146] loss: 1.283935308456421\n",
      "[step: 8147] loss: 1.2793318033218384\n",
      "[step: 8148] loss: 1.2756582498550415\n",
      "[step: 8149] loss: 1.2752913236618042\n",
      "[step: 8150] loss: 1.2766740322113037\n",
      "[step: 8151] loss: 1.2768425941467285\n",
      "[step: 8152] loss: 1.2747981548309326\n",
      "[step: 8153] loss: 1.272405982017517\n",
      "[step: 8154] loss: 1.2715706825256348\n",
      "[step: 8155] loss: 1.272278070449829\n",
      "[step: 8156] loss: 1.2730225324630737\n",
      "[step: 8157] loss: 1.2726712226867676\n",
      "[step: 8158] loss: 1.2717113494873047\n",
      "[step: 8159] loss: 1.271310567855835\n",
      "[step: 8160] loss: 1.2724162340164185\n",
      "[step: 8161] loss: 1.2749567031860352\n",
      "[step: 8162] loss: 1.2787435054779053\n",
      "[step: 8163] loss: 1.2843472957611084\n",
      "[step: 8164] loss: 1.294520378112793\n",
      "[step: 8165] loss: 1.3128669261932373\n",
      "[step: 8166] loss: 1.347665548324585\n",
      "[step: 8167] loss: 1.4084904193878174\n",
      "[step: 8168] loss: 1.523695945739746\n",
      "[step: 8169] loss: 1.7174469232559204\n",
      "[step: 8170] loss: 2.083853006362915\n",
      "[step: 8171] loss: 2.613999605178833\n",
      "[step: 8172] loss: 3.4858386516571045\n",
      "[step: 8173] loss: 4.124971389770508\n",
      "[step: 8174] loss: 4.457043170928955\n",
      "[step: 8175] loss: 3.3531885147094727\n",
      "[step: 8176] loss: 1.9441728591918945\n",
      "[step: 8177] loss: 1.3311283588409424\n",
      "[step: 8178] loss: 1.8823275566101074\n",
      "[step: 8179] loss: 2.7291746139526367\n",
      "[step: 8180] loss: 2.7256860733032227\n",
      "[step: 8181] loss: 2.0078554153442383\n",
      "[step: 8182] loss: 1.4142625331878662\n",
      "[step: 8183] loss: 1.6485434770584106\n",
      "[step: 8184] loss: 2.1227190494537354\n",
      "[step: 8185] loss: 2.036116600036621\n",
      "[step: 8186] loss: 1.639301061630249\n",
      "[step: 8187] loss: 1.4644051790237427\n",
      "[step: 8188] loss: 1.6094772815704346\n",
      "[step: 8189] loss: 1.6934794187545776\n",
      "[step: 8190] loss: 1.5740454196929932\n",
      "[step: 8191] loss: 1.4976539611816406\n",
      "[step: 8192] loss: 1.5033061504364014\n",
      "[step: 8193] loss: 1.4873809814453125\n",
      "[step: 8194] loss: 1.4522771835327148\n",
      "[step: 8195] loss: 1.4599252939224243\n",
      "[step: 8196] loss: 1.4841350317001343\n",
      "[step: 8197] loss: 1.4259991645812988\n",
      "[step: 8198] loss: 1.3465819358825684\n",
      "[step: 8199] loss: 1.3723771572113037\n",
      "[step: 8200] loss: 1.4368672370910645\n",
      "[step: 8201] loss: 1.4136039018630981\n",
      "[step: 8202] loss: 1.3241990804672241\n",
      "[step: 8203] loss: 1.295485496520996\n",
      "[step: 8204] loss: 1.3555231094360352\n",
      "[step: 8205] loss: 1.3886802196502686\n",
      "[step: 8206] loss: 1.3396004438400269\n",
      "[step: 8207] loss: 1.2811111211776733\n",
      "[step: 8208] loss: 1.291642665863037\n",
      "[step: 8209] loss: 1.3360981941223145\n",
      "[step: 8210] loss: 1.3354957103729248\n",
      "[step: 8211] loss: 1.2948248386383057\n",
      "[step: 8212] loss: 1.2736856937408447\n",
      "[step: 8213] loss: 1.2908239364624023\n",
      "[step: 8214] loss: 1.3075660467147827\n",
      "[step: 8215] loss: 1.2954614162445068\n",
      "[step: 8216] loss: 1.2731393575668335\n",
      "[step: 8217] loss: 1.2716524600982666\n",
      "[step: 8218] loss: 1.285449743270874\n",
      "[step: 8219] loss: 1.2871030569076538\n",
      "[step: 8220] loss: 1.272498369216919\n",
      "[step: 8221] loss: 1.2612130641937256\n",
      "[step: 8222] loss: 1.2654576301574707\n",
      "[step: 8223] loss: 1.2749333381652832\n",
      "[step: 8224] loss: 1.2745871543884277\n",
      "[step: 8225] loss: 1.2652709484100342\n",
      "[step: 8226] loss: 1.2579840421676636\n",
      "[step: 8227] loss: 1.2593919038772583\n",
      "[step: 8228] loss: 1.2630584239959717\n",
      "[step: 8229] loss: 1.2614552974700928\n",
      "[step: 8230] loss: 1.2563889026641846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8231] loss: 1.2539581060409546\n",
      "[step: 8232] loss: 1.2560110092163086\n",
      "[step: 8233] loss: 1.258235216140747\n",
      "[step: 8234] loss: 1.2567156553268433\n",
      "[step: 8235] loss: 1.2525269985198975\n",
      "[step: 8236] loss: 1.2498589754104614\n",
      "[step: 8237] loss: 1.2500011920928955\n",
      "[step: 8238] loss: 1.250811219215393\n",
      "[step: 8239] loss: 1.2498564720153809\n",
      "[step: 8240] loss: 1.2474474906921387\n",
      "[step: 8241] loss: 1.2455110549926758\n",
      "[step: 8242] loss: 1.245276927947998\n",
      "[step: 8243] loss: 1.2458677291870117\n",
      "[step: 8244] loss: 1.2456574440002441\n",
      "[step: 8245] loss: 1.2443926334381104\n",
      "[step: 8246] loss: 1.2430922985076904\n",
      "[step: 8247] loss: 1.2426226139068604\n",
      "[step: 8248] loss: 1.2429460287094116\n",
      "[step: 8249] loss: 1.2433295249938965\n",
      "[step: 8250] loss: 1.2433040142059326\n",
      "[step: 8251] loss: 1.243373155593872\n",
      "[step: 8252] loss: 1.2444325685501099\n",
      "[step: 8253] loss: 1.2473759651184082\n",
      "[step: 8254] loss: 1.2528445720672607\n",
      "[step: 8255] loss: 1.2625367641448975\n",
      "[step: 8256] loss: 1.2793467044830322\n",
      "[step: 8257] loss: 1.3111233711242676\n",
      "[step: 8258] loss: 1.3683416843414307\n",
      "[step: 8259] loss: 1.4793741703033447\n",
      "[step: 8260] loss: 1.6720647811889648\n",
      "[step: 8261] loss: 2.043673515319824\n",
      "[step: 8262] loss: 2.600043296813965\n",
      "[step: 8263] loss: 3.539726734161377\n",
      "[step: 8264] loss: 4.274542331695557\n",
      "[step: 8265] loss: 4.699463844299316\n",
      "[step: 8266] loss: 3.5087976455688477\n",
      "[step: 8267] loss: 1.9512301683425903\n",
      "[step: 8268] loss: 1.3213062286376953\n",
      "[step: 8269] loss: 1.9903937578201294\n",
      "[step: 8270] loss: 2.8877453804016113\n",
      "[step: 8271] loss: 2.7377023696899414\n",
      "[step: 8272] loss: 1.8992431163787842\n",
      "[step: 8273] loss: 1.409238338470459\n",
      "[step: 8274] loss: 1.7838081121444702\n",
      "[step: 8275] loss: 2.184798240661621\n",
      "[step: 8276] loss: 1.926591396331787\n",
      "[step: 8277] loss: 1.5564931631088257\n",
      "[step: 8278] loss: 1.5270472764968872\n",
      "[step: 8279] loss: 1.6694836616516113\n",
      "[step: 8280] loss: 1.6443051099777222\n",
      "[step: 8281] loss: 1.5259099006652832\n",
      "[step: 8282] loss: 1.5173903703689575\n",
      "[step: 8283] loss: 1.5094021558761597\n",
      "[step: 8284] loss: 1.4412422180175781\n",
      "[step: 8285] loss: 1.4512263536453247\n",
      "[step: 8286] loss: 1.5018372535705566\n",
      "[step: 8287] loss: 1.450195074081421\n",
      "[step: 8288] loss: 1.3379954099655151\n",
      "[step: 8289] loss: 1.3340473175048828\n",
      "[step: 8290] loss: 1.4391911029815674\n",
      "[step: 8291] loss: 1.4299015998840332\n",
      "[step: 8292] loss: 1.30897855758667\n",
      "[step: 8293] loss: 1.2702462673187256\n",
      "[step: 8294] loss: 1.3489513397216797\n",
      "[step: 8295] loss: 1.3887726068496704\n",
      "[step: 8296] loss: 1.3143830299377441\n",
      "[step: 8297] loss: 1.25608491897583\n",
      "[step: 8298] loss: 1.2871172428131104\n",
      "[step: 8299] loss: 1.331276297569275\n",
      "[step: 8300] loss: 1.3070876598358154\n",
      "[step: 8301] loss: 1.2594313621520996\n",
      "[step: 8302] loss: 1.260402798652649\n",
      "[step: 8303] loss: 1.2883447408676147\n",
      "[step: 8304] loss: 1.2838609218597412\n",
      "[step: 8305] loss: 1.2554807662963867\n",
      "[step: 8306] loss: 1.2492691278457642\n",
      "[step: 8307] loss: 1.2663084268569946\n",
      "[step: 8308] loss: 1.2706966400146484\n",
      "[step: 8309] loss: 1.2522294521331787\n",
      "[step: 8310] loss: 1.2377753257751465\n",
      "[step: 8311] loss: 1.2449227571487427\n",
      "[step: 8312] loss: 1.2573678493499756\n",
      "[step: 8313] loss: 1.2528893947601318\n",
      "[step: 8314] loss: 1.2394373416900635\n",
      "[step: 8315] loss: 1.2347427606582642\n",
      "[step: 8316] loss: 1.2401599884033203\n",
      "[step: 8317] loss: 1.242120385169983\n",
      "[step: 8318] loss: 1.2366664409637451\n",
      "[step: 8319] loss: 1.231801986694336\n",
      "[step: 8320] loss: 1.2333955764770508\n",
      "[step: 8321] loss: 1.2368264198303223\n",
      "[step: 8322] loss: 1.2347325086593628\n",
      "[step: 8323] loss: 1.2288371324539185\n",
      "[step: 8324] loss: 1.2255898714065552\n",
      "[step: 8325] loss: 1.2269680500030518\n",
      "[step: 8326] loss: 1.2285993099212646\n",
      "[step: 8327] loss: 1.227181315422058\n",
      "[step: 8328] loss: 1.2242534160614014\n",
      "[step: 8329] loss: 1.22324800491333\n",
      "[step: 8330] loss: 1.224429965019226\n",
      "[step: 8331] loss: 1.2251074314117432\n",
      "[step: 8332] loss: 1.2235682010650635\n",
      "[step: 8333] loss: 1.221318006515503\n",
      "[step: 8334] loss: 1.2203291654586792\n",
      "[step: 8335] loss: 1.2205168008804321\n",
      "[step: 8336] loss: 1.2204298973083496\n",
      "[step: 8337] loss: 1.219249963760376\n",
      "[step: 8338] loss: 1.2176668643951416\n",
      "[step: 8339] loss: 1.2168399095535278\n",
      "[step: 8340] loss: 1.216870903968811\n",
      "[step: 8341] loss: 1.2168889045715332\n",
      "[step: 8342] loss: 1.2163665294647217\n",
      "[step: 8343] loss: 1.2156615257263184\n",
      "[step: 8344] loss: 1.2155405282974243\n",
      "[step: 8345] loss: 1.2163069248199463\n",
      "[step: 8346] loss: 1.2178778648376465\n",
      "[step: 8347] loss: 1.2202200889587402\n",
      "[step: 8348] loss: 1.2244081497192383\n",
      "[step: 8349] loss: 1.2322087287902832\n",
      "[step: 8350] loss: 1.2471609115600586\n",
      "[step: 8351] loss: 1.2737650871276855\n",
      "[step: 8352] loss: 1.32362699508667\n",
      "[step: 8353] loss: 1.4108126163482666\n",
      "[step: 8354] loss: 1.5782160758972168\n",
      "[step: 8355] loss: 1.8573439121246338\n",
      "[step: 8356] loss: 2.3807473182678223\n",
      "[step: 8357] loss: 3.071432590484619\n",
      "[step: 8358] loss: 4.081350326538086\n",
      "[step: 8359] loss: 4.4046173095703125\n",
      "[step: 8360] loss: 3.982189416885376\n",
      "[step: 8361] loss: 2.3939270973205566\n",
      "[step: 8362] loss: 1.340321660041809\n",
      "[step: 8363] loss: 1.5867424011230469\n",
      "[step: 8364] loss: 2.502995491027832\n",
      "[step: 8365] loss: 2.8957414627075195\n",
      "[step: 8366] loss: 2.1787312030792236\n",
      "[step: 8367] loss: 1.4311612844467163\n",
      "[step: 8368] loss: 1.4935221672058105\n",
      "[step: 8369] loss: 2.0093305110931396\n",
      "[step: 8370] loss: 2.1018147468566895\n",
      "[step: 8371] loss: 1.6492669582366943\n",
      "[step: 8372] loss: 1.3913363218307495\n",
      "[step: 8373] loss: 1.5230445861816406\n",
      "[step: 8374] loss: 1.6726276874542236\n",
      "[step: 8375] loss: 1.6048120260238647\n",
      "[step: 8376] loss: 1.4411120414733887\n",
      "[step: 8377] loss: 1.401168704032898\n",
      "[step: 8378] loss: 1.4427374601364136\n",
      "[step: 8379] loss: 1.4589457511901855\n",
      "[step: 8380] loss: 1.457460641860962\n",
      "[step: 8381] loss: 1.3977700471878052\n",
      "[step: 8382] loss: 1.328061819076538\n",
      "[step: 8383] loss: 1.3257368803024292\n",
      "[step: 8384] loss: 1.3864367008209229\n",
      "[step: 8385] loss: 1.4005463123321533\n",
      "[step: 8386] loss: 1.3055744171142578\n",
      "[step: 8387] loss: 1.2491885423660278\n",
      "[step: 8388] loss: 1.2998535633087158\n",
      "[step: 8389] loss: 1.3549065589904785\n",
      "[step: 8390] loss: 1.3164467811584473\n",
      "[step: 8391] loss: 1.2396495342254639\n",
      "[step: 8392] loss: 1.2371017932891846\n",
      "[step: 8393] loss: 1.290175199508667\n",
      "[step: 8394] loss: 1.2998325824737549\n",
      "[step: 8395] loss: 1.2542163133621216\n",
      "[step: 8396] loss: 1.223117470741272\n",
      "[step: 8397] loss: 1.2403931617736816\n",
      "[step: 8398] loss: 1.2607296705245972\n",
      "[step: 8399] loss: 1.2472134828567505\n",
      "[step: 8400] loss: 1.2272017002105713\n",
      "[step: 8401] loss: 1.2265474796295166\n",
      "[step: 8402] loss: 1.2367045879364014\n",
      "[step: 8403] loss: 1.2321349382400513\n",
      "[step: 8404] loss: 1.217423677444458\n",
      "[step: 8405] loss: 1.2142581939697266\n",
      "[step: 8406] loss: 1.2245168685913086\n",
      "[step: 8407] loss: 1.229099988937378\n",
      "[step: 8408] loss: 1.2185957431793213\n",
      "[step: 8409] loss: 1.207147479057312\n",
      "[step: 8410] loss: 1.2068493366241455\n",
      "[step: 8411] loss: 1.2128503322601318\n",
      "[step: 8412] loss: 1.2142502069473267\n",
      "[step: 8413] loss: 1.2095669507980347\n",
      "[step: 8414] loss: 1.206015706062317\n",
      "[step: 8415] loss: 1.2071188688278198\n",
      "[step: 8416] loss: 1.2079495191574097\n",
      "[step: 8417] loss: 1.2044931650161743\n",
      "[step: 8418] loss: 1.1996808052062988\n",
      "[step: 8419] loss: 1.1981914043426514\n",
      "[step: 8420] loss: 1.200047492980957\n",
      "[step: 8421] loss: 1.201483964920044\n",
      "[step: 8422] loss: 1.2001229524612427\n",
      "[step: 8423] loss: 1.1979435682296753\n",
      "[step: 8424] loss: 1.1974189281463623\n",
      "[step: 8425] loss: 1.1983041763305664\n",
      "[step: 8426] loss: 1.19821035861969\n",
      "[step: 8427] loss: 1.1965203285217285\n",
      "[step: 8428] loss: 1.194558024406433\n",
      "[step: 8429] loss: 1.193659782409668\n",
      "[step: 8430] loss: 1.1936650276184082\n",
      "[step: 8431] loss: 1.1933948993682861\n",
      "[step: 8432] loss: 1.1922342777252197\n",
      "[step: 8433] loss: 1.1909339427947998\n",
      "[step: 8434] loss: 1.1903536319732666\n",
      "[step: 8435] loss: 1.1904574632644653\n",
      "[step: 8436] loss: 1.190603494644165\n",
      "[step: 8437] loss: 1.190476417541504\n",
      "[step: 8438] loss: 1.1904969215393066\n",
      "[step: 8439] loss: 1.191446304321289\n",
      "[step: 8440] loss: 1.193812608718872\n",
      "[step: 8441] loss: 1.1980793476104736\n",
      "[step: 8442] loss: 1.2051374912261963\n",
      "[step: 8443] loss: 1.2179110050201416\n",
      "[step: 8444] loss: 1.2405866384506226\n",
      "[step: 8445] loss: 1.283279538154602\n",
      "[step: 8446] loss: 1.3578799962997437\n",
      "[step: 8447] loss: 1.4996167421340942\n",
      "[step: 8448] loss: 1.7357354164123535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8449] loss: 2.178340435028076\n",
      "[step: 8450] loss: 2.7881813049316406\n",
      "[step: 8451] loss: 3.731309652328491\n",
      "[step: 8452] loss: 4.224793910980225\n",
      "[step: 8453] loss: 4.167815208435059\n",
      "[step: 8454] loss: 2.772524356842041\n",
      "[step: 8455] loss: 1.4983919858932495\n",
      "[step: 8456] loss: 1.346914291381836\n",
      "[step: 8457] loss: 2.1435751914978027\n",
      "[step: 8458] loss: 2.7970199584960938\n",
      "[step: 8459] loss: 2.394529342651367\n",
      "[step: 8460] loss: 1.5886908769607544\n",
      "[step: 8461] loss: 1.3321053981781006\n",
      "[step: 8462] loss: 1.7853326797485352\n",
      "[step: 8463] loss: 2.1021738052368164\n",
      "[step: 8464] loss: 1.7795913219451904\n",
      "[step: 8465] loss: 1.4016931056976318\n",
      "[step: 8466] loss: 1.4006922245025635\n",
      "[step: 8467] loss: 1.607541799545288\n",
      "[step: 8468] loss: 1.6288243532180786\n",
      "[step: 8469] loss: 1.4442718029022217\n",
      "[step: 8470] loss: 1.354774832725525\n",
      "[step: 8471] loss: 1.3955798149108887\n",
      "[step: 8472] loss: 1.4330501556396484\n",
      "[step: 8473] loss: 1.4298028945922852\n",
      "[step: 8474] loss: 1.3784692287445068\n",
      "[step: 8475] loss: 1.320335865020752\n",
      "[step: 8476] loss: 1.2913615703582764\n",
      "[step: 8477] loss: 1.3250738382339478\n",
      "[step: 8478] loss: 1.3714981079101562\n",
      "[step: 8479] loss: 1.3192522525787354\n",
      "[step: 8480] loss: 1.2402119636535645\n",
      "[step: 8481] loss: 1.2361602783203125\n",
      "[step: 8482] loss: 1.296195387840271\n",
      "[step: 8483] loss: 1.3158605098724365\n",
      "[step: 8484] loss: 1.250427484512329\n",
      "[step: 8485] loss: 1.2015432119369507\n",
      "[step: 8486] loss: 1.2266186475753784\n",
      "[step: 8487] loss: 1.2701817750930786\n",
      "[step: 8488] loss: 1.259621024131775\n",
      "[step: 8489] loss: 1.2110276222229004\n",
      "[step: 8490] loss: 1.1956367492675781\n",
      "[step: 8491] loss: 1.2184817790985107\n",
      "[step: 8492] loss: 1.2318503856658936\n",
      "[step: 8493] loss: 1.2172324657440186\n",
      "[step: 8494] loss: 1.1974678039550781\n",
      "[step: 8495] loss: 1.1983954906463623\n",
      "[step: 8496] loss: 1.20730721950531\n",
      "[step: 8497] loss: 1.2029504776000977\n",
      "[step: 8498] loss: 1.1906867027282715\n",
      "[step: 8499] loss: 1.1887129545211792\n",
      "[step: 8500] loss: 1.1974377632141113\n",
      "[step: 8501] loss: 1.199967384338379\n",
      "[step: 8502] loss: 1.1910068988800049\n",
      "[step: 8503] loss: 1.1806882619857788\n",
      "[step: 8504] loss: 1.1798124313354492\n",
      "[step: 8505] loss: 1.1855213642120361\n",
      "[step: 8506] loss: 1.1876784563064575\n",
      "[step: 8507] loss: 1.1837340593338013\n",
      "[step: 8508] loss: 1.179912805557251\n",
      "[step: 8509] loss: 1.1799724102020264\n",
      "[step: 8510] loss: 1.1808568239212036\n",
      "[step: 8511] loss: 1.1787621974945068\n",
      "[step: 8512] loss: 1.174332618713379\n",
      "[step: 8513] loss: 1.1718909740447998\n",
      "[step: 8514] loss: 1.1729419231414795\n",
      "[step: 8515] loss: 1.174609661102295\n",
      "[step: 8516] loss: 1.1741981506347656\n",
      "[step: 8517] loss: 1.1722798347473145\n",
      "[step: 8518] loss: 1.1712822914123535\n",
      "[step: 8519] loss: 1.1717530488967896\n",
      "[step: 8520] loss: 1.1723523139953613\n",
      "[step: 8521] loss: 1.1717090606689453\n",
      "[step: 8522] loss: 1.1701221466064453\n",
      "[step: 8523] loss: 1.1689945459365845\n",
      "[step: 8524] loss: 1.1689746379852295\n",
      "[step: 8525] loss: 1.169287919998169\n",
      "[step: 8526] loss: 1.1691806316375732\n",
      "[step: 8527] loss: 1.1686807870864868\n",
      "[step: 8528] loss: 1.1687102317810059\n",
      "[step: 8529] loss: 1.1699116230010986\n",
      "[step: 8530] loss: 1.1724278926849365\n",
      "[step: 8531] loss: 1.1761833429336548\n",
      "[step: 8532] loss: 1.1822588443756104\n",
      "[step: 8533] loss: 1.1923166513442993\n",
      "[step: 8534] loss: 1.2103731632232666\n",
      "[step: 8535] loss: 1.240553855895996\n",
      "[step: 8536] loss: 1.294633388519287\n",
      "[step: 8537] loss: 1.3835463523864746\n",
      "[step: 8538] loss: 1.5460660457611084\n",
      "[step: 8539] loss: 1.7976748943328857\n",
      "[step: 8540] loss: 2.2410624027252197\n",
      "[step: 8541] loss: 2.7769997119903564\n",
      "[step: 8542] loss: 3.5143930912017822\n",
      "[step: 8543] loss: 3.7291741371154785\n",
      "[step: 8544] loss: 3.4440274238586426\n",
      "[step: 8545] loss: 2.2964329719543457\n",
      "[step: 8546] loss: 1.37673020362854\n",
      "[step: 8547] loss: 1.2916128635406494\n",
      "[step: 8548] loss: 1.8683593273162842\n",
      "[step: 8549] loss: 2.390547752380371\n",
      "[step: 8550] loss: 2.20566725730896\n",
      "[step: 8551] loss: 1.6244957447052002\n",
      "[step: 8552] loss: 1.256792426109314\n",
      "[step: 8553] loss: 1.4662331342697144\n",
      "[step: 8554] loss: 1.824671983718872\n",
      "[step: 8555] loss: 1.8011199235916138\n",
      "[step: 8556] loss: 1.504319190979004\n",
      "[step: 8557] loss: 1.2728358507156372\n",
      "[step: 8558] loss: 1.3485444784164429\n",
      "[step: 8559] loss: 1.5254515409469604\n",
      "[step: 8560] loss: 1.5135912895202637\n",
      "[step: 8561] loss: 1.3752892017364502\n",
      "[step: 8562] loss: 1.261175274848938\n",
      "[step: 8563] loss: 1.2795240879058838\n",
      "[step: 8564] loss: 1.3610246181488037\n",
      "[step: 8565] loss: 1.3879146575927734\n",
      "[step: 8566] loss: 1.336193561553955\n",
      "[step: 8567] loss: 1.2373214960098267\n",
      "[step: 8568] loss: 1.2097747325897217\n",
      "[step: 8569] loss: 1.2650556564331055\n",
      "[step: 8570] loss: 1.3101954460144043\n",
      "[step: 8571] loss: 1.2920199632644653\n",
      "[step: 8572] loss: 1.2163121700286865\n",
      "[step: 8573] loss: 1.1758486032485962\n",
      "[step: 8574] loss: 1.2082128524780273\n",
      "[step: 8575] loss: 1.2526674270629883\n",
      "[step: 8576] loss: 1.250459909439087\n",
      "[step: 8577] loss: 1.2014236450195312\n",
      "[step: 8578] loss: 1.1677645444869995\n",
      "[step: 8579] loss: 1.1789398193359375\n",
      "[step: 8580] loss: 1.2058610916137695\n",
      "[step: 8581] loss: 1.2120765447616577\n",
      "[step: 8582] loss: 1.1925382614135742\n",
      "[step: 8583] loss: 1.1746214628219604\n",
      "[step: 8584] loss: 1.1729984283447266\n",
      "[step: 8585] loss: 1.1771388053894043\n",
      "[step: 8586] loss: 1.1753654479980469\n",
      "[step: 8587] loss: 1.1711883544921875\n",
      "[step: 8588] loss: 1.171276569366455\n",
      "[step: 8589] loss: 1.1755270957946777\n",
      "[step: 8590] loss: 1.1759471893310547\n",
      "[step: 8591] loss: 1.1668293476104736\n",
      "[step: 8592] loss: 1.1565738916397095\n",
      "[step: 8593] loss: 1.1530052423477173\n",
      "[step: 8594] loss: 1.1568682193756104\n",
      "[step: 8595] loss: 1.1620255708694458\n",
      "[step: 8596] loss: 1.1625103950500488\n",
      "[step: 8597] loss: 1.1601301431655884\n",
      "[step: 8598] loss: 1.1581871509552002\n",
      "[step: 8599] loss: 1.1580917835235596\n",
      "[step: 8600] loss: 1.1576220989227295\n",
      "[step: 8601] loss: 1.1547881364822388\n",
      "[step: 8602] loss: 1.1505770683288574\n",
      "[step: 8603] loss: 1.1474809646606445\n",
      "[step: 8604] loss: 1.1466305255889893\n",
      "[step: 8605] loss: 1.1470518112182617\n",
      "[step: 8606] loss: 1.1466197967529297\n",
      "[step: 8607] loss: 1.1452358961105347\n",
      "[step: 8608] loss: 1.1440448760986328\n",
      "[step: 8609] loss: 1.1438668966293335\n",
      "[step: 8610] loss: 1.1446936130523682\n",
      "[step: 8611] loss: 1.145538330078125\n",
      "[step: 8612] loss: 1.1460349559783936\n",
      "[step: 8613] loss: 1.1466189622879028\n",
      "[step: 8614] loss: 1.148362159729004\n",
      "[step: 8615] loss: 1.152107834815979\n",
      "[step: 8616] loss: 1.158599853515625\n",
      "[step: 8617] loss: 1.1687860488891602\n",
      "[step: 8618] loss: 1.1861231327056885\n",
      "[step: 8619] loss: 1.2150664329528809\n",
      "[step: 8620] loss: 1.268114447593689\n",
      "[step: 8621] loss: 1.357435703277588\n",
      "[step: 8622] loss: 1.5228263139724731\n",
      "[step: 8623] loss: 1.7822785377502441\n",
      "[step: 8624] loss: 2.2450833320617676\n",
      "[step: 8625] loss: 2.8142082691192627\n",
      "[step: 8626] loss: 3.60493803024292\n",
      "[step: 8627] loss: 3.83843994140625\n",
      "[step: 8628] loss: 3.5162177085876465\n",
      "[step: 8629] loss: 2.2758891582489014\n",
      "[step: 8630] loss: 1.3268523216247559\n",
      "[step: 8631] loss: 1.3171780109405518\n",
      "[step: 8632] loss: 1.9762755632400513\n",
      "[step: 8633] loss: 2.474924087524414\n",
      "[step: 8634] loss: 2.161184310913086\n",
      "[step: 8635] loss: 1.5170485973358154\n",
      "[step: 8636] loss: 1.2543237209320068\n",
      "[step: 8637] loss: 1.5707030296325684\n",
      "[step: 8638] loss: 1.8790266513824463\n",
      "[step: 8639] loss: 1.7167866230010986\n",
      "[step: 8640] loss: 1.3959678411483765\n",
      "[step: 8641] loss: 1.2881736755371094\n",
      "[step: 8642] loss: 1.4301161766052246\n",
      "[step: 8643] loss: 1.519869089126587\n",
      "[step: 8644] loss: 1.4141038656234741\n",
      "[step: 8645] loss: 1.3147122859954834\n",
      "[step: 8646] loss: 1.303536295890808\n",
      "[step: 8647] loss: 1.3196158409118652\n",
      "[step: 8648] loss: 1.3193376064300537\n",
      "[step: 8649] loss: 1.312436580657959\n",
      "[step: 8650] loss: 1.307018518447876\n",
      "[step: 8651] loss: 1.2626821994781494\n",
      "[step: 8652] loss: 1.2187138795852661\n",
      "[step: 8653] loss: 1.2261525392532349\n",
      "[step: 8654] loss: 1.2620480060577393\n",
      "[step: 8655] loss: 1.2712128162384033\n",
      "[step: 8656] loss: 1.216313123703003\n",
      "[step: 8657] loss: 1.166041612625122\n",
      "[step: 8658] loss: 1.1826071739196777\n",
      "[step: 8659] loss: 1.2245738506317139\n",
      "[step: 8660] loss: 1.227259874343872\n",
      "[step: 8661] loss: 1.1807937622070312\n",
      "[step: 8662] loss: 1.1468689441680908\n",
      "[step: 8663] loss: 1.1618250608444214\n",
      "[step: 8664] loss: 1.1919348239898682\n",
      "[step: 8665] loss: 1.192657709121704\n",
      "[step: 8666] loss: 1.1624528169631958\n",
      "[step: 8667] loss: 1.142238736152649\n",
      "[step: 8668] loss: 1.150719165802002\n",
      "[step: 8669] loss: 1.1659411191940308\n",
      "[step: 8670] loss: 1.165155291557312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8671] loss: 1.1502633094787598\n",
      "[step: 8672] loss: 1.1406008005142212\n",
      "[step: 8673] loss: 1.1454846858978271\n",
      "[step: 8674] loss: 1.1526343822479248\n",
      "[step: 8675] loss: 1.148775577545166\n",
      "[step: 8676] loss: 1.1384520530700684\n",
      "[step: 8677] loss: 1.1327846050262451\n",
      "[step: 8678] loss: 1.1366310119628906\n",
      "[step: 8679] loss: 1.1428368091583252\n",
      "[step: 8680] loss: 1.142383337020874\n",
      "[step: 8681] loss: 1.1363999843597412\n",
      "[step: 8682] loss: 1.1310784816741943\n",
      "[step: 8683] loss: 1.1307815313339233\n",
      "[step: 8684] loss: 1.1328331232070923\n",
      "[step: 8685] loss: 1.1326258182525635\n",
      "[step: 8686] loss: 1.1293572187423706\n",
      "[step: 8687] loss: 1.1259384155273438\n",
      "[step: 8688] loss: 1.1254737377166748\n",
      "[step: 8689] loss: 1.1272706985473633\n",
      "[step: 8690] loss: 1.1282908916473389\n",
      "[step: 8691] loss: 1.1272990703582764\n",
      "[step: 8692] loss: 1.125342845916748\n",
      "[step: 8693] loss: 1.1243094205856323\n",
      "[step: 8694] loss: 1.1250189542770386\n",
      "[step: 8695] loss: 1.1262786388397217\n",
      "[step: 8696] loss: 1.1269986629486084\n",
      "[step: 8697] loss: 1.1270173788070679\n",
      "[step: 8698] loss: 1.1276628971099854\n",
      "[step: 8699] loss: 1.1301393508911133\n",
      "[step: 8700] loss: 1.135148525238037\n",
      "[step: 8701] loss: 1.142899751663208\n",
      "[step: 8702] loss: 1.1552650928497314\n",
      "[step: 8703] loss: 1.174999475479126\n",
      "[step: 8704] loss: 1.2105228900909424\n",
      "[step: 8705] loss: 1.2700207233428955\n",
      "[step: 8706] loss: 1.3781791925430298\n",
      "[step: 8707] loss: 1.5498193502426147\n",
      "[step: 8708] loss: 1.857125163078308\n",
      "[step: 8709] loss: 2.279049873352051\n",
      "[step: 8710] loss: 2.945009231567383\n",
      "[step: 8711] loss: 3.4501407146453857\n",
      "[step: 8712] loss: 3.772677421569824\n",
      "[step: 8713] loss: 3.0711989402770996\n",
      "[step: 8714] loss: 1.994287371635437\n",
      "[step: 8715] loss: 1.22721266746521\n",
      "[step: 8716] loss: 1.3341548442840576\n",
      "[step: 8717] loss: 1.9653326272964478\n",
      "[step: 8718] loss: 2.319716453552246\n",
      "[step: 8719] loss: 2.066716194152832\n",
      "[step: 8720] loss: 1.4316184520721436\n",
      "[step: 8721] loss: 1.2156484127044678\n",
      "[step: 8722] loss: 1.5186381340026855\n",
      "[step: 8723] loss: 1.7913687229156494\n",
      "[step: 8724] loss: 1.6895500421524048\n",
      "[step: 8725] loss: 1.3590697050094604\n",
      "[step: 8726] loss: 1.2347452640533447\n",
      "[step: 8727] loss: 1.3655951023101807\n",
      "[step: 8728] loss: 1.4720144271850586\n",
      "[step: 8729] loss: 1.412998914718628\n",
      "[step: 8730] loss: 1.2843773365020752\n",
      "[step: 8731] loss: 1.2493858337402344\n",
      "[step: 8732] loss: 1.274159550666809\n",
      "[step: 8733] loss: 1.2899272441864014\n",
      "[step: 8734] loss: 1.2927641868591309\n",
      "[step: 8735] loss: 1.2689721584320068\n",
      "[step: 8736] loss: 1.235264778137207\n",
      "[step: 8737] loss: 1.1996979713439941\n",
      "[step: 8738] loss: 1.1898306608200073\n",
      "[step: 8739] loss: 1.2182226181030273\n",
      "[step: 8740] loss: 1.2391564846038818\n",
      "[step: 8741] loss: 1.2142977714538574\n",
      "[step: 8742] loss: 1.159590482711792\n",
      "[step: 8743] loss: 1.1417310237884521\n",
      "[step: 8744] loss: 1.1717041730880737\n",
      "[step: 8745] loss: 1.2001194953918457\n",
      "[step: 8746] loss: 1.1880607604980469\n",
      "[step: 8747] loss: 1.143451452255249\n",
      "[step: 8748] loss: 1.1225908994674683\n",
      "[step: 8749] loss: 1.140087366104126\n",
      "[step: 8750] loss: 1.1630085706710815\n",
      "[step: 8751] loss: 1.1620073318481445\n",
      "[step: 8752] loss: 1.1374797821044922\n",
      "[step: 8753] loss: 1.120133399963379\n",
      "[step: 8754] loss: 1.123673677444458\n",
      "[step: 8755] loss: 1.135581612586975\n",
      "[step: 8756] loss: 1.1373507976531982\n",
      "[step: 8757] loss: 1.1265649795532227\n",
      "[step: 8758] loss: 1.118220329284668\n",
      "[step: 8759] loss: 1.119842767715454\n",
      "[step: 8760] loss: 1.1255717277526855\n",
      "[step: 8761] loss: 1.1256320476531982\n",
      "[step: 8762] loss: 1.1181011199951172\n",
      "[step: 8763] loss: 1.1104508638381958\n",
      "[step: 8764] loss: 1.1095224618911743\n",
      "[step: 8765] loss: 1.113541603088379\n",
      "[step: 8766] loss: 1.1166764497756958\n",
      "[step: 8767] loss: 1.1152725219726562\n",
      "[step: 8768] loss: 1.1111314296722412\n",
      "[step: 8769] loss: 1.1089122295379639\n",
      "[step: 8770] loss: 1.109663963317871\n",
      "[step: 8771] loss: 1.1110903024673462\n",
      "[step: 8772] loss: 1.1103076934814453\n",
      "[step: 8773] loss: 1.1074931621551514\n",
      "[step: 8774] loss: 1.1046024560928345\n",
      "[step: 8775] loss: 1.1034661531448364\n",
      "[step: 8776] loss: 1.1039838790893555\n",
      "[step: 8777] loss: 1.1044013500213623\n",
      "[step: 8778] loss: 1.1035513877868652\n",
      "[step: 8779] loss: 1.1019270420074463\n",
      "[step: 8780] loss: 1.1005480289459229\n",
      "[step: 8781] loss: 1.1002707481384277\n",
      "[step: 8782] loss: 1.100832462310791\n",
      "[step: 8783] loss: 1.1014564037322998\n",
      "[step: 8784] loss: 1.1018011569976807\n",
      "[step: 8785] loss: 1.1023797988891602\n",
      "[step: 8786] loss: 1.104204535484314\n",
      "[step: 8787] loss: 1.1087021827697754\n",
      "[step: 8788] loss: 1.1173217296600342\n",
      "[step: 8789] loss: 1.1332879066467285\n",
      "[step: 8790] loss: 1.1615526676177979\n",
      "[step: 8791] loss: 1.2155901193618774\n",
      "[step: 8792] loss: 1.3126859664916992\n",
      "[step: 8793] loss: 1.5029268264770508\n",
      "[step: 8794] loss: 1.826768159866333\n",
      "[step: 8795] loss: 2.439152956008911\n",
      "[step: 8796] loss: 3.2418293952941895\n",
      "[step: 8797] loss: 4.358935356140137\n",
      "[step: 8798] loss: 4.544785022735596\n",
      "[step: 8799] loss: 3.741690158843994\n",
      "[step: 8800] loss: 1.9712483882904053\n",
      "[step: 8801] loss: 1.185025930404663\n",
      "[step: 8802] loss: 1.850250244140625\n",
      "[step: 8803] loss: 2.7688801288604736\n",
      "[step: 8804] loss: 2.674736738204956\n",
      "[step: 8805] loss: 1.679757833480835\n",
      "[step: 8806] loss: 1.296043872833252\n",
      "[step: 8807] loss: 1.7912838459014893\n",
      "[step: 8808] loss: 2.036536931991577\n",
      "[step: 8809] loss: 1.6466913223266602\n",
      "[step: 8810] loss: 1.3427133560180664\n",
      "[step: 8811] loss: 1.5126583576202393\n",
      "[step: 8812] loss: 1.6177968978881836\n",
      "[step: 8813] loss: 1.3807153701782227\n",
      "[step: 8814] loss: 1.3072943687438965\n",
      "[step: 8815] loss: 1.4834699630737305\n",
      "[step: 8816] loss: 1.4552664756774902\n",
      "[step: 8817] loss: 1.2330656051635742\n",
      "[step: 8818] loss: 1.2087961435317993\n",
      "[step: 8819] loss: 1.3900260925292969\n",
      "[step: 8820] loss: 1.4027369022369385\n",
      "[step: 8821] loss: 1.1941678524017334\n",
      "[step: 8822] loss: 1.146201491355896\n",
      "[step: 8823] loss: 1.2862704992294312\n",
      "[step: 8824] loss: 1.3025102615356445\n",
      "[step: 8825] loss: 1.1809641122817993\n",
      "[step: 8826] loss: 1.1345757246017456\n",
      "[step: 8827] loss: 1.20682954788208\n",
      "[step: 8828] loss: 1.2344311475753784\n",
      "[step: 8829] loss: 1.1628506183624268\n",
      "[step: 8830] loss: 1.1278109550476074\n",
      "[step: 8831] loss: 1.1723647117614746\n",
      "[step: 8832] loss: 1.192762851715088\n",
      "[step: 8833] loss: 1.1501495838165283\n",
      "[step: 8834] loss: 1.1141791343688965\n",
      "[step: 8835] loss: 1.1363673210144043\n",
      "[step: 8836] loss: 1.165088176727295\n",
      "[step: 8837] loss: 1.1471595764160156\n",
      "[step: 8838] loss: 1.1129119396209717\n",
      "[step: 8839] loss: 1.1085401773452759\n",
      "[step: 8840] loss: 1.129929542541504\n",
      "[step: 8841] loss: 1.138321876525879\n",
      "[step: 8842] loss: 1.120558261871338\n",
      "[step: 8843] loss: 1.1049833297729492\n",
      "[step: 8844] loss: 1.108078956604004\n",
      "[step: 8845] loss: 1.1160881519317627\n",
      "[step: 8846] loss: 1.1145012378692627\n",
      "[step: 8847] loss: 1.1058365106582642\n",
      "[step: 8848] loss: 1.102806806564331\n",
      "[step: 8849] loss: 1.10599684715271\n",
      "[step: 8850] loss: 1.1063385009765625\n",
      "[step: 8851] loss: 1.100062370300293\n",
      "[step: 8852] loss: 1.0943174362182617\n",
      "[step: 8853] loss: 1.0953147411346436\n",
      "[step: 8854] loss: 1.099708914756775\n",
      "[step: 8855] loss: 1.1004748344421387\n",
      "[step: 8856] loss: 1.0957924127578735\n",
      "[step: 8857] loss: 1.090479850769043\n",
      "[step: 8858] loss: 1.0893367528915405\n",
      "[step: 8859] loss: 1.0913081169128418\n",
      "[step: 8860] loss: 1.091994047164917\n",
      "[step: 8861] loss: 1.0899739265441895\n",
      "[step: 8862] loss: 1.0874700546264648\n",
      "[step: 8863] loss: 1.0868330001831055\n",
      "[step: 8864] loss: 1.0878691673278809\n",
      "[step: 8865] loss: 1.0882668495178223\n",
      "[step: 8866] loss: 1.0868923664093018\n",
      "[step: 8867] loss: 1.0846238136291504\n",
      "[step: 8868] loss: 1.0831221342086792\n",
      "[step: 8869] loss: 1.0828852653503418\n",
      "[step: 8870] loss: 1.0829792022705078\n",
      "[step: 8871] loss: 1.0824041366577148\n",
      "[step: 8872] loss: 1.0811164379119873\n",
      "[step: 8873] loss: 1.0799683332443237\n",
      "[step: 8874] loss: 1.079544186592102\n",
      "[step: 8875] loss: 1.0795855522155762\n",
      "[step: 8876] loss: 1.0795092582702637\n",
      "[step: 8877] loss: 1.0789875984191895\n",
      "[step: 8878] loss: 1.078190803527832\n",
      "[step: 8879] loss: 1.0776500701904297\n",
      "[step: 8880] loss: 1.077579379081726\n",
      "[step: 8881] loss: 1.077857494354248\n",
      "[step: 8882] loss: 1.0782418251037598\n",
      "[step: 8883] loss: 1.0787913799285889\n",
      "[step: 8884] loss: 1.0798835754394531\n",
      "[step: 8885] loss: 1.0822327136993408\n",
      "[step: 8886] loss: 1.08665132522583\n",
      "[step: 8887] loss: 1.0946381092071533\n",
      "[step: 8888] loss: 1.108244776725769\n",
      "[step: 8889] loss: 1.132865309715271\n",
      "[step: 8890] loss: 1.1754896640777588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8891] loss: 1.2552626132965088\n",
      "[step: 8892] loss: 1.3917107582092285\n",
      "[step: 8893] loss: 1.648447036743164\n",
      "[step: 8894] loss: 2.0465784072875977\n",
      "[step: 8895] loss: 2.737281322479248\n",
      "[step: 8896] loss: 3.457933187484741\n",
      "[step: 8897] loss: 4.218636512756348\n",
      "[step: 8898] loss: 3.834613084793091\n",
      "[step: 8899] loss: 2.6927871704101562\n",
      "[step: 8900] loss: 1.4099096059799194\n",
      "[step: 8901] loss: 1.2021106481552124\n",
      "[step: 8902] loss: 1.9413058757781982\n",
      "[step: 8903] loss: 2.5519092082977295\n",
      "[step: 8904] loss: 2.330477714538574\n",
      "[step: 8905] loss: 1.492282509803772\n",
      "[step: 8906] loss: 1.2030210494995117\n",
      "[step: 8907] loss: 1.626828670501709\n",
      "[step: 8908] loss: 1.9233646392822266\n",
      "[step: 8909] loss: 1.6850559711456299\n",
      "[step: 8910] loss: 1.2960344552993774\n",
      "[step: 8911] loss: 1.287656545639038\n",
      "[step: 8912] loss: 1.4821964502334595\n",
      "[step: 8913] loss: 1.4662420749664307\n",
      "[step: 8914] loss: 1.320737600326538\n",
      "[step: 8915] loss: 1.2779176235198975\n",
      "[step: 8916] loss: 1.3218791484832764\n",
      "[step: 8917] loss: 1.2925033569335938\n",
      "[step: 8918] loss: 1.22786545753479\n",
      "[step: 8919] loss: 1.2626903057098389\n",
      "[step: 8920] loss: 1.2993463277816772\n",
      "[step: 8921] loss: 1.2255076169967651\n",
      "[step: 8922] loss: 1.144216775894165\n",
      "[step: 8923] loss: 1.1658120155334473\n",
      "[step: 8924] loss: 1.239249348640442\n",
      "[step: 8925] loss: 1.2302539348602295\n",
      "[step: 8926] loss: 1.1353766918182373\n",
      "[step: 8927] loss: 1.0972366333007812\n",
      "[step: 8928] loss: 1.1523419618606567\n",
      "[step: 8929] loss: 1.1918888092041016\n",
      "[step: 8930] loss: 1.1533973217010498\n",
      "[step: 8931] loss: 1.0947647094726562\n",
      "[step: 8932] loss: 1.0939562320709229\n",
      "[step: 8933] loss: 1.1348440647125244\n",
      "[step: 8934] loss: 1.1443862915039062\n",
      "[step: 8935] loss: 1.1102802753448486\n",
      "[step: 8936] loss: 1.082529902458191\n",
      "[step: 8937] loss: 1.0936188697814941\n",
      "[step: 8938] loss: 1.115790605545044\n",
      "[step: 8939] loss: 1.1106816530227661\n",
      "[step: 8940] loss: 1.0881427526474\n",
      "[step: 8941] loss: 1.077537178993225\n",
      "[step: 8942] loss: 1.0877054929733276\n",
      "[step: 8943] loss: 1.0992908477783203\n",
      "[step: 8944] loss: 1.0923423767089844\n",
      "[step: 8945] loss: 1.0771205425262451\n",
      "[step: 8946] loss: 1.0714943408966064\n",
      "[step: 8947] loss: 1.0780971050262451\n",
      "[step: 8948] loss: 1.085310697555542\n",
      "[step: 8949] loss: 1.0820939540863037\n",
      "[step: 8950] loss: 1.073227047920227\n",
      "[step: 8951] loss: 1.0687673091888428\n",
      "[step: 8952] loss: 1.0711243152618408\n",
      "[step: 8953] loss: 1.0741428136825562\n",
      "[step: 8954] loss: 1.0723176002502441\n",
      "[step: 8955] loss: 1.0676624774932861\n",
      "[step: 8956] loss: 1.065166711807251\n",
      "[step: 8957] loss: 1.0665149688720703\n",
      "[step: 8958] loss: 1.0688297748565674\n",
      "[step: 8959] loss: 1.0682514905929565\n",
      "[step: 8960] loss: 1.0649555921554565\n",
      "[step: 8961] loss: 1.0621612071990967\n",
      "[step: 8962] loss: 1.0616014003753662\n",
      "[step: 8963] loss: 1.0625653266906738\n",
      "[step: 8964] loss: 1.0626989603042603\n",
      "[step: 8965] loss: 1.0611542463302612\n",
      "[step: 8966] loss: 1.0591238737106323\n",
      "[step: 8967] loss: 1.0580207109451294\n",
      "[step: 8968] loss: 1.05818510055542\n",
      "[step: 8969] loss: 1.058588981628418\n",
      "[step: 8970] loss: 1.0582338571548462\n",
      "[step: 8971] loss: 1.0571017265319824\n",
      "[step: 8972] loss: 1.0559977293014526\n",
      "[step: 8973] loss: 1.0556094646453857\n",
      "[step: 8974] loss: 1.0558030605316162\n",
      "[step: 8975] loss: 1.056058645248413\n",
      "[step: 8976] loss: 1.056074857711792\n",
      "[step: 8977] loss: 1.0559628009796143\n",
      "[step: 8978] loss: 1.0564491748809814\n",
      "[step: 8979] loss: 1.058159351348877\n",
      "[step: 8980] loss: 1.0617257356643677\n",
      "[step: 8981] loss: 1.0679774284362793\n",
      "[step: 8982] loss: 1.0791206359863281\n",
      "[step: 8983] loss: 1.0988154411315918\n",
      "[step: 8984] loss: 1.1360266208648682\n",
      "[step: 8985] loss: 1.202907681465149\n",
      "[step: 8986] loss: 1.33184015750885\n",
      "[step: 8987] loss: 1.5545824766159058\n",
      "[step: 8988] loss: 1.9781112670898438\n",
      "[step: 8989] loss: 2.5994796752929688\n",
      "[step: 8990] loss: 3.597120761871338\n",
      "[step: 8991] loss: 4.269895076751709\n",
      "[step: 8992] loss: 4.428256034851074\n",
      "[step: 8993] loss: 3.014033317565918\n",
      "[step: 8994] loss: 1.5051114559173584\n",
      "[step: 8995] loss: 1.191771149635315\n",
      "[step: 8996] loss: 2.0464529991149902\n",
      "[step: 8997] loss: 2.7844715118408203\n",
      "[step: 8998] loss: 2.3234505653381348\n",
      "[step: 8999] loss: 1.4417452812194824\n",
      "[step: 9000] loss: 1.275212049484253\n",
      "[step: 9001] loss: 1.798200249671936\n",
      "[step: 9002] loss: 1.9655977487564087\n",
      "[step: 9003] loss: 1.5174132585525513\n",
      "[step: 9004] loss: 1.283928394317627\n",
      "[step: 9005] loss: 1.4624531269073486\n",
      "[step: 9006] loss: 1.5314068794250488\n",
      "[step: 9007] loss: 1.3467031717300415\n",
      "[step: 9008] loss: 1.27566659450531\n",
      "[step: 9009] loss: 1.3887782096862793\n",
      "[step: 9010] loss: 1.367661476135254\n",
      "[step: 9011] loss: 1.1912097930908203\n",
      "[step: 9012] loss: 1.2090637683868408\n",
      "[step: 9013] loss: 1.3577990531921387\n",
      "[step: 9014] loss: 1.302334189414978\n",
      "[step: 9015] loss: 1.1268653869628906\n",
      "[step: 9016] loss: 1.117471694946289\n",
      "[step: 9017] loss: 1.244786024093628\n",
      "[step: 9018] loss: 1.2539721727371216\n",
      "[step: 9019] loss: 1.128581166267395\n",
      "[step: 9020] loss: 1.0823451280593872\n",
      "[step: 9021] loss: 1.157443881034851\n",
      "[step: 9022] loss: 1.187220573425293\n",
      "[step: 9023] loss: 1.1226470470428467\n",
      "[step: 9024] loss: 1.0785020589828491\n",
      "[step: 9025] loss: 1.1121861934661865\n",
      "[step: 9026] loss: 1.1437208652496338\n",
      "[step: 9027] loss: 1.1119024753570557\n",
      "[step: 9028] loss: 1.0736804008483887\n",
      "[step: 9029] loss: 1.0839022397994995\n",
      "[step: 9030] loss: 1.1108338832855225\n",
      "[step: 9031] loss: 1.104722499847412\n",
      "[step: 9032] loss: 1.0721476078033447\n",
      "[step: 9033] loss: 1.0608386993408203\n",
      "[step: 9034] loss: 1.080075740814209\n",
      "[step: 9035] loss: 1.09209144115448\n",
      "[step: 9036] loss: 1.0787222385406494\n",
      "[step: 9037] loss: 1.0596661567687988\n",
      "[step: 9038] loss: 1.0576828718185425\n",
      "[step: 9039] loss: 1.0684654712677002\n",
      "[step: 9040] loss: 1.0721771717071533\n",
      "[step: 9041] loss: 1.0643787384033203\n",
      "[step: 9042] loss: 1.0569663047790527\n",
      "[step: 9043] loss: 1.0572798252105713\n",
      "[step: 9044] loss: 1.0597456693649292\n",
      "[step: 9045] loss: 1.057328224182129\n",
      "[step: 9046] loss: 1.0519671440124512\n",
      "[step: 9047] loss: 1.050165057182312\n",
      "[step: 9048] loss: 1.053126573562622\n",
      "[step: 9049] loss: 1.0555082559585571\n",
      "[step: 9050] loss: 1.0529701709747314\n",
      "[step: 9051] loss: 1.0474247932434082\n",
      "[step: 9052] loss: 1.044520616531372\n",
      "[step: 9053] loss: 1.0453592538833618\n",
      "[step: 9054] loss: 1.047101616859436\n",
      "[step: 9055] loss: 1.0467312335968018\n",
      "[step: 9056] loss: 1.0443921089172363\n",
      "[step: 9057] loss: 1.042792797088623\n",
      "[step: 9058] loss: 1.0429847240447998\n",
      "[step: 9059] loss: 1.0437465906143188\n",
      "[step: 9060] loss: 1.0433528423309326\n",
      "[step: 9061] loss: 1.0415290594100952\n",
      "[step: 9062] loss: 1.0395866632461548\n",
      "[step: 9063] loss: 1.0387049913406372\n",
      "[step: 9064] loss: 1.0386981964111328\n",
      "[step: 9065] loss: 1.0385510921478271\n",
      "[step: 9066] loss: 1.0376834869384766\n",
      "[step: 9067] loss: 1.0364842414855957\n",
      "[step: 9068] loss: 1.0356090068817139\n",
      "[step: 9069] loss: 1.0353566408157349\n",
      "[step: 9070] loss: 1.0353760719299316\n",
      "[step: 9071] loss: 1.0351059436798096\n",
      "[step: 9072] loss: 1.0344752073287964\n",
      "[step: 9073] loss: 1.0337722301483154\n",
      "[step: 9074] loss: 1.0333714485168457\n",
      "[step: 9075] loss: 1.0333836078643799\n",
      "[step: 9076] loss: 1.0336264371871948\n",
      "[step: 9077] loss: 1.0339769124984741\n",
      "[step: 9078] loss: 1.0346088409423828\n",
      "[step: 9079] loss: 1.0359690189361572\n",
      "[step: 9080] loss: 1.0388668775558472\n",
      "[step: 9081] loss: 1.0442825555801392\n",
      "[step: 9082] loss: 1.054207444190979\n",
      "[step: 9083] loss: 1.0716636180877686\n",
      "[step: 9084] loss: 1.104184627532959\n",
      "[step: 9085] loss: 1.1620908975601196\n",
      "[step: 9086] loss: 1.2729798555374146\n",
      "[step: 9087] loss: 1.4652986526489258\n",
      "[step: 9088] loss: 1.8308403491973877\n",
      "[step: 9089] loss: 2.3838119506835938\n",
      "[step: 9090] loss: 3.301344394683838\n",
      "[step: 9091] loss: 4.054266929626465\n",
      "[step: 9092] loss: 4.493759632110596\n",
      "[step: 9093] loss: 3.3733572959899902\n",
      "[step: 9094] loss: 1.810927152633667\n",
      "[step: 9095] loss: 1.0986812114715576\n",
      "[step: 9096] loss: 1.7092978954315186\n",
      "[step: 9097] loss: 2.622258424758911\n",
      "[step: 9098] loss: 2.520094871520996\n",
      "[step: 9099] loss: 1.6596332788467407\n",
      "[step: 9100] loss: 1.1681809425354004\n",
      "[step: 9101] loss: 1.5888700485229492\n",
      "[step: 9102] loss: 1.996246337890625\n",
      "[step: 9103] loss: 1.6767821311950684\n",
      "[step: 9104] loss: 1.2774066925048828\n",
      "[step: 9105] loss: 1.3236734867095947\n",
      "[step: 9106] loss: 1.5179778337478638\n",
      "[step: 9107] loss: 1.4301363229751587\n",
      "[step: 9108] loss: 1.2494465112686157\n",
      "[step: 9109] loss: 1.2962744235992432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9110] loss: 1.3679518699645996\n",
      "[step: 9111] loss: 1.2417373657226562\n",
      "[step: 9112] loss: 1.1727120876312256\n",
      "[step: 9113] loss: 1.2748322486877441\n",
      "[step: 9114] loss: 1.3049849271774292\n",
      "[step: 9115] loss: 1.1669315099716187\n",
      "[step: 9116] loss: 1.0828306674957275\n",
      "[step: 9117] loss: 1.1771056652069092\n",
      "[step: 9118] loss: 1.248645305633545\n",
      "[step: 9119] loss: 1.157923936843872\n",
      "[step: 9120] loss: 1.0584444999694824\n",
      "[step: 9121] loss: 1.0932191610336304\n",
      "[step: 9122] loss: 1.166327953338623\n",
      "[step: 9123] loss: 1.1416223049163818\n",
      "[step: 9124] loss: 1.0667545795440674\n",
      "[step: 9125] loss: 1.0606532096862793\n",
      "[step: 9126] loss: 1.1075509786605835\n",
      "[step: 9127] loss: 1.1137769222259521\n",
      "[step: 9128] loss: 1.0719882249832153\n",
      "[step: 9129] loss: 1.048799753189087\n",
      "[step: 9130] loss: 1.069610357284546\n",
      "[step: 9131] loss: 1.0892640352249146\n",
      "[step: 9132] loss: 1.0704396963119507\n",
      "[step: 9133] loss: 1.042435884475708\n",
      "[step: 9134] loss: 1.0438815355300903\n",
      "[step: 9135] loss: 1.063154935836792\n",
      "[step: 9136] loss: 1.0666289329528809\n",
      "[step: 9137] loss: 1.04831063747406\n",
      "[step: 9138] loss: 1.0332221984863281\n",
      "[step: 9139] loss: 1.0376698970794678\n",
      "[step: 9140] loss: 1.049180507659912\n",
      "[step: 9141] loss: 1.0495327711105347\n",
      "[step: 9142] loss: 1.039512038230896\n",
      "[step: 9143] loss: 1.0322951078414917\n",
      "[step: 9144] loss: 1.0334646701812744\n",
      "[step: 9145] loss: 1.036707878112793\n",
      "[step: 9146] loss: 1.035073161125183\n",
      "[step: 9147] loss: 1.030342936515808\n",
      "[step: 9148] loss: 1.0286037921905518\n",
      "[step: 9149] loss: 1.0309109687805176\n",
      "[step: 9150] loss: 1.0323830842971802\n",
      "[step: 9151] loss: 1.029311180114746\n",
      "[step: 9152] loss: 1.0245813131332397\n",
      "[step: 9153] loss: 1.0221984386444092\n",
      "[step: 9154] loss: 1.023356318473816\n",
      "[step: 9155] loss: 1.0252611637115479\n",
      "[step: 9156] loss: 1.0248651504516602\n",
      "[step: 9157] loss: 1.0227314233779907\n",
      "[step: 9158] loss: 1.0209379196166992\n",
      "[step: 9159] loss: 1.020754098892212\n",
      "[step: 9160] loss: 1.021346092224121\n",
      "[step: 9161] loss: 1.0209938287734985\n",
      "[step: 9162] loss: 1.0193207263946533\n",
      "[step: 9163] loss: 1.017486572265625\n",
      "[step: 9164] loss: 1.016547679901123\n",
      "[step: 9165] loss: 1.0164988040924072\n",
      "[step: 9166] loss: 1.016491174697876\n",
      "[step: 9167] loss: 1.015887975692749\n",
      "[step: 9168] loss: 1.0148074626922607\n",
      "[step: 9169] loss: 1.0139071941375732\n",
      "[step: 9170] loss: 1.013575792312622\n",
      "[step: 9171] loss: 1.013563632965088\n",
      "[step: 9172] loss: 1.013452410697937\n",
      "[step: 9173] loss: 1.0129997730255127\n",
      "[step: 9174] loss: 1.0124083757400513\n",
      "[step: 9175] loss: 1.0120636224746704\n",
      "[step: 9176] loss: 1.0121536254882812\n",
      "[step: 9177] loss: 1.0126771926879883\n",
      "[step: 9178] loss: 1.0136005878448486\n",
      "[step: 9179] loss: 1.0150964260101318\n",
      "[step: 9180] loss: 1.0178791284561157\n",
      "[step: 9181] loss: 1.0230352878570557\n",
      "[step: 9182] loss: 1.0326663255691528\n",
      "[step: 9183] loss: 1.049757480621338\n",
      "[step: 9184] loss: 1.0813803672790527\n",
      "[step: 9185] loss: 1.13716721534729\n",
      "[step: 9186] loss: 1.24300217628479\n",
      "[step: 9187] loss: 1.4256482124328613\n",
      "[step: 9188] loss: 1.771000862121582\n",
      "[step: 9189] loss: 2.296018362045288\n",
      "[step: 9190] loss: 3.1702914237976074\n",
      "[step: 9191] loss: 3.9270145893096924\n",
      "[step: 9192] loss: 4.431037902832031\n",
      "[step: 9193] loss: 3.444976806640625\n",
      "[step: 9194] loss: 1.9188960790634155\n",
      "[step: 9195] loss: 1.082398533821106\n",
      "[step: 9196] loss: 1.565865397453308\n",
      "[step: 9197] loss: 2.483569622039795\n",
      "[step: 9198] loss: 2.516261339187622\n",
      "[step: 9199] loss: 1.726601243019104\n",
      "[step: 9200] loss: 1.1520286798477173\n",
      "[step: 9201] loss: 1.4846160411834717\n",
      "[step: 9202] loss: 1.9267151355743408\n",
      "[step: 9203] loss: 1.6863398551940918\n",
      "[step: 9204] loss: 1.2827959060668945\n",
      "[step: 9205] loss: 1.2793234586715698\n",
      "[step: 9206] loss: 1.4705226421356201\n",
      "[step: 9207] loss: 1.4099305868148804\n",
      "[step: 9208] loss: 1.223846435546875\n",
      "[step: 9209] loss: 1.2624297142028809\n",
      "[step: 9210] loss: 1.3542898893356323\n",
      "[step: 9211] loss: 1.2298531532287598\n",
      "[step: 9212] loss: 1.1277871131896973\n",
      "[step: 9213] loss: 1.2223879098892212\n",
      "[step: 9214] loss: 1.2950160503387451\n",
      "[step: 9215] loss: 1.1779255867004395\n",
      "[step: 9216] loss: 1.0542538166046143\n",
      "[step: 9217] loss: 1.1193408966064453\n",
      "[step: 9218] loss: 1.219529628753662\n",
      "[step: 9219] loss: 1.163618564605713\n",
      "[step: 9220] loss: 1.0503878593444824\n",
      "[step: 9221] loss: 1.0508015155792236\n",
      "[step: 9222] loss: 1.1269919872283936\n",
      "[step: 9223] loss: 1.1297965049743652\n",
      "[step: 9224] loss: 1.0600018501281738\n",
      "[step: 9225] loss: 1.0352296829223633\n",
      "[step: 9226] loss: 1.0723297595977783\n",
      "[step: 9227] loss: 1.0909984111785889\n",
      "[step: 9228] loss: 1.0590181350708008\n",
      "[step: 9229] loss: 1.0278939008712769\n",
      "[step: 9230] loss: 1.040766716003418\n",
      "[step: 9231] loss: 1.0653091669082642\n",
      "[step: 9232] loss: 1.0555695295333862\n",
      "[step: 9233] loss: 1.0259997844696045\n",
      "[step: 9234] loss: 1.0175368785858154\n",
      "[step: 9235] loss: 1.0346238613128662\n",
      "[step: 9236] loss: 1.0462709665298462\n",
      "[step: 9237] loss: 1.034565806388855\n",
      "[step: 9238] loss: 1.016082525253296\n",
      "[step: 9239] loss: 1.0126862525939941\n",
      "[step: 9240] loss: 1.0222837924957275\n",
      "[step: 9241] loss: 1.0273079872131348\n",
      "[step: 9242] loss: 1.0217242240905762\n",
      "[step: 9243] loss: 1.0139555931091309\n",
      "[step: 9244] loss: 1.0120714902877808\n",
      "[step: 9245] loss: 1.0145998001098633\n",
      "[step: 9246] loss: 1.0141173601150513\n",
      "[step: 9247] loss: 1.0096744298934937\n",
      "[step: 9248] loss: 1.006592035293579\n",
      "[step: 9249] loss: 1.0079410076141357\n",
      "[step: 9250] loss: 1.010837197303772\n",
      "[step: 9251] loss: 1.0102672576904297\n",
      "[step: 9252] loss: 1.0062365531921387\n",
      "[step: 9253] loss: 1.002124547958374\n",
      "[step: 9254] loss: 1.0011370182037354\n",
      "[step: 9255] loss: 1.0025193691253662\n",
      "[step: 9256] loss: 1.003260850906372\n",
      "[step: 9257] loss: 1.002145767211914\n",
      "[step: 9258] loss: 1.0001899003982544\n",
      "[step: 9259] loss: 0.9992581605911255\n",
      "[step: 9260] loss: 0.9997729063034058\n",
      "[step: 9261] loss: 1.000295877456665\n",
      "[step: 9262] loss: 0.9996720552444458\n",
      "[step: 9263] loss: 0.9981257915496826\n",
      "[step: 9264] loss: 0.9966058135032654\n",
      "[step: 9265] loss: 0.9959203004837036\n",
      "[step: 9266] loss: 0.9958379864692688\n",
      "[step: 9267] loss: 0.9956048727035522\n",
      "[step: 9268] loss: 0.9947957992553711\n",
      "[step: 9269] loss: 0.9936915040016174\n",
      "[step: 9270] loss: 0.9928206205368042\n",
      "[step: 9271] loss: 0.9924201965332031\n",
      "[step: 9272] loss: 0.9922925233840942\n",
      "[step: 9273] loss: 0.9920293092727661\n",
      "[step: 9274] loss: 0.9914665818214417\n",
      "[step: 9275] loss: 0.9907789826393127\n",
      "[step: 9276] loss: 0.9902141094207764\n",
      "[step: 9277] loss: 0.9899526238441467\n",
      "[step: 9278] loss: 0.9899159073829651\n",
      "[step: 9279] loss: 0.9899603128433228\n",
      "[step: 9280] loss: 0.9901025891304016\n",
      "[step: 9281] loss: 0.9905955791473389\n",
      "[step: 9282] loss: 0.9918968081474304\n",
      "[step: 9283] loss: 0.9948168992996216\n",
      "[step: 9284] loss: 1.0005240440368652\n",
      "[step: 9285] loss: 1.0115054845809937\n",
      "[step: 9286] loss: 1.0319784879684448\n",
      "[step: 9287] loss: 1.0718767642974854\n",
      "[step: 9288] loss: 1.1466028690338135\n",
      "[step: 9289] loss: 1.2958579063415527\n",
      "[step: 9290] loss: 1.565671443939209\n",
      "[step: 9291] loss: 2.094182014465332\n",
      "[step: 9292] loss: 2.887817859649658\n",
      "[step: 9293] loss: 4.142444133758545\n",
      "[step: 9294] loss: 4.846365451812744\n",
      "[step: 9295] loss: 4.639176368713379\n",
      "[step: 9296] loss: 2.6577720642089844\n",
      "[step: 9297] loss: 1.1703877449035645\n",
      "[step: 9298] loss: 1.5119476318359375\n",
      "[step: 9299] loss: 2.741983652114868\n",
      "[step: 9300] loss: 2.9658679962158203\n",
      "[step: 9301] loss: 1.798827052116394\n",
      "[step: 9302] loss: 1.167593240737915\n",
      "[step: 9303] loss: 1.7505723237991333\n",
      "[step: 9304] loss: 2.121614456176758\n",
      "[step: 9305] loss: 1.6055575609207153\n",
      "[step: 9306] loss: 1.2417775392532349\n",
      "[step: 9307] loss: 1.517985463142395\n",
      "[step: 9308] loss: 1.6014090776443481\n",
      "[step: 9309] loss: 1.262252688407898\n",
      "[step: 9310] loss: 1.2606147527694702\n",
      "[step: 9311] loss: 1.5133657455444336\n",
      "[step: 9312] loss: 1.3410569429397583\n",
      "[step: 9313] loss: 1.0838316679000854\n",
      "[step: 9314] loss: 1.2264831066131592\n",
      "[step: 9315] loss: 1.391057014465332\n",
      "[step: 9316] loss: 1.221489667892456\n",
      "[step: 9317] loss: 1.0379884243011475\n",
      "[step: 9318] loss: 1.1637303829193115\n",
      "[step: 9319] loss: 1.2698874473571777\n",
      "[step: 9320] loss: 1.1195764541625977\n",
      "[step: 9321] loss: 1.0499821901321411\n",
      "[step: 9322] loss: 1.1414475440979004\n",
      "[step: 9323] loss: 1.1507680416107178\n",
      "[step: 9324] loss: 1.0594875812530518\n",
      "[step: 9325] loss: 1.0458955764770508\n",
      "[step: 9326] loss: 1.1147449016571045\n",
      "[step: 9327] loss: 1.105383276939392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9328] loss: 1.0299007892608643\n",
      "[step: 9329] loss: 1.023071527481079\n",
      "[step: 9330] loss: 1.0754332542419434\n",
      "[step: 9331] loss: 1.0760952234268188\n",
      "[step: 9332] loss: 1.0253548622131348\n",
      "[step: 9333] loss: 1.0088059902191162\n",
      "[step: 9334] loss: 1.0379528999328613\n",
      "[step: 9335] loss: 1.0501174926757812\n",
      "[step: 9336] loss: 1.0266196727752686\n",
      "[step: 9337] loss: 1.0082541704177856\n",
      "[step: 9338] loss: 1.0160220861434937\n",
      "[step: 9339] loss: 1.0250903367996216\n",
      "[step: 9340] loss: 1.0158367156982422\n",
      "[step: 9341] loss: 1.0044264793395996\n",
      "[step: 9342] loss: 1.0071001052856445\n",
      "[step: 9343] loss: 1.012624979019165\n",
      "[step: 9344] loss: 1.0075603723526\n",
      "[step: 9345] loss: 0.9974216222763062\n",
      "[step: 9346] loss: 0.9949027299880981\n",
      "[step: 9347] loss: 1.0013401508331299\n",
      "[step: 9348] loss: 1.0040924549102783\n",
      "[step: 9349] loss: 0.9975194334983826\n",
      "[step: 9350] loss: 0.9894742965698242\n",
      "[step: 9351] loss: 0.9886670708656311\n",
      "[step: 9352] loss: 0.9932323098182678\n",
      "[step: 9353] loss: 0.995156466960907\n",
      "[step: 9354] loss: 0.991317629814148\n",
      "[step: 9355] loss: 0.9862263798713684\n",
      "[step: 9356] loss: 0.9849773645401001\n",
      "[step: 9357] loss: 0.9870229959487915\n",
      "[step: 9358] loss: 0.9879390001296997\n",
      "[step: 9359] loss: 0.9859006404876709\n",
      "[step: 9360] loss: 0.9828822016716003\n",
      "[step: 9361] loss: 0.981661319732666\n",
      "[step: 9362] loss: 0.9826170802116394\n",
      "[step: 9363] loss: 0.9834437370300293\n",
      "[step: 9364] loss: 0.9823949337005615\n",
      "[step: 9365] loss: 0.9802545309066772\n",
      "[step: 9366] loss: 0.978614330291748\n",
      "[step: 9367] loss: 0.9783639311790466\n",
      "[step: 9368] loss: 0.9787943959236145\n",
      "[step: 9369] loss: 0.9786914587020874\n",
      "[step: 9370] loss: 0.977725088596344\n",
      "[step: 9371] loss: 0.9765185117721558\n",
      "[step: 9372] loss: 0.9758234620094299\n",
      "[step: 9373] loss: 0.9757115244865417\n",
      "[step: 9374] loss: 0.9756674766540527\n",
      "[step: 9375] loss: 0.9751729965209961\n",
      "[step: 9376] loss: 0.9742514491081238\n",
      "[step: 9377] loss: 0.9733269810676575\n",
      "[step: 9378] loss: 0.972720205783844\n",
      "[step: 9379] loss: 0.972447395324707\n",
      "[step: 9380] loss: 0.972226619720459\n",
      "[step: 9381] loss: 0.9717763662338257\n",
      "[step: 9382] loss: 0.9711165428161621\n",
      "[step: 9383] loss: 0.9704330563545227\n",
      "[step: 9384] loss: 0.9698972702026367\n",
      "[step: 9385] loss: 0.969545841217041\n",
      "[step: 9386] loss: 0.9692480564117432\n",
      "[step: 9387] loss: 0.9688599705696106\n",
      "[step: 9388] loss: 0.9683578610420227\n",
      "[step: 9389] loss: 0.9678032398223877\n",
      "[step: 9390] loss: 0.9672964811325073\n",
      "[step: 9391] loss: 0.9668903350830078\n",
      "[step: 9392] loss: 0.966545581817627\n",
      "[step: 9393] loss: 0.9661999940872192\n",
      "[step: 9394] loss: 0.965816080570221\n",
      "[step: 9395] loss: 0.9654020071029663\n",
      "[step: 9396] loss: 0.9650161862373352\n",
      "[step: 9397] loss: 0.9647348523139954\n",
      "[step: 9398] loss: 0.9646198749542236\n",
      "[step: 9399] loss: 0.9647687673568726\n",
      "[step: 9400] loss: 0.9653745889663696\n",
      "[step: 9401] loss: 0.9668226838111877\n",
      "[step: 9402] loss: 0.9700061678886414\n",
      "[step: 9403] loss: 0.9765945672988892\n",
      "[step: 9404] loss: 0.9903206825256348\n",
      "[step: 9405] loss: 1.0180649757385254\n",
      "[step: 9406] loss: 1.0761915445327759\n",
      "[step: 9407] loss: 1.192394495010376\n",
      "[step: 9408] loss: 1.4391090869903564\n",
      "[step: 9409] loss: 1.9031347036361694\n",
      "[step: 9410] loss: 2.8386104106903076\n",
      "[step: 9411] loss: 4.131250381469727\n",
      "[step: 9412] loss: 5.825896263122559\n",
      "[step: 9413] loss: 5.570224285125732\n",
      "[step: 9414] loss: 3.462857484817505\n",
      "[step: 9415] loss: 1.2621400356292725\n",
      "[step: 9416] loss: 1.6732462644577026\n",
      "[step: 9417] loss: 3.425597667694092\n",
      "[step: 9418] loss: 3.367466688156128\n",
      "[step: 9419] loss: 1.6828888654708862\n",
      "[step: 9420] loss: 1.3287088871002197\n",
      "[step: 9421] loss: 2.376987934112549\n",
      "[step: 9422] loss: 2.244919538497925\n",
      "[step: 9423] loss: 1.2881381511688232\n",
      "[step: 9424] loss: 1.556065320968628\n",
      "[step: 9425] loss: 2.018728256225586\n",
      "[step: 9426] loss: 1.3861619234085083\n",
      "[step: 9427] loss: 1.253899097442627\n",
      "[step: 9428] loss: 1.801577091217041\n",
      "[step: 9429] loss: 1.5356847047805786\n",
      "[step: 9430] loss: 1.0469576120376587\n",
      "[step: 9431] loss: 1.4403142929077148\n",
      "[step: 9432] loss: 1.5482168197631836\n",
      "[step: 9433] loss: 1.1333165168762207\n",
      "[step: 9434] loss: 1.1973565816879272\n",
      "[step: 9435] loss: 1.41441810131073\n",
      "[step: 9436] loss: 1.2287324666976929\n",
      "[step: 9437] loss: 1.0697228908538818\n",
      "[step: 9438] loss: 1.3146588802337646\n",
      "[step: 9439] loss: 1.2789236307144165\n",
      "[step: 9440] loss: 1.0336920022964478\n",
      "[step: 9441] loss: 1.1537877321243286\n",
      "[step: 9442] loss: 1.2499217987060547\n",
      "[step: 9443] loss: 1.1076066493988037\n",
      "[step: 9444] loss: 1.0463736057281494\n",
      "[step: 9445] loss: 1.1804141998291016\n",
      "[step: 9446] loss: 1.140267252922058\n",
      "[step: 9447] loss: 1.014098048210144\n",
      "[step: 9448] loss: 1.0929596424102783\n",
      "[step: 9449] loss: 1.12868070602417\n",
      "[step: 9450] loss: 1.0497996807098389\n",
      "[step: 9451] loss: 1.008103370666504\n",
      "[step: 9452] loss: 1.0785634517669678\n",
      "[step: 9453] loss: 1.0887043476104736\n",
      "[step: 9454] loss: 1.0043257474899292\n",
      "[step: 9455] loss: 1.0181342363357544\n",
      "[step: 9456] loss: 1.0580792427062988\n",
      "[step: 9457] loss: 1.038252353668213\n",
      "[step: 9458] loss: 0.9924221038818359\n",
      "[step: 9459] loss: 1.0064029693603516\n",
      "[step: 9460] loss: 1.0403324365615845\n",
      "[step: 9461] loss: 1.0101732015609741\n",
      "[step: 9462] loss: 0.9855154752731323\n",
      "[step: 9463] loss: 0.9947077035903931\n",
      "[step: 9464] loss: 1.0134410858154297\n",
      "[step: 9465] loss: 1.0014359951019287\n",
      "[step: 9466] loss: 0.978643536567688\n",
      "[step: 9467] loss: 0.984977662563324\n",
      "[step: 9468] loss: 0.9944419264793396\n",
      "[step: 9469] loss: 0.9910115003585815\n",
      "[step: 9470] loss: 0.9775956869125366\n",
      "[step: 9471] loss: 0.9745211601257324\n",
      "[step: 9472] loss: 0.9840124249458313\n",
      "[step: 9473] loss: 0.9831002354621887\n",
      "[step: 9474] loss: 0.9752980470657349\n",
      "[step: 9475] loss: 0.9693617820739746\n",
      "[step: 9476] loss: 0.972386360168457\n",
      "[step: 9477] loss: 0.9774742126464844\n",
      "[step: 9478] loss: 0.9733845591545105\n",
      "[step: 9479] loss: 0.9676373600959778\n",
      "[step: 9480] loss: 0.9653881788253784\n",
      "[step: 9481] loss: 0.9682708978652954\n",
      "[step: 9482] loss: 0.9701408743858337\n",
      "[step: 9483] loss: 0.9668958783149719\n",
      "[step: 9484] loss: 0.9634861946105957\n",
      "[step: 9485] loss: 0.9621239304542542\n",
      "[step: 9486] loss: 0.9637006521224976\n",
      "[step: 9487] loss: 0.96462482213974\n",
      "[step: 9488] loss: 0.9625682830810547\n",
      "[step: 9489] loss: 0.9602607488632202\n",
      "[step: 9490] loss: 0.9589201211929321\n",
      "[step: 9491] loss: 0.9595434665679932\n",
      "[step: 9492] loss: 0.9601290225982666\n",
      "[step: 9493] loss: 0.9591771960258484\n",
      "[step: 9494] loss: 0.9576584100723267\n",
      "[step: 9495] loss: 0.9562522768974304\n",
      "[step: 9496] loss: 0.9560211896896362\n",
      "[step: 9497] loss: 0.9562307596206665\n",
      "[step: 9498] loss: 0.955909788608551\n",
      "[step: 9499] loss: 0.9550929069519043\n",
      "[step: 9500] loss: 0.9539225101470947\n",
      "[step: 9501] loss: 0.9532745480537415\n",
      "[step: 9502] loss: 0.953065037727356\n",
      "[step: 9503] loss: 0.9528821110725403\n",
      "[step: 9504] loss: 0.9525046348571777\n",
      "[step: 9505] loss: 0.9516880512237549\n",
      "[step: 9506] loss: 0.9508994221687317\n",
      "[step: 9507] loss: 0.9503368735313416\n",
      "[step: 9508] loss: 0.9499924778938293\n",
      "[step: 9509] loss: 0.9497826099395752\n",
      "[step: 9510] loss: 0.949370265007019\n",
      "[step: 9511] loss: 0.9488110542297363\n",
      "[step: 9512] loss: 0.9481902718544006\n",
      "[step: 9513] loss: 0.9476239681243896\n",
      "[step: 9514] loss: 0.9472360610961914\n",
      "[step: 9515] loss: 0.9468936324119568\n",
      "[step: 9516] loss: 0.9465439915657043\n",
      "[step: 9517] loss: 0.9461114406585693\n",
      "[step: 9518] loss: 0.9455915093421936\n",
      "[step: 9519] loss: 0.945094108581543\n",
      "[step: 9520] loss: 0.944631814956665\n",
      "[step: 9521] loss: 0.9442384243011475\n",
      "[step: 9522] loss: 0.9438832998275757\n",
      "[step: 9523] loss: 0.9435039162635803\n",
      "[step: 9524] loss: 0.9431039094924927\n",
      "[step: 9525] loss: 0.9426704049110413\n",
      "[step: 9526] loss: 0.9422460794448853\n",
      "[step: 9527] loss: 0.9418668150901794\n",
      "[step: 9528] loss: 0.9415360689163208\n",
      "[step: 9529] loss: 0.9412796497344971\n",
      "[step: 9530] loss: 0.9411011934280396\n",
      "[step: 9531] loss: 0.9410348534584045\n",
      "[step: 9532] loss: 0.9411823153495789\n",
      "[step: 9533] loss: 0.9416889548301697\n",
      "[step: 9534] loss: 0.942879855632782\n",
      "[step: 9535] loss: 0.9452534914016724\n",
      "[step: 9536] loss: 0.9498454928398132\n",
      "[step: 9537] loss: 0.9583303928375244\n",
      "[step: 9538] loss: 0.9744596481323242\n",
      "[step: 9539] loss: 1.0039842128753662\n",
      "[step: 9540] loss: 1.0612876415252686\n",
      "[step: 9541] loss: 1.164886236190796\n",
      "[step: 9542] loss: 1.368852138519287\n",
      "[step: 9543] loss: 1.7146406173706055\n",
      "[step: 9544] loss: 2.3653903007507324\n",
      "[step: 9545] loss: 3.2020294666290283\n",
      "[step: 9546] loss: 4.340872287750244\n",
      "[step: 9547] loss: 4.485367298126221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9548] loss: 3.5964372158050537\n",
      "[step: 9549] loss: 1.789034128189087\n",
      "[step: 9550] loss: 1.0217998027801514\n",
      "[step: 9551] loss: 1.7567479610443115\n",
      "[step: 9552] loss: 2.7105677127838135\n",
      "[step: 9553] loss: 2.5444259643554688\n",
      "[step: 9554] loss: 1.4204726219177246\n",
      "[step: 9555] loss: 1.0803570747375488\n",
      "[step: 9556] loss: 1.7489049434661865\n",
      "[step: 9557] loss: 2.0147860050201416\n",
      "[step: 9558] loss: 1.4780359268188477\n",
      "[step: 9559] loss: 1.1016738414764404\n",
      "[step: 9560] loss: 1.381596565246582\n",
      "[step: 9561] loss: 1.5620652437210083\n",
      "[step: 9562] loss: 1.2436132431030273\n",
      "[step: 9563] loss: 1.105193853378296\n",
      "[step: 9564] loss: 1.3444364070892334\n",
      "[step: 9565] loss: 1.349536418914795\n",
      "[step: 9566] loss: 1.0770416259765625\n",
      "[step: 9567] loss: 1.0372717380523682\n",
      "[step: 9568] loss: 1.2520098686218262\n",
      "[step: 9569] loss: 1.2718749046325684\n",
      "[step: 9570] loss: 1.046320915222168\n",
      "[step: 9571] loss: 1.0021905899047852\n",
      "[step: 9572] loss: 1.13563072681427\n",
      "[step: 9573] loss: 1.1435976028442383\n",
      "[step: 9574] loss: 1.027134895324707\n",
      "[step: 9575] loss: 0.9982955455780029\n",
      "[step: 9576] loss: 1.0767848491668701\n",
      "[step: 9577] loss: 1.074514389038086\n",
      "[step: 9578] loss: 0.9896011352539062\n",
      "[step: 9579] loss: 0.9766687154769897\n",
      "[step: 9580] loss: 1.0351035594940186\n",
      "[step: 9581] loss: 1.0493696928024292\n",
      "[step: 9582] loss: 0.9957709312438965\n",
      "[step: 9583] loss: 0.9594758749008179\n",
      "[step: 9584] loss: 0.9843852519989014\n",
      "[step: 9585] loss: 1.0128272771835327\n",
      "[step: 9586] loss: 0.9954858422279358\n",
      "[step: 9587] loss: 0.9643633365631104\n",
      "[step: 9588] loss: 0.9635130167007446\n",
      "[step: 9589] loss: 0.980617344379425\n",
      "[step: 9590] loss: 0.9809340834617615\n",
      "[step: 9591] loss: 0.9624948501586914\n",
      "[step: 9592] loss: 0.9509511590003967\n",
      "[step: 9593] loss: 0.9593198895454407\n",
      "[step: 9594] loss: 0.9701161980628967\n",
      "[step: 9595] loss: 0.9652940034866333\n",
      "[step: 9596] loss: 0.9506813287734985\n",
      "[step: 9597] loss: 0.9430726766586304\n",
      "[step: 9598] loss: 0.9477464556694031\n",
      "[step: 9599] loss: 0.9557861089706421\n",
      "[step: 9600] loss: 0.9556731581687927\n",
      "[step: 9601] loss: 0.9472529888153076\n",
      "[step: 9602] loss: 0.9398930072784424\n",
      "[step: 9603] loss: 0.9394412040710449\n",
      "[step: 9604] loss: 0.9436689019203186\n",
      "[step: 9605] loss: 0.946347713470459\n",
      "[step: 9606] loss: 0.943942129611969\n",
      "[step: 9607] loss: 0.9388309121131897\n",
      "[step: 9608] loss: 0.935475766658783\n",
      "[step: 9609] loss: 0.9356788992881775\n",
      "[step: 9610] loss: 0.9377748966217041\n",
      "[step: 9611] loss: 0.9390121102333069\n",
      "[step: 9612] loss: 0.9376712441444397\n",
      "[step: 9613] loss: 0.9347053170204163\n",
      "[step: 9614] loss: 0.9322671890258789\n",
      "[step: 9615] loss: 0.931359589099884\n",
      "[step: 9616] loss: 0.9319345355033875\n",
      "[step: 9617] loss: 0.9327841401100159\n",
      "[step: 9618] loss: 0.9327685236930847\n",
      "[step: 9619] loss: 0.931783139705658\n",
      "[step: 9620] loss: 0.9302565455436707\n",
      "[step: 9621] loss: 0.928956925868988\n",
      "[step: 9622] loss: 0.9282805323600769\n",
      "[step: 9623] loss: 0.9281167387962341\n",
      "[step: 9624] loss: 0.9280669093132019\n",
      "[step: 9625] loss: 0.9277774095535278\n",
      "[step: 9626] loss: 0.9271169900894165\n",
      "[step: 9627] loss: 0.9262071251869202\n",
      "[step: 9628] loss: 0.9253507256507874\n",
      "[step: 9629] loss: 0.924720287322998\n",
      "[step: 9630] loss: 0.9243625998497009\n",
      "[step: 9631] loss: 0.9242139458656311\n",
      "[step: 9632] loss: 0.924068033695221\n",
      "[step: 9633] loss: 0.9238043427467346\n",
      "[step: 9634] loss: 0.9233940839767456\n",
      "[step: 9635] loss: 0.9228544235229492\n",
      "[step: 9636] loss: 0.922309160232544\n",
      "[step: 9637] loss: 0.9218443632125854\n",
      "[step: 9638] loss: 0.9215441346168518\n",
      "[step: 9639] loss: 0.921437680721283\n",
      "[step: 9640] loss: 0.9215373992919922\n",
      "[step: 9641] loss: 0.9218815565109253\n",
      "[step: 9642] loss: 0.9225905537605286\n",
      "[step: 9643] loss: 0.9238625764846802\n",
      "[step: 9644] loss: 0.9261679649353027\n",
      "[step: 9645] loss: 0.9302012920379639\n",
      "[step: 9646] loss: 0.9374107122421265\n",
      "[step: 9647] loss: 0.9499053955078125\n",
      "[step: 9648] loss: 0.9725167751312256\n",
      "[step: 9649] loss: 1.0116229057312012\n",
      "[step: 9650] loss: 1.0839675664901733\n",
      "[step: 9651] loss: 1.2069668769836426\n",
      "[step: 9652] loss: 1.436347484588623\n",
      "[step: 9653] loss: 1.7944552898406982\n",
      "[step: 9654] loss: 2.418708324432373\n",
      "[step: 9655] loss: 3.1193103790283203\n",
      "[step: 9656] loss: 3.929560422897339\n",
      "[step: 9657] loss: 3.8068692684173584\n",
      "[step: 9658] loss: 2.885896682739258\n",
      "[step: 9659] loss: 1.4986438751220703\n",
      "[step: 9660] loss: 0.977205753326416\n",
      "[step: 9661] loss: 1.5450307130813599\n",
      "[step: 9662] loss: 2.281437873840332\n",
      "[step: 9663] loss: 2.2474961280822754\n",
      "[step: 9664] loss: 1.4243168830871582\n",
      "[step: 9665] loss: 1.0020630359649658\n",
      "[step: 9666] loss: 1.3867805004119873\n",
      "[step: 9667] loss: 1.7555122375488281\n",
      "[step: 9668] loss: 1.5154063701629639\n",
      "[step: 9669] loss: 1.083803653717041\n",
      "[step: 9670] loss: 1.1206746101379395\n",
      "[step: 9671] loss: 1.4100406169891357\n",
      "[step: 9672] loss: 1.3301405906677246\n",
      "[step: 9673] loss: 1.0317583084106445\n",
      "[step: 9674] loss: 1.025341510772705\n",
      "[step: 9675] loss: 1.238937258720398\n",
      "[step: 9676] loss: 1.2541155815124512\n",
      "[step: 9677] loss: 1.0361214876174927\n",
      "[step: 9678] loss: 0.959338903427124\n",
      "[step: 9679] loss: 1.083561897277832\n",
      "[step: 9680] loss: 1.1436786651611328\n",
      "[step: 9681] loss: 1.0444194078445435\n",
      "[step: 9682] loss: 0.9566771388053894\n",
      "[step: 9683] loss: 1.0111689567565918\n",
      "[step: 9684] loss: 1.079559326171875\n",
      "[step: 9685] loss: 1.0319600105285645\n",
      "[step: 9686] loss: 0.9513855576515198\n",
      "[step: 9687] loss: 0.9522240161895752\n",
      "[step: 9688] loss: 1.010462999343872\n",
      "[step: 9689] loss: 1.024981141090393\n",
      "[step: 9690] loss: 0.9740842580795288\n",
      "[step: 9691] loss: 0.938145101070404\n",
      "[step: 9692] loss: 0.955914318561554\n",
      "[step: 9693] loss: 0.9830043911933899\n",
      "[step: 9694] loss: 0.9758991003036499\n",
      "[step: 9695] loss: 0.9436318874359131\n",
      "[step: 9696] loss: 0.9285532236099243\n",
      "[step: 9697] loss: 0.9429449439048767\n",
      "[step: 9698] loss: 0.9601726531982422\n",
      "[step: 9699] loss: 0.9567763805389404\n",
      "[step: 9700] loss: 0.9367877244949341\n",
      "[step: 9701] loss: 0.9235545992851257\n",
      "[step: 9702] loss: 0.9272644519805908\n",
      "[step: 9703] loss: 0.9386077523231506\n",
      "[step: 9704] loss: 0.942320704460144\n",
      "[step: 9705] loss: 0.9342139959335327\n",
      "[step: 9706] loss: 0.9234619140625\n",
      "[step: 9707] loss: 0.9185618162155151\n",
      "[step: 9708] loss: 0.9218106269836426\n",
      "[step: 9709] loss: 0.9278953075408936\n",
      "[step: 9710] loss: 0.929796040058136\n",
      "[step: 9711] loss: 0.9261643886566162\n",
      "[step: 9712] loss: 0.9198119044303894\n",
      "[step: 9713] loss: 0.9151016473770142\n",
      "[step: 9714] loss: 0.9142542481422424\n",
      "[step: 9715] loss: 0.9162571430206299\n",
      "[step: 9716] loss: 0.9186708927154541\n",
      "[step: 9717] loss: 0.9192996621131897\n",
      "[step: 9718] loss: 0.9177495837211609\n",
      "[step: 9719] loss: 0.9149312973022461\n",
      "[step: 9720] loss: 0.9123325943946838\n",
      "[step: 9721] loss: 0.9108483791351318\n",
      "[step: 9722] loss: 0.9106477499008179\n",
      "[step: 9723] loss: 0.9112719893455505\n",
      "[step: 9724] loss: 0.9118584990501404\n",
      "[step: 9725] loss: 0.9119055271148682\n",
      "[step: 9726] loss: 0.9112223982810974\n",
      "[step: 9727] loss: 0.9099523425102234\n",
      "[step: 9728] loss: 0.9084343910217285\n",
      "[step: 9729] loss: 0.9070345163345337\n",
      "[step: 9730] loss: 0.9059635400772095\n",
      "[step: 9731] loss: 0.9053189754486084\n",
      "[step: 9732] loss: 0.9050172567367554\n",
      "[step: 9733] loss: 0.904961109161377\n",
      "[step: 9734] loss: 0.905002236366272\n",
      "[step: 9735] loss: 0.9050010442733765\n",
      "[step: 9736] loss: 0.9049026966094971\n",
      "[step: 9737] loss: 0.9046651124954224\n",
      "[step: 9738] loss: 0.9042930006980896\n",
      "[step: 9739] loss: 0.9038074016571045\n",
      "[step: 9740] loss: 0.903268575668335\n",
      "[step: 9741] loss: 0.902687668800354\n",
      "[step: 9742] loss: 0.9021092057228088\n",
      "[step: 9743] loss: 0.9015671014785767\n",
      "[step: 9744] loss: 0.9010788798332214\n",
      "[step: 9745] loss: 0.9006907939910889\n",
      "[step: 9746] loss: 0.9004397988319397\n",
      "[step: 9747] loss: 0.9004302024841309\n",
      "[step: 9748] loss: 0.9007741808891296\n",
      "[step: 9749] loss: 0.9017945528030396\n",
      "[step: 9750] loss: 0.9039542078971863\n",
      "[step: 9751] loss: 0.9083958864212036\n",
      "[step: 9752] loss: 0.9168460369110107\n",
      "[step: 9753] loss: 0.9336448907852173\n",
      "[step: 9754] loss: 0.9652056694030762\n",
      "[step: 9755] loss: 1.0289881229400635\n",
      "[step: 9756] loss: 1.1477839946746826\n",
      "[step: 9757] loss: 1.391722559928894\n",
      "[step: 9758] loss: 1.8157033920288086\n",
      "[step: 9759] loss: 2.6389336585998535\n",
      "[step: 9760] loss: 3.689195156097412\n",
      "[step: 9761] loss: 5.046600341796875\n",
      "[step: 9762] loss: 4.962507724761963\n",
      "[step: 9763] loss: 3.4693965911865234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9764] loss: 1.4740290641784668\n",
      "[step: 9765] loss: 1.2805511951446533\n",
      "[step: 9766] loss: 2.6628565788269043\n",
      "[step: 9767] loss: 3.170039176940918\n",
      "[step: 9768] loss: 1.990686297416687\n",
      "[step: 9769] loss: 1.0201597213745117\n",
      "[step: 9770] loss: 1.7042511701583862\n",
      "[step: 9771] loss: 2.3883824348449707\n",
      "[step: 9772] loss: 1.6360169649124146\n",
      "[step: 9773] loss: 1.1083930730819702\n",
      "[step: 9774] loss: 1.5717623233795166\n",
      "[step: 9775] loss: 1.7280387878417969\n",
      "[step: 9776] loss: 1.1761693954467773\n",
      "[step: 9777] loss: 1.1559569835662842\n",
      "[step: 9778] loss: 1.5572001934051514\n",
      "[step: 9779] loss: 1.3804335594177246\n",
      "[step: 9780] loss: 0.9905152320861816\n",
      "[step: 9781] loss: 1.1620347499847412\n",
      "[step: 9782] loss: 1.3946306705474854\n",
      "[step: 9783] loss: 1.131791114807129\n",
      "[step: 9784] loss: 0.9978002905845642\n",
      "[step: 9785] loss: 1.2003170251846313\n",
      "[step: 9786] loss: 1.1998697519302368\n",
      "[step: 9787] loss: 0.9849445819854736\n",
      "[step: 9788] loss: 0.9933534264564514\n",
      "[step: 9789] loss: 1.1347332000732422\n",
      "[step: 9790] loss: 1.0774368047714233\n",
      "[step: 9791] loss: 0.9604413509368896\n",
      "[step: 9792] loss: 1.011068344116211\n",
      "[step: 9793] loss: 1.0782743692398071\n",
      "[step: 9794] loss: 0.9994714260101318\n",
      "[step: 9795] loss: 0.9320924878120422\n",
      "[step: 9796] loss: 0.9844181537628174\n",
      "[step: 9797] loss: 1.0222504138946533\n",
      "[step: 9798] loss: 0.9695621728897095\n",
      "[step: 9799] loss: 0.9304627180099487\n",
      "[step: 9800] loss: 0.9630125164985657\n",
      "[step: 9801] loss: 0.9842996597290039\n",
      "[step: 9802] loss: 0.9498690962791443\n",
      "[step: 9803] loss: 0.9195334315299988\n",
      "[step: 9804] loss: 0.9375089406967163\n",
      "[step: 9805] loss: 0.9608500003814697\n",
      "[step: 9806] loss: 0.9460831880569458\n",
      "[step: 9807] loss: 0.9196074604988098\n",
      "[step: 9808] loss: 0.9198665022850037\n",
      "[step: 9809] loss: 0.9360237121582031\n",
      "[step: 9810] loss: 0.9368327856063843\n",
      "[step: 9811] loss: 0.9197983145713806\n",
      "[step: 9812] loss: 0.9089671969413757\n",
      "[step: 9813] loss: 0.9157819747924805\n",
      "[step: 9814] loss: 0.925380527973175\n",
      "[step: 9815] loss: 0.9224227070808411\n",
      "[step: 9816] loss: 0.9111602306365967\n",
      "[step: 9817] loss: 0.9046968221664429\n",
      "[step: 9818] loss: 0.9078707695007324\n",
      "[step: 9819] loss: 0.9132309556007385\n",
      "[step: 9820] loss: 0.912194550037384\n",
      "[step: 9821] loss: 0.9059265851974487\n",
      "[step: 9822] loss: 0.9011944532394409\n",
      "[step: 9823] loss: 0.9014677405357361\n",
      "[step: 9824] loss: 0.9046598672866821\n",
      "[step: 9825] loss: 0.906000018119812\n",
      "[step: 9826] loss: 0.9034984111785889\n",
      "[step: 9827] loss: 0.8993620872497559\n",
      "[step: 9828] loss: 0.8967459201812744\n",
      "[step: 9829] loss: 0.8968204259872437\n",
      "[step: 9830] loss: 0.8982496857643127\n",
      "[step: 9831] loss: 0.8989456295967102\n",
      "[step: 9832] loss: 0.8979285359382629\n",
      "[step: 9833] loss: 0.8957833647727966\n",
      "[step: 9834] loss: 0.8938300013542175\n",
      "[step: 9835] loss: 0.8929131031036377\n",
      "[step: 9836] loss: 0.8930649757385254\n",
      "[step: 9837] loss: 0.8935176134109497\n",
      "[step: 9838] loss: 0.8935498595237732\n",
      "[step: 9839] loss: 0.8928990364074707\n",
      "[step: 9840] loss: 0.8916617631912231\n",
      "[step: 9841] loss: 0.8903281688690186\n",
      "[step: 9842] loss: 0.8892983198165894\n",
      "[step: 9843] loss: 0.8886817693710327\n",
      "[step: 9844] loss: 0.8884822130203247\n",
      "[step: 9845] loss: 0.8884263038635254\n",
      "[step: 9846] loss: 0.8883111476898193\n",
      "[step: 9847] loss: 0.8880172967910767\n",
      "[step: 9848] loss: 0.8874852061271667\n",
      "[step: 9849] loss: 0.8868137001991272\n",
      "[step: 9850] loss: 0.8860671520233154\n",
      "[step: 9851] loss: 0.885344922542572\n",
      "[step: 9852] loss: 0.8847199082374573\n",
      "[step: 9853] loss: 0.8841878175735474\n",
      "[step: 9854] loss: 0.8837565183639526\n",
      "[step: 9855] loss: 0.8834015130996704\n",
      "[step: 9856] loss: 0.8830955028533936\n",
      "[step: 9857] loss: 0.8828200101852417\n",
      "[step: 9858] loss: 0.8825559616088867\n",
      "[step: 9859] loss: 0.8823032379150391\n",
      "[step: 9860] loss: 0.8820610046386719\n",
      "[step: 9861] loss: 0.8818437457084656\n",
      "[step: 9862] loss: 0.8816720247268677\n",
      "[step: 9863] loss: 0.8815672397613525\n",
      "[step: 9864] loss: 0.8815857172012329\n",
      "[step: 9865] loss: 0.8817652463912964\n",
      "[step: 9866] loss: 0.8822259902954102\n",
      "[step: 9867] loss: 0.8830693960189819\n",
      "[step: 9868] loss: 0.8845779895782471\n",
      "[step: 9869] loss: 0.8870140314102173\n",
      "[step: 9870] loss: 0.8911458253860474\n",
      "[step: 9871] loss: 0.8976311087608337\n",
      "[step: 9872] loss: 0.9086629152297974\n",
      "[step: 9873] loss: 0.9258596897125244\n",
      "[step: 9874] loss: 0.9558783173561096\n",
      "[step: 9875] loss: 1.0022087097167969\n",
      "[step: 9876] loss: 1.08510422706604\n",
      "[step: 9877] loss: 1.2080614566802979\n",
      "[step: 9878] loss: 1.4290200471878052\n",
      "[step: 9879] loss: 1.7202599048614502\n",
      "[step: 9880] loss: 2.204174518585205\n",
      "[step: 9881] loss: 2.657111406326294\n",
      "[step: 9882] loss: 3.1762709617614746\n",
      "[step: 9883] loss: 3.1617276668548584\n",
      "[step: 9884] loss: 2.757277488708496\n",
      "[step: 9885] loss: 1.8873525857925415\n",
      "[step: 9886] loss: 1.353772521018982\n",
      "[step: 9887] loss: 1.4567673206329346\n",
      "[step: 9888] loss: 1.8718607425689697\n",
      "[step: 9889] loss: 2.129514455795288\n",
      "[step: 9890] loss: 1.5888347625732422\n",
      "[step: 9891] loss: 1.0438960790634155\n",
      "[step: 9892] loss: 1.0765397548675537\n",
      "[step: 9893] loss: 1.5093868970870972\n",
      "[step: 9894] loss: 1.793919563293457\n",
      "[step: 9895] loss: 1.3662371635437012\n",
      "[step: 9896] loss: 0.9513862133026123\n",
      "[step: 9897] loss: 1.0309637784957886\n",
      "[step: 9898] loss: 1.3545804023742676\n",
      "[step: 9899] loss: 1.450451374053955\n",
      "[step: 9900] loss: 1.1376192569732666\n",
      "[step: 9901] loss: 0.9312615394592285\n",
      "[step: 9902] loss: 1.0403980016708374\n",
      "[step: 9903] loss: 1.2210803031921387\n",
      "[step: 9904] loss: 1.2317023277282715\n",
      "[step: 9905] loss: 1.0425403118133545\n",
      "[step: 9906] loss: 0.9331384301185608\n",
      "[step: 9907] loss: 1.010779619216919\n",
      "[step: 9908] loss: 1.1049070358276367\n",
      "[step: 9909] loss: 1.091309666633606\n",
      "[step: 9910] loss: 0.981295645236969\n",
      "[step: 9911] loss: 0.9168708324432373\n",
      "[step: 9912] loss: 0.9678235650062561\n",
      "[step: 9913] loss: 1.0264240503311157\n",
      "[step: 9914] loss: 1.0181827545166016\n",
      "[step: 9915] loss: 0.9512216448783875\n",
      "[step: 9916] loss: 0.8958781361579895\n",
      "[step: 9917] loss: 0.9161461591720581\n",
      "[step: 9918] loss: 0.9624946117401123\n",
      "[step: 9919] loss: 0.9779825806617737\n",
      "[step: 9920] loss: 0.9482667446136475\n",
      "[step: 9921] loss: 0.8990800976753235\n",
      "[step: 9922] loss: 0.883679986000061\n",
      "[step: 9923] loss: 0.9018214344978333\n",
      "[step: 9924] loss: 0.9286282658576965\n",
      "[step: 9925] loss: 0.9383301138877869\n",
      "[step: 9926] loss: 0.919166624546051\n",
      "[step: 9927] loss: 0.8947948217391968\n",
      "[step: 9928] loss: 0.8805297613143921\n",
      "[step: 9929] loss: 0.8822668790817261\n",
      "[step: 9930] loss: 0.8942372798919678\n",
      "[step: 9931] loss: 0.9014153480529785\n",
      "[step: 9932] loss: 0.9016660451889038\n",
      "[step: 9933] loss: 0.8953964114189148\n",
      "[step: 9934] loss: 0.8881785273551941\n",
      "[step: 9935] loss: 0.8843579292297363\n",
      "[step: 9936] loss: 0.8814378380775452\n",
      "[step: 9937] loss: 0.880256712436676\n",
      "[step: 9938] loss: 0.8779094219207764\n",
      "[step: 9939] loss: 0.8756104111671448\n",
      "[step: 9940] loss: 0.8751352429389954\n",
      "[step: 9941] loss: 0.8760508298873901\n",
      "[step: 9942] loss: 0.8793016672134399\n",
      "[step: 9943] loss: 0.8817224502563477\n",
      "[step: 9944] loss: 0.8827168941497803\n",
      "[step: 9945] loss: 0.8815021514892578\n",
      "[step: 9946] loss: 0.87867271900177\n",
      "[step: 9947] loss: 0.8760254383087158\n",
      "[step: 9948] loss: 0.8741739988327026\n",
      "[step: 9949] loss: 0.8736128807067871\n",
      "[step: 9950] loss: 0.8739990592002869\n",
      "[step: 9951] loss: 0.8742645978927612\n",
      "[step: 9952] loss: 0.8745664358139038\n",
      "[step: 9953] loss: 0.8745114803314209\n",
      "[step: 9954] loss: 0.8750102519989014\n",
      "[step: 9955] loss: 0.8765857219696045\n",
      "[step: 9956] loss: 0.880146861076355\n",
      "[step: 9957] loss: 0.8865528106689453\n",
      "[step: 9958] loss: 0.8973867893218994\n",
      "[step: 9959] loss: 0.9141439199447632\n",
      "[step: 9960] loss: 0.942850649356842\n",
      "[step: 9961] loss: 0.9871237277984619\n",
      "[step: 9962] loss: 1.0658624172210693\n",
      "[step: 9963] loss: 1.18578040599823\n",
      "[step: 9964] loss: 1.399282455444336\n",
      "[step: 9965] loss: 1.6926429271697998\n",
      "[step: 9966] loss: 2.1695754528045654\n",
      "[step: 9967] loss: 2.6077451705932617\n",
      "[step: 9968] loss: 3.0457005500793457\n",
      "[step: 9969] loss: 2.832160472869873\n",
      "[step: 9970] loss: 2.178804874420166\n",
      "[step: 9971] loss: 1.318082571029663\n",
      "[step: 9972] loss: 0.9490488171577454\n",
      "[step: 9973] loss: 1.203664779663086\n",
      "[step: 9974] loss: 1.6203675270080566\n",
      "[step: 9975] loss: 1.7218822240829468\n",
      "[step: 9976] loss: 1.3275022506713867\n",
      "[step: 9977] loss: 0.9801422357559204\n",
      "[step: 9978] loss: 1.048384428024292\n",
      "[step: 9979] loss: 1.3152778148651123\n",
      "[step: 9980] loss: 1.3773000240325928\n",
      "[step: 9981] loss: 1.102919101715088\n",
      "[step: 9982] loss: 0.9020681381225586\n",
      "[step: 9983] loss: 0.9953044652938843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9984] loss: 1.1689890623092651\n",
      "[step: 9985] loss: 1.1562604904174805\n",
      "[step: 9986] loss: 0.967316210269928\n",
      "[step: 9987] loss: 0.8875994682312012\n",
      "[step: 9988] loss: 0.9804215431213379\n",
      "[step: 9989] loss: 1.0649731159210205\n",
      "[step: 9990] loss: 1.017586350440979\n",
      "[step: 9991] loss: 0.9092426300048828\n",
      "[step: 9992] loss: 0.8934248089790344\n",
      "[step: 9993] loss: 0.9599166512489319\n",
      "[step: 9994] loss: 0.9919776916503906\n",
      "[step: 9995] loss: 0.9457440376281738\n",
      "[step: 9996] loss: 0.8829129934310913\n",
      "[step: 9997] loss: 0.881706953048706\n",
      "[step: 9998] loss: 0.9255145192146301\n",
      "[step: 9999] loss: 0.9456993341445923\n",
      "[step: 10000] loss: 0.920383632183075\n",
      "[step: 10001] loss: 0.8820508718490601\n",
      "[step: 10002] loss: 0.8723617792129517\n",
      "[step: 10003] loss: 0.8914725184440613\n",
      "[step: 10004] loss: 0.9073714017868042\n",
      "[step: 10005] loss: 0.9013916254043579\n",
      "[step: 10006] loss: 0.8809604644775391\n",
      "[step: 10007] loss: 0.8685742616653442\n",
      "[step: 10008] loss: 0.8730458617210388\n",
      "[step: 10009] loss: 0.8838855624198914\n",
      "[step: 10010] loss: 0.8868663907051086\n",
      "[step: 10011] loss: 0.8781198263168335\n",
      "[step: 10012] loss: 0.8665898442268372\n",
      "[step: 10013] loss: 0.8613615036010742\n",
      "[step: 10014] loss: 0.8648960590362549\n",
      "[step: 10015] loss: 0.8714371919631958\n",
      "[step: 10016] loss: 0.8737438321113586\n",
      "[step: 10017] loss: 0.8698959946632385\n",
      "[step: 10018] loss: 0.8627955913543701\n",
      "[step: 10019] loss: 0.8574807643890381\n",
      "[step: 10020] loss: 0.8566762208938599\n",
      "[step: 10021] loss: 0.8593771457672119\n",
      "[step: 10022] loss: 0.8623527884483337\n",
      "[step: 10023] loss: 0.8628253936767578\n",
      "[step: 10024] loss: 0.8605169057846069\n",
      "[step: 10025] loss: 0.8568661212921143\n",
      "[step: 10026] loss: 0.8539057374000549\n",
      "[step: 10027] loss: 0.85288405418396\n",
      "[step: 10028] loss: 0.8536301851272583\n",
      "[step: 10029] loss: 0.8549154996871948\n",
      "[step: 10030] loss: 0.8555647730827332\n",
      "[step: 10031] loss: 0.8550623059272766\n",
      "[step: 10032] loss: 0.8535323143005371\n",
      "[step: 10033] loss: 0.8517634868621826\n",
      "[step: 10034] loss: 0.8504624366760254\n",
      "[step: 10035] loss: 0.8499717116355896\n",
      "[step: 10036] loss: 0.8502044677734375\n",
      "[step: 10037] loss: 0.8508291244506836\n",
      "[step: 10038] loss: 0.8514231443405151\n",
      "[step: 10039] loss: 0.8518818616867065\n",
      "[step: 10040] loss: 0.8522679209709167\n",
      "[step: 10041] loss: 0.8532029390335083\n",
      "[step: 10042] loss: 0.8553465008735657\n",
      "[step: 10043] loss: 0.8600873947143555\n",
      "[step: 10044] loss: 0.8691931366920471\n",
      "[step: 10045] loss: 0.8864898681640625\n",
      "[step: 10046] loss: 0.917827844619751\n",
      "[step: 10047] loss: 0.9768617749214172\n",
      "[step: 10048] loss: 1.0830233097076416\n",
      "[step: 10049] loss: 1.2858396768569946\n",
      "[step: 10050] loss: 1.632794976234436\n",
      "[step: 10051] loss: 2.2647042274475098\n",
      "[step: 10052] loss: 3.106281280517578\n",
      "[step: 10053] loss: 4.193881511688232\n",
      "[step: 10054] loss: 4.4117431640625\n",
      "[step: 10055] loss: 3.5293493270874023\n",
      "[step: 10056] loss: 1.7377349138259888\n",
      "[step: 10057] loss: 0.9193170070648193\n",
      "[step: 10058] loss: 1.6101913452148438\n",
      "[step: 10059] loss: 2.5524959564208984\n",
      "[step: 10060] loss: 2.3596086502075195\n",
      "[step: 10061] loss: 1.2827785015106201\n",
      "[step: 10062] loss: 1.0111267566680908\n",
      "[step: 10063] loss: 1.6713213920593262\n",
      "[step: 10064] loss: 1.8443901538848877\n",
      "[step: 10065] loss: 1.2716047763824463\n",
      "[step: 10066] loss: 1.020304799079895\n",
      "[step: 10067] loss: 1.3640027046203613\n",
      "[step: 10068] loss: 1.4433069229125977\n",
      "[step: 10069] loss: 1.0494053363800049\n",
      "[step: 10070] loss: 0.9897627830505371\n",
      "[step: 10071] loss: 1.2999835014343262\n",
      "[step: 10072] loss: 1.2452616691589355\n",
      "[step: 10073] loss: 0.9430655241012573\n",
      "[step: 10074] loss: 0.9555826187133789\n",
      "[step: 10075] loss: 1.1612145900726318\n",
      "[step: 10076] loss: 1.1105602979660034\n",
      "[step: 10077] loss: 0.926044225692749\n",
      "[step: 10078] loss: 0.9609803557395935\n",
      "[step: 10079] loss: 1.077040195465088\n",
      "[step: 10080] loss: 1.0094530582427979\n",
      "[step: 10081] loss: 0.8879635334014893\n",
      "[step: 10082] loss: 0.9246564507484436\n",
      "[step: 10083] loss: 1.0078790187835693\n",
      "[step: 10084] loss: 0.9626146554946899\n",
      "[step: 10085] loss: 0.8855814337730408\n",
      "[step: 10086] loss: 0.9032012224197388\n",
      "[step: 10087] loss: 0.9527938365936279\n",
      "[step: 10088] loss: 0.9334415197372437\n",
      "[step: 10089] loss: 0.8792555332183838\n",
      "[step: 10090] loss: 0.8800361156463623\n",
      "[step: 10091] loss: 0.9191237688064575\n",
      "[step: 10092] loss: 0.9207888841629028\n",
      "[step: 10093] loss: 0.8821215629577637\n",
      "[step: 10094] loss: 0.8629721999168396\n",
      "[step: 10095] loss: 0.8817822337150574\n",
      "[step: 10096] loss: 0.898635745048523\n",
      "[step: 10097] loss: 0.8856412172317505\n",
      "[step: 10098] loss: 0.8628458380699158\n",
      "[step: 10099] loss: 0.8606095314025879\n",
      "[step: 10100] loss: 0.8748870491981506\n",
      "[step: 10101] loss: 0.8804925680160522\n",
      "[step: 10102] loss: 0.8696340322494507\n",
      "[step: 10103] loss: 0.8557634353637695\n",
      "[step: 10104] loss: 0.8534430265426636\n",
      "[step: 10105] loss: 0.8616994619369507\n",
      "[step: 10106] loss: 0.866950511932373\n",
      "[step: 10107] loss: 0.8627885580062866\n",
      "[step: 10108] loss: 0.8542256355285645\n",
      "[step: 10109] loss: 0.8493334054946899\n",
      "[step: 10110] loss: 0.8513051271438599\n",
      "[step: 10111] loss: 0.8552379012107849\n",
      "[step: 10112] loss: 0.8558806777000427\n",
      "[step: 10113] loss: 0.852406919002533\n",
      "[step: 10114] loss: 0.8477345705032349\n",
      "[step: 10115] loss: 0.8455178737640381\n",
      "[step: 10116] loss: 0.8464521169662476\n",
      "[step: 10117] loss: 0.8484121561050415\n",
      "[step: 10118] loss: 0.8489917516708374\n",
      "[step: 10119] loss: 0.8474223613739014\n",
      "[step: 10120] loss: 0.8446282148361206\n",
      "[step: 10121] loss: 0.8422449827194214\n",
      "[step: 10122] loss: 0.8413326144218445\n",
      "[step: 10123] loss: 0.8417356014251709\n",
      "[step: 10124] loss: 0.8425720930099487\n",
      "[step: 10125] loss: 0.842877209186554\n",
      "[step: 10126] loss: 0.8422407507896423\n",
      "[step: 10127] loss: 0.8408956527709961\n",
      "[step: 10128] loss: 0.8393417596817017\n",
      "[step: 10129] loss: 0.8381073474884033\n",
      "[step: 10130] loss: 0.8374946713447571\n",
      "[step: 10131] loss: 0.8373396396636963\n",
      "[step: 10132] loss: 0.837446928024292\n",
      "[step: 10133] loss: 0.837500810623169\n",
      "[step: 10134] loss: 0.8372831344604492\n",
      "[step: 10135] loss: 0.8367940783500671\n",
      "[step: 10136] loss: 0.836071789264679\n",
      "[step: 10137] loss: 0.8352706432342529\n",
      "[step: 10138] loss: 0.8345025181770325\n",
      "[step: 10139] loss: 0.8338513374328613\n",
      "[step: 10140] loss: 0.8333609104156494\n",
      "[step: 10141] loss: 0.8329968452453613\n",
      "[step: 10142] loss: 0.8327235579490662\n",
      "[step: 10143] loss: 0.8324897289276123\n",
      "[step: 10144] loss: 0.8322511911392212\n",
      "[step: 10145] loss: 0.8319733142852783\n",
      "[step: 10146] loss: 0.8316528797149658\n",
      "[step: 10147] loss: 0.8313026428222656\n",
      "[step: 10148] loss: 0.8309282064437866\n",
      "[step: 10149] loss: 0.8305641412734985\n",
      "[step: 10150] loss: 0.8302350044250488\n",
      "[step: 10151] loss: 0.8299924731254578\n",
      "[step: 10152] loss: 0.8298882246017456\n",
      "[step: 10153] loss: 0.8300417065620422\n",
      "[step: 10154] loss: 0.8306068181991577\n",
      "[step: 10155] loss: 0.8319253921508789\n",
      "[step: 10156] loss: 0.8344966173171997\n",
      "[step: 10157] loss: 0.8394796848297119\n",
      "[step: 10158] loss: 0.8486244678497314\n",
      "[step: 10159] loss: 0.8660479784011841\n",
      "[step: 10160] loss: 0.8978678584098816\n",
      "[step: 10161] loss: 0.9595374464988708\n",
      "[step: 10162] loss: 1.071448802947998\n",
      "[step: 10163] loss: 1.2910070419311523\n",
      "[step: 10164] loss: 1.6655523777008057\n",
      "[step: 10165] loss: 2.3593544960021973\n",
      "[step: 10166] loss: 3.2636494636535645\n",
      "[step: 10167] loss: 4.4180450439453125\n",
      "[step: 10168] loss: 4.565817832946777\n",
      "[step: 10169] loss: 3.4951322078704834\n",
      "[step: 10170] loss: 1.6331018209457397\n",
      "[step: 10171] loss: 0.9765529632568359\n",
      "[step: 10172] loss: 1.8972868919372559\n",
      "[step: 10173] loss: 2.7578561305999756\n",
      "[step: 10174] loss: 2.2392578125\n",
      "[step: 10175] loss: 1.0882885456085205\n",
      "[step: 10176] loss: 1.145555019378662\n",
      "[step: 10177] loss: 1.9265477657318115\n",
      "[step: 10178] loss: 1.7920247316360474\n",
      "[step: 10179] loss: 1.1282448768615723\n",
      "[step: 10180] loss: 1.1440751552581787\n",
      "[step: 10181] loss: 1.5475528240203857\n",
      "[step: 10182] loss: 1.3396873474121094\n",
      "[step: 10183] loss: 0.931793749332428\n",
      "[step: 10184] loss: 1.132493019104004\n",
      "[step: 10185] loss: 1.4044079780578613\n",
      "[step: 10186] loss: 1.1188325881958008\n",
      "[step: 10187] loss: 0.89766526222229\n",
      "[step: 10188] loss: 1.1006190776824951\n",
      "[step: 10189] loss: 1.1910572052001953\n",
      "[step: 10190] loss: 0.9662964344024658\n",
      "[step: 10191] loss: 0.9085258841514587\n",
      "[step: 10192] loss: 1.0746850967407227\n",
      "[step: 10193] loss: 1.0771743059158325\n",
      "[step: 10194] loss: 0.9150995016098022\n",
      "[step: 10195] loss: 0.8974854350090027\n",
      "[step: 10196] loss: 1.0041368007659912\n",
      "[step: 10197] loss: 0.9733625650405884\n",
      "[step: 10198] loss: 0.8724818229675293\n",
      "[step: 10199] loss: 0.8941565752029419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10200] loss: 0.963750422000885\n",
      "[step: 10201] loss: 0.9414619207382202\n",
      "[step: 10202] loss: 0.8709675669670105\n",
      "[step: 10203] loss: 0.8713319301605225\n",
      "[step: 10204] loss: 0.9169189929962158\n",
      "[step: 10205] loss: 0.9070974588394165\n",
      "[step: 10206] loss: 0.8605207204818726\n",
      "[step: 10207] loss: 0.8536196947097778\n",
      "[step: 10208] loss: 0.8850160837173462\n",
      "[step: 10209] loss: 0.8939977884292603\n",
      "[step: 10210] loss: 0.8652981519699097\n",
      "[step: 10211] loss: 0.8451988697052002\n",
      "[step: 10212] loss: 0.8562192320823669\n",
      "[step: 10213] loss: 0.8709872961044312\n",
      "[step: 10214] loss: 0.8632910847663879\n",
      "[step: 10215] loss: 0.8435440063476562\n",
      "[step: 10216] loss: 0.8384953141212463\n",
      "[step: 10217] loss: 0.8487513065338135\n",
      "[step: 10218] loss: 0.8567047119140625\n",
      "[step: 10219] loss: 0.8511453866958618\n",
      "[step: 10220] loss: 0.8387818336486816\n",
      "[step: 10221] loss: 0.8342932462692261\n",
      "[step: 10222] loss: 0.8388667106628418\n",
      "[step: 10223] loss: 0.8438014984130859\n",
      "[step: 10224] loss: 0.8421411514282227\n",
      "[step: 10225] loss: 0.8349025249481201\n",
      "[step: 10226] loss: 0.8298401832580566\n",
      "[step: 10227] loss: 0.8302826881408691\n",
      "[step: 10228] loss: 0.8337892889976501\n",
      "[step: 10229] loss: 0.8358743190765381\n",
      "[step: 10230] loss: 0.8340023756027222\n",
      "[step: 10231] loss: 0.8299287557601929\n",
      "[step: 10232] loss: 0.826806902885437\n",
      "[step: 10233] loss: 0.8263154029846191\n",
      "[step: 10234] loss: 0.8276245594024658\n",
      "[step: 10235] loss: 0.8287453055381775\n",
      "[step: 10236] loss: 0.828348696231842\n",
      "[step: 10237] loss: 0.8263964653015137\n",
      "[step: 10238] loss: 0.8240698575973511\n",
      "[step: 10239] loss: 0.8223713636398315\n",
      "[step: 10240] loss: 0.8218556046485901\n",
      "[step: 10241] loss: 0.8222125768661499\n",
      "[step: 10242] loss: 0.8226597905158997\n",
      "[step: 10243] loss: 0.8227555751800537\n",
      "[step: 10244] loss: 0.8221642374992371\n",
      "[step: 10245] loss: 0.8210886716842651\n",
      "[step: 10246] loss: 0.8198875784873962\n",
      "[step: 10247] loss: 0.8188389539718628\n",
      "[step: 10248] loss: 0.8181618452072144\n",
      "[step: 10249] loss: 0.8178373575210571\n",
      "[step: 10250] loss: 0.8177489042282104\n",
      "[step: 10251] loss: 0.8177533149719238\n",
      "[step: 10252] loss: 0.8176963329315186\n",
      "[step: 10253] loss: 0.8175376653671265\n",
      "[step: 10254] loss: 0.8172539472579956\n",
      "[step: 10255] loss: 0.8168801069259644\n",
      "[step: 10256] loss: 0.8164911866188049\n",
      "[step: 10257] loss: 0.8161388635635376\n",
      "[step: 10258] loss: 0.8159046173095703\n",
      "[step: 10259] loss: 0.8158314228057861\n",
      "[step: 10260] loss: 0.8160594701766968\n",
      "[step: 10261] loss: 0.8166314363479614\n",
      "[step: 10262] loss: 0.8178114891052246\n",
      "[step: 10263] loss: 0.8196983933448792\n",
      "[step: 10264] loss: 0.8229040503501892\n",
      "[step: 10265] loss: 0.8276618123054504\n",
      "[step: 10266] loss: 0.8355878591537476\n",
      "[step: 10267] loss: 0.8469563126564026\n",
      "[step: 10268] loss: 0.8659531474113464\n",
      "[step: 10269] loss: 0.8916038274765015\n",
      "[step: 10270] loss: 0.9349311590194702\n",
      "[step: 10271] loss: 0.9869688749313354\n",
      "[step: 10272] loss: 1.0736281871795654\n",
      "[step: 10273] loss: 1.1494089365005493\n",
      "[step: 10274] loss: 1.2647569179534912\n",
      "[step: 10275] loss: 1.2859880924224854\n",
      "[step: 10276] loss: 1.307051658630371\n",
      "[step: 10277] loss: 1.1738574504852295\n",
      "[step: 10278] loss: 1.0544662475585938\n",
      "[step: 10279] loss: 0.9515196681022644\n",
      "[step: 10280] loss: 0.9644328951835632\n",
      "[step: 10281] loss: 1.0983469486236572\n",
      "[step: 10282] loss: 1.3601278066635132\n",
      "[step: 10283] loss: 1.642082929611206\n",
      "[step: 10284] loss: 2.0637218952178955\n",
      "[step: 10285] loss: 2.324740171432495\n",
      "[step: 10286] loss: 2.6731414794921875\n",
      "[step: 10287] loss: 2.722017526626587\n",
      "[step: 10288] loss: 2.60878324508667\n",
      "[step: 10289] loss: 2.0867462158203125\n",
      "[step: 10290] loss: 1.5572972297668457\n",
      "[step: 10291] loss: 1.0758180618286133\n",
      "[step: 10292] loss: 1.0772852897644043\n",
      "[step: 10293] loss: 1.4293566942214966\n",
      "[step: 10294] loss: 1.6899633407592773\n",
      "[step: 10295] loss: 1.632139801979065\n",
      "[step: 10296] loss: 1.1381769180297852\n",
      "[step: 10297] loss: 0.8888298869132996\n",
      "[step: 10298] loss: 1.0762841701507568\n",
      "[step: 10299] loss: 1.358872413635254\n",
      "[step: 10300] loss: 1.3927698135375977\n",
      "[step: 10301] loss: 1.050095558166504\n",
      "[step: 10302] loss: 0.8424718379974365\n",
      "[step: 10303] loss: 0.9805315136909485\n",
      "[step: 10304] loss: 1.1821366548538208\n",
      "[step: 10305] loss: 1.195776343345642\n",
      "[step: 10306] loss: 0.9769393801689148\n",
      "[step: 10307] loss: 0.8306545615196228\n",
      "[step: 10308] loss: 0.9115071296691895\n",
      "[step: 10309] loss: 1.0540217161178589\n",
      "[step: 10310] loss: 1.0842173099517822\n",
      "[step: 10311] loss: 0.9516116976737976\n",
      "[step: 10312] loss: 0.8350738286972046\n",
      "[step: 10313] loss: 0.8628021478652954\n",
      "[step: 10314] loss: 0.9525790810585022\n",
      "[step: 10315] loss: 0.9940676689147949\n",
      "[step: 10316] loss: 0.9354931116104126\n",
      "[step: 10317] loss: 0.856034517288208\n",
      "[step: 10318] loss: 0.842556357383728\n",
      "[step: 10319] loss: 0.8745056390762329\n",
      "[step: 10320] loss: 0.9062380790710449\n",
      "[step: 10321] loss: 0.9014377593994141\n",
      "[step: 10322] loss: 0.8737843036651611\n",
      "[step: 10323] loss: 0.857900857925415\n",
      "[step: 10324] loss: 0.8469851613044739\n",
      "[step: 10325] loss: 0.8393638134002686\n",
      "[step: 10326] loss: 0.8396540880203247\n",
      "[step: 10327] loss: 0.8452776074409485\n",
      "[step: 10328] loss: 0.8595497608184814\n",
      "[step: 10329] loss: 0.8608280420303345\n",
      "[step: 10330] loss: 0.8422927856445312\n",
      "[step: 10331] loss: 0.8210724592208862\n",
      "[step: 10332] loss: 0.8068561553955078\n",
      "[step: 10333] loss: 0.8129245042800903\n",
      "[step: 10334] loss: 0.8264203667640686\n",
      "[step: 10335] loss: 0.8336823582649231\n",
      "[step: 10336] loss: 0.8340233564376831\n",
      "[step: 10337] loss: 0.8269948959350586\n",
      "[step: 10338] loss: 0.822296142578125\n",
      "[step: 10339] loss: 0.8191140294075012\n",
      "[step: 10340] loss: 0.8150389790534973\n",
      "[step: 10341] loss: 0.8094998002052307\n",
      "[step: 10342] loss: 0.8033990859985352\n",
      "[step: 10343] loss: 0.8016682863235474\n",
      "[step: 10344] loss: 0.8038654327392578\n",
      "[step: 10345] loss: 0.8076494932174683\n",
      "[step: 10346] loss: 0.8104076385498047\n",
      "[step: 10347] loss: 0.8108479976654053\n",
      "[step: 10348] loss: 0.8114398717880249\n",
      "[step: 10349] loss: 0.8131614923477173\n",
      "[step: 10350] loss: 0.8162790536880493\n",
      "[step: 10351] loss: 0.8201131820678711\n",
      "[step: 10352] loss: 0.8230124711990356\n",
      "[step: 10353] loss: 0.8269558548927307\n",
      "[step: 10354] loss: 0.8324551582336426\n",
      "[step: 10355] loss: 0.8421700596809387\n",
      "[step: 10356] loss: 0.8569644689559937\n",
      "[step: 10357] loss: 0.8790355920791626\n",
      "[step: 10358] loss: 0.9108544588088989\n",
      "[step: 10359] loss: 0.9607306718826294\n",
      "[step: 10360] loss: 1.0343616008758545\n",
      "[step: 10361] loss: 1.1512049436569214\n",
      "[step: 10362] loss: 1.3133808374404907\n",
      "[step: 10363] loss: 1.5565857887268066\n",
      "[step: 10364] loss: 1.83805251121521\n",
      "[step: 10365] loss: 2.1763999462127686\n",
      "[step: 10366] loss: 2.3599019050598145\n",
      "[step: 10367] loss: 2.3428001403808594\n",
      "[step: 10368] loss: 1.9304206371307373\n",
      "[step: 10369] loss: 1.3617548942565918\n",
      "[step: 10370] loss: 0.9157967567443848\n",
      "[step: 10371] loss: 0.8363547325134277\n",
      "[step: 10372] loss: 1.0651565790176392\n",
      "[step: 10373] loss: 1.328643560409546\n",
      "[step: 10374] loss: 1.3826905488967896\n",
      "[step: 10375] loss: 1.1672799587249756\n",
      "[step: 10376] loss: 0.9150044918060303\n",
      "[step: 10377] loss: 0.8373380899429321\n",
      "[step: 10378] loss: 0.949630618095398\n",
      "[step: 10379] loss: 1.083927869796753\n",
      "[step: 10380] loss: 1.0850757360458374\n",
      "[step: 10381] loss: 0.9726096391677856\n",
      "[step: 10382] loss: 0.8619320392608643\n",
      "[step: 10383] loss: 0.8520476818084717\n",
      "[step: 10384] loss: 0.9102997779846191\n",
      "[step: 10385] loss: 0.9520195126533508\n",
      "[step: 10386] loss: 0.9407072067260742\n",
      "[step: 10387] loss: 0.8934099674224854\n",
      "[step: 10388] loss: 0.8610479235649109\n",
      "[step: 10389] loss: 0.8512464761734009\n",
      "[step: 10390] loss: 0.8548397421836853\n",
      "[step: 10391] loss: 0.8610525131225586\n",
      "[step: 10392] loss: 0.8628576993942261\n",
      "[step: 10393] loss: 0.8613485097885132\n",
      "[step: 10394] loss: 0.8508567214012146\n",
      "[step: 10395] loss: 0.8366850018501282\n",
      "[step: 10396] loss: 0.8216546773910522\n",
      "[step: 10397] loss: 0.8161088228225708\n",
      "[step: 10398] loss: 0.824026882648468\n",
      "[step: 10399] loss: 0.8357559442520142\n",
      "[step: 10400] loss: 0.840233325958252\n",
      "[step: 10401] loss: 0.8286734819412231\n",
      "[step: 10402] loss: 0.8109328150749207\n",
      "[step: 10403] loss: 0.7973543405532837\n",
      "[step: 10404] loss: 0.796937108039856\n",
      "[step: 10405] loss: 0.8071015477180481\n",
      "[step: 10406] loss: 0.8169176578521729\n",
      "[step: 10407] loss: 0.8195605278015137\n",
      "[step: 10408] loss: 0.8126518726348877\n",
      "[step: 10409] loss: 0.8033336400985718\n",
      "[step: 10410] loss: 0.7970168590545654\n",
      "[step: 10411] loss: 0.7962660193443298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10412] loss: 0.7983182668685913\n",
      "[step: 10413] loss: 0.7990766763687134\n",
      "[step: 10414] loss: 0.7966164946556091\n",
      "[step: 10415] loss: 0.7924835681915283\n",
      "[step: 10416] loss: 0.7893808484077454\n",
      "[step: 10417] loss: 0.789024829864502\n",
      "[step: 10418] loss: 0.7912881374359131\n",
      "[step: 10419] loss: 0.7941751480102539\n",
      "[step: 10420] loss: 0.7959948182106018\n",
      "[step: 10421] loss: 0.7963675260543823\n",
      "[step: 10422] loss: 0.7966196537017822\n",
      "[step: 10423] loss: 0.7980466485023499\n",
      "[step: 10424] loss: 0.8021389245986938\n",
      "[step: 10425] loss: 0.8095093369483948\n",
      "[step: 10426] loss: 0.8211243748664856\n",
      "[step: 10427] loss: 0.8381527662277222\n",
      "[step: 10428] loss: 0.8655827045440674\n",
      "[step: 10429] loss: 0.9089977741241455\n",
      "[step: 10430] loss: 0.9833391904830933\n",
      "[step: 10431] loss: 1.1034281253814697\n",
      "[step: 10432] loss: 1.3084131479263306\n",
      "[step: 10433] loss: 1.6168365478515625\n",
      "[step: 10434] loss: 2.098020553588867\n",
      "[step: 10435] loss: 2.639349937438965\n",
      "[step: 10436] loss: 3.176457405090332\n",
      "[step: 10437] loss: 3.1521449089050293\n",
      "[step: 10438] loss: 2.5097460746765137\n",
      "[step: 10439] loss: 1.4606072902679443\n",
      "[step: 10440] loss: 0.8642046451568604\n",
      "[step: 10441] loss: 1.0572925806045532\n",
      "[step: 10442] loss: 1.5965747833251953\n",
      "[step: 10443] loss: 1.7820451259613037\n",
      "[step: 10444] loss: 1.3767096996307373\n",
      "[step: 10445] loss: 0.9502259492874146\n",
      "[step: 10446] loss: 0.9912859201431274\n",
      "[step: 10447] loss: 1.2715598344802856\n",
      "[step: 10448] loss: 1.2943847179412842\n",
      "[step: 10449] loss: 1.021141529083252\n",
      "[step: 10450] loss: 0.8876450657844543\n",
      "[step: 10451] loss: 1.0392394065856934\n",
      "[step: 10452] loss: 1.13761568069458\n",
      "[step: 10453] loss: 0.9979028105735779\n",
      "[step: 10454] loss: 0.8334483504295349\n",
      "[step: 10455] loss: 0.8860005736351013\n",
      "[step: 10456] loss: 1.0261212587356567\n",
      "[step: 10457] loss: 1.0020217895507812\n",
      "[step: 10458] loss: 0.8665286898612976\n",
      "[step: 10459] loss: 0.8116307258605957\n",
      "[step: 10460] loss: 0.8850862979888916\n",
      "[step: 10461] loss: 0.940543532371521\n",
      "[step: 10462] loss: 0.8869215250015259\n",
      "[step: 10463] loss: 0.8179330825805664\n",
      "[step: 10464] loss: 0.8249324560165405\n",
      "[step: 10465] loss: 0.874855637550354\n",
      "[step: 10466] loss: 0.8796063661575317\n",
      "[step: 10467] loss: 0.8314429521560669\n",
      "[step: 10468] loss: 0.7984891533851624\n",
      "[step: 10469] loss: 0.8147627115249634\n",
      "[step: 10470] loss: 0.8468772768974304\n",
      "[step: 10471] loss: 0.8488011360168457\n",
      "[step: 10472] loss: 0.8192894458770752\n",
      "[step: 10473] loss: 0.7947485446929932\n",
      "[step: 10474] loss: 0.7976090908050537\n",
      "[step: 10475] loss: 0.8151981830596924\n",
      "[step: 10476] loss: 0.8228511810302734\n",
      "[step: 10477] loss: 0.8124903440475464\n",
      "[step: 10478] loss: 0.7979699969291687\n",
      "[step: 10479] loss: 0.7931704521179199\n",
      "[step: 10480] loss: 0.79791259765625\n",
      "[step: 10481] loss: 0.8029009699821472\n",
      "[step: 10482] loss: 0.8008873462677002\n",
      "[step: 10483] loss: 0.7936996817588806\n",
      "[step: 10484] loss: 0.7882032990455627\n",
      "[step: 10485] loss: 0.7884936332702637\n",
      "[step: 10486] loss: 0.7920058965682983\n",
      "[step: 10487] loss: 0.7940283417701721\n",
      "[step: 10488] loss: 0.7916271090507507\n",
      "[step: 10489] loss: 0.786207914352417\n",
      "[step: 10490] loss: 0.7815628051757812\n",
      "[step: 10491] loss: 0.7803102731704712\n",
      "[step: 10492] loss: 0.7822591662406921\n",
      "[step: 10493] loss: 0.784812867641449\n",
      "[step: 10494] loss: 0.7856362462043762\n",
      "[step: 10495] loss: 0.7839744687080383\n",
      "[step: 10496] loss: 0.780911922454834\n",
      "[step: 10497] loss: 0.7781844139099121\n",
      "[step: 10498] loss: 0.7770117521286011\n",
      "[step: 10499] loss: 0.7773298025131226\n",
      "[step: 10500] loss: 0.7782403230667114\n",
      "[step: 10501] loss: 0.7786965370178223\n",
      "[step: 10502] loss: 0.7781844139099121\n",
      "[step: 10503] loss: 0.7768020629882812\n",
      "[step: 10504] loss: 0.7752038240432739\n",
      "[step: 10505] loss: 0.7739652395248413\n",
      "[step: 10506] loss: 0.7733969688415527\n",
      "[step: 10507] loss: 0.7733747959136963\n",
      "[step: 10508] loss: 0.7735720872879028\n",
      "[step: 10509] loss: 0.7736169099807739\n",
      "[step: 10510] loss: 0.7733142375946045\n",
      "[step: 10511] loss: 0.7726824283599854\n",
      "[step: 10512] loss: 0.7719333171844482\n",
      "[step: 10513] loss: 0.7712845802307129\n",
      "[step: 10514] loss: 0.7709677219390869\n",
      "[step: 10515] loss: 0.7710883617401123\n",
      "[step: 10516] loss: 0.771752119064331\n",
      "[step: 10517] loss: 0.7731224894523621\n",
      "[step: 10518] loss: 0.7756356596946716\n",
      "[step: 10519] loss: 0.7801868915557861\n",
      "[step: 10520] loss: 0.7887153625488281\n",
      "[step: 10521] loss: 0.8047329783439636\n",
      "[step: 10522] loss: 0.8356838822364807\n",
      "[step: 10523] loss: 0.8944299817085266\n",
      "[step: 10524] loss: 1.009922981262207\n",
      "[step: 10525] loss: 1.2247025966644287\n",
      "[step: 10526] loss: 1.6414591073989868\n",
      "[step: 10527] loss: 2.3331522941589355\n",
      "[step: 10528] loss: 3.4776134490966797\n",
      "[step: 10529] loss: 4.583314895629883\n",
      "[step: 10530] loss: 5.079254627227783\n",
      "[step: 10531] loss: 3.6108946800231934\n",
      "[step: 10532] loss: 1.484557867050171\n",
      "[step: 10533] loss: 0.9345007538795471\n",
      "[step: 10534] loss: 2.12392520904541\n",
      "[step: 10535] loss: 2.92974591255188\n",
      "[step: 10536] loss: 1.9378609657287598\n",
      "[step: 10537] loss: 0.9163728952407837\n",
      "[step: 10538] loss: 1.446002721786499\n",
      "[step: 10539] loss: 2.0644805431365967\n",
      "[step: 10540] loss: 1.423824429512024\n",
      "[step: 10541] loss: 0.9702830910682678\n",
      "[step: 10542] loss: 1.4319097995758057\n",
      "[step: 10543] loss: 1.5003228187561035\n",
      "[step: 10544] loss: 0.9611804485321045\n",
      "[step: 10545] loss: 1.0309953689575195\n",
      "[step: 10546] loss: 1.4176898002624512\n",
      "[step: 10547] loss: 1.1282188892364502\n",
      "[step: 10548] loss: 0.8590702414512634\n",
      "[step: 10549] loss: 1.1146501302719116\n",
      "[step: 10550] loss: 1.1702426671981812\n",
      "[step: 10551] loss: 0.8861846923828125\n",
      "[step: 10552] loss: 0.9267657399177551\n",
      "[step: 10553] loss: 1.1036701202392578\n",
      "[step: 10554] loss: 0.9584347009658813\n",
      "[step: 10555] loss: 0.8294532299041748\n",
      "[step: 10556] loss: 0.9489898085594177\n",
      "[step: 10557] loss: 0.982602059841156\n",
      "[step: 10558] loss: 0.8473318815231323\n",
      "[step: 10559] loss: 0.8501299619674683\n",
      "[step: 10560] loss: 0.9458965063095093\n",
      "[step: 10561] loss: 0.8945552110671997\n",
      "[step: 10562] loss: 0.8089743852615356\n",
      "[step: 10563] loss: 0.8493286371231079\n",
      "[step: 10564] loss: 0.8931695818901062\n",
      "[step: 10565] loss: 0.8389127850532532\n",
      "[step: 10566] loss: 0.8029742240905762\n",
      "[step: 10567] loss: 0.8410965204238892\n",
      "[step: 10568] loss: 0.856731653213501\n",
      "[step: 10569] loss: 0.8155341148376465\n",
      "[step: 10570] loss: 0.7935266494750977\n",
      "[step: 10571] loss: 0.8204587697982788\n",
      "[step: 10572] loss: 0.8320711851119995\n",
      "[step: 10573] loss: 0.8058760762214661\n",
      "[step: 10574] loss: 0.7893519401550293\n",
      "[step: 10575] loss: 0.8030441999435425\n",
      "[step: 10576] loss: 0.8144750595092773\n",
      "[step: 10577] loss: 0.8010368347167969\n",
      "[step: 10578] loss: 0.7839221954345703\n",
      "[step: 10579] loss: 0.7874363660812378\n",
      "[step: 10580] loss: 0.7990231513977051\n",
      "[step: 10581] loss: 0.7973443269729614\n",
      "[step: 10582] loss: 0.7852066159248352\n",
      "[step: 10583] loss: 0.7789726257324219\n",
      "[step: 10584] loss: 0.7840182781219482\n",
      "[step: 10585] loss: 0.7893977761268616\n",
      "[step: 10586] loss: 0.7855527400970459\n",
      "[step: 10587] loss: 0.7779219746589661\n",
      "[step: 10588] loss: 0.7752237319946289\n",
      "[step: 10589] loss: 0.7784092426300049\n",
      "[step: 10590] loss: 0.7815327048301697\n",
      "[step: 10591] loss: 0.779491662979126\n",
      "[step: 10592] loss: 0.7746495008468628\n",
      "[step: 10593] loss: 0.7718584537506104\n",
      "[step: 10594] loss: 0.7725706696510315\n",
      "[step: 10595] loss: 0.7746164798736572\n",
      "[step: 10596] loss: 0.7745726108551025\n",
      "[step: 10597] loss: 0.7721702456474304\n",
      "[step: 10598] loss: 0.7695764899253845\n",
      "[step: 10599] loss: 0.7685073614120483\n",
      "[step: 10600] loss: 0.7691301107406616\n",
      "[step: 10601] loss: 0.7698922157287598\n",
      "[step: 10602] loss: 0.7695150375366211\n",
      "[step: 10603] loss: 0.7680283188819885\n",
      "[step: 10604] loss: 0.7662744522094727\n",
      "[step: 10605] loss: 0.7652631998062134\n",
      "[step: 10606] loss: 0.7651363015174866\n",
      "[step: 10607] loss: 0.7654174566268921\n",
      "[step: 10608] loss: 0.765451192855835\n",
      "[step: 10609] loss: 0.7648650407791138\n",
      "[step: 10610] loss: 0.7638728022575378\n",
      "[step: 10611] loss: 0.7628335952758789\n",
      "[step: 10612] loss: 0.7620977163314819\n",
      "[step: 10613] loss: 0.7617560625076294\n",
      "[step: 10614] loss: 0.7616443037986755\n",
      "[step: 10615] loss: 0.761540412902832\n",
      "[step: 10616] loss: 0.761243462562561\n",
      "[step: 10617] loss: 0.7607288360595703\n",
      "[step: 10618] loss: 0.7600700855255127\n",
      "[step: 10619] loss: 0.7593961954116821\n",
      "[step: 10620] loss: 0.7588200569152832\n",
      "[step: 10621] loss: 0.7583738565444946\n",
      "[step: 10622] loss: 0.7580446004867554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10623] loss: 0.7577720880508423\n",
      "[step: 10624] loss: 0.7574931979179382\n",
      "[step: 10625] loss: 0.7571679353713989\n",
      "[step: 10626] loss: 0.7567846179008484\n",
      "[step: 10627] loss: 0.7563484311103821\n",
      "[step: 10628] loss: 0.7558838129043579\n",
      "[step: 10629] loss: 0.7554136514663696\n",
      "[step: 10630] loss: 0.7549617290496826\n",
      "[step: 10631] loss: 0.7545392513275146\n",
      "[step: 10632] loss: 0.7541460990905762\n",
      "[step: 10633] loss: 0.7537808418273926\n",
      "[step: 10634] loss: 0.7534397840499878\n",
      "[step: 10635] loss: 0.7531089186668396\n",
      "[step: 10636] loss: 0.7527830600738525\n",
      "[step: 10637] loss: 0.7524609565734863\n",
      "[step: 10638] loss: 0.7521377205848694\n",
      "[step: 10639] loss: 0.7518131732940674\n",
      "[step: 10640] loss: 0.7514867782592773\n",
      "[step: 10641] loss: 0.7511603832244873\n",
      "[step: 10642] loss: 0.7508364915847778\n",
      "[step: 10643] loss: 0.7505150437355042\n",
      "[step: 10644] loss: 0.750200629234314\n",
      "[step: 10645] loss: 0.749897837638855\n",
      "[step: 10646] loss: 0.7496144771575928\n",
      "[step: 10647] loss: 0.7493500709533691\n",
      "[step: 10648] loss: 0.7491286993026733\n",
      "[step: 10649] loss: 0.7489597797393799\n",
      "[step: 10650] loss: 0.7488877177238464\n",
      "[step: 10651] loss: 0.7489703893661499\n",
      "[step: 10652] loss: 0.7493358850479126\n",
      "[step: 10653] loss: 0.7501698732376099\n",
      "[step: 10654] loss: 0.7519661784172058\n",
      "[step: 10655] loss: 0.7554187178611755\n",
      "[step: 10656] loss: 0.7625210285186768\n",
      "[step: 10657] loss: 0.7760829925537109\n",
      "[step: 10658] loss: 0.804948091506958\n",
      "[step: 10659] loss: 0.8605191111564636\n",
      "[step: 10660] loss: 0.9836963415145874\n",
      "[step: 10661] loss: 1.2152562141418457\n",
      "[step: 10662] loss: 1.7313991785049438\n",
      "[step: 10663] loss: 2.5775907039642334\n",
      "[step: 10664] loss: 4.184831142425537\n",
      "[step: 10665] loss: 5.711824417114258\n",
      "[step: 10666] loss: 6.557092666625977\n",
      "[step: 10667] loss: 4.690865516662598\n",
      "[step: 10668] loss: 2.1595778465270996\n",
      "[step: 10669] loss: 2.0533807277679443\n",
      "[step: 10670] loss: 3.5453667640686035\n",
      "[step: 10671] loss: 3.2829782962799072\n",
      "[step: 10672] loss: 1.3401546478271484\n",
      "[step: 10673] loss: 1.8688170909881592\n",
      "[step: 10674] loss: 3.4065072536468506\n",
      "[step: 10675] loss: 1.8794695138931274\n",
      "[step: 10676] loss: 1.0545501708984375\n",
      "[step: 10677] loss: 2.3396120071411133\n",
      "[step: 10678] loss: 1.8961713314056396\n",
      "[step: 10679] loss: 0.9611433148384094\n",
      "[step: 10680] loss: 1.6608808040618896\n",
      "[step: 10681] loss: 1.6865490674972534\n",
      "[step: 10682] loss: 1.0110265016555786\n",
      "[step: 10683] loss: 1.4111354351043701\n",
      "[step: 10684] loss: 1.4544286727905273\n",
      "[step: 10685] loss: 1.0576872825622559\n",
      "[step: 10686] loss: 1.2652723789215088\n",
      "[step: 10687] loss: 1.3338160514831543\n",
      "[step: 10688] loss: 0.9188460111618042\n",
      "[step: 10689] loss: 1.1180450916290283\n",
      "[step: 10690] loss: 1.252225399017334\n",
      "[step: 10691] loss: 0.903380274772644\n",
      "[step: 10692] loss: 0.9343773126602173\n",
      "[step: 10693] loss: 1.1278951168060303\n",
      "[step: 10694] loss: 0.9280452132225037\n",
      "[step: 10695] loss: 0.876198947429657\n",
      "[step: 10696] loss: 1.0096157789230347\n",
      "[step: 10697] loss: 0.9252716302871704\n",
      "[step: 10698] loss: 0.8431538939476013\n",
      "[step: 10699] loss: 0.9482544660568237\n",
      "[step: 10700] loss: 0.8988818526268005\n",
      "[step: 10701] loss: 0.8107987642288208\n",
      "[step: 10702] loss: 0.8780623078346252\n",
      "[step: 10703] loss: 0.9022952914237976\n",
      "[step: 10704] loss: 0.8146486282348633\n",
      "[step: 10705] loss: 0.8127403855323792\n",
      "[step: 10706] loss: 0.8676743507385254\n",
      "[step: 10707] loss: 0.8346784710884094\n",
      "[step: 10708] loss: 0.7927035093307495\n",
      "[step: 10709] loss: 0.8221126198768616\n",
      "[step: 10710] loss: 0.8294858336448669\n",
      "[step: 10711] loss: 0.7963607311248779\n",
      "[step: 10712] loss: 0.7932877540588379\n",
      "[step: 10713] loss: 0.8135526180267334\n",
      "[step: 10714] loss: 0.8006472587585449\n",
      "[step: 10715] loss: 0.7787172794342041\n",
      "[step: 10716] loss: 0.7886731028556824\n",
      "[step: 10717] loss: 0.8000268340110779\n",
      "[step: 10718] loss: 0.7851113080978394\n",
      "[step: 10719] loss: 0.7708753347396851\n",
      "[step: 10720] loss: 0.7793362140655518\n",
      "[step: 10721] loss: 0.787492036819458\n",
      "[step: 10722] loss: 0.7773762941360474\n",
      "[step: 10723] loss: 0.7662949562072754\n",
      "[step: 10724] loss: 0.7705516815185547\n",
      "[step: 10725] loss: 0.777130126953125\n",
      "[step: 10726] loss: 0.772369384765625\n",
      "[step: 10727] loss: 0.7637120485305786\n",
      "[step: 10728] loss: 0.7637506723403931\n",
      "[step: 10729] loss: 0.7682303190231323\n",
      "[step: 10730] loss: 0.7676302194595337\n",
      "[step: 10731] loss: 0.7619553804397583\n",
      "[step: 10732] loss: 0.7594026327133179\n",
      "[step: 10733] loss: 0.7612985372543335\n",
      "[step: 10734] loss: 0.7625342607498169\n",
      "[step: 10735] loss: 0.7600692510604858\n",
      "[step: 10736] loss: 0.7568449974060059\n",
      "[step: 10737] loss: 0.7562267780303955\n",
      "[step: 10738] loss: 0.7574809789657593\n",
      "[step: 10739] loss: 0.7572961449623108\n",
      "[step: 10740] loss: 0.7553059458732605\n",
      "[step: 10741] loss: 0.7532938718795776\n",
      "[step: 10742] loss: 0.7530156970024109\n",
      "[step: 10743] loss: 0.7535504102706909\n",
      "[step: 10744] loss: 0.7533169388771057\n",
      "[step: 10745] loss: 0.7519323825836182\n",
      "[step: 10746] loss: 0.7504858374595642\n",
      "[step: 10747] loss: 0.7499104142189026\n",
      "[step: 10748] loss: 0.7500641942024231\n",
      "[step: 10749] loss: 0.7499675154685974\n",
      "[step: 10750] loss: 0.7492172122001648\n",
      "[step: 10751] loss: 0.7480810284614563\n",
      "[step: 10752] loss: 0.7472671270370483\n",
      "[step: 10753] loss: 0.7469550371170044\n",
      "[step: 10754] loss: 0.7468783259391785\n",
      "[step: 10755] loss: 0.7465858459472656\n",
      "[step: 10756] loss: 0.7459287643432617\n",
      "[step: 10757] loss: 0.7451217174530029\n",
      "[step: 10758] loss: 0.744473397731781\n",
      "[step: 10759] loss: 0.7440890073776245\n",
      "[step: 10760] loss: 0.7438721656799316\n",
      "[step: 10761] loss: 0.7435824275016785\n",
      "[step: 10762] loss: 0.7431280612945557\n",
      "[step: 10763] loss: 0.7425331473350525\n",
      "[step: 10764] loss: 0.7419401407241821\n",
      "[step: 10765] loss: 0.7414594292640686\n",
      "[step: 10766] loss: 0.7411004900932312\n",
      "[step: 10767] loss: 0.7408024668693542\n",
      "[step: 10768] loss: 0.7404786944389343\n",
      "[step: 10769] loss: 0.7400768995285034\n",
      "[step: 10770] loss: 0.7396135330200195\n",
      "[step: 10771] loss: 0.7391284108161926\n",
      "[step: 10772] loss: 0.7386720180511475\n",
      "[step: 10773] loss: 0.7382709383964539\n",
      "[step: 10774] loss: 0.7379158139228821\n",
      "[step: 10775] loss: 0.737579345703125\n",
      "[step: 10776] loss: 0.7372391223907471\n",
      "[step: 10777] loss: 0.7368766665458679\n",
      "[step: 10778] loss: 0.7364925742149353\n",
      "[step: 10779] loss: 0.7360948920249939\n",
      "[step: 10780] loss: 0.73569256067276\n",
      "[step: 10781] loss: 0.7352981567382812\n",
      "[step: 10782] loss: 0.7349152565002441\n",
      "[step: 10783] loss: 0.7345503568649292\n",
      "[step: 10784] loss: 0.7341938614845276\n",
      "[step: 10785] loss: 0.7338509559631348\n",
      "[step: 10786] loss: 0.7335101366043091\n",
      "[step: 10787] loss: 0.7331721186637878\n",
      "[step: 10788] loss: 0.7328330278396606\n",
      "[step: 10789] loss: 0.7324931621551514\n",
      "[step: 10790] loss: 0.7321487665176392\n",
      "[step: 10791] loss: 0.7318071126937866\n",
      "[step: 10792] loss: 0.731464147567749\n",
      "[step: 10793] loss: 0.731122612953186\n",
      "[step: 10794] loss: 0.7307847738265991\n",
      "[step: 10795] loss: 0.7304501533508301\n",
      "[step: 10796] loss: 0.7301191091537476\n",
      "[step: 10797] loss: 0.7297927737236023\n",
      "[step: 10798] loss: 0.7294712066650391\n",
      "[step: 10799] loss: 0.7291611433029175\n",
      "[step: 10800] loss: 0.728861391544342\n",
      "[step: 10801] loss: 0.7285791635513306\n",
      "[step: 10802] loss: 0.7283240556716919\n",
      "[step: 10803] loss: 0.7281103730201721\n",
      "[step: 10804] loss: 0.7279617190361023\n",
      "[step: 10805] loss: 0.7279180288314819\n",
      "[step: 10806] loss: 0.7280557155609131\n",
      "[step: 10807] loss: 0.7284978628158569\n",
      "[step: 10808] loss: 0.7294666171073914\n",
      "[step: 10809] loss: 0.7313790321350098\n",
      "[step: 10810] loss: 0.7349549531936646\n",
      "[step: 10811] loss: 0.7416273951530457\n",
      "[step: 10812] loss: 0.7538855671882629\n",
      "[step: 10813] loss: 0.7769009470939636\n",
      "[step: 10814] loss: 0.8194292783737183\n",
      "[step: 10815] loss: 0.9007594585418701\n",
      "[step: 10816] loss: 1.050095558166504\n",
      "[step: 10817] loss: 1.3352220058441162\n",
      "[step: 10818] loss: 1.821427822113037\n",
      "[step: 10819] loss: 2.6598901748657227\n",
      "[step: 10820] loss: 3.6699461936950684\n",
      "[step: 10821] loss: 4.606355667114258\n",
      "[step: 10822] loss: 4.157580375671387\n",
      "[step: 10823] loss: 2.448110818862915\n",
      "[step: 10824] loss: 0.9647176265716553\n",
      "[step: 10825] loss: 1.2061071395874023\n",
      "[step: 10826] loss: 2.2916035652160645\n",
      "[step: 10827] loss: 2.2480697631835938\n",
      "[step: 10828] loss: 1.19978666305542\n",
      "[step: 10829] loss: 0.9684717059135437\n",
      "[step: 10830] loss: 1.6535924673080444\n",
      "[step: 10831] loss: 1.6488860845565796\n",
      "[step: 10832] loss: 0.9057173728942871\n",
      "[step: 10833] loss: 0.9536295533180237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10834] loss: 1.430349588394165\n",
      "[step: 10835] loss: 1.1602940559387207\n",
      "[step: 10836] loss: 0.8177376985549927\n",
      "[step: 10837] loss: 1.0806498527526855\n",
      "[step: 10838] loss: 1.1849327087402344\n",
      "[step: 10839] loss: 0.8528059720993042\n",
      "[step: 10840] loss: 0.858171820640564\n",
      "[step: 10841] loss: 1.0918419361114502\n",
      "[step: 10842] loss: 0.966310441493988\n",
      "[step: 10843] loss: 0.7990891933441162\n",
      "[step: 10844] loss: 0.9262118935585022\n",
      "[step: 10845] loss: 0.9750679731369019\n",
      "[step: 10846] loss: 0.8069255352020264\n",
      "[step: 10847] loss: 0.8048365712165833\n",
      "[step: 10848] loss: 0.921045184135437\n",
      "[step: 10849] loss: 0.8670437335968018\n",
      "[step: 10850] loss: 0.7749137282371521\n",
      "[step: 10851] loss: 0.8237589001655579\n",
      "[step: 10852] loss: 0.8659878969192505\n",
      "[step: 10853] loss: 0.7934129238128662\n",
      "[step: 10854] loss: 0.7614350914955139\n",
      "[step: 10855] loss: 0.8155642151832581\n",
      "[step: 10856] loss: 0.8238744735717773\n",
      "[step: 10857] loss: 0.771622896194458\n",
      "[step: 10858] loss: 0.7590265870094299\n",
      "[step: 10859] loss: 0.792518675327301\n",
      "[step: 10860] loss: 0.7896517515182495\n",
      "[step: 10861] loss: 0.7546142935752869\n",
      "[step: 10862] loss: 0.7516317367553711\n",
      "[step: 10863] loss: 0.775692343711853\n",
      "[step: 10864] loss: 0.7763611078262329\n",
      "[step: 10865] loss: 0.7514920830726624\n",
      "[step: 10866] loss: 0.7423017024993896\n",
      "[step: 10867] loss: 0.7559365034103394\n",
      "[step: 10868] loss: 0.7616943717002869\n",
      "[step: 10869] loss: 0.7490458488464355\n",
      "[step: 10870] loss: 0.7380825281143188\n",
      "[step: 10871] loss: 0.7428689002990723\n",
      "[step: 10872] loss: 0.7513356804847717\n",
      "[step: 10873] loss: 0.748001217842102\n",
      "[step: 10874] loss: 0.737614095211029\n",
      "[step: 10875] loss: 0.7336207628250122\n",
      "[step: 10876] loss: 0.7382262349128723\n",
      "[step: 10877] loss: 0.7418655157089233\n",
      "[step: 10878] loss: 0.7384891510009766\n",
      "[step: 10879] loss: 0.732803463935852\n",
      "[step: 10880] loss: 0.7311793565750122\n",
      "[step: 10881] loss: 0.7337254285812378\n",
      "[step: 10882] loss: 0.7355155348777771\n",
      "[step: 10883] loss: 0.7331968545913696\n",
      "[step: 10884] loss: 0.7292375564575195\n",
      "[step: 10885] loss: 0.7273549437522888\n",
      "[step: 10886] loss: 0.7283530235290527\n",
      "[step: 10887] loss: 0.7299255132675171\n",
      "[step: 10888] loss: 0.7295650243759155\n",
      "[step: 10889] loss: 0.7274338603019714\n",
      "[step: 10890] loss: 0.7253352999687195\n",
      "[step: 10891] loss: 0.7246720790863037\n",
      "[step: 10892] loss: 0.7252599000930786\n",
      "[step: 10893] loss: 0.7257059812545776\n",
      "[step: 10894] loss: 0.725140392780304\n",
      "[step: 10895] loss: 0.7237569093704224\n",
      "[step: 10896] loss: 0.7223834991455078\n",
      "[step: 10897] loss: 0.7217585444450378\n",
      "[step: 10898] loss: 0.7218025922775269\n",
      "[step: 10899] loss: 0.7219597101211548\n",
      "[step: 10900] loss: 0.721755862236023\n",
      "[step: 10901] loss: 0.7210639715194702\n",
      "[step: 10902] loss: 0.7201814651489258\n",
      "[step: 10903] loss: 0.7194709777832031\n",
      "[step: 10904] loss: 0.7190829515457153\n",
      "[step: 10905] loss: 0.7189433574676514\n",
      "[step: 10906] loss: 0.7188084125518799\n",
      "[step: 10907] loss: 0.7184971570968628\n",
      "[step: 10908] loss: 0.7179824709892273\n",
      "[step: 10909] loss: 0.7173474431037903\n",
      "[step: 10910] loss: 0.7167597413063049\n",
      "[step: 10911] loss: 0.7163023948669434\n",
      "[step: 10912] loss: 0.7159839868545532\n",
      "[step: 10913] loss: 0.715749979019165\n",
      "[step: 10914] loss: 0.7155083417892456\n",
      "[step: 10915] loss: 0.7152056694030762\n",
      "[step: 10916] loss: 0.714826762676239\n",
      "[step: 10917] loss: 0.7144007682800293\n",
      "[step: 10918] loss: 0.7139710187911987\n",
      "[step: 10919] loss: 0.7135810852050781\n",
      "[step: 10920] loss: 0.7132525444030762\n",
      "[step: 10921] loss: 0.7129863500595093\n",
      "[step: 10922] loss: 0.7127730846405029\n",
      "[step: 10923] loss: 0.7126076817512512\n",
      "[step: 10924] loss: 0.7124912738800049\n",
      "[step: 10925] loss: 0.712460458278656\n",
      "[step: 10926] loss: 0.7125649452209473\n",
      "[step: 10927] loss: 0.7129448056221008\n",
      "[step: 10928] loss: 0.7137535214424133\n",
      "[step: 10929] loss: 0.7153860926628113\n",
      "[step: 10930] loss: 0.7183008790016174\n",
      "[step: 10931] loss: 0.7236860990524292\n",
      "[step: 10932] loss: 0.7329546213150024\n",
      "[step: 10933] loss: 0.7500382661819458\n",
      "[step: 10934] loss: 0.7791699171066284\n",
      "[step: 10935] loss: 0.8339864611625671\n",
      "[step: 10936] loss: 0.925204873085022\n",
      "[step: 10937] loss: 1.0995476245880127\n",
      "[step: 10938] loss: 1.364898920059204\n",
      "[step: 10939] loss: 1.850758671760559\n",
      "[step: 10940] loss: 2.409749984741211\n",
      "[step: 10941] loss: 3.1817073822021484\n",
      "[step: 10942] loss: 3.4403491020202637\n",
      "[step: 10943] loss: 3.1593899726867676\n",
      "[step: 10944] loss: 2.1698250770568848\n",
      "[step: 10945] loss: 1.2454348802566528\n",
      "[step: 10946] loss: 1.0331244468688965\n",
      "[step: 10947] loss: 1.490734338760376\n",
      "[step: 10948] loss: 1.9655102491378784\n",
      "[step: 10949] loss: 1.6208882331848145\n",
      "[step: 10950] loss: 1.0005440711975098\n",
      "[step: 10951] loss: 0.9221641421318054\n",
      "[step: 10952] loss: 1.345932960510254\n",
      "[step: 10953] loss: 1.6138843297958374\n",
      "[step: 10954] loss: 1.1610887050628662\n",
      "[step: 10955] loss: 0.7654339671134949\n",
      "[step: 10956] loss: 0.9650534987449646\n",
      "[step: 10957] loss: 1.2894775867462158\n",
      "[step: 10958] loss: 1.2482481002807617\n",
      "[step: 10959] loss: 0.8901235461235046\n",
      "[step: 10960] loss: 0.796411395072937\n",
      "[step: 10961] loss: 1.0065135955810547\n",
      "[step: 10962] loss: 1.100168228149414\n",
      "[step: 10963] loss: 0.9972438812255859\n",
      "[step: 10964] loss: 0.8612973690032959\n",
      "[step: 10965] loss: 0.8632069826126099\n",
      "[step: 10966] loss: 0.9267212152481079\n",
      "[step: 10967] loss: 0.8974061608314514\n",
      "[step: 10968] loss: 0.8559433817863464\n",
      "[step: 10969] loss: 0.8631408214569092\n",
      "[step: 10970] loss: 0.8683351278305054\n",
      "[step: 10971] loss: 0.8355884552001953\n",
      "[step: 10972] loss: 0.7806320190429688\n",
      "[step: 10973] loss: 0.7889021635055542\n",
      "[step: 10974] loss: 0.837097704410553\n",
      "[step: 10975] loss: 0.8397179245948792\n",
      "[step: 10976] loss: 0.7925964593887329\n",
      "[step: 10977] loss: 0.7378596663475037\n",
      "[step: 10978] loss: 0.7477999925613403\n",
      "[step: 10979] loss: 0.7925920486450195\n",
      "[step: 10980] loss: 0.804245114326477\n",
      "[step: 10981] loss: 0.7749330997467041\n",
      "[step: 10982] loss: 0.7289351224899292\n",
      "[step: 10983] loss: 0.7236127853393555\n",
      "[step: 10984] loss: 0.7502194046974182\n",
      "[step: 10985] loss: 0.7686829566955566\n",
      "[step: 10986] loss: 0.7634849548339844\n",
      "[step: 10987] loss: 0.7329889535903931\n",
      "[step: 10988] loss: 0.7153688073158264\n",
      "[step: 10989] loss: 0.7210497856140137\n",
      "[step: 10990] loss: 0.735536515712738\n",
      "[step: 10991] loss: 0.7452544569969177\n",
      "[step: 10992] loss: 0.7354640364646912\n",
      "[step: 10993] loss: 0.7201303243637085\n",
      "[step: 10994] loss: 0.7117422819137573\n",
      "[step: 10995] loss: 0.713178813457489\n",
      "[step: 10996] loss: 0.7219189405441284\n",
      "[step: 10997] loss: 0.7258189916610718\n",
      "[step: 10998] loss: 0.722883939743042\n",
      "[step: 10999] loss: 0.7158243656158447\n",
      "[step: 11000] loss: 0.7097029089927673\n",
      "[step: 11001] loss: 0.7088135480880737\n",
      "[step: 11002] loss: 0.7107678651809692\n",
      "[step: 11003] loss: 0.7129172086715698\n",
      "[step: 11004] loss: 0.712732195854187\n",
      "[step: 11005] loss: 0.7104368209838867\n",
      "[step: 11006] loss: 0.7081847190856934\n",
      "[step: 11007] loss: 0.7069590091705322\n",
      "[step: 11008] loss: 0.7072012424468994\n",
      "[step: 11009] loss: 0.7074792385101318\n",
      "[step: 11010] loss: 0.7067883014678955\n",
      "[step: 11011] loss: 0.7051947116851807\n",
      "[step: 11012] loss: 0.7030863165855408\n",
      "[step: 11013] loss: 0.7018306255340576\n",
      "[step: 11014] loss: 0.7015049457550049\n",
      "[step: 11015] loss: 0.7021164894104004\n",
      "[step: 11016] loss: 0.7029503583908081\n",
      "[step: 11017] loss: 0.703134298324585\n",
      "[step: 11018] loss: 0.7028193473815918\n",
      "[step: 11019] loss: 0.7018634676933289\n",
      "[step: 11020] loss: 0.7010151147842407\n",
      "[step: 11021] loss: 0.7005751729011536\n",
      "[step: 11022] loss: 0.7005190849304199\n",
      "[step: 11023] loss: 0.7008643746376038\n",
      "[step: 11024] loss: 0.7011868953704834\n",
      "[step: 11025] loss: 0.7014710903167725\n",
      "[step: 11026] loss: 0.7018185257911682\n",
      "[step: 11027] loss: 0.7023809552192688\n",
      "[step: 11028] loss: 0.7037278413772583\n",
      "[step: 11029] loss: 0.7061726450920105\n",
      "[step: 11030] loss: 0.7104493379592896\n",
      "[step: 11031] loss: 0.7174699306488037\n",
      "[step: 11032] loss: 0.7290255427360535\n",
      "[step: 11033] loss: 0.7478519678115845\n",
      "[step: 11034] loss: 0.7795517444610596\n",
      "[step: 11035] loss: 0.8322557806968689\n",
      "[step: 11036] loss: 0.923227846622467\n",
      "[step: 11037] loss: 1.0738239288330078\n",
      "[step: 11038] loss: 1.3294215202331543\n",
      "[step: 11039] loss: 1.7169413566589355\n",
      "[step: 11040] loss: 2.2931880950927734\n",
      "[step: 11041] loss: 2.9094619750976562\n",
      "[step: 11042] loss: 3.3733880519866943\n",
      "[step: 11043] loss: 3.092805862426758\n",
      "[step: 11044] loss: 2.0946762561798096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11045] loss: 1.0284671783447266\n",
      "[step: 11046] loss: 0.7861551642417908\n",
      "[step: 11047] loss: 1.3305366039276123\n",
      "[step: 11048] loss: 1.7800006866455078\n",
      "[step: 11049] loss: 1.5061854124069214\n",
      "[step: 11050] loss: 0.9194159507751465\n",
      "[step: 11051] loss: 0.855668842792511\n",
      "[step: 11052] loss: 1.240661382675171\n",
      "[step: 11053] loss: 1.2915335893630981\n",
      "[step: 11054] loss: 0.9267857074737549\n",
      "[step: 11055] loss: 0.7523001432418823\n",
      "[step: 11056] loss: 0.9819911122322083\n",
      "[step: 11057] loss: 1.1255784034729004\n",
      "[step: 11058] loss: 0.8981868624687195\n",
      "[step: 11059] loss: 0.735586404800415\n",
      "[step: 11060] loss: 0.8612565994262695\n",
      "[step: 11061] loss: 0.9665335416793823\n",
      "[step: 11062] loss: 0.8504782915115356\n",
      "[step: 11063] loss: 0.7335105538368225\n",
      "[step: 11064] loss: 0.8032093644142151\n",
      "[step: 11065] loss: 0.8892925977706909\n",
      "[step: 11066] loss: 0.8229609727859497\n",
      "[step: 11067] loss: 0.7274583578109741\n",
      "[step: 11068] loss: 0.7496955394744873\n",
      "[step: 11069] loss: 0.8152817487716675\n",
      "[step: 11070] loss: 0.79917311668396\n",
      "[step: 11071] loss: 0.7329810261726379\n",
      "[step: 11072] loss: 0.724557638168335\n",
      "[step: 11073] loss: 0.7697709798812866\n",
      "[step: 11074] loss: 0.7799111604690552\n",
      "[step: 11075] loss: 0.7394632697105408\n",
      "[step: 11076] loss: 0.7092368602752686\n",
      "[step: 11077] loss: 0.7253880500793457\n",
      "[step: 11078] loss: 0.7519587278366089\n",
      "[step: 11079] loss: 0.7453221082687378\n",
      "[step: 11080] loss: 0.7179539203643799\n",
      "[step: 11081] loss: 0.7057214975357056\n",
      "[step: 11082] loss: 0.7181302309036255\n",
      "[step: 11083] loss: 0.7323472499847412\n",
      "[step: 11084] loss: 0.7274472713470459\n",
      "[step: 11085] loss: 0.7103685140609741\n",
      "[step: 11086] loss: 0.700657844543457\n",
      "[step: 11087] loss: 0.7054651975631714\n",
      "[step: 11088] loss: 0.7151073217391968\n",
      "[step: 11089] loss: 0.716951847076416\n",
      "[step: 11090] loss: 0.7093856930732727\n",
      "[step: 11091] loss: 0.700527012348175\n",
      "[step: 11092] loss: 0.6978864669799805\n",
      "[step: 11093] loss: 0.7016057968139648\n",
      "[step: 11094] loss: 0.705744743347168\n",
      "[step: 11095] loss: 0.7055479884147644\n",
      "[step: 11096] loss: 0.7010818123817444\n",
      "[step: 11097] loss: 0.6960648894309998\n",
      "[step: 11098] loss: 0.6938983201980591\n",
      "[step: 11099] loss: 0.6951956748962402\n",
      "[step: 11100] loss: 0.6976630091667175\n",
      "[step: 11101] loss: 0.6987497806549072\n",
      "[step: 11102] loss: 0.6975295543670654\n",
      "[step: 11103] loss: 0.6946418285369873\n",
      "[step: 11104] loss: 0.6918917298316956\n",
      "[step: 11105] loss: 0.6904734373092651\n",
      "[step: 11106] loss: 0.6905906796455383\n",
      "[step: 11107] loss: 0.6915384531021118\n",
      "[step: 11108] loss: 0.6922730803489685\n",
      "[step: 11109] loss: 0.6922203302383423\n",
      "[step: 11110] loss: 0.691245436668396\n",
      "[step: 11111] loss: 0.6897806525230408\n",
      "[step: 11112] loss: 0.6883530616760254\n",
      "[step: 11113] loss: 0.6873506307601929\n",
      "[step: 11114] loss: 0.6869224309921265\n",
      "[step: 11115] loss: 0.6869211196899414\n",
      "[step: 11116] loss: 0.6870764493942261\n",
      "[step: 11117] loss: 0.6871473789215088\n",
      "[step: 11118] loss: 0.6869676113128662\n",
      "[step: 11119] loss: 0.6865032911300659\n",
      "[step: 11120] loss: 0.6858525276184082\n",
      "[step: 11121] loss: 0.6851076483726501\n",
      "[step: 11122] loss: 0.6844074726104736\n",
      "[step: 11123] loss: 0.6838275194168091\n",
      "[step: 11124] loss: 0.6833958029747009\n",
      "[step: 11125] loss: 0.6831196546554565\n",
      "[step: 11126] loss: 0.6829679608345032\n",
      "[step: 11127] loss: 0.682909369468689\n",
      "[step: 11128] loss: 0.6829347014427185\n",
      "[step: 11129] loss: 0.6830462217330933\n",
      "[step: 11130] loss: 0.6833388805389404\n",
      "[step: 11131] loss: 0.6839221715927124\n",
      "[step: 11132] loss: 0.685151219367981\n",
      "[step: 11133] loss: 0.6874427795410156\n",
      "[step: 11134] loss: 0.691959023475647\n",
      "[step: 11135] loss: 0.7002102136611938\n",
      "[step: 11136] loss: 0.7163329124450684\n",
      "[step: 11137] loss: 0.745805025100708\n",
      "[step: 11138] loss: 0.8041578531265259\n",
      "[step: 11139] loss: 0.9100296497344971\n",
      "[step: 11140] loss: 1.1213933229446411\n",
      "[step: 11141] loss: 1.483646035194397\n",
      "[step: 11142] loss: 2.166422128677368\n",
      "[step: 11143] loss: 3.086033821105957\n",
      "[step: 11144] loss: 4.285942077636719\n",
      "[step: 11145] loss: 4.686971664428711\n",
      "[step: 11146] loss: 3.764324903488159\n",
      "[step: 11147] loss: 1.885964035987854\n",
      "[step: 11148] loss: 0.9663596153259277\n",
      "[step: 11149] loss: 1.7797846794128418\n",
      "[step: 11150] loss: 2.639960289001465\n",
      "[step: 11151] loss: 2.050640106201172\n",
      "[step: 11152] loss: 0.8924448490142822\n",
      "[step: 11153] loss: 1.1639037132263184\n",
      "[step: 11154] loss: 2.069471836090088\n",
      "[step: 11155] loss: 1.6505475044250488\n",
      "[step: 11156] loss: 0.9022848606109619\n",
      "[step: 11157] loss: 1.1117058992385864\n",
      "[step: 11158] loss: 1.5242912769317627\n",
      "[step: 11159] loss: 1.1952242851257324\n",
      "[step: 11160] loss: 0.9612363576889038\n",
      "[step: 11161] loss: 1.2538038492202759\n",
      "[step: 11162] loss: 1.1401469707489014\n",
      "[step: 11163] loss: 0.8000515103340149\n",
      "[step: 11164] loss: 0.9655386805534363\n",
      "[step: 11165] loss: 1.2085143327713013\n",
      "[step: 11166] loss: 0.9445411562919617\n",
      "[step: 11167] loss: 0.7656490206718445\n",
      "[step: 11168] loss: 0.9333485960960388\n",
      "[step: 11169] loss: 0.968870997428894\n",
      "[step: 11170] loss: 0.8007387518882751\n",
      "[step: 11171] loss: 0.8221479654312134\n",
      "[step: 11172] loss: 0.9220633506774902\n",
      "[step: 11173] loss: 0.8234535455703735\n",
      "[step: 11174] loss: 0.7232478260993958\n",
      "[step: 11175] loss: 0.806728720664978\n",
      "[step: 11176] loss: 0.8636184930801392\n",
      "[step: 11177] loss: 0.7792947292327881\n",
      "[step: 11178] loss: 0.7290017604827881\n",
      "[step: 11179] loss: 0.7773661017417908\n",
      "[step: 11180] loss: 0.7899036407470703\n",
      "[step: 11181] loss: 0.7361471056938171\n",
      "[step: 11182] loss: 0.7268065810203552\n",
      "[step: 11183] loss: 0.765511691570282\n",
      "[step: 11184] loss: 0.7608832120895386\n",
      "[step: 11185] loss: 0.7164109945297241\n",
      "[step: 11186] loss: 0.7053794264793396\n",
      "[step: 11187] loss: 0.734251081943512\n",
      "[step: 11188] loss: 0.7442457675933838\n",
      "[step: 11189] loss: 0.7183969020843506\n",
      "[step: 11190] loss: 0.7010819911956787\n",
      "[step: 11191] loss: 0.7116954326629639\n",
      "[step: 11192] loss: 0.7219729423522949\n",
      "[step: 11193] loss: 0.7124408483505249\n",
      "[step: 11194] loss: 0.6978968381881714\n",
      "[step: 11195] loss: 0.6990788578987122\n",
      "[step: 11196] loss: 0.7093841433525085\n",
      "[step: 11197] loss: 0.7091525793075562\n",
      "[step: 11198] loss: 0.6981251239776611\n",
      "[step: 11199] loss: 0.689730167388916\n",
      "[step: 11200] loss: 0.6925100684165955\n",
      "[step: 11201] loss: 0.6996353268623352\n",
      "[step: 11202] loss: 0.7000759840011597\n",
      "[step: 11203] loss: 0.6934084892272949\n",
      "[step: 11204] loss: 0.6873358488082886\n",
      "[step: 11205] loss: 0.6869060397148132\n",
      "[step: 11206] loss: 0.6903085708618164\n",
      "[step: 11207] loss: 0.6916602253913879\n",
      "[step: 11208] loss: 0.6892205476760864\n",
      "[step: 11209] loss: 0.685583233833313\n",
      "[step: 11210] loss: 0.6837471723556519\n",
      "[step: 11211] loss: 0.6844213008880615\n",
      "[step: 11212] loss: 0.6855854988098145\n",
      "[step: 11213] loss: 0.6851186156272888\n",
      "[step: 11214] loss: 0.683203935623169\n",
      "[step: 11215] loss: 0.6811180114746094\n",
      "[step: 11216] loss: 0.6801893711090088\n",
      "[step: 11217] loss: 0.6805456876754761\n",
      "[step: 11218] loss: 0.6810890436172485\n",
      "[step: 11219] loss: 0.680932879447937\n",
      "[step: 11220] loss: 0.6798312067985535\n",
      "[step: 11221] loss: 0.6783169507980347\n",
      "[step: 11222] loss: 0.677180290222168\n",
      "[step: 11223] loss: 0.6767147183418274\n",
      "[step: 11224] loss: 0.6767997741699219\n",
      "[step: 11225] loss: 0.6769868731498718\n",
      "[step: 11226] loss: 0.6768103837966919\n",
      "[step: 11227] loss: 0.6762186288833618\n",
      "[step: 11228] loss: 0.6753287315368652\n",
      "[step: 11229] loss: 0.6744269728660583\n",
      "[step: 11230] loss: 0.673764705657959\n",
      "[step: 11231] loss: 0.6733709573745728\n",
      "[step: 11232] loss: 0.6732020378112793\n",
      "[step: 11233] loss: 0.6730977296829224\n",
      "[step: 11234] loss: 0.6729004383087158\n",
      "[step: 11235] loss: 0.6725741624832153\n",
      "[step: 11236] loss: 0.6720893383026123\n",
      "[step: 11237] loss: 0.6715350151062012\n",
      "[step: 11238] loss: 0.6709786057472229\n",
      "[step: 11239] loss: 0.6704744100570679\n",
      "[step: 11240] loss: 0.6700584888458252\n",
      "[step: 11241] loss: 0.6697181463241577\n",
      "[step: 11242] loss: 0.6694229245185852\n",
      "[step: 11243] loss: 0.6691511869430542\n",
      "[step: 11244] loss: 0.6688629984855652\n",
      "[step: 11245] loss: 0.6685522198677063\n",
      "[step: 11246] loss: 0.6682149171829224\n",
      "[step: 11247] loss: 0.6678547859191895\n",
      "[step: 11248] loss: 0.6674902439117432\n",
      "[step: 11249] loss: 0.667128324508667\n",
      "[step: 11250] loss: 0.6667823195457458\n",
      "[step: 11251] loss: 0.6664623022079468\n",
      "[step: 11252] loss: 0.666169285774231\n",
      "[step: 11253] loss: 0.6659135222434998\n",
      "[step: 11254] loss: 0.665700376033783\n",
      "[step: 11255] loss: 0.6655396223068237\n",
      "[step: 11256] loss: 0.665460467338562\n",
      "[step: 11257] loss: 0.6654999852180481\n",
      "[step: 11258] loss: 0.6657451391220093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11259] loss: 0.6663223505020142\n",
      "[step: 11260] loss: 0.6674865484237671\n",
      "[step: 11261] loss: 0.6696447134017944\n",
      "[step: 11262] loss: 0.6736077666282654\n",
      "[step: 11263] loss: 0.6807371377944946\n",
      "[step: 11264] loss: 0.6937686204910278\n",
      "[step: 11265] loss: 0.7174232602119446\n",
      "[step: 11266] loss: 0.7614067196846008\n",
      "[step: 11267] loss: 0.8420729637145996\n",
      "[step: 11268] loss: 0.9936460256576538\n",
      "[step: 11269] loss: 1.2663954496383667\n",
      "[step: 11270] loss: 1.7589871883392334\n",
      "[step: 11271] loss: 2.531449794769287\n",
      "[step: 11272] loss: 3.6163182258605957\n",
      "[step: 11273] loss: 4.446896553039551\n",
      "[step: 11274] loss: 4.255784511566162\n",
      "[step: 11275] loss: 2.5667710304260254\n",
      "[step: 11276] loss: 0.932939887046814\n",
      "[step: 11277] loss: 1.018194556236267\n",
      "[step: 11278] loss: 2.1393229961395264\n",
      "[step: 11279] loss: 2.2564749717712402\n",
      "[step: 11280] loss: 1.1978203058242798\n",
      "[step: 11281] loss: 0.8435466289520264\n",
      "[step: 11282] loss: 1.563748836517334\n",
      "[step: 11283] loss: 1.6077852249145508\n",
      "[step: 11284] loss: 0.8679602742195129\n",
      "[step: 11285] loss: 0.883266806602478\n",
      "[step: 11286] loss: 1.3485488891601562\n",
      "[step: 11287] loss: 1.0900883674621582\n",
      "[step: 11288] loss: 0.7444746494293213\n",
      "[step: 11289] loss: 1.0355417728424072\n",
      "[step: 11290] loss: 1.1292442083358765\n",
      "[step: 11291] loss: 0.7937571406364441\n",
      "[step: 11292] loss: 0.8123065233230591\n",
      "[step: 11293] loss: 1.026911735534668\n",
      "[step: 11294] loss: 0.8646780252456665\n",
      "[step: 11295] loss: 0.7237510681152344\n",
      "[step: 11296] loss: 0.8776100277900696\n",
      "[step: 11297] loss: 0.9026824235916138\n",
      "[step: 11298] loss: 0.740307092666626\n",
      "[step: 11299] loss: 0.7624279856681824\n",
      "[step: 11300] loss: 0.8651448488235474\n",
      "[step: 11301] loss: 0.7824316024780273\n",
      "[step: 11302] loss: 0.7069745063781738\n",
      "[step: 11303] loss: 0.7775655388832092\n",
      "[step: 11304] loss: 0.7982096672058105\n",
      "[step: 11305] loss: 0.7186626195907593\n",
      "[step: 11306] loss: 0.7064050436019897\n",
      "[step: 11307] loss: 0.7619187831878662\n",
      "[step: 11308] loss: 0.7508744597434998\n",
      "[step: 11309] loss: 0.6977403163909912\n",
      "[step: 11310] loss: 0.7058666348457336\n",
      "[step: 11311] loss: 0.740391731262207\n",
      "[step: 11312] loss: 0.7239695191383362\n",
      "[step: 11313] loss: 0.6895488500595093\n",
      "[step: 11314] loss: 0.6960514187812805\n",
      "[step: 11315] loss: 0.7178491353988647\n",
      "[step: 11316] loss: 0.7084330916404724\n",
      "[step: 11317] loss: 0.6841334700584412\n",
      "[step: 11318] loss: 0.6836615204811096\n",
      "[step: 11319] loss: 0.6989939212799072\n",
      "[step: 11320] loss: 0.69899582862854\n",
      "[step: 11321] loss: 0.6830096244812012\n",
      "[step: 11322] loss: 0.6758558750152588\n",
      "[step: 11323] loss: 0.6841957569122314\n",
      "[step: 11324] loss: 0.6905408501625061\n",
      "[step: 11325] loss: 0.6839107275009155\n",
      "[step: 11326] loss: 0.6745060682296753\n",
      "[step: 11327] loss: 0.6740879416465759\n",
      "[step: 11328] loss: 0.6799149513244629\n",
      "[step: 11329] loss: 0.6817290186882019\n",
      "[step: 11330] loss: 0.6765228509902954\n",
      "[step: 11331] loss: 0.6708812713623047\n",
      "[step: 11332] loss: 0.670695424079895\n",
      "[step: 11333] loss: 0.6740606427192688\n",
      "[step: 11334] loss: 0.6753715872764587\n",
      "[step: 11335] loss: 0.6726410388946533\n",
      "[step: 11336] loss: 0.6689274907112122\n",
      "[step: 11337] loss: 0.667805552482605\n",
      "[step: 11338] loss: 0.6693291664123535\n",
      "[step: 11339] loss: 0.6710701584815979\n",
      "[step: 11340] loss: 0.6709034442901611\n",
      "[step: 11341] loss: 0.6692632436752319\n",
      "[step: 11342] loss: 0.6681210398674011\n",
      "[step: 11343] loss: 0.6688456535339355\n",
      "[step: 11344] loss: 0.6712092757225037\n",
      "[step: 11345] loss: 0.6744445562362671\n",
      "[step: 11346] loss: 0.6774255037307739\n",
      "[step: 11347] loss: 0.6815784573554993\n",
      "[step: 11348] loss: 0.6870987415313721\n",
      "[step: 11349] loss: 0.6974581480026245\n",
      "[step: 11350] loss: 0.711361289024353\n",
      "[step: 11351] loss: 0.7342755198478699\n",
      "[step: 11352] loss: 0.7611096501350403\n",
      "[step: 11353] loss: 0.8038442730903625\n",
      "[step: 11354] loss: 0.8450444340705872\n",
      "[step: 11355] loss: 0.9120959043502808\n",
      "[step: 11356] loss: 0.9505860805511475\n",
      "[step: 11357] loss: 1.011486530303955\n",
      "[step: 11358] loss: 0.9887533783912659\n",
      "[step: 11359] loss: 0.9672024250030518\n",
      "[step: 11360] loss: 0.8655192255973816\n",
      "[step: 11361] loss: 0.7790395021438599\n",
      "[step: 11362] loss: 0.7028974294662476\n",
      "[step: 11363] loss: 0.6692388653755188\n",
      "[step: 11364] loss: 0.6736209392547607\n",
      "[step: 11365] loss: 0.7017347812652588\n",
      "[step: 11366] loss: 0.7372732162475586\n",
      "[step: 11367] loss: 0.7591006755828857\n",
      "[step: 11368] loss: 0.7726795077323914\n",
      "[step: 11369] loss: 0.7607424259185791\n",
      "[step: 11370] loss: 0.7478322386741638\n",
      "[step: 11371] loss: 0.7285373210906982\n",
      "[step: 11372] loss: 0.7158238887786865\n",
      "[step: 11373] loss: 0.7075089812278748\n",
      "[step: 11374] loss: 0.7019084095954895\n",
      "[step: 11375] loss: 0.6951134204864502\n",
      "[step: 11376] loss: 0.6865663528442383\n",
      "[step: 11377] loss: 0.6754094362258911\n",
      "[step: 11378] loss: 0.6649011969566345\n",
      "[step: 11379] loss: 0.6569198369979858\n",
      "[step: 11380] loss: 0.6533339023590088\n",
      "[step: 11381] loss: 0.6545376777648926\n",
      "[step: 11382] loss: 0.6596366763114929\n",
      "[step: 11383] loss: 0.6671656370162964\n",
      "[step: 11384] loss: 0.675592303276062\n",
      "[step: 11385] loss: 0.6843358278274536\n",
      "[step: 11386] loss: 0.6932583451271057\n",
      "[step: 11387] loss: 0.70391845703125\n",
      "[step: 11388] loss: 0.7180020213127136\n",
      "[step: 11389] loss: 0.7388513684272766\n",
      "[step: 11390] loss: 0.7695457339286804\n",
      "[step: 11391] loss: 0.8168478608131409\n",
      "[step: 11392] loss: 0.8825781345367432\n",
      "[step: 11393] loss: 0.9831606149673462\n",
      "[step: 11394] loss: 1.1069992780685425\n",
      "[step: 11395] loss: 1.2869575023651123\n",
      "[step: 11396] loss: 1.4624589681625366\n",
      "[step: 11397] loss: 1.6689326763153076\n",
      "[step: 11398] loss: 1.7639524936676025\n",
      "[step: 11399] loss: 1.7595226764678955\n",
      "[step: 11400] loss: 1.5606048107147217\n",
      "[step: 11401] loss: 1.2419612407684326\n",
      "[step: 11402] loss: 0.9147175550460815\n",
      "[step: 11403] loss: 0.7210363149642944\n",
      "[step: 11404] loss: 0.7217754125595093\n",
      "[step: 11405] loss: 0.8606630563735962\n",
      "[step: 11406] loss: 1.0161114931106567\n",
      "[step: 11407] loss: 1.040748119354248\n",
      "[step: 11408] loss: 0.9308692216873169\n",
      "[step: 11409] loss: 0.7631176710128784\n",
      "[step: 11410] loss: 0.6634830832481384\n",
      "[step: 11411] loss: 0.6821489930152893\n",
      "[step: 11412] loss: 0.7729980945587158\n",
      "[step: 11413] loss: 0.8491729497909546\n",
      "[step: 11414] loss: 0.8464374542236328\n",
      "[step: 11415] loss: 0.7828064560890198\n",
      "[step: 11416] loss: 0.7067764401435852\n",
      "[step: 11417] loss: 0.6748427152633667\n",
      "[step: 11418] loss: 0.6895601153373718\n",
      "[step: 11419] loss: 0.7213684916496277\n",
      "[step: 11420] loss: 0.7383759021759033\n",
      "[step: 11421] loss: 0.7266223430633545\n",
      "[step: 11422] loss: 0.6986085772514343\n",
      "[step: 11423] loss: 0.67561274766922\n",
      "[step: 11424] loss: 0.6722031831741333\n",
      "[step: 11425] loss: 0.6847378015518188\n",
      "[step: 11426] loss: 0.7004653215408325\n",
      "[step: 11427] loss: 0.7081092000007629\n",
      "[step: 11428] loss: 0.6996734142303467\n",
      "[step: 11429] loss: 0.6823046207427979\n",
      "[step: 11430] loss: 0.6628928184509277\n",
      "[step: 11431] loss: 0.6499322652816772\n",
      "[step: 11432] loss: 0.6465725302696228\n",
      "[step: 11433] loss: 0.6515843868255615\n",
      "[step: 11434] loss: 0.6607464551925659\n",
      "[step: 11435] loss: 0.6691413521766663\n",
      "[step: 11436] loss: 0.6743556261062622\n",
      "[step: 11437] loss: 0.6742286682128906\n",
      "[step: 11438] loss: 0.6704350709915161\n",
      "[step: 11439] loss: 0.6635454893112183\n",
      "[step: 11440] loss: 0.6563433408737183\n",
      "[step: 11441] loss: 0.6497870087623596\n",
      "[step: 11442] loss: 0.6449946761131287\n",
      "[step: 11443] loss: 0.6421773433685303\n",
      "[step: 11444] loss: 0.6412179470062256\n",
      "[step: 11445] loss: 0.6416671872138977\n",
      "[step: 11446] loss: 0.6430215835571289\n",
      "[step: 11447] loss: 0.6448895931243896\n",
      "[step: 11448] loss: 0.646939754486084\n",
      "[step: 11449] loss: 0.6491621136665344\n",
      "[step: 11450] loss: 0.6514354944229126\n",
      "[step: 11451] loss: 0.6542556285858154\n",
      "[step: 11452] loss: 0.6577351093292236\n",
      "[step: 11453] loss: 0.6631537675857544\n",
      "[step: 11454] loss: 0.6713522672653198\n",
      "[step: 11455] loss: 0.6855137944221497\n",
      "[step: 11456] loss: 0.7090953588485718\n",
      "[step: 11457] loss: 0.7507811784744263\n",
      "[step: 11458] loss: 0.8227522373199463\n",
      "[step: 11459] loss: 0.9503211975097656\n",
      "[step: 11460] loss: 1.168811559677124\n",
      "[step: 11461] loss: 1.541351556777954\n",
      "[step: 11462] loss: 2.1117897033691406\n",
      "[step: 11463] loss: 2.904006004333496\n",
      "[step: 11464] loss: 3.6119894981384277\n",
      "[step: 11465] loss: 3.759561061859131\n",
      "[step: 11466] loss: 2.774465560913086\n",
      "[step: 11467] loss: 1.314208984375\n",
      "[step: 11468] loss: 0.6957608461380005\n",
      "[step: 11469] loss: 1.2778677940368652\n",
      "[step: 11470] loss: 1.9684326648712158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11471] loss: 1.7019703388214111\n",
      "[step: 11472] loss: 0.9194916486740112\n",
      "[step: 11473] loss: 0.8565157055854797\n",
      "[step: 11474] loss: 1.3748970031738281\n",
      "[step: 11475] loss: 1.3489453792572021\n",
      "[step: 11476] loss: 0.824282169342041\n",
      "[step: 11477] loss: 0.7564352750778198\n",
      "[step: 11478] loss: 1.1170485019683838\n",
      "[step: 11479] loss: 1.0856738090515137\n",
      "[step: 11480] loss: 0.7462987899780273\n",
      "[step: 11481] loss: 0.7795162200927734\n",
      "[step: 11482] loss: 1.0048598051071167\n",
      "[step: 11483] loss: 0.9018571376800537\n",
      "[step: 11484] loss: 0.6978369355201721\n",
      "[step: 11485] loss: 0.7679370641708374\n",
      "[step: 11486] loss: 0.8923585414886475\n",
      "[step: 11487] loss: 0.7906730771064758\n",
      "[step: 11488] loss: 0.675049901008606\n",
      "[step: 11489] loss: 0.7494322657585144\n",
      "[step: 11490] loss: 0.8166395425796509\n",
      "[step: 11491] loss: 0.7368825674057007\n",
      "[step: 11492] loss: 0.6711089611053467\n",
      "[step: 11493] loss: 0.7249566316604614\n",
      "[step: 11494] loss: 0.7671875357627869\n",
      "[step: 11495] loss: 0.7125265002250671\n",
      "[step: 11496] loss: 0.6644855737686157\n",
      "[step: 11497] loss: 0.6952317357063293\n",
      "[step: 11498] loss: 0.7280673384666443\n",
      "[step: 11499] loss: 0.6993759870529175\n",
      "[step: 11500] loss: 0.6596856117248535\n",
      "[step: 11501] loss: 0.6684058904647827\n",
      "[step: 11502] loss: 0.6966462731361389\n",
      "[step: 11503] loss: 0.6915594339370728\n",
      "[step: 11504] loss: 0.6628314256668091\n",
      "[step: 11505] loss: 0.6522679328918457\n",
      "[step: 11506] loss: 0.6680241823196411\n",
      "[step: 11507] loss: 0.6798105835914612\n",
      "[step: 11508] loss: 0.6693012714385986\n",
      "[step: 11509] loss: 0.65185546875\n",
      "[step: 11510] loss: 0.6489977240562439\n",
      "[step: 11511] loss: 0.6590557098388672\n",
      "[step: 11512] loss: 0.6654605865478516\n",
      "[step: 11513] loss: 0.6592341065406799\n",
      "[step: 11514] loss: 0.6485016345977783\n",
      "[step: 11515] loss: 0.6444629430770874\n",
      "[step: 11516] loss: 0.6488745212554932\n",
      "[step: 11517] loss: 0.6543090343475342\n",
      "[step: 11518] loss: 0.6537498235702515\n",
      "[step: 11519] loss: 0.6479721665382385\n",
      "[step: 11520] loss: 0.64268559217453\n",
      "[step: 11521] loss: 0.6418124437332153\n",
      "[step: 11522] loss: 0.6446571350097656\n",
      "[step: 11523] loss: 0.6474289894104004\n",
      "[step: 11524] loss: 0.6474204063415527\n",
      "[step: 11525] loss: 0.6448372006416321\n",
      "[step: 11526] loss: 0.6420340538024902\n",
      "[step: 11527] loss: 0.6412366032600403\n",
      "[step: 11528] loss: 0.6429051756858826\n",
      "[step: 11529] loss: 0.6463456749916077\n",
      "[step: 11530] loss: 0.6500063538551331\n",
      "[step: 11531] loss: 0.6540655493736267\n",
      "[step: 11532] loss: 0.6584464311599731\n",
      "[step: 11533] loss: 0.6662688851356506\n",
      "[step: 11534] loss: 0.6773809194564819\n",
      "[step: 11535] loss: 0.6976478099822998\n",
      "[step: 11536] loss: 0.7239509224891663\n",
      "[step: 11537] loss: 0.7674258351325989\n",
      "[step: 11538] loss: 0.8152183890342712\n",
      "[step: 11539] loss: 0.8908615112304688\n",
      "[step: 11540] loss: 0.947485625743866\n",
      "[step: 11541] loss: 1.0294363498687744\n",
      "[step: 11542] loss: 1.023747205734253\n",
      "[step: 11543] loss: 1.0128514766693115\n",
      "[step: 11544] loss: 0.8918353319168091\n",
      "[step: 11545] loss: 0.7831488847732544\n",
      "[step: 11546] loss: 0.6874529123306274\n",
      "[step: 11547] loss: 0.6542369723320007\n",
      "[step: 11548] loss: 0.6748656034469604\n",
      "[step: 11549] loss: 0.7226245403289795\n",
      "[step: 11550] loss: 0.7636175751686096\n",
      "[step: 11551] loss: 0.7714779376983643\n",
      "[step: 11552] loss: 0.7498170137405396\n",
      "[step: 11553] loss: 0.7022877931594849\n",
      "[step: 11554] loss: 0.6631283760070801\n",
      "[step: 11555] loss: 0.6452397108078003\n",
      "[step: 11556] loss: 0.6547985076904297\n",
      "[step: 11557] loss: 0.6814354658126831\n",
      "[step: 11558] loss: 0.7114543914794922\n",
      "[step: 11559] loss: 0.7279857397079468\n",
      "[step: 11560] loss: 0.7328879833221436\n",
      "[step: 11561] loss: 0.7234331369400024\n",
      "[step: 11562] loss: 0.7167229652404785\n",
      "[step: 11563] loss: 0.7193964719772339\n",
      "[step: 11564] loss: 0.7417124509811401\n",
      "[step: 11565] loss: 0.7858994007110596\n",
      "[step: 11566] loss: 0.8561529517173767\n",
      "[step: 11567] loss: 0.9400603175163269\n",
      "[step: 11568] loss: 1.0594654083251953\n",
      "[step: 11569] loss: 1.1789019107818604\n",
      "[step: 11570] loss: 1.339756727218628\n",
      "[step: 11571] loss: 1.4879915714263916\n",
      "[step: 11572] loss: 1.630426287651062\n",
      "[step: 11573] loss: 1.692720651626587\n",
      "[step: 11574] loss: 1.620240330696106\n",
      "[step: 11575] loss: 1.3784575462341309\n",
      "[step: 11576] loss: 1.0393213033676147\n",
      "[step: 11577] loss: 0.7460743188858032\n",
      "[step: 11578] loss: 0.6366559267044067\n",
      "[step: 11579] loss: 0.7242110967636108\n",
      "[step: 11580] loss: 0.8944988250732422\n",
      "[step: 11581] loss: 0.9920210838317871\n",
      "[step: 11582] loss: 0.9372103214263916\n",
      "[step: 11583] loss: 0.7842094898223877\n",
      "[step: 11584] loss: 0.6631073355674744\n",
      "[step: 11585] loss: 0.6554551124572754\n",
      "[step: 11586] loss: 0.7310181260108948\n",
      "[step: 11587] loss: 0.7941442728042603\n",
      "[step: 11588] loss: 0.7851080298423767\n",
      "[step: 11589] loss: 0.7177290916442871\n",
      "[step: 11590] loss: 0.6598131060600281\n",
      "[step: 11591] loss: 0.6560342311859131\n",
      "[step: 11592] loss: 0.6936523914337158\n",
      "[step: 11593] loss: 0.7276790142059326\n",
      "[step: 11594] loss: 0.7198219299316406\n",
      "[step: 11595] loss: 0.6813687682151794\n",
      "[step: 11596] loss: 0.6415219306945801\n",
      "[step: 11597] loss: 0.629767656326294\n",
      "[step: 11598] loss: 0.6466385126113892\n",
      "[step: 11599] loss: 0.6713851094245911\n",
      "[step: 11600] loss: 0.6838111281394958\n",
      "[step: 11601] loss: 0.6737267971038818\n",
      "[step: 11602] loss: 0.652352511882782\n",
      "[step: 11603] loss: 0.6331861019134521\n",
      "[step: 11604] loss: 0.6266446113586426\n",
      "[step: 11605] loss: 0.6326663494110107\n",
      "[step: 11606] loss: 0.6441425681114197\n",
      "[step: 11607] loss: 0.6527210474014282\n",
      "[step: 11608] loss: 0.653134822845459\n",
      "[step: 11609] loss: 0.6465882062911987\n",
      "[step: 11610] loss: 0.636510968208313\n",
      "[step: 11611] loss: 0.6278228759765625\n",
      "[step: 11612] loss: 0.6232590675354004\n",
      "[step: 11613] loss: 0.6231685876846313\n",
      "[step: 11614] loss: 0.626017689704895\n",
      "[step: 11615] loss: 0.6295419931411743\n",
      "[step: 11616] loss: 0.6319103240966797\n",
      "[step: 11617] loss: 0.6320762038230896\n",
      "[step: 11618] loss: 0.6304560899734497\n",
      "[step: 11619] loss: 0.6275568008422852\n",
      "[step: 11620] loss: 0.6246939301490784\n",
      "[step: 11621] loss: 0.6226921081542969\n",
      "[step: 11622] loss: 0.6221802234649658\n",
      "[step: 11623] loss: 0.6234052181243896\n",
      "[step: 11624] loss: 0.6265372037887573\n",
      "[step: 11625] loss: 0.6315206289291382\n",
      "[step: 11626] loss: 0.6394526958465576\n",
      "[step: 11627] loss: 0.6505348682403564\n",
      "[step: 11628] loss: 0.669279932975769\n",
      "[step: 11629] loss: 0.6972371935844421\n",
      "[step: 11630] loss: 0.7486850023269653\n",
      "[step: 11631] loss: 0.8299009203910828\n",
      "[step: 11632] loss: 0.9815496206283569\n",
      "[step: 11633] loss: 1.2200454473495483\n",
      "[step: 11634] loss: 1.6395065784454346\n",
      "[step: 11635] loss: 2.2262043952941895\n",
      "[step: 11636] loss: 3.021152973175049\n",
      "[step: 11637] loss: 3.652740955352783\n",
      "[step: 11638] loss: 3.6268105506896973\n",
      "[step: 11639] loss: 2.607264280319214\n",
      "[step: 11640] loss: 1.219743251800537\n",
      "[step: 11641] loss: 0.7341456413269043\n",
      "[step: 11642] loss: 1.3733183145523071\n",
      "[step: 11643] loss: 2.0246455669403076\n",
      "[step: 11644] loss: 1.624293565750122\n",
      "[step: 11645] loss: 0.805310845375061\n",
      "[step: 11646] loss: 0.820708155632019\n",
      "[step: 11647] loss: 1.3704272508621216\n",
      "[step: 11648] loss: 1.3318136930465698\n",
      "[step: 11649] loss: 0.8427187204360962\n",
      "[step: 11650] loss: 0.8200851678848267\n",
      "[step: 11651] loss: 1.1357569694519043\n",
      "[step: 11652] loss: 1.0102603435516357\n",
      "[step: 11653] loss: 0.6941018104553223\n",
      "[step: 11654] loss: 0.8112771511077881\n",
      "[step: 11655] loss: 1.0164191722869873\n",
      "[step: 11656] loss: 0.878952145576477\n",
      "[step: 11657] loss: 0.6930497884750366\n",
      "[step: 11658] loss: 0.792316734790802\n",
      "[step: 11659] loss: 0.8798274993896484\n",
      "[step: 11660] loss: 0.7404017448425293\n",
      "[step: 11661] loss: 0.6587903499603271\n",
      "[step: 11662] loss: 0.7635710835456848\n",
      "[step: 11663] loss: 0.811151921749115\n",
      "[step: 11664] loss: 0.7160372138023376\n",
      "[step: 11665] loss: 0.66888427734375\n",
      "[step: 11666] loss: 0.7338442802429199\n",
      "[step: 11667] loss: 0.7494117021560669\n",
      "[step: 11668] loss: 0.6737254858016968\n",
      "[step: 11669] loss: 0.6442068815231323\n",
      "[step: 11670] loss: 0.6912832260131836\n",
      "[step: 11671] loss: 0.7142274975776672\n",
      "[step: 11672] loss: 0.6738020777702332\n",
      "[step: 11673] loss: 0.6429859399795532\n",
      "[step: 11674] loss: 0.6641708612442017\n",
      "[step: 11675] loss: 0.6852643489837646\n",
      "[step: 11676] loss: 0.6654398441314697\n",
      "[step: 11677] loss: 0.6354462504386902\n",
      "[step: 11678] loss: 0.6350250840187073\n",
      "[step: 11679] loss: 0.6556379795074463\n",
      "[step: 11680] loss: 0.6604893207550049\n",
      "[step: 11681] loss: 0.6439393758773804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11682] loss: 0.6303216814994812\n",
      "[step: 11683] loss: 0.6347243189811707\n",
      "[step: 11684] loss: 0.6453496217727661\n",
      "[step: 11685] loss: 0.6452445983886719\n",
      "[step: 11686] loss: 0.6333726048469543\n",
      "[step: 11687] loss: 0.6237473487854004\n",
      "[step: 11688] loss: 0.6246412992477417\n",
      "[step: 11689] loss: 0.6315305233001709\n",
      "[step: 11690] loss: 0.634634256362915\n",
      "[step: 11691] loss: 0.6300104856491089\n",
      "[step: 11692] loss: 0.6233435869216919\n",
      "[step: 11693] loss: 0.6204977035522461\n",
      "[step: 11694] loss: 0.6226487159729004\n",
      "[step: 11695] loss: 0.6260578632354736\n",
      "[step: 11696] loss: 0.6264857649803162\n",
      "[step: 11697] loss: 0.6233848333358765\n",
      "[step: 11698] loss: 0.619196891784668\n",
      "[step: 11699] loss: 0.616690993309021\n",
      "[step: 11700] loss: 0.6168259382247925\n",
      "[step: 11701] loss: 0.6182283163070679\n",
      "[step: 11702] loss: 0.6192252039909363\n",
      "[step: 11703] loss: 0.6186015009880066\n",
      "[step: 11704] loss: 0.6167742013931274\n",
      "[step: 11705] loss: 0.6147944331169128\n",
      "[step: 11706] loss: 0.613518476486206\n",
      "[step: 11707] loss: 0.6133686304092407\n",
      "[step: 11708] loss: 0.6138683557510376\n",
      "[step: 11709] loss: 0.6144226789474487\n",
      "[step: 11710] loss: 0.6144855618476868\n",
      "[step: 11711] loss: 0.6139193773269653\n",
      "[step: 11712] loss: 0.6129120588302612\n",
      "[step: 11713] loss: 0.6117958426475525\n",
      "[step: 11714] loss: 0.6108877658843994\n",
      "[step: 11715] loss: 0.6103490591049194\n",
      "[step: 11716] loss: 0.6101677417755127\n",
      "[step: 11717] loss: 0.6102670431137085\n",
      "[step: 11718] loss: 0.6104903221130371\n",
      "[step: 11719] loss: 0.6107543110847473\n",
      "[step: 11720] loss: 0.6110254526138306\n",
      "[step: 11721] loss: 0.6113157272338867\n",
      "[step: 11722] loss: 0.6117821931838989\n",
      "[step: 11723] loss: 0.6125271320343018\n",
      "[step: 11724] loss: 0.6139041185379028\n",
      "[step: 11725] loss: 0.6160590052604675\n",
      "[step: 11726] loss: 0.6197705268859863\n",
      "[step: 11727] loss: 0.6252050399780273\n",
      "[step: 11728] loss: 0.6344276666641235\n",
      "[step: 11729] loss: 0.6473748683929443\n",
      "[step: 11730] loss: 0.6697928309440613\n",
      "[step: 11731] loss: 0.6996022462844849\n",
      "[step: 11732] loss: 0.7528212070465088\n",
      "[step: 11733] loss: 0.8169273138046265\n",
      "[step: 11734] loss: 0.9337146282196045\n",
      "[step: 11735] loss: 1.0469077825546265\n",
      "[step: 11736] loss: 1.2469171285629272\n",
      "[step: 11737] loss: 1.370652198791504\n",
      "[step: 11738] loss: 1.5681953430175781\n",
      "[step: 11739] loss: 1.6380503177642822\n",
      "[step: 11740] loss: 1.728898286819458\n",
      "[step: 11741] loss: 1.7987542152404785\n",
      "[step: 11742] loss: 1.8306876420974731\n",
      "[step: 11743] loss: 1.693873643875122\n",
      "[step: 11744] loss: 1.4527133703231812\n",
      "[step: 11745] loss: 1.0566303730010986\n",
      "[step: 11746] loss: 0.8078367710113525\n",
      "[step: 11747] loss: 0.8077795505523682\n",
      "[step: 11748] loss: 0.9654467105865479\n",
      "[step: 11749] loss: 1.0805866718292236\n",
      "[step: 11750] loss: 1.000105381011963\n",
      "[step: 11751] loss: 0.8010877370834351\n",
      "[step: 11752] loss: 0.6705018281936646\n",
      "[step: 11753] loss: 0.7250970602035522\n",
      "[step: 11754] loss: 0.8655098676681519\n",
      "[step: 11755] loss: 0.9109893441200256\n",
      "[step: 11756] loss: 0.8039208054542542\n",
      "[step: 11757] loss: 0.659538984298706\n",
      "[step: 11758] loss: 0.6187880039215088\n",
      "[step: 11759] loss: 0.6922191977500916\n",
      "[step: 11760] loss: 0.771873414516449\n",
      "[step: 11761] loss: 0.7687986493110657\n",
      "[step: 11762] loss: 0.6998927593231201\n",
      "[step: 11763] loss: 0.644999623298645\n",
      "[step: 11764] loss: 0.6487653255462646\n",
      "[step: 11765] loss: 0.6834275126457214\n",
      "[step: 11766] loss: 0.6960681676864624\n",
      "[step: 11767] loss: 0.667542576789856\n",
      "[step: 11768] loss: 0.6311761140823364\n",
      "[step: 11769] loss: 0.6215260028839111\n",
      "[step: 11770] loss: 0.6428261399269104\n",
      "[step: 11771] loss: 0.6676895022392273\n",
      "[step: 11772] loss: 0.6679906845092773\n",
      "[step: 11773] loss: 0.6458605527877808\n",
      "[step: 11774] loss: 0.6182936429977417\n",
      "[step: 11775] loss: 0.6062279939651489\n",
      "[step: 11776] loss: 0.6140633225440979\n",
      "[step: 11777] loss: 0.6295466423034668\n",
      "[step: 11778] loss: 0.6382500529289246\n",
      "[step: 11779] loss: 0.6332324743270874\n",
      "[step: 11780] loss: 0.6201044321060181\n",
      "[step: 11781] loss: 0.6078135967254639\n",
      "[step: 11782] loss: 0.6037944555282593\n",
      "[step: 11783] loss: 0.6080279350280762\n",
      "[step: 11784] loss: 0.6155200004577637\n",
      "[step: 11785] loss: 0.620791494846344\n",
      "[step: 11786] loss: 0.6203267574310303\n",
      "[step: 11787] loss: 0.6153144836425781\n",
      "[step: 11788] loss: 0.6087205410003662\n",
      "[step: 11789] loss: 0.6038055419921875\n",
      "[step: 11790] loss: 0.6020827293395996\n",
      "[step: 11791] loss: 0.6032119393348694\n",
      "[step: 11792] loss: 0.6054004430770874\n",
      "[step: 11793] loss: 0.607052206993103\n",
      "[step: 11794] loss: 0.6069632768630981\n",
      "[step: 11795] loss: 0.6051629781723022\n",
      "[step: 11796] loss: 0.6023247838020325\n",
      "[step: 11797] loss: 0.5995904207229614\n",
      "[step: 11798] loss: 0.597683846950531\n",
      "[step: 11799] loss: 0.5970214605331421\n",
      "[step: 11800] loss: 0.5975258946418762\n",
      "[step: 11801] loss: 0.5988324880599976\n",
      "[step: 11802] loss: 0.6005764007568359\n",
      "[step: 11803] loss: 0.602681040763855\n",
      "[step: 11804] loss: 0.6053851246833801\n",
      "[step: 11805] loss: 0.6096416711807251\n",
      "[step: 11806] loss: 0.6171934008598328\n",
      "[step: 11807] loss: 0.6312428712844849\n",
      "[step: 11808] loss: 0.657406210899353\n",
      "[step: 11809] loss: 0.7064335346221924\n",
      "[step: 11810] loss: 0.7958380579948425\n",
      "[step: 11811] loss: 0.9641993641853333\n",
      "[step: 11812] loss: 1.2560772895812988\n",
      "[step: 11813] loss: 1.781965970993042\n",
      "[step: 11814] loss: 2.5460329055786133\n",
      "[step: 11815] loss: 3.5748369693756104\n",
      "[step: 11816] loss: 4.2438812255859375\n",
      "[step: 11817] loss: 3.859799861907959\n",
      "[step: 11818] loss: 2.333937406539917\n",
      "[step: 11819] loss: 0.9299507141113281\n",
      "[step: 11820] loss: 1.0380208492279053\n",
      "[step: 11821] loss: 2.0331335067749023\n",
      "[step: 11822] loss: 2.2042393684387207\n",
      "[step: 11823] loss: 1.1520969867706299\n",
      "[step: 11824] loss: 0.7149781584739685\n",
      "[step: 11825] loss: 1.4304145574569702\n",
      "[step: 11826] loss: 1.6773531436920166\n",
      "[step: 11827] loss: 1.0619480609893799\n",
      "[step: 11828] loss: 0.8247694969177246\n",
      "[step: 11829] loss: 1.2199193239212036\n",
      "[step: 11830] loss: 1.1564584970474243\n",
      "[step: 11831] loss: 0.7933869361877441\n",
      "[step: 11832] loss: 0.9908146858215332\n",
      "[step: 11833] loss: 1.1666455268859863\n",
      "[step: 11834] loss: 0.8166937828063965\n",
      "[step: 11835] loss: 0.680741012096405\n",
      "[step: 11836] loss: 0.930756151676178\n",
      "[step: 11837] loss: 0.9483016729354858\n",
      "[step: 11838] loss: 0.7375876903533936\n",
      "[step: 11839] loss: 0.758609414100647\n",
      "[step: 11840] loss: 0.8714079260826111\n",
      "[step: 11841] loss: 0.7404899597167969\n",
      "[step: 11842] loss: 0.6504335999488831\n",
      "[step: 11843] loss: 0.760102391242981\n",
      "[step: 11844] loss: 0.7947752475738525\n",
      "[step: 11845] loss: 0.6843370199203491\n",
      "[step: 11846] loss: 0.6489499807357788\n",
      "[step: 11847] loss: 0.7168231010437012\n",
      "[step: 11848] loss: 0.7090511918067932\n",
      "[step: 11849] loss: 0.6420067548751831\n",
      "[step: 11850] loss: 0.6535981893539429\n",
      "[step: 11851] loss: 0.6999186873435974\n",
      "[step: 11852] loss: 0.6773990988731384\n",
      "[step: 11853] loss: 0.6257373094558716\n",
      "[step: 11854] loss: 0.6295852065086365\n",
      "[step: 11855] loss: 0.6649616956710815\n",
      "[step: 11856] loss: 0.6571632623672485\n",
      "[step: 11857] loss: 0.625150203704834\n",
      "[step: 11858] loss: 0.6221022605895996\n",
      "[step: 11859] loss: 0.6413237452507019\n",
      "[step: 11860] loss: 0.6417011022567749\n",
      "[step: 11861] loss: 0.6200411319732666\n",
      "[step: 11862] loss: 0.6093466877937317\n",
      "[step: 11863] loss: 0.6214748620986938\n",
      "[step: 11864] loss: 0.6314432621002197\n",
      "[step: 11865] loss: 0.6237797737121582\n",
      "[step: 11866] loss: 0.6094451546669006\n",
      "[step: 11867] loss: 0.6062400937080383\n",
      "[step: 11868] loss: 0.6138314604759216\n",
      "[step: 11869] loss: 0.6172148585319519\n",
      "[step: 11870] loss: 0.6118261814117432\n",
      "[step: 11871] loss: 0.6046061515808105\n",
      "[step: 11872] loss: 0.6036760807037354\n",
      "[step: 11873] loss: 0.6076878309249878\n",
      "[step: 11874] loss: 0.6092559695243835\n",
      "[step: 11875] loss: 0.6057804822921753\n",
      "[step: 11876] loss: 0.600701093673706\n",
      "[step: 11877] loss: 0.5986064076423645\n",
      "[step: 11878] loss: 0.6005595922470093\n",
      "[step: 11879] loss: 0.6027612090110779\n",
      "[step: 11880] loss: 0.6025608777999878\n",
      "[step: 11881] loss: 0.5996617078781128\n",
      "[step: 11882] loss: 0.5966004133224487\n",
      "[step: 11883] loss: 0.5955163240432739\n",
      "[step: 11884] loss: 0.5962023735046387\n",
      "[step: 11885] loss: 0.5972867012023926\n",
      "[step: 11886] loss: 0.5972104072570801\n",
      "[step: 11887] loss: 0.5958466529846191\n",
      "[step: 11888] loss: 0.5940802097320557\n",
      "[step: 11889] loss: 0.5928720235824585\n",
      "[step: 11890] loss: 0.5926623940467834\n",
      "[step: 11891] loss: 0.593021035194397\n",
      "[step: 11892] loss: 0.5932414531707764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11893] loss: 0.5929433107376099\n",
      "[step: 11894] loss: 0.5920416116714478\n",
      "[step: 11895] loss: 0.5909709930419922\n",
      "[step: 11896] loss: 0.5900673866271973\n",
      "[step: 11897] loss: 0.5895634889602661\n",
      "[step: 11898] loss: 0.5894198417663574\n",
      "[step: 11899] loss: 0.5894137620925903\n",
      "[step: 11900] loss: 0.5893462896347046\n",
      "[step: 11901] loss: 0.5890806913375854\n",
      "[step: 11902] loss: 0.5886207818984985\n",
      "[step: 11903] loss: 0.588051438331604\n",
      "[step: 11904] loss: 0.5874714255332947\n",
      "[step: 11905] loss: 0.5869730710983276\n",
      "[step: 11906] loss: 0.5865720510482788\n",
      "[step: 11907] loss: 0.5862521529197693\n",
      "[step: 11908] loss: 0.5859830379486084\n",
      "[step: 11909] loss: 0.5857022404670715\n",
      "[step: 11910] loss: 0.5853980779647827\n",
      "[step: 11911] loss: 0.5850513577461243\n",
      "[step: 11912] loss: 0.5846733450889587\n",
      "[step: 11913] loss: 0.584283709526062\n",
      "[step: 11914] loss: 0.5838952660560608\n",
      "[step: 11915] loss: 0.5835301280021667\n",
      "[step: 11916] loss: 0.5831937193870544\n",
      "[step: 11917] loss: 0.5828861594200134\n",
      "[step: 11918] loss: 0.5826097726821899\n",
      "[step: 11919] loss: 0.5823560953140259\n",
      "[step: 11920] loss: 0.5821259617805481\n",
      "[step: 11921] loss: 0.5819188952445984\n",
      "[step: 11922] loss: 0.5817430019378662\n",
      "[step: 11923] loss: 0.5816134214401245\n",
      "[step: 11924] loss: 0.5815615653991699\n",
      "[step: 11925] loss: 0.5816408395767212\n",
      "[step: 11926] loss: 0.5819612741470337\n",
      "[step: 11927] loss: 0.58268141746521\n",
      "[step: 11928] loss: 0.5841430425643921\n",
      "[step: 11929] loss: 0.586870551109314\n",
      "[step: 11930] loss: 0.5920169353485107\n",
      "[step: 11931] loss: 0.6014372110366821\n",
      "[step: 11932] loss: 0.619230329990387\n",
      "[step: 11933] loss: 0.6521028280258179\n",
      "[step: 11934] loss: 0.7151520252227783\n",
      "[step: 11935] loss: 0.8324645757675171\n",
      "[step: 11936] loss: 1.0579566955566406\n",
      "[step: 11937] loss: 1.4624760150909424\n",
      "[step: 11938] loss: 2.181427001953125\n",
      "[step: 11939] loss: 3.2343578338623047\n",
      "[step: 11940] loss: 4.436323165893555\n",
      "[step: 11941] loss: 4.861293792724609\n",
      "[step: 11942] loss: 3.528735637664795\n",
      "[step: 11943] loss: 1.4322844743728638\n",
      "[step: 11944] loss: 0.779984176158905\n",
      "[step: 11945] loss: 1.9697800874710083\n",
      "[step: 11946] loss: 2.622206926345825\n",
      "[step: 11947] loss: 1.4935834407806396\n",
      "[step: 11948] loss: 0.7172130346298218\n",
      "[step: 11949] loss: 1.4935460090637207\n",
      "[step: 11950] loss: 1.7051947116851807\n",
      "[step: 11951] loss: 0.9375828504562378\n",
      "[step: 11952] loss: 0.9950957298278809\n",
      "[step: 11953] loss: 1.451651692390442\n",
      "[step: 11954] loss: 1.0169093608856201\n",
      "[step: 11955] loss: 0.6933547258377075\n",
      "[step: 11956] loss: 1.121711254119873\n",
      "[step: 11957] loss: 1.0826560258865356\n",
      "[step: 11958] loss: 0.7216986417770386\n",
      "[step: 11959] loss: 0.9546691179275513\n",
      "[step: 11960] loss: 1.0529571771621704\n",
      "[step: 11961] loss: 0.7139724493026733\n",
      "[step: 11962] loss: 0.7668282985687256\n",
      "[step: 11963] loss: 0.9254044890403748\n",
      "[step: 11964] loss: 0.7284984588623047\n",
      "[step: 11965] loss: 0.6879855394363403\n",
      "[step: 11966] loss: 0.8519056439399719\n",
      "[step: 11967] loss: 0.7715213894844055\n",
      "[step: 11968] loss: 0.6562098264694214\n",
      "[step: 11969] loss: 0.7465276122093201\n",
      "[step: 11970] loss: 0.7484806776046753\n",
      "[step: 11971] loss: 0.6365501880645752\n",
      "[step: 11972] loss: 0.6616142988204956\n",
      "[step: 11973] loss: 0.7271240949630737\n",
      "[step: 11974] loss: 0.6697601675987244\n",
      "[step: 11975] loss: 0.6303170323371887\n",
      "[step: 11976] loss: 0.6803910732269287\n",
      "[step: 11977] loss: 0.6788643598556519\n",
      "[step: 11978] loss: 0.6198450326919556\n",
      "[step: 11979] loss: 0.6223127245903015\n",
      "[step: 11980] loss: 0.6582093238830566\n",
      "[step: 11981] loss: 0.6421056389808655\n",
      "[step: 11982] loss: 0.6104525327682495\n",
      "[step: 11983] loss: 0.6233158111572266\n",
      "[step: 11984] loss: 0.6428656578063965\n",
      "[step: 11985] loss: 0.6246978044509888\n",
      "[step: 11986] loss: 0.6020476818084717\n",
      "[step: 11987] loss: 0.6106791496276855\n",
      "[step: 11988] loss: 0.6225405335426331\n",
      "[step: 11989] loss: 0.6126156449317932\n",
      "[step: 11990] loss: 0.5983215570449829\n",
      "[step: 11991] loss: 0.6025518178939819\n",
      "[step: 11992] loss: 0.6132767200469971\n",
      "[step: 11993] loss: 0.608964204788208\n",
      "[step: 11994] loss: 0.5970702767372131\n",
      "[step: 11995] loss: 0.5947086215019226\n",
      "[step: 11996] loss: 0.6005407571792603\n",
      "[step: 11997] loss: 0.6019078493118286\n",
      "[step: 11998] loss: 0.595086932182312\n",
      "[step: 11999] loss: 0.5897555947303772\n",
      "[step: 12000] loss: 0.5918010473251343\n",
      "[step: 12001] loss: 0.5956932306289673\n",
      "[step: 12002] loss: 0.5949657559394836\n",
      "[step: 12003] loss: 0.5902889966964722\n",
      "[step: 12004] loss: 0.587602436542511\n",
      "[step: 12005] loss: 0.5886746644973755\n",
      "[step: 12006] loss: 0.5903595685958862\n",
      "[step: 12007] loss: 0.5893627405166626\n",
      "[step: 12008] loss: 0.5861984491348267\n",
      "[step: 12009] loss: 0.584026575088501\n",
      "[step: 12010] loss: 0.5842121839523315\n",
      "[step: 12011] loss: 0.5853273272514343\n",
      "[step: 12012] loss: 0.5853676199913025\n",
      "[step: 12013] loss: 0.5838137865066528\n",
      "[step: 12014] loss: 0.5821279287338257\n",
      "[step: 12015] loss: 0.581445574760437\n",
      "[step: 12016] loss: 0.5817760229110718\n",
      "[step: 12017] loss: 0.5822041630744934\n",
      "[step: 12018] loss: 0.5818085074424744\n",
      "[step: 12019] loss: 0.5807411074638367\n",
      "[step: 12020] loss: 0.579610288143158\n",
      "[step: 12021] loss: 0.5789892673492432\n",
      "[step: 12022] loss: 0.5789042711257935\n",
      "[step: 12023] loss: 0.578934907913208\n",
      "[step: 12024] loss: 0.5786770582199097\n",
      "[step: 12025] loss: 0.5780478119850159\n",
      "[step: 12026] loss: 0.5772477388381958\n",
      "[step: 12027] loss: 0.5765665173530579\n",
      "[step: 12028] loss: 0.5761517882347107\n",
      "[step: 12029] loss: 0.5759653449058533\n",
      "[step: 12030] loss: 0.5758227705955505\n",
      "[step: 12031] loss: 0.5755773782730103\n",
      "[step: 12032] loss: 0.57518470287323\n",
      "[step: 12033] loss: 0.5746867656707764\n",
      "[step: 12034] loss: 0.5741956830024719\n",
      "[step: 12035] loss: 0.573787271976471\n",
      "[step: 12036] loss: 0.5734995603561401\n",
      "[step: 12037] loss: 0.5733102560043335\n",
      "[step: 12038] loss: 0.5731792449951172\n",
      "[step: 12039] loss: 0.5730737447738647\n",
      "[step: 12040] loss: 0.5729849338531494\n",
      "[step: 12041] loss: 0.5729410648345947\n",
      "[step: 12042] loss: 0.5730007886886597\n",
      "[step: 12043] loss: 0.5732660293579102\n",
      "[step: 12044] loss: 0.5738359689712524\n",
      "[step: 12045] loss: 0.574944257736206\n",
      "[step: 12046] loss: 0.5767461061477661\n",
      "[step: 12047] loss: 0.5798445343971252\n",
      "[step: 12048] loss: 0.5844669342041016\n",
      "[step: 12049] loss: 0.5923324823379517\n",
      "[step: 12050] loss: 0.6036120653152466\n",
      "[step: 12051] loss: 0.6232019662857056\n",
      "[step: 12052] loss: 0.6495793461799622\n",
      "[step: 12053] loss: 0.6964590549468994\n",
      "[step: 12054] loss: 0.7515784502029419\n",
      "[step: 12055] loss: 0.8504112958908081\n",
      "[step: 12056] loss: 0.9323882460594177\n",
      "[step: 12057] loss: 1.0724360942840576\n",
      "[step: 12058] loss: 1.0893393754959106\n",
      "[step: 12059] loss: 1.1230809688568115\n",
      "[step: 12060] loss: 0.970608651638031\n",
      "[step: 12061] loss: 0.8460161685943604\n",
      "[step: 12062] loss: 0.7446961998939514\n",
      "[step: 12063] loss: 0.750444769859314\n",
      "[step: 12064] loss: 0.8447756767272949\n",
      "[step: 12065] loss: 0.9904170632362366\n",
      "[step: 12066] loss: 1.0795670747756958\n",
      "[step: 12067] loss: 1.1276943683624268\n",
      "[step: 12068] loss: 1.0527405738830566\n",
      "[step: 12069] loss: 0.9724915027618408\n",
      "[step: 12070] loss: 0.9059430360794067\n",
      "[step: 12071] loss: 0.9049944877624512\n",
      "[step: 12072] loss: 0.9408324956893921\n",
      "[step: 12073] loss: 0.9878767728805542\n",
      "[step: 12074] loss: 0.914901614189148\n",
      "[step: 12075] loss: 0.812218189239502\n",
      "[step: 12076] loss: 0.6702092885971069\n",
      "[step: 12077] loss: 0.5914196968078613\n",
      "[step: 12078] loss: 0.5868078470230103\n",
      "[step: 12079] loss: 0.6288642287254333\n",
      "[step: 12080] loss: 0.6758568286895752\n",
      "[step: 12081] loss: 0.684309720993042\n",
      "[step: 12082] loss: 0.6747130155563354\n",
      "[step: 12083] loss: 0.654626190662384\n",
      "[step: 12084] loss: 0.6514784097671509\n",
      "[step: 12085] loss: 0.660625696182251\n",
      "[step: 12086] loss: 0.6673827171325684\n",
      "[step: 12087] loss: 0.6548349857330322\n",
      "[step: 12088] loss: 0.6280009150505066\n",
      "[step: 12089] loss: 0.5952553749084473\n",
      "[step: 12090] loss: 0.5732002258300781\n",
      "[step: 12091] loss: 0.5676578283309937\n",
      "[step: 12092] loss: 0.5748911499977112\n",
      "[step: 12093] loss: 0.5850287675857544\n",
      "[step: 12094] loss: 0.589916467666626\n",
      "[step: 12095] loss: 0.5881491899490356\n",
      "[step: 12096] loss: 0.5833160281181335\n",
      "[step: 12097] loss: 0.5820038318634033\n",
      "[step: 12098] loss: 0.5876920223236084\n",
      "[step: 12099] loss: 0.6006723642349243\n",
      "[step: 12100] loss: 0.6176434755325317\n",
      "[step: 12101] loss: 0.6385838985443115\n",
      "[step: 12102] loss: 0.6605196595191956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12103] loss: 0.6933226585388184\n",
      "[step: 12104] loss: 0.7409835457801819\n",
      "[step: 12105] loss: 0.8188748359680176\n",
      "[step: 12106] loss: 0.9398319125175476\n",
      "[step: 12107] loss: 1.1190435886383057\n",
      "[step: 12108] loss: 1.3732377290725708\n",
      "[step: 12109] loss: 1.6924054622650146\n",
      "[step: 12110] loss: 2.0392982959747314\n",
      "[step: 12111] loss: 2.265453815460205\n",
      "[step: 12112] loss: 2.2152438163757324\n",
      "[step: 12113] loss: 1.7594877481460571\n",
      "[step: 12114] loss: 1.106722116470337\n",
      "[step: 12115] loss: 0.6593112349510193\n",
      "[step: 12116] loss: 0.6652314066886902\n",
      "[step: 12117] loss: 0.9674966931343079\n",
      "[step: 12118] loss: 1.162886142730713\n",
      "[step: 12119] loss: 1.037554144859314\n",
      "[step: 12120] loss: 0.7628074288368225\n",
      "[step: 12121] loss: 0.6581020355224609\n",
      "[step: 12122] loss: 0.7770624756813049\n",
      "[step: 12123] loss: 0.8762739896774292\n",
      "[step: 12124] loss: 0.789938747882843\n",
      "[step: 12125] loss: 0.6513458490371704\n",
      "[step: 12126] loss: 0.6512449979782104\n",
      "[step: 12127] loss: 0.7559077739715576\n",
      "[step: 12128] loss: 0.7889628410339355\n",
      "[step: 12129] loss: 0.6833559274673462\n",
      "[step: 12130] loss: 0.5863616466522217\n",
      "[step: 12131] loss: 0.6048235893249512\n",
      "[step: 12132] loss: 0.6843633651733398\n",
      "[step: 12133] loss: 0.7092258930206299\n",
      "[step: 12134] loss: 0.6476742029190063\n",
      "[step: 12135] loss: 0.5894392728805542\n",
      "[step: 12136] loss: 0.5944271087646484\n",
      "[step: 12137] loss: 0.6332955360412598\n",
      "[step: 12138] loss: 0.6444783210754395\n",
      "[step: 12139] loss: 0.6128340363502502\n",
      "[step: 12140] loss: 0.5792549848556519\n",
      "[step: 12141] loss: 0.5793014764785767\n",
      "[step: 12142] loss: 0.6041568517684937\n",
      "[step: 12143] loss: 0.6198194622993469\n",
      "[step: 12144] loss: 0.6090488433837891\n",
      "[step: 12145] loss: 0.583401620388031\n",
      "[step: 12146] loss: 0.5669287443161011\n",
      "[step: 12147] loss: 0.5699256658554077\n",
      "[step: 12148] loss: 0.5833384990692139\n",
      "[step: 12149] loss: 0.5920647978782654\n",
      "[step: 12150] loss: 0.5892964005470276\n",
      "[step: 12151] loss: 0.5795425772666931\n",
      "[step: 12152] loss: 0.5713191032409668\n",
      "[step: 12153] loss: 0.5690861940383911\n",
      "[step: 12154] loss: 0.5713281631469727\n",
      "[step: 12155] loss: 0.5731186270713806\n",
      "[step: 12156] loss: 0.5716268420219421\n",
      "[step: 12157] loss: 0.5677558183670044\n",
      "[step: 12158] loss: 0.5641427636146545\n",
      "[step: 12159] loss: 0.5632140636444092\n",
      "[step: 12160] loss: 0.5649584531784058\n",
      "[step: 12161] loss: 0.5677464604377747\n",
      "[step: 12162] loss: 0.5690298080444336\n",
      "[step: 12163] loss: 0.5680734515190125\n",
      "[step: 12164] loss: 0.5648472905158997\n",
      "[step: 12165] loss: 0.561095118522644\n",
      "[step: 12166] loss: 0.5582259893417358\n",
      "[step: 12167] loss: 0.5570652484893799\n",
      "[step: 12168] loss: 0.5573848485946655\n",
      "[step: 12169] loss: 0.5583882927894592\n",
      "[step: 12170] loss: 0.5590940713882446\n",
      "[step: 12171] loss: 0.5590094327926636\n",
      "[step: 12172] loss: 0.5580499172210693\n",
      "[step: 12173] loss: 0.5566365718841553\n",
      "[step: 12174] loss: 0.5552645325660706\n",
      "[step: 12175] loss: 0.5543713569641113\n",
      "[step: 12176] loss: 0.5541301965713501\n",
      "[step: 12177] loss: 0.554496169090271\n",
      "[step: 12178] loss: 0.5552997589111328\n",
      "[step: 12179] loss: 0.5564462542533875\n",
      "[step: 12180] loss: 0.5578816533088684\n",
      "[step: 12181] loss: 0.5601648092269897\n",
      "[step: 12182] loss: 0.5638795495033264\n",
      "[step: 12183] loss: 0.5709186792373657\n",
      "[step: 12184] loss: 0.5836081504821777\n",
      "[step: 12185] loss: 0.6076342463493347\n",
      "[step: 12186] loss: 0.6516152024269104\n",
      "[step: 12187] loss: 0.7344101667404175\n",
      "[step: 12188] loss: 0.8860978484153748\n",
      "[step: 12189] loss: 1.1666526794433594\n",
      "[step: 12190] loss: 1.6551424264907837\n",
      "[step: 12191] loss: 2.4583935737609863\n",
      "[step: 12192] loss: 3.524548053741455\n",
      "[step: 12193] loss: 4.425356388092041\n",
      "[step: 12194] loss: 4.253222465515137\n",
      "[step: 12195] loss: 2.498786449432373\n",
      "[step: 12196] loss: 0.8448672294616699\n",
      "[step: 12197] loss: 0.9259399771690369\n",
      "[step: 12198] loss: 2.0958008766174316\n",
      "[step: 12199] loss: 2.2010650634765625\n",
      "[step: 12200] loss: 1.0359177589416504\n",
      "[step: 12201] loss: 0.7602646350860596\n",
      "[step: 12202] loss: 1.5275866985321045\n",
      "[step: 12203] loss: 1.4192734956741333\n",
      "[step: 12204] loss: 0.7765698432922363\n",
      "[step: 12205] loss: 0.9792377352714539\n",
      "[step: 12206] loss: 1.2723510265350342\n",
      "[step: 12207] loss: 0.8365465998649597\n",
      "[step: 12208] loss: 0.6806021332740784\n",
      "[step: 12209] loss: 1.0724831819534302\n",
      "[step: 12210] loss: 0.958458423614502\n",
      "[step: 12211] loss: 0.6643214225769043\n",
      "[step: 12212] loss: 0.8588172197341919\n",
      "[step: 12213] loss: 0.9240056276321411\n",
      "[step: 12214] loss: 0.6546066999435425\n",
      "[step: 12215] loss: 0.6892352104187012\n",
      "[step: 12216] loss: 0.8540281057357788\n",
      "[step: 12217] loss: 0.7158764600753784\n",
      "[step: 12218] loss: 0.6342657804489136\n",
      "[step: 12219] loss: 0.7596537470817566\n",
      "[step: 12220] loss: 0.7280632257461548\n",
      "[step: 12221] loss: 0.6032285690307617\n",
      "[step: 12222] loss: 0.6499596834182739\n",
      "[step: 12223] loss: 0.7067627906799316\n",
      "[step: 12224] loss: 0.6360733509063721\n",
      "[step: 12225] loss: 0.6019219160079956\n",
      "[step: 12226] loss: 0.6617778539657593\n",
      "[step: 12227] loss: 0.6580219864845276\n",
      "[step: 12228] loss: 0.5925270318984985\n",
      "[step: 12229] loss: 0.595974326133728\n",
      "[step: 12230] loss: 0.6347101330757141\n",
      "[step: 12231] loss: 0.6164065003395081\n",
      "[step: 12232] loss: 0.5823357105255127\n",
      "[step: 12233] loss: 0.5951457023620605\n",
      "[step: 12234] loss: 0.6176467537879944\n",
      "[step: 12235] loss: 0.5984951257705688\n",
      "[step: 12236] loss: 0.572838544845581\n",
      "[step: 12237] loss: 0.5807912945747375\n",
      "[step: 12238] loss: 0.5966458320617676\n",
      "[step: 12239] loss: 0.5881514549255371\n",
      "[step: 12240] loss: 0.5709498524665833\n",
      "[step: 12241] loss: 0.5727602243423462\n",
      "[step: 12242] loss: 0.5842850208282471\n",
      "[step: 12243] loss: 0.5825512409210205\n",
      "[step: 12244] loss: 0.5698261260986328\n",
      "[step: 12245] loss: 0.5639994144439697\n",
      "[step: 12246] loss: 0.5700266361236572\n",
      "[step: 12247] loss: 0.5747745037078857\n",
      "[step: 12248] loss: 0.5698552131652832\n",
      "[step: 12249] loss: 0.5629662275314331\n",
      "[step: 12250] loss: 0.5623192191123962\n",
      "[step: 12251] loss: 0.5664125680923462\n",
      "[step: 12252] loss: 0.5675748586654663\n",
      "[step: 12253] loss: 0.5632429122924805\n",
      "[step: 12254] loss: 0.5586031675338745\n",
      "[step: 12255] loss: 0.5579136610031128\n",
      "[step: 12256] loss: 0.560312032699585\n",
      "[step: 12257] loss: 0.5614719390869141\n",
      "[step: 12258] loss: 0.5594940185546875\n",
      "[step: 12259] loss: 0.5565416812896729\n",
      "[step: 12260] loss: 0.555192768573761\n",
      "[step: 12261] loss: 0.5559778213500977\n",
      "[step: 12262] loss: 0.5570030808448792\n",
      "[step: 12263] loss: 0.5565786957740784\n",
      "[step: 12264] loss: 0.5548071265220642\n",
      "[step: 12265] loss: 0.5529688596725464\n",
      "[step: 12266] loss: 0.552288293838501\n",
      "[step: 12267] loss: 0.5525705218315125\n",
      "[step: 12268] loss: 0.5529488325119019\n",
      "[step: 12269] loss: 0.5526909828186035\n",
      "[step: 12270] loss: 0.5517114400863647\n",
      "[step: 12271] loss: 0.5506283044815063\n",
      "[step: 12272] loss: 0.5499382019042969\n",
      "[step: 12273] loss: 0.5497718453407288\n",
      "[step: 12274] loss: 0.549854576587677\n",
      "[step: 12275] loss: 0.5497698187828064\n",
      "[step: 12276] loss: 0.5493420362472534\n",
      "[step: 12277] loss: 0.5486286878585815\n",
      "[step: 12278] loss: 0.5478861331939697\n",
      "[step: 12279] loss: 0.547309398651123\n",
      "[step: 12280] loss: 0.5469684600830078\n",
      "[step: 12281] loss: 0.546803891658783\n",
      "[step: 12282] loss: 0.5466530323028564\n",
      "[step: 12283] loss: 0.546414315700531\n",
      "[step: 12284] loss: 0.5460465550422668\n",
      "[step: 12285] loss: 0.5455780625343323\n",
      "[step: 12286] loss: 0.5450896620750427\n",
      "[step: 12287] loss: 0.5446343421936035\n",
      "[step: 12288] loss: 0.5442560911178589\n",
      "[step: 12289] loss: 0.5439548492431641\n",
      "[step: 12290] loss: 0.543702244758606\n",
      "[step: 12291] loss: 0.5434664487838745\n",
      "[step: 12292] loss: 0.5432162284851074\n",
      "[step: 12293] loss: 0.5429400205612183\n",
      "[step: 12294] loss: 0.5426304936408997\n",
      "[step: 12295] loss: 0.5422990322113037\n",
      "[step: 12296] loss: 0.5419561862945557\n",
      "[step: 12297] loss: 0.5416122674942017\n",
      "[step: 12298] loss: 0.5412790179252625\n",
      "[step: 12299] loss: 0.540962278842926\n",
      "[step: 12300] loss: 0.5406657457351685\n",
      "[step: 12301] loss: 0.5403956174850464\n",
      "[step: 12302] loss: 0.5401526689529419\n",
      "[step: 12303] loss: 0.5399484634399414\n",
      "[step: 12304] loss: 0.539798378944397\n",
      "[step: 12305] loss: 0.5397384166717529\n",
      "[step: 12306] loss: 0.5398181080818176\n",
      "[step: 12307] loss: 0.540151834487915\n",
      "[step: 12308] loss: 0.5409130454063416\n",
      "[step: 12309] loss: 0.5424616932868958\n",
      "[step: 12310] loss: 0.5453557372093201\n",
      "[step: 12311] loss: 0.550870418548584\n",
      "[step: 12312] loss: 0.5608040690422058\n",
      "[step: 12313] loss: 0.5798445343971252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12314] loss: 0.6135867834091187\n",
      "[step: 12315] loss: 0.6799775958061218\n",
      "[step: 12316] loss: 0.7933278679847717\n",
      "[step: 12317] loss: 1.0193655490875244\n",
      "[step: 12318] loss: 1.360613465309143\n",
      "[step: 12319] loss: 1.9938063621520996\n",
      "[step: 12320] loss: 2.6658573150634766\n",
      "[step: 12321] loss: 3.4768009185791016\n",
      "[step: 12322] loss: 3.7140426635742188\n",
      "[step: 12323] loss: 3.184262752532959\n",
      "[step: 12324] loss: 2.2156994342803955\n",
      "[step: 12325] loss: 1.5335512161254883\n",
      "[step: 12326] loss: 1.6631224155426025\n",
      "[step: 12327] loss: 1.8479983806610107\n",
      "[step: 12328] loss: 1.570739507675171\n",
      "[step: 12329] loss: 1.0121210813522339\n",
      "[step: 12330] loss: 1.2017453908920288\n",
      "[step: 12331] loss: 1.8938815593719482\n",
      "[step: 12332] loss: 1.501046895980835\n",
      "[step: 12333] loss: 0.7806273698806763\n",
      "[step: 12334] loss: 0.8946494460105896\n",
      "[step: 12335] loss: 1.4539997577667236\n",
      "[step: 12336] loss: 1.3945488929748535\n",
      "[step: 12337] loss: 0.7220718860626221\n",
      "[step: 12338] loss: 0.7941159605979919\n",
      "[step: 12339] loss: 1.281437635421753\n",
      "[step: 12340] loss: 1.055936336517334\n",
      "[step: 12341] loss: 0.6495567560195923\n",
      "[step: 12342] loss: 0.7227062582969666\n",
      "[step: 12343] loss: 0.9316140413284302\n",
      "[step: 12344] loss: 0.8828002214431763\n",
      "[step: 12345] loss: 0.63482666015625\n",
      "[step: 12346] loss: 0.675636351108551\n",
      "[step: 12347] loss: 0.8242979645729065\n",
      "[step: 12348] loss: 0.729718804359436\n",
      "[step: 12349] loss: 0.6035719513893127\n",
      "[step: 12350] loss: 0.656517505645752\n",
      "[step: 12351] loss: 0.7231624126434326\n",
      "[step: 12352] loss: 0.689032793045044\n",
      "[step: 12353] loss: 0.5931680798530579\n",
      "[step: 12354] loss: 0.6155346632003784\n",
      "[step: 12355] loss: 0.6805251240730286\n",
      "[step: 12356] loss: 0.6543374061584473\n",
      "[step: 12357] loss: 0.5862557888031006\n",
      "[step: 12358] loss: 0.5793887376785278\n",
      "[step: 12359] loss: 0.6250259876251221\n",
      "[step: 12360] loss: 0.642722487449646\n",
      "[step: 12361] loss: 0.5889975428581238\n",
      "[step: 12362] loss: 0.5589699745178223\n",
      "[step: 12363] loss: 0.5797502398490906\n",
      "[step: 12364] loss: 0.6076763868331909\n",
      "[step: 12365] loss: 0.5963249206542969\n",
      "[step: 12366] loss: 0.5599609017372131\n",
      "[step: 12367] loss: 0.5517861843109131\n",
      "[step: 12368] loss: 0.5721421241760254\n",
      "[step: 12369] loss: 0.582456648349762\n",
      "[step: 12370] loss: 0.5708363056182861\n",
      "[step: 12371] loss: 0.5520434379577637\n",
      "[step: 12372] loss: 0.5491939783096313\n",
      "[step: 12373] loss: 0.5586228966712952\n",
      "[step: 12374] loss: 0.5619629621505737\n",
      "[step: 12375] loss: 0.5577915906906128\n",
      "[step: 12376] loss: 0.5511987209320068\n",
      "[step: 12377] loss: 0.549248218536377\n",
      "[step: 12378] loss: 0.5487949848175049\n",
      "[step: 12379] loss: 0.5475403070449829\n",
      "[step: 12380] loss: 0.5468320250511169\n",
      "[step: 12381] loss: 0.5482718348503113\n",
      "[step: 12382] loss: 0.5491124391555786\n",
      "[step: 12383] loss: 0.5466669797897339\n",
      "[step: 12384] loss: 0.541612982749939\n",
      "[step: 12385] loss: 0.5393295288085938\n",
      "[step: 12386] loss: 0.540472686290741\n",
      "[step: 12387] loss: 0.5432240962982178\n",
      "[step: 12388] loss: 0.5436456203460693\n",
      "[step: 12389] loss: 0.541556715965271\n",
      "[step: 12390] loss: 0.5392621755599976\n",
      "[step: 12391] loss: 0.5381408929824829\n",
      "[step: 12392] loss: 0.5377553701400757\n",
      "[step: 12393] loss: 0.5372358560562134\n",
      "[step: 12394] loss: 0.5363636016845703\n",
      "[step: 12395] loss: 0.5361726880073547\n",
      "[step: 12396] loss: 0.536659300327301\n",
      "[step: 12397] loss: 0.5370761752128601\n",
      "[step: 12398] loss: 0.5367134809494019\n",
      "[step: 12399] loss: 0.5355038046836853\n",
      "[step: 12400] loss: 0.5344054102897644\n",
      "[step: 12401] loss: 0.5337170958518982\n",
      "[step: 12402] loss: 0.5333856344223022\n",
      "[step: 12403] loss: 0.532926082611084\n",
      "[step: 12404] loss: 0.5323053598403931\n",
      "[step: 12405] loss: 0.5317928194999695\n",
      "[step: 12406] loss: 0.5316175818443298\n",
      "[step: 12407] loss: 0.5316484570503235\n",
      "[step: 12408] loss: 0.5316472053527832\n",
      "[step: 12409] loss: 0.5314269065856934\n",
      "[step: 12410] loss: 0.5311724543571472\n",
      "[step: 12411] loss: 0.5310102701187134\n",
      "[step: 12412] loss: 0.5310270190238953\n",
      "[step: 12413] loss: 0.5311154127120972\n",
      "[step: 12414] loss: 0.5311859846115112\n",
      "[step: 12415] loss: 0.531318187713623\n",
      "[step: 12416] loss: 0.5316751599311829\n",
      "[step: 12417] loss: 0.5324440002441406\n",
      "[step: 12418] loss: 0.5337821245193481\n",
      "[step: 12419] loss: 0.5359274744987488\n",
      "[step: 12420] loss: 0.5393542051315308\n",
      "[step: 12421] loss: 0.5449519753456116\n",
      "[step: 12422] loss: 0.554242730140686\n",
      "[step: 12423] loss: 0.5697159171104431\n",
      "[step: 12424] loss: 0.5958342552185059\n",
      "[step: 12425] loss: 0.63993239402771\n",
      "[step: 12426] loss: 0.7157090306282043\n",
      "[step: 12427] loss: 0.8441141247749329\n",
      "[step: 12428] loss: 1.0624217987060547\n",
      "[step: 12429] loss: 1.4123858213424683\n",
      "[step: 12430] loss: 1.9465839862823486\n",
      "[step: 12431] loss: 2.6131486892700195\n",
      "[step: 12432] loss: 3.2303600311279297\n",
      "[step: 12433] loss: 3.2341580390930176\n",
      "[step: 12434] loss: 2.400981903076172\n",
      "[step: 12435] loss: 1.1157336235046387\n",
      "[step: 12436] loss: 0.5754718780517578\n",
      "[step: 12437] loss: 1.0713471174240112\n",
      "[step: 12438] loss: 1.6153005361557007\n",
      "[step: 12439] loss: 1.3427783250808716\n",
      "[step: 12440] loss: 0.7098759412765503\n",
      "[step: 12441] loss: 0.741778552532196\n",
      "[step: 12442] loss: 1.1415071487426758\n",
      "[step: 12443] loss: 1.0149182081222534\n",
      "[step: 12444] loss: 0.6443303823471069\n",
      "[step: 12445] loss: 0.7313536405563354\n",
      "[step: 12446] loss: 0.9675757884979248\n",
      "[step: 12447] loss: 0.7863646745681763\n",
      "[step: 12448] loss: 0.576484203338623\n",
      "[step: 12449] loss: 0.731969952583313\n",
      "[step: 12450] loss: 0.8385788202285767\n",
      "[step: 12451] loss: 0.6637541055679321\n",
      "[step: 12452] loss: 0.5784577131271362\n",
      "[step: 12453] loss: 0.706282913684845\n",
      "[step: 12454] loss: 0.7227307558059692\n",
      "[step: 12455] loss: 0.5954117774963379\n",
      "[step: 12456] loss: 0.5895305275917053\n",
      "[step: 12457] loss: 0.6758579015731812\n",
      "[step: 12458] loss: 0.6484050154685974\n",
      "[step: 12459] loss: 0.5629315376281738\n",
      "[step: 12460] loss: 0.5798506736755371\n",
      "[step: 12461] loss: 0.6386961936950684\n",
      "[step: 12462] loss: 0.6141129732131958\n",
      "[step: 12463] loss: 0.556279182434082\n",
      "[step: 12464] loss: 0.565097451210022\n",
      "[step: 12465] loss: 0.6019686460494995\n",
      "[step: 12466] loss: 0.5902904272079468\n",
      "[step: 12467] loss: 0.5537267923355103\n",
      "[step: 12468] loss: 0.5539447069168091\n",
      "[step: 12469] loss: 0.5771068334579468\n",
      "[step: 12470] loss: 0.5754225254058838\n",
      "[step: 12471] loss: 0.5506631135940552\n",
      "[step: 12472] loss: 0.54254549741745\n",
      "[step: 12473] loss: 0.5570189952850342\n",
      "[step: 12474] loss: 0.5651768445968628\n",
      "[step: 12475] loss: 0.5525949001312256\n",
      "[step: 12476] loss: 0.5380631685256958\n",
      "[step: 12477] loss: 0.5398006439208984\n",
      "[step: 12478] loss: 0.5502700805664062\n",
      "[step: 12479] loss: 0.551554262638092\n",
      "[step: 12480] loss: 0.5419268012046814\n",
      "[step: 12481] loss: 0.5338784456253052\n",
      "[step: 12482] loss: 0.5357210040092468\n",
      "[step: 12483] loss: 0.5418487787246704\n",
      "[step: 12484] loss: 0.5426254272460938\n",
      "[step: 12485] loss: 0.5369803309440613\n",
      "[step: 12486] loss: 0.531458854675293\n",
      "[step: 12487] loss: 0.5313993096351624\n",
      "[step: 12488] loss: 0.5347592234611511\n",
      "[step: 12489] loss: 0.5363413095474243\n",
      "[step: 12490] loss: 0.5339228510856628\n",
      "[step: 12491] loss: 0.5300514698028564\n",
      "[step: 12492] loss: 0.5282814502716064\n",
      "[step: 12493] loss: 0.5292967557907104\n",
      "[step: 12494] loss: 0.5308221578598022\n",
      "[step: 12495] loss: 0.5306984186172485\n",
      "[step: 12496] loss: 0.5287953615188599\n",
      "[step: 12497] loss: 0.5268248319625854\n",
      "[step: 12498] loss: 0.5261290669441223\n",
      "[step: 12499] loss: 0.5266234874725342\n",
      "[step: 12500] loss: 0.5270907878875732\n",
      "[step: 12501] loss: 0.5266482830047607\n",
      "[step: 12502] loss: 0.5254731178283691\n",
      "[step: 12503] loss: 0.5243445634841919\n",
      "[step: 12504] loss: 0.5239094495773315\n",
      "[step: 12505] loss: 0.5240538120269775\n",
      "[step: 12506] loss: 0.524217963218689\n",
      "[step: 12507] loss: 0.5239148139953613\n",
      "[step: 12508] loss: 0.5231531858444214\n",
      "[step: 12509] loss: 0.5222971439361572\n",
      "[step: 12510] loss: 0.5217419266700745\n",
      "[step: 12511] loss: 0.5215648412704468\n",
      "[step: 12512] loss: 0.5215681791305542\n",
      "[step: 12513] loss: 0.5214472413063049\n",
      "[step: 12514] loss: 0.521080493927002\n",
      "[step: 12515] loss: 0.5205357074737549\n",
      "[step: 12516] loss: 0.5200142860412598\n",
      "[step: 12517] loss: 0.5196545720100403\n",
      "[step: 12518] loss: 0.5194623470306396\n",
      "[step: 12519] loss: 0.5193334817886353\n",
      "[step: 12520] loss: 0.5191426277160645\n",
      "[step: 12521] loss: 0.5188466906547546\n",
      "[step: 12522] loss: 0.5184717774391174\n",
      "[step: 12523] loss: 0.5181043744087219\n",
      "[step: 12524] loss: 0.5178132057189941\n",
      "[step: 12525] loss: 0.517616331577301\n",
      "[step: 12526] loss: 0.5174844264984131\n",
      "[step: 12527] loss: 0.5173825025558472\n",
      "[step: 12528] loss: 0.5172907114028931\n",
      "[step: 12529] loss: 0.5172551870346069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12530] loss: 0.5173588395118713\n",
      "[step: 12531] loss: 0.5177491903305054\n",
      "[step: 12532] loss: 0.5186383724212646\n",
      "[step: 12533] loss: 0.5203742384910583\n",
      "[step: 12534] loss: 0.5235708355903625\n",
      "[step: 12535] loss: 0.5293506979942322\n",
      "[step: 12536] loss: 0.5398489236831665\n",
      "[step: 12537] loss: 0.5590225458145142\n",
      "[step: 12538] loss: 0.5944793224334717\n",
      "[step: 12539] loss: 0.6603172421455383\n",
      "[step: 12540] loss: 0.7831623554229736\n",
      "[step: 12541] loss: 1.0103271007537842\n",
      "[step: 12542] loss: 1.4207711219787598\n",
      "[step: 12543] loss: 2.1124963760375977\n",
      "[step: 12544] loss: 3.1408121585845947\n",
      "[step: 12545] loss: 4.198484420776367\n",
      "[step: 12546] loss: 4.562819957733154\n",
      "[step: 12547] loss: 3.1826791763305664\n",
      "[step: 12548] loss: 1.221252202987671\n",
      "[step: 12549] loss: 0.6525750160217285\n",
      "[step: 12550] loss: 1.7381436824798584\n",
      "[step: 12551] loss: 2.319398880004883\n",
      "[step: 12552] loss: 1.267909049987793\n",
      "[step: 12553] loss: 0.6532952189445496\n",
      "[step: 12554] loss: 1.360081672668457\n",
      "[step: 12555] loss: 1.4486808776855469\n",
      "[step: 12556] loss: 0.7693348526954651\n",
      "[step: 12557] loss: 0.8817418813705444\n",
      "[step: 12558] loss: 1.2450611591339111\n",
      "[step: 12559] loss: 0.8346197605133057\n",
      "[step: 12560] loss: 0.642569899559021\n",
      "[step: 12561] loss: 1.025686502456665\n",
      "[step: 12562] loss: 0.9131338000297546\n",
      "[step: 12563] loss: 0.6254912614822388\n",
      "[step: 12564] loss: 0.8374108076095581\n",
      "[step: 12565] loss: 0.8685024976730347\n",
      "[step: 12566] loss: 0.5982217788696289\n",
      "[step: 12567] loss: 0.6989161968231201\n",
      "[step: 12568] loss: 0.8214659094810486\n",
      "[step: 12569] loss: 0.6367896199226379\n",
      "[step: 12570] loss: 0.6216375827789307\n",
      "[step: 12571] loss: 0.7422621846199036\n",
      "[step: 12572] loss: 0.6403215527534485\n",
      "[step: 12573] loss: 0.5674178600311279\n",
      "[step: 12574] loss: 0.6649128794670105\n",
      "[step: 12575] loss: 0.6559221148490906\n",
      "[step: 12576] loss: 0.5688437223434448\n",
      "[step: 12577] loss: 0.5977092981338501\n",
      "[step: 12578] loss: 0.636313796043396\n",
      "[step: 12579] loss: 0.5808653235435486\n",
      "[step: 12580] loss: 0.551772952079773\n",
      "[step: 12581] loss: 0.596792459487915\n",
      "[step: 12582] loss: 0.6000716090202332\n",
      "[step: 12583] loss: 0.5535138845443726\n",
      "[step: 12584] loss: 0.555300235748291\n",
      "[step: 12585] loss: 0.5827537178993225\n",
      "[step: 12586] loss: 0.5673266649246216\n",
      "[step: 12587] loss: 0.5397396683692932\n",
      "[step: 12588] loss: 0.5497885942459106\n",
      "[step: 12589] loss: 0.5676683187484741\n",
      "[step: 12590] loss: 0.5548796653747559\n",
      "[step: 12591] loss: 0.5355052947998047\n",
      "[step: 12592] loss: 0.5409504771232605\n",
      "[step: 12593] loss: 0.5526745319366455\n",
      "[step: 12594] loss: 0.5455136299133301\n",
      "[step: 12595] loss: 0.5319689512252808\n",
      "[step: 12596] loss: 0.5336431264877319\n",
      "[step: 12597] loss: 0.542240560054779\n",
      "[step: 12598] loss: 0.5408299565315247\n",
      "[step: 12599] loss: 0.5313361883163452\n",
      "[step: 12600] loss: 0.5272355079650879\n",
      "[step: 12601] loss: 0.5319144129753113\n",
      "[step: 12602] loss: 0.5349006056785583\n",
      "[step: 12603] loss: 0.5305091142654419\n",
      "[step: 12604] loss: 0.5253698825836182\n",
      "[step: 12605] loss: 0.5254655480384827\n",
      "[step: 12606] loss: 0.5283887982368469\n",
      "[step: 12607] loss: 0.528567910194397\n",
      "[step: 12608] loss: 0.5249063968658447\n",
      "[step: 12609] loss: 0.5218203067779541\n",
      "[step: 12610] loss: 0.5220867991447449\n",
      "[step: 12611] loss: 0.5239150524139404\n",
      "[step: 12612] loss: 0.5239819884300232\n",
      "[step: 12613] loss: 0.5218862295150757\n",
      "[step: 12614] loss: 0.5197204947471619\n",
      "[step: 12615] loss: 0.5192765593528748\n",
      "[step: 12616] loss: 0.520117998123169\n",
      "[step: 12617] loss: 0.5204087495803833\n",
      "[step: 12618] loss: 0.5193886756896973\n",
      "[step: 12619] loss: 0.5178380012512207\n",
      "[step: 12620] loss: 0.5168643593788147\n",
      "[step: 12621] loss: 0.5168840885162354\n",
      "[step: 12622] loss: 0.5172398686408997\n",
      "[step: 12623] loss: 0.5171063542366028\n",
      "[step: 12624] loss: 0.5163449645042419\n",
      "[step: 12625] loss: 0.5153598189353943\n",
      "[step: 12626] loss: 0.5146960020065308\n",
      "[step: 12627] loss: 0.5144928693771362\n",
      "[step: 12628] loss: 0.5144919157028198\n",
      "[step: 12629] loss: 0.5143148899078369\n",
      "[step: 12630] loss: 0.5138423442840576\n",
      "[step: 12631] loss: 0.5131991505622864\n",
      "[step: 12632] loss: 0.5126155614852905\n",
      "[step: 12633] loss: 0.5122569799423218\n",
      "[step: 12634] loss: 0.5120776891708374\n",
      "[step: 12635] loss: 0.5119301676750183\n",
      "[step: 12636] loss: 0.5116930603981018\n",
      "[step: 12637] loss: 0.5113135576248169\n",
      "[step: 12638] loss: 0.5108451843261719\n",
      "[step: 12639] loss: 0.5103923678398132\n",
      "[step: 12640] loss: 0.510017991065979\n",
      "[step: 12641] loss: 0.5097368955612183\n",
      "[step: 12642] loss: 0.509511411190033\n",
      "[step: 12643] loss: 0.5092893838882446\n",
      "[step: 12644] loss: 0.5090263485908508\n",
      "[step: 12645] loss: 0.508713960647583\n",
      "[step: 12646] loss: 0.508366584777832\n",
      "[step: 12647] loss: 0.5080111622810364\n",
      "[step: 12648] loss: 0.5076781511306763\n",
      "[step: 12649] loss: 0.5073757171630859\n",
      "[step: 12650] loss: 0.5071024298667908\n",
      "[step: 12651] loss: 0.5068503618240356\n",
      "[step: 12652] loss: 0.50660240650177\n",
      "[step: 12653] loss: 0.5063477754592896\n",
      "[step: 12654] loss: 0.506083607673645\n",
      "[step: 12655] loss: 0.5058066844940186\n",
      "[step: 12656] loss: 0.505521297454834\n",
      "[step: 12657] loss: 0.5052366852760315\n",
      "[step: 12658] loss: 0.5049559473991394\n",
      "[step: 12659] loss: 0.5046858787536621\n",
      "[step: 12660] loss: 0.5044283270835876\n",
      "[step: 12661] loss: 0.5041899681091309\n",
      "[step: 12662] loss: 0.5039736032485962\n",
      "[step: 12663] loss: 0.503792941570282\n",
      "[step: 12664] loss: 0.5036633014678955\n",
      "[step: 12665] loss: 0.5036200284957886\n",
      "[step: 12666] loss: 0.5037238597869873\n",
      "[step: 12667] loss: 0.5040917992591858\n",
      "[step: 12668] loss: 0.5049270391464233\n",
      "[step: 12669] loss: 0.5066310167312622\n",
      "[step: 12670] loss: 0.509893536567688\n",
      "[step: 12671] loss: 0.5161774158477783\n",
      "[step: 12672] loss: 0.5279004573822021\n",
      "[step: 12673] loss: 0.5506449937820435\n",
      "[step: 12674] loss: 0.5928800106048584\n",
      "[step: 12675] loss: 0.6764926910400391\n",
      "[step: 12676] loss: 0.8281558752059937\n",
      "[step: 12677] loss: 1.1280996799468994\n",
      "[step: 12678] loss: 1.620434045791626\n",
      "[step: 12679] loss: 2.487078905105591\n",
      "[step: 12680] loss: 3.4854469299316406\n",
      "[step: 12681] loss: 4.341738700866699\n",
      "[step: 12682] loss: 4.1786370277404785\n",
      "[step: 12683] loss: 2.6198835372924805\n",
      "[step: 12684] loss: 1.1890027523040771\n",
      "[step: 12685] loss: 1.2428131103515625\n",
      "[step: 12686] loss: 2.3983500003814697\n",
      "[step: 12687] loss: 2.1020667552948\n",
      "[step: 12688] loss: 0.8847017288208008\n",
      "[step: 12689] loss: 0.9992464184761047\n",
      "[step: 12690] loss: 1.90493643283844\n",
      "[step: 12691] loss: 1.8438658714294434\n",
      "[step: 12692] loss: 0.8183066844940186\n",
      "[step: 12693] loss: 1.0348820686340332\n",
      "[step: 12694] loss: 1.6451233625411987\n",
      "[step: 12695] loss: 1.1661174297332764\n",
      "[step: 12696] loss: 0.8874205350875854\n",
      "[step: 12697] loss: 1.0928823947906494\n",
      "[step: 12698] loss: 0.9056510925292969\n",
      "[step: 12699] loss: 0.8241169452667236\n",
      "[step: 12700] loss: 1.0507633686065674\n",
      "[step: 12701] loss: 0.9380433559417725\n",
      "[step: 12702] loss: 0.6076295375823975\n",
      "[step: 12703] loss: 0.7428439855575562\n",
      "[step: 12704] loss: 0.9447883367538452\n",
      "[step: 12705] loss: 0.721158504486084\n",
      "[step: 12706] loss: 0.6040781140327454\n",
      "[step: 12707] loss: 0.7499041557312012\n",
      "[step: 12708] loss: 0.7126815319061279\n",
      "[step: 12709] loss: 0.6269669532775879\n",
      "[step: 12710] loss: 0.6675023436546326\n",
      "[step: 12711] loss: 0.6876564621925354\n",
      "[step: 12712] loss: 0.5926694869995117\n",
      "[step: 12713] loss: 0.5735500454902649\n",
      "[step: 12714] loss: 0.6727392673492432\n",
      "[step: 12715] loss: 0.6426873207092285\n",
      "[step: 12716] loss: 0.5473514199256897\n",
      "[step: 12717] loss: 0.5631392002105713\n",
      "[step: 12718] loss: 0.612682580947876\n",
      "[step: 12719] loss: 0.5962030291557312\n",
      "[step: 12720] loss: 0.5467731952667236\n",
      "[step: 12721] loss: 0.5526807904243469\n",
      "[step: 12722] loss: 0.5762342214584351\n",
      "[step: 12723] loss: 0.5558514595031738\n",
      "[step: 12724] loss: 0.5393253564834595\n",
      "[step: 12725] loss: 0.5488544702529907\n",
      "[step: 12726] loss: 0.554945707321167\n",
      "[step: 12727] loss: 0.5401444435119629\n",
      "[step: 12728] loss: 0.5254846811294556\n",
      "[step: 12729] loss: 0.5354806780815125\n",
      "[step: 12730] loss: 0.5450472831726074\n",
      "[step: 12731] loss: 0.5336393117904663\n",
      "[step: 12732] loss: 0.5202831625938416\n",
      "[step: 12733] loss: 0.5206613540649414\n",
      "[step: 12734] loss: 0.531642735004425\n",
      "[step: 12735] loss: 0.5314795970916748\n",
      "[step: 12736] loss: 0.5200783014297485\n",
      "[step: 12737] loss: 0.5139721632003784\n",
      "[step: 12738] loss: 0.5177203416824341\n",
      "[step: 12739] loss: 0.5236226916313171\n",
      "[step: 12740] loss: 0.521381139755249\n",
      "[step: 12741] loss: 0.5137945413589478\n",
      "[step: 12742] loss: 0.5108773708343506\n",
      "[step: 12743] loss: 0.5131025910377502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 12744] loss: 0.5165367722511292\n",
      "[step: 12745] loss: 0.5151134729385376\n",
      "[step: 12746] loss: 0.5104891657829285\n",
      "[step: 12747] loss: 0.5081457495689392\n",
      "[step: 12748] loss: 0.5088577270507812\n",
      "[step: 12749] loss: 0.5109390020370483\n",
      "[step: 12750] loss: 0.5108341574668884\n",
      "[step: 12751] loss: 0.5082623958587646\n",
      "[step: 12752] loss: 0.5060651898384094\n",
      "[step: 12753] loss: 0.5054833889007568\n",
      "[step: 12754] loss: 0.5064694285392761\n",
      "[step: 12755] loss: 0.507098376750946\n",
      "[step: 12756] loss: 0.5062519907951355\n",
      "[step: 12757] loss: 0.5046994686126709\n",
      "[step: 12758] loss: 0.5033519268035889\n",
      "[step: 12759] loss: 0.503158688545227\n",
      "[step: 12760] loss: 0.5034955143928528\n",
      "[step: 12761] loss: 0.5036174058914185\n",
      "[step: 12762] loss: 0.5031512379646301\n",
      "[step: 12763] loss: 0.502170205116272\n",
      "[step: 12764] loss: 0.5013638734817505\n",
      "[step: 12765] loss: 0.5009356737136841\n",
      "[step: 12766] loss: 0.5007976293563843\n",
      "[step: 12767] loss: 0.5007447600364685\n",
      "[step: 12768] loss: 0.5004358291625977\n",
      "[step: 12769] loss: 0.5000083446502686\n",
      "[step: 12770] loss: 0.4995265603065491\n"
     ]
    }
   ],
   "source": [
    "a = LSTM(txsForRealForecastLstm, forecastDay,feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18021.525390625,\n",
       " 17133.32421875,\n",
       " 17810.654296875,\n",
       " 23324.0859375,\n",
       " 24855.05859375,\n",
       " 26545.142578125,\n",
       " 28114.302734375]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18021.525390625,\n",
       " 17133.32421875,\n",
       " 17810.654296875,\n",
       " 23324.0859375,\n",
       " 24855.05859375,\n",
       " 26545.142578125,\n",
       " 28114.302734375]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-26e1a31ae645>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-26e1a31ae645>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "ab=[item for sublist in [1,2] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float32' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-4095a5ba5537>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-4095a5ba5537>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float32' object is not iterable"
     ]
    }
   ],
   "source": [
    "ab=[item for sublist in a for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " data = rawArrayDatas[1][:-forecastDay] + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
