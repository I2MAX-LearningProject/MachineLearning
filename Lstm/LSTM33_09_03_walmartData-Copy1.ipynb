{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walmart 주단위 데이터 139주로 향후 4주 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime  \n",
    "# tf.set_random_seed(77)\n",
    "columns=['date','sales']\n",
    "txs=pd.read_table('./lstmData/lstmPrac12.csv', sep=',',header=None,names=columns )\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-03-12</td>\n",
       "      <td>21043.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-03-19</td>\n",
       "      <td>22136.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-03-26</td>\n",
       "      <td>26229.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-04-02</td>\n",
       "      <td>57258.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-04-09</td>\n",
       "      <td>42960.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>17596.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2010-04-23</td>\n",
       "      <td>16145.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>16555.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010-05-07</td>\n",
       "      <td>17413.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2010-05-14</td>\n",
       "      <td>18926.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010-05-21</td>\n",
       "      <td>14773.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2010-05-28</td>\n",
       "      <td>15580.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>17558.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2010-06-11</td>\n",
       "      <td>16637.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2010-06-18</td>\n",
       "      <td>16216.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2010-06-25</td>\n",
       "      <td>16328.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2010-07-02</td>\n",
       "      <td>16333.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2010-07-09</td>\n",
       "      <td>17688.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2010-07-16</td>\n",
       "      <td>17150.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010-07-23</td>\n",
       "      <td>15360.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2010-07-30</td>\n",
       "      <td>15381.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2010-08-06</td>\n",
       "      <td>17508.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2010-08-13</td>\n",
       "      <td>15536.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-08-20</td>\n",
       "      <td>15740.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2010-08-27</td>\n",
       "      <td>15793.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2012-04-13</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2012-04-20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2012-04-27</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2012-05-04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2012-05-11</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2012-05-18</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2012-05-25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2012-06-08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2012-06-15</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2012-06-29</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2012-07-06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2012-07-13</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2012-07-27</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2012-08-03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2012-08-17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2012-08-31</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2012-09-07</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2012-09-21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2012-10-12</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     sales\n",
       "0    2010-02-05  24924.50\n",
       "1    2010-02-12  46039.49\n",
       "2    2010-02-19  41595.55\n",
       "3    2010-02-26  19403.54\n",
       "4    2010-03-05  21827.90\n",
       "5    2010-03-12  21043.39\n",
       "6    2010-03-19  22136.64\n",
       "7    2010-03-26  26229.21\n",
       "8    2010-04-02  57258.43\n",
       "9    2010-04-09  42960.91\n",
       "10   2010-04-16  17596.96\n",
       "11   2010-04-23  16145.35\n",
       "12   2010-04-30  16555.11\n",
       "13   2010-05-07  17413.94\n",
       "14   2010-05-14  18926.74\n",
       "15   2010-05-21  14773.04\n",
       "16   2010-05-28  15580.43\n",
       "17   2010-06-04  17558.09\n",
       "18   2010-06-11  16637.62\n",
       "19   2010-06-18  16216.27\n",
       "20   2010-06-25  16328.72\n",
       "21   2010-07-02  16333.14\n",
       "22   2010-07-09  17688.76\n",
       "23   2010-07-16  17150.84\n",
       "24   2010-07-23  15360.45\n",
       "25   2010-07-30  15381.82\n",
       "26   2010-08-06  17508.41\n",
       "27   2010-08-13  15536.40\n",
       "28   2010-08-20  15740.13\n",
       "29   2010-08-27  15793.87\n",
       "..          ...       ...\n",
       "113  2012-04-06      0.00\n",
       "114  2012-04-13      0.00\n",
       "115  2012-04-20      0.00\n",
       "116  2012-04-27      0.00\n",
       "117  2012-05-04      0.00\n",
       "118  2012-05-11      0.00\n",
       "119  2012-05-18      0.00\n",
       "120  2012-05-25      0.00\n",
       "121  2012-06-01      0.00\n",
       "122  2012-06-08      0.00\n",
       "123  2012-06-15      0.00\n",
       "124  2012-06-22      0.00\n",
       "125  2012-06-29      0.00\n",
       "126  2012-07-06      0.00\n",
       "127  2012-07-13      0.00\n",
       "128  2012-07-20      0.00\n",
       "129  2012-07-27      0.00\n",
       "130  2012-08-03      0.00\n",
       "131  2012-08-10      0.00\n",
       "132  2012-08-17      0.00\n",
       "133  2012-08-24      0.00\n",
       "134  2012-08-31      0.00\n",
       "135  2012-09-07      0.00\n",
       "136  2012-09-14      0.00\n",
       "137  2012-09-21      0.00\n",
       "138  2012-09-28      0.00\n",
       "139  2012-10-05      0.00\n",
       "140  2012-10-12      0.00\n",
       "141  2012-10-19      0.00\n",
       "142  2012-10-26      0.00\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noOutlierSales(sales):\n",
    "    mean=np.mean(sales)\n",
    "    std=np.std(sales)\n",
    "    for i in range(len(sales)):\n",
    "        if (sales[i]<mean-2*std or sales[i]>mean+2*std):\n",
    "             sales[i]=int(mean)\n",
    "    return sales\n",
    "def logSales(sales):\n",
    "    for i in range(len(sales)):\n",
    "        if sales[i] is 0:\n",
    "            sales[i]=1\n",
    "    return np.log(sales)\n",
    "def sqrtSales(sales):\n",
    "    return np.sqrt(sales)\n",
    "\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LSTM(txs, forecastDay, features):\n",
    "\n",
    "    #Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['date'].map(year)\n",
    "    txs['month'] = txs['date'].map(month)\n",
    "    txs['weekNumber'] = txs['date'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['date'].map(dayOfWeek)\n",
    "\n",
    "    #Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['date'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['date'].map(day_of_week01)\n",
    "\n",
    "    #Backup originalSales\n",
    "    originalSales = list(txs['sales'])\n",
    "    sales = list(txs['sales'])\n",
    "\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season' :\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']),list(txs['month']),list(txs['season']) , sales]\n",
    "    elif features is'DayOfWeek01_WeekNumber_Month_Season' :\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']),list(txs['month']),list(txs['season']) , sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year' :\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']),list(txs['year']), sales]\n",
    "\n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "\n",
    "    #Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    #TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 5\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 50\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    # iterations는 반복 횟수\n",
    "    iterations = 5000\n",
    "\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay+1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        print(_x,\"->\",_y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    print('data set length:',len(y) - seq_length - forecastDay+1)\n",
    "    \n",
    "    train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "    print('train size:' , train_size)\n",
    "    print('test size:' , test_size)\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "    print('trainX:', trainX)\n",
    "    print('testX:', testX)\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    denormalizedTestY=originalSales[train_size+seq_length:]\n",
    "#     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "    \n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "       \n",
    "\n",
    "        # Test step\n",
    "        # test_predict= sess.run(Y_pred, feed_dict={X: testX}\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalXY)\n",
    "        realSale= minMaxDeNormalizer(testY,originalXY)\n",
    "        # Plot predictions\n",
    "#         plt.plot(denormalizedTestY_feed) #실제 sales 파란색\n",
    "#         plt.plot(realSale)      #실제 sales 파란색\n",
    "        plt.plot(test_predict) #예측 sales 주황색\n",
    "               \n",
    "        plt.xlabel(\"Time Period\")\n",
    "        plt.ylabel(\"Stock Price\")\n",
    "        plt.show()\n",
    "        \n",
    "    return (test_predict), realSale\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    4.35298348e-01]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    8.04064834e-01]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    7.26452856e-01]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    3.38876564e-01]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.81217229e-01]] -> [ 0.36751601]\n",
      "[[  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    8.04064834e-01]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    7.26452856e-01]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    3.38876564e-01]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.81217229e-01]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.67516015e-01]] -> [ 0.38660927]\n",
      "[[  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    7.26452856e-01]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    3.38876564e-01]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.81217229e-01]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.67516015e-01]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.86609273e-01]] -> [ 0.45808469]\n",
      "[[  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "    3.38876564e-01]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.81217229e-01]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.67516015e-01]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.86609273e-01]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    4.58084687e-01]] -> [ 1.]\n",
      "[[  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.81217229e-01]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.67516015e-01]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.86609273e-01]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    4.58084687e-01]\n",
      " [  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    1.00000000e+00]] -> [ 0.75029843]\n",
      "[[  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.67516015e-01]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.86609273e-01]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    4.58084687e-01]\n",
      " [  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    1.00000000e+00]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    7.50298428e-01]] -> [ 0.30732523]\n",
      "[[  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.86609273e-01]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    4.58084687e-01]\n",
      " [  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    1.00000000e+00]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    7.50298428e-01]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.07325227e-01]] -> [ 0.28197333]\n",
      "[[  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "    4.58084687e-01]\n",
      " [  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    1.00000000e+00]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    7.50298428e-01]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.07325227e-01]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.81973327e-01]] -> [ 0.28912965]\n",
      "[[  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    1.00000000e+00]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    7.50298428e-01]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.07325227e-01]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.81973327e-01]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.89129653e-01]] -> [ 0.30412884]\n",
      "[[  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    7.50298428e-01]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.07325227e-01]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.81973327e-01]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.89129653e-01]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.04128842e-01]] -> [ 0.33054941]\n",
      "[[  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.07325227e-01]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.81973327e-01]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.89129653e-01]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.04128842e-01]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.30549406e-01]] -> [ 0.25800638]\n",
      "[[  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.81973327e-01]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.89129653e-01]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.04128842e-01]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.30549406e-01]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.58006376e-01]] -> [ 0.27210718]\n",
      "[[  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.89129653e-01]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.04128842e-01]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.30549406e-01]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.58006376e-01]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.72107181e-01]] -> [ 0.30664638]\n",
      "[[  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.04128842e-01]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.30549406e-01]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.58006376e-01]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.72107181e-01]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.06646375e-01]] -> [ 0.29057066]\n",
      "[[  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    3.30549406e-01]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.58006376e-01]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.72107181e-01]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.06646375e-01]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.90570664e-01]] -> [ 0.28321192]\n",
      "[[  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.58006376e-01]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.72107181e-01]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.06646375e-01]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.90570664e-01]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.83211922e-01]] -> [ 0.28517582]\n",
      "[[  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51040013e-02\n",
      "    2.72107181e-01]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.06646375e-01]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.90570664e-01]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.83211922e-01]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85175825e-01]] -> [ 0.28525302]\n",
      "[[  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.06646375e-01]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.90570664e-01]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.83211922e-01]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85175825e-01]\n",
      " [  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85253019e-01]] -> [ 0.30892848]\n",
      "[[  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.90570664e-01]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.83211922e-01]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85175825e-01]\n",
      " [  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85253019e-01]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.08928484e-01]] -> [ 0.29953389]\n",
      "[[  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.83211922e-01]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85175825e-01]\n",
      " [  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85253019e-01]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.08928484e-01]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.99533885e-01]] -> [ 0.2682653]\n",
      "[[  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85175825e-01]\n",
      " [  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85253019e-01]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.08928484e-01]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.99533885e-01]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68265302e-01]] -> [ 0.26863852]\n",
      "[[  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.85253019e-01]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.08928484e-01]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.99533885e-01]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68265302e-01]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68638522e-01]] -> [ 0.30577873]\n",
      "[[  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.08928484e-01]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.99533885e-01]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68265302e-01]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68638522e-01]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.05778730e-01]] -> [ 0.27133821]\n",
      "[[  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.99533885e-01]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68265302e-01]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68638522e-01]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.05778730e-01]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.71338212e-01]] -> [ 0.27489629]\n",
      "[[  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68265302e-01]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68638522e-01]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.05778730e-01]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.71338212e-01]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.74896290e-01]] -> [ 0.27583484]\n",
      "[[  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.68638522e-01]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.05778730e-01]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.71338212e-01]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.74896290e-01]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.75834842e-01]] -> [ 0.28365745]\n",
      "[[  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    3.05778730e-01]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.71338212e-01]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.74896290e-01]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.75834842e-01]\n",
      " [  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    2.83657446e-01]] -> [ 0.31776526]\n",
      "[[  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.71338212e-01]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.74896290e-01]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.75834842e-01]\n",
      " [  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    2.83657446e-01]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.17765262e-01]] -> [ 0.33801538]\n",
      "[[  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.74896290e-01]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.75834842e-01]\n",
      " [  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    2.83657446e-01]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.17765262e-01]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.38015380e-01]] -> [ 0.31650396]\n",
      "[[  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51040013e-02\n",
      "    2.75834842e-01]\n",
      " [  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    2.83657446e-01]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.17765262e-01]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.38015380e-01]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.16503963e-01]] -> [ 0.35093854]\n",
      "[[  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    2.83657446e-01]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.17765262e-01]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.38015380e-01]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.16503963e-01]\n",
      " [  6.81122413e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.50938543e-01]] -> [ 0.4084644]\n",
      "[[  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.17765262e-01]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.38015380e-01]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.16503963e-01]\n",
      " [  6.81122413e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.50938543e-01]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.08464396e-01]] -> [ 0.471168]\n",
      "[[  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.38015380e-01]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.16503963e-01]\n",
      " [  6.81122413e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.50938543e-01]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.08464396e-01]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.71168001e-01]] -> [ 0.44610095]\n",
      "[[  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.16503963e-01]\n",
      " [  6.81122413e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.50938543e-01]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.08464396e-01]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.71168001e-01]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.46100950e-01]] -> [ 0.67485137]\n",
      "[[  6.81122413e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.50938543e-01]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.08464396e-01]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.71168001e-01]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.46100950e-01]\n",
      " [  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    6.74851371e-01]] -> [ 0.59797099]\n",
      "[[  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.08464396e-01]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.71168001e-01]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.46100950e-01]\n",
      " [  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    6.74851371e-01]\n",
      " [  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    5.97970989e-01]] -> [ 0.34142379]\n",
      "[[  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.71168001e-01]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.46100950e-01]\n",
      " [  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    6.74851371e-01]\n",
      " [  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    5.97970989e-01]\n",
      " [  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41423787e-01]] -> [ 0.34148404]\n",
      "[[  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    4.46100950e-01]\n",
      " [  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    6.74851371e-01]\n",
      " [  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    5.97970989e-01]\n",
      " [  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41423787e-01]\n",
      " [  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41484040e-01]] -> [ 0.32869029]\n",
      "[[  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51040013e-02\n",
      "    6.74851371e-01]\n",
      " [  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    5.97970989e-01]\n",
      " [  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41423787e-01]\n",
      " [  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41484040e-01]\n",
      " [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.28690291e-01]] -> [ 0.39326192]\n",
      "[[  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    5.97970989e-01]\n",
      " [  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41423787e-01]\n",
      " [  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41484040e-01]\n",
      " [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.28690291e-01]\n",
      " [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.93261918e-01]] -> [ 0.55009629]\n",
      "[[  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41423787e-01]\n",
      " [  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41484040e-01]\n",
      " [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.28690291e-01]\n",
      " [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.93261918e-01]\n",
      " [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5.50096291e-01]] -> [ 0.7843886]\n",
      "[[  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.41484040e-01]\n",
      " [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.28690291e-01]\n",
      " [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.93261918e-01]\n",
      " [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    5.50096291e-01]\n",
      " [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    7.84388604e-01]] -> [ 0.97682088]\n",
      "[[  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51040013e-02\n",
      "    3.28690291e-01]\n",
      " [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.93261918e-01]\n",
      " [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    5.50096291e-01]\n",
      " [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    7.84388604e-01]\n",
      " [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    9.76820880e-01]] -> [ 0.33400462]\n",
      "[[  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.93261918e-01]\n",
      " [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    5.50096291e-01]\n",
      " [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    7.84388604e-01]\n",
      " [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    9.76820880e-01]\n",
      " [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.34004617e-01]] -> [ 0.27915959]\n",
      "[[  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    5.50096291e-01]\n",
      " [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    7.84388604e-01]\n",
      " [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    9.76820880e-01]\n",
      " [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.34004617e-01]\n",
      " [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    2.79159593e-01]] -> [ 0.30318156]\n",
      "[[  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    7.84388604e-01]\n",
      " [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    9.76820880e-01]\n",
      " [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.34004617e-01]\n",
      " [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    2.79159593e-01]\n",
      " [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.03181558e-01]] -> [ 0.30286318]\n",
      "[[  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    9.76820880e-01]\n",
      " [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.34004617e-01]\n",
      " [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    2.79159593e-01]\n",
      " [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.03181558e-01]\n",
      " [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.02863177e-01]] -> [ 0.32241855]\n",
      "[[  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51040013e-02\n",
      "    3.34004617e-01]\n",
      " [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    2.79159593e-01]\n",
      " [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.03181558e-01]\n",
      " [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.02863177e-01]\n",
      " [  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.22418550e-01]] -> [ 0.37838551]\n",
      "[[  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    2.79159593e-01]\n",
      " [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.03181558e-01]\n",
      " [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.02863177e-01]\n",
      " [  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.22418550e-01]\n",
      " [  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.78385506e-01]] -> [ 0.6616872]\n",
      "[[  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.03181558e-01]\n",
      " [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.02863177e-01]\n",
      " [  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.22418550e-01]\n",
      " [  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.78385506e-01]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    6.61687196e-01]] -> [ 0.818148]\n",
      "[[  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.02863177e-01]\n",
      " [  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.22418550e-01]\n",
      " [  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.78385506e-01]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    6.61687196e-01]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    8.18148000e-01]] -> [ 0.33818304]\n",
      "[[  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.22418550e-01]\n",
      " [  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.78385506e-01]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    6.61687196e-01]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    8.18148000e-01]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.38183041e-01]] -> [ 0.35501515]\n",
      "[[  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.78385506e-01]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    6.61687196e-01]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    8.18148000e-01]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.38183041e-01]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55015148e-01]] -> [ 0.37165532]\n",
      "[[  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    6.61687196e-01]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    8.18148000e-01]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.38183041e-01]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55015148e-01]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.71655318e-01]] -> [ 0.35513076]\n",
      "[[  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    8.18148000e-01]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.38183041e-01]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55015148e-01]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.71655318e-01]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55130764e-01]] -> [ 0.36468167]\n",
      "[[  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51214660e-02\n",
      "    3.38183041e-01]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55015148e-01]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.71655318e-01]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55130764e-01]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.64681672e-01]] -> [ 0.35624606]\n",
      "[[  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55015148e-01]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.71655318e-01]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55130764e-01]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.64681672e-01]\n",
      " [  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.56246058e-01]] -> [ 0.41694804]\n",
      "[[  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.71655318e-01]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55130764e-01]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.64681672e-01]\n",
      " [  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.56246058e-01]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    4.16948037e-01]] -> [ 0.50232551]\n",
      "[[  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.55130764e-01]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.64681672e-01]\n",
      " [  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.56246058e-01]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    4.16948037e-01]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    5.02325509e-01]] -> [ 0.88214626]\n",
      "[[  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.64681672e-01]\n",
      " [  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.56246058e-01]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    4.16948037e-01]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    5.02325509e-01]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    8.82146262e-01]] -> [ 0.72500049]\n",
      "[[  2.27040804e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.56246058e-01]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    4.16948037e-01]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    5.02325509e-01]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    8.82146262e-01]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    7.25000493e-01]] -> [ 0.35170699]\n",
      "[[  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    4.16948037e-01]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    5.02325509e-01]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    8.82146262e-01]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    7.25000493e-01]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.51706989e-01]] -> [ 0.30100633]\n",
      "[[  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    5.02325509e-01]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    8.82146262e-01]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    7.25000493e-01]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.51706989e-01]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.01006332e-01]] -> [ 0.26435898]\n",
      "[[  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    8.82146262e-01]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    7.25000493e-01]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.51706989e-01]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.01006332e-01]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.64358977e-01]] -> [ 0.27492196]\n",
      "[[  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51214660e-02\n",
      "    7.25000493e-01]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.51706989e-01]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.01006332e-01]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.64358977e-01]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.74921963e-01]] -> [ 0.28701713]\n",
      "[[  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.51706989e-01]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.01006332e-01]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.64358977e-01]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.74921963e-01]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.87017126e-01]] -> [ 0.27740055]\n",
      "[[  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    3.01006332e-01]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.64358977e-01]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.74921963e-01]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.87017126e-01]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.77400550e-01]] -> [ 0.26158751]\n",
      "[[  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.64358977e-01]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.74921963e-01]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.87017126e-01]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.77400550e-01]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.61587508e-01]] -> [ 0.27389522]\n",
      "[[  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51214660e-02\n",
      "    2.74921963e-01]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.87017126e-01]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.77400550e-01]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.61587508e-01]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73895215e-01]] -> [ 0.26831857]\n",
      "[[  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.87017126e-01]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.77400550e-01]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.61587508e-01]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73895215e-01]\n",
      " [  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.68318569e-01]] -> [ 0.2820348]\n",
      "[[  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.77400550e-01]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.61587508e-01]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73895215e-01]\n",
      " [  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.68318569e-01]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.82034803e-01]] -> [ 0.2734069]\n",
      "[[  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.61587508e-01]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73895215e-01]\n",
      " [  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.68318569e-01]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.82034803e-01]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73406903e-01]] -> [ 0.27535858]\n",
      "[[  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73895215e-01]\n",
      " [  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.68318569e-01]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.82034803e-01]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73406903e-01]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.75358580e-01]] -> [ 0.27807975]\n",
      "[[  4.54081609e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.68318569e-01]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.82034803e-01]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73406903e-01]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.75358580e-01]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.78079752e-01]] -> [ 0.26713184]\n",
      "[[  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.82034803e-01]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73406903e-01]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.75358580e-01]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.78079752e-01]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.67131844e-01]] -> [ 0.25393274]\n",
      "[[  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.73406903e-01]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.75358580e-01]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.78079752e-01]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.67131844e-01]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53932740e-01]] -> [ 0.25654284]\n",
      "[[  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.75358580e-01]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.78079752e-01]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.67131844e-01]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53932740e-01]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.56542836e-01]] -> [ 0.25389048]\n",
      "[[  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.78079752e-01]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.67131844e-01]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53932740e-01]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.56542836e-01]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53890475e-01]] -> [ 0.26681259]\n",
      "[[  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.67131844e-01]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53932740e-01]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.56542836e-01]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53890475e-01]\n",
      " [  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    2.66812590e-01]] -> [ 0.30994004]\n",
      "[[  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53932740e-01]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.56542836e-01]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53890475e-01]\n",
      " [  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    2.66812590e-01]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.09940039e-01]] -> [ 0.32371618]\n",
      "[[  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.56542836e-01]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53890475e-01]\n",
      " [  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    2.66812590e-01]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.09940039e-01]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.23716176e-01]] -> [ 0.31190691]\n",
      "[[  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51214660e-02\n",
      "    2.53890475e-01]\n",
      " [  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    2.66812590e-01]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.09940039e-01]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.23716176e-01]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.11906910e-01]] -> [ 0.32026166]\n",
      "[[  6.11263704e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    2.66812590e-01]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.09940039e-01]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.23716176e-01]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.11906910e-01]\n",
      " [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.20261663e-01]] -> [ 0.36322302]\n",
      "[[  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.09940039e-01]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.23716176e-01]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.11906910e-01]\n",
      " [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.20261663e-01]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.63223022e-01]] -> [ 0.40304196]\n",
      "[[  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.23716176e-01]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.11906910e-01]\n",
      " [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.20261663e-01]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.63223022e-01]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.03041963e-01]] -> [ 0.40783165]\n",
      "[[  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.11906910e-01]\n",
      " [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.20261663e-01]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.63223022e-01]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.03041963e-01]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.07831650e-01]] -> [ 0.55153276]\n",
      "[[  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.20261663e-01]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.63223022e-01]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.03041963e-01]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.07831650e-01]\n",
      " [  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    5.51532761e-01]] -> [ 0.69659716]\n",
      "[[  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.63223022e-01]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.03041963e-01]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.07831650e-01]\n",
      " [  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    5.51532761e-01]\n",
      " [  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    6.96597165e-01]] -> [ 0.32640678]\n",
      "[[  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.03041963e-01]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.07831650e-01]\n",
      " [  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    5.51532761e-01]\n",
      " [  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    6.96597165e-01]\n",
      " [  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.26406784e-01]] -> [ 0.33271363]\n",
      "[[  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    4.07831650e-01]\n",
      " [  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    5.51532761e-01]\n",
      " [  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    6.96597165e-01]\n",
      " [  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.26406784e-01]\n",
      " [  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.32713628e-01]] -> [ 0.36520823]\n",
      "[[  7.50981122e-04   1.74646773e-04   5.23940318e-05   3.51214660e-02\n",
      "    5.51532761e-01]\n",
      " [  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    6.96597165e-01]\n",
      " [  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.26406784e-01]\n",
      " [  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.32713628e-01]\n",
      " [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.65208232e-01]] -> [ 0.44174264]\n",
      "[[  7.68445799e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    6.96597165e-01]\n",
      " [  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.26406784e-01]\n",
      " [  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.32713628e-01]\n",
      " [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.65208232e-01]\n",
      " [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.41742639e-01]] -> [ 0.58167714]\n",
      "[[  7.85910476e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.26406784e-01]\n",
      " [  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.32713628e-01]\n",
      " [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.65208232e-01]\n",
      " [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.41742639e-01]\n",
      " [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    5.81677143e-01]] -> [ 0.7994112]\n",
      "[[  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.32713628e-01]\n",
      " [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.65208232e-01]\n",
      " [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.41742639e-01]\n",
      " [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    5.81677143e-01]\n",
      " [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    7.99411196e-01]] -> [ 0.81715042]\n",
      "[[  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "    3.65208232e-01]\n",
      " [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.41742639e-01]\n",
      " [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    5.81677143e-01]\n",
      " [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    7.99411196e-01]\n",
      " [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    8.17150418e-01]] -> [ 0.40781558]\n",
      "[[  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.41742639e-01]\n",
      " [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    5.81677143e-01]\n",
      " [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    7.99411196e-01]\n",
      " [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    8.17150418e-01]\n",
      " [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.07815583e-01]] -> [ 0.28934936]\n",
      "[[  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    5.81677143e-01]\n",
      " [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    7.99411196e-01]\n",
      " [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    8.17150418e-01]\n",
      " [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.07815583e-01]\n",
      " [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.89349359e-01]] -> [ 0.29505524]\n",
      "[[  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    7.99411196e-01]\n",
      " [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    8.17150418e-01]\n",
      " [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.07815583e-01]\n",
      " [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.89349359e-01]\n",
      " [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.95055243e-01]] -> [ 0.32074054]\n",
      "[[  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    8.17150418e-01]\n",
      " [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.07815583e-01]\n",
      " [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.89349359e-01]\n",
      " [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.95055243e-01]\n",
      " [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    3.20740544e-01]] -> [ 0.]\n",
      "[[  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "    4.07815583e-01]\n",
      " [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.89349359e-01]\n",
      " [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.95055243e-01]\n",
      " [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    3.20740544e-01]\n",
      " [  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.89349359e-01]\n",
      " [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.95055243e-01]\n",
      " [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    3.20740544e-01]\n",
      " [  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    2.95055243e-01]\n",
      " [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    3.20740544e-01]\n",
      " [  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    3.20740544e-01]\n",
      " [  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  6.98587090e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.27040804e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.27040804e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.27040804e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  2.09576127e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.27040804e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  2.27040804e-04   5.23940318e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  2.44505482e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  2.61970159e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  2.79434836e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  2.96899513e-04   6.98587090e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  3.14364191e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  3.31828868e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  3.49293545e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  3.66758222e-04   8.73233863e-05   1.74646773e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  3.84222900e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.54081609e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  4.01687577e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.54081609e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  4.19152254e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.54081609e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  4.36616931e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.54081609e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  4.54081609e-04   1.04788064e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  4.71546286e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  4.89010963e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  5.06475640e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  5.23940318e-04   1.22252741e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  5.41404995e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.11263704e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  5.58869672e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.11263704e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  5.76334349e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.11263704e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  5.93799027e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.11263704e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  6.11263704e-04   1.39717418e-04   3.49293545e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "[[  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]\n",
      " [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "    0.00000000e+00]] -> [ 0.]\n",
      "data set length: 138\n",
      "train size: 96\n",
      "test size: 42\n",
      "trainX: [[[  8.73233863e-05   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     4.35298348e-01]\n",
      "  [  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     8.04064834e-01]\n",
      "  [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     7.26452856e-01]\n",
      "  [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     3.38876564e-01]\n",
      "  [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "     3.81217229e-01]]\n",
      "\n",
      " [[  1.04788064e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     8.04064834e-01]\n",
      "  [  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     7.26452856e-01]\n",
      "  [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     3.38876564e-01]\n",
      "  [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "     3.81217229e-01]\n",
      "  [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "     3.67516015e-01]]\n",
      "\n",
      " [[  1.22252741e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     7.26452856e-01]\n",
      "  [  1.39717418e-04   3.49293545e-05   0.00000000e+00   3.51040013e-02\n",
      "     3.38876564e-01]\n",
      "  [  1.57182095e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "     3.81217229e-01]\n",
      "  [  1.74646773e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "     3.67516015e-01]\n",
      "  [  1.92111450e-04   5.23940318e-05   1.74646773e-05   3.51040013e-02\n",
      "     3.86609273e-01]]\n",
      "\n",
      " ..., \n",
      " [[  8.03375154e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "     3.32713628e-01]\n",
      "  [  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "     3.65208232e-01]\n",
      "  [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     4.41742639e-01]\n",
      "  [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     5.81677143e-01]\n",
      "  [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     7.99411196e-01]]\n",
      "\n",
      " [[  8.20839831e-04   1.92111450e-04   5.23940318e-05   3.51214660e-02\n",
      "     3.65208232e-01]\n",
      "  [  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     4.41742639e-01]\n",
      "  [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     5.81677143e-01]\n",
      "  [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     7.99411196e-01]\n",
      "  [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     8.17150418e-01]]\n",
      "\n",
      " [[  8.38304508e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     4.41742639e-01]\n",
      "  [  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     5.81677143e-01]\n",
      "  [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     7.99411196e-01]\n",
      "  [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     8.17150418e-01]\n",
      "  [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     4.07815583e-01]]]\n",
      "testX: [[[  8.55769185e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     5.81677143e-01]\n",
      "  [  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     7.99411196e-01]\n",
      "  [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     8.17150418e-01]\n",
      "  [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     4.07815583e-01]\n",
      "  [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "     2.89349359e-01]]\n",
      "\n",
      " [[  8.73233863e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     7.99411196e-01]\n",
      "  [  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     8.17150418e-01]\n",
      "  [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     4.07815583e-01]\n",
      "  [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "     2.89349359e-01]\n",
      "  [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "     2.95055243e-01]]\n",
      "\n",
      " [[  8.90698540e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     8.17150418e-01]\n",
      "  [  9.08163217e-04   2.09576127e-04   0.00000000e+00   3.51214660e-02\n",
      "     4.07815583e-01]\n",
      "  [  1.74646773e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "     2.89349359e-01]\n",
      "  [  3.49293545e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "     2.95055243e-01]\n",
      "  [  5.23940318e-05   1.74646773e-05   0.00000000e+00   3.51389306e-02\n",
      "     3.20740544e-01]]\n",
      "\n",
      " ..., \n",
      " [[  6.28728381e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]]\n",
      "\n",
      " [[  6.46193058e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]]\n",
      "\n",
      " [[  6.63657736e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.81122413e-04   1.57182095e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  6.98587090e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  7.16051767e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]\n",
      "  [  7.33516445e-04   1.74646773e-04   5.23940318e-05   3.51389306e-02\n",
      "     0.00000000e+00]]]\n",
      "[step: 0] loss: 18.616710662841797\n",
      "[step: 1] loss: 17.37733268737793\n",
      "[step: 2] loss: 16.18134117126465\n",
      "[step: 3] loss: 15.027997970581055\n",
      "[step: 4] loss: 13.916631698608398\n",
      "[step: 5] loss: 12.84693717956543\n",
      "[step: 6] loss: 11.819221496582031\n",
      "[step: 7] loss: 10.834298133850098\n",
      "[step: 8] loss: 9.89342975616455\n",
      "[step: 9] loss: 8.998298645019531\n",
      "[step: 10] loss: 8.151002883911133\n",
      "[step: 11] loss: 7.354099273681641\n",
      "[step: 12] loss: 6.6106367111206055\n",
      "[step: 13] loss: 5.924196243286133\n",
      "[step: 14] loss: 5.298892021179199\n",
      "[step: 15] loss: 4.73930025100708\n",
      "[step: 16] loss: 4.250312328338623\n",
      "[step: 17] loss: 3.836841583251953\n",
      "[step: 18] loss: 3.5033621788024902\n",
      "[step: 19] loss: 3.253213882446289\n",
      "[step: 20] loss: 3.0876193046569824\n",
      "[step: 21] loss: 3.0044164657592773\n",
      "[step: 22] loss: 2.9966228008270264\n",
      "[step: 23] loss: 3.0512239933013916\n",
      "[step: 24] loss: 3.1489710807800293\n",
      "[step: 25] loss: 3.26615571975708\n",
      "[step: 26] loss: 3.3785319328308105\n",
      "[step: 27] loss: 3.466010093688965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 28] loss: 3.516055107116699\n",
      "[step: 29] loss: 3.5246710777282715\n",
      "[step: 30] loss: 3.495209217071533\n",
      "[step: 31] loss: 3.4359798431396484\n",
      "[step: 32] loss: 3.357630729675293\n",
      "[step: 33] loss: 3.2709341049194336\n",
      "[step: 34] loss: 3.1852433681488037\n",
      "[step: 35] loss: 3.107642650604248\n",
      "[step: 36] loss: 3.0426833629608154\n",
      "[step: 37] loss: 2.9925336837768555\n",
      "[step: 38] loss: 2.9573869705200195\n",
      "[step: 39] loss: 2.935976028442383\n",
      "[step: 40] loss: 2.926098108291626\n",
      "[step: 41] loss: 2.925079345703125\n",
      "[step: 42] loss: 2.9301462173461914\n",
      "[step: 43] loss: 2.93869686126709\n",
      "[step: 44] loss: 2.948477029800415\n",
      "[step: 45] loss: 2.9576756954193115\n",
      "[step: 46] loss: 2.964960813522339\n",
      "[step: 47] loss: 2.9694676399230957\n",
      "[step: 48] loss: 2.9707605838775635\n",
      "[step: 49] loss: 2.968773365020752\n",
      "[step: 50] loss: 2.963740348815918\n",
      "[step: 51] loss: 2.956122636795044\n",
      "[step: 52] loss: 2.9465341567993164\n",
      "[step: 53] loss: 2.935671806335449\n",
      "[step: 54] loss: 2.9242489337921143\n",
      "[step: 55] loss: 2.912938356399536\n",
      "[step: 56] loss: 2.90232515335083\n",
      "[step: 57] loss: 2.8928680419921875\n",
      "[step: 58] loss: 2.884875774383545\n",
      "[step: 59] loss: 2.8784971237182617\n",
      "[step: 60] loss: 2.8737258911132812\n",
      "[step: 61] loss: 2.8704187870025635\n",
      "[step: 62] loss: 2.868320941925049\n",
      "[step: 63] loss: 2.867107391357422\n",
      "[step: 64] loss: 2.866422653198242\n",
      "[step: 65] loss: 2.865919589996338\n",
      "[step: 66] loss: 2.8652961254119873\n",
      "[step: 67] loss: 2.864320993423462\n",
      "[step: 68] loss: 2.862846851348877\n",
      "[step: 69] loss: 2.8608150482177734\n",
      "[step: 70] loss: 2.8582468032836914\n",
      "[step: 71] loss: 2.8552255630493164\n",
      "[step: 72] loss: 2.8518788814544678\n",
      "[step: 73] loss: 2.8483529090881348\n",
      "[step: 74] loss: 2.8447928428649902\n",
      "[step: 75] loss: 2.8413262367248535\n",
      "[step: 76] loss: 2.8380494117736816\n",
      "[step: 77] loss: 2.8350236415863037\n",
      "[step: 78] loss: 2.8322739601135254\n",
      "[step: 79] loss: 2.829792022705078\n",
      "[step: 80] loss: 2.827544689178467\n",
      "[step: 81] loss: 2.8254799842834473\n",
      "[step: 82] loss: 2.8235392570495605\n",
      "[step: 83] loss: 2.8216609954833984\n",
      "[step: 84] loss: 2.8197927474975586\n",
      "[step: 85] loss: 2.8178906440734863\n",
      "[step: 86] loss: 2.815925121307373\n",
      "[step: 87] loss: 2.8138811588287354\n",
      "[step: 88] loss: 2.8117566108703613\n",
      "[step: 89] loss: 2.809561252593994\n",
      "[step: 90] loss: 2.807311773300171\n",
      "[step: 91] loss: 2.8050296306610107\n",
      "[step: 92] loss: 2.802739143371582\n",
      "[step: 93] loss: 2.8004603385925293\n",
      "[step: 94] loss: 2.7982120513916016\n",
      "[step: 95] loss: 2.7960054874420166\n",
      "[step: 96] loss: 2.79384708404541\n",
      "[step: 97] loss: 2.7917380332946777\n",
      "[step: 98] loss: 2.7896728515625\n",
      "[step: 99] loss: 2.787644386291504\n",
      "[step: 100] loss: 2.785642385482788\n",
      "[step: 101] loss: 2.783656120300293\n",
      "[step: 102] loss: 2.7816760540008545\n",
      "[step: 103] loss: 2.779694080352783\n",
      "[step: 104] loss: 2.7777047157287598\n",
      "[step: 105] loss: 2.7757058143615723\n",
      "[step: 106] loss: 2.7736968994140625\n",
      "[step: 107] loss: 2.7716803550720215\n",
      "[step: 108] loss: 2.7696590423583984\n",
      "[step: 109] loss: 2.767637252807617\n",
      "[step: 110] loss: 2.7656192779541016\n",
      "[step: 111] loss: 2.763608932495117\n",
      "[step: 112] loss: 2.7616090774536133\n",
      "[step: 113] loss: 2.7596206665039062\n",
      "[step: 114] loss: 2.75764536857605\n",
      "[step: 115] loss: 2.7556815147399902\n",
      "[step: 116] loss: 2.7537283897399902\n",
      "[step: 117] loss: 2.751783609390259\n",
      "[step: 118] loss: 2.749845504760742\n",
      "[step: 119] loss: 2.7479124069213867\n",
      "[step: 120] loss: 2.7459826469421387\n",
      "[step: 121] loss: 2.7440547943115234\n",
      "[step: 122] loss: 2.742128372192383\n",
      "[step: 123] loss: 2.740203857421875\n",
      "[step: 124] loss: 2.738280773162842\n",
      "[step: 125] loss: 2.736359119415283\n",
      "[step: 126] loss: 2.734440326690674\n",
      "[step: 127] loss: 2.73252534866333\n",
      "[step: 128] loss: 2.73061466217041\n",
      "[step: 129] loss: 2.7287087440490723\n",
      "[step: 130] loss: 2.7268073558807373\n",
      "[step: 131] loss: 2.7249109745025635\n",
      "[step: 132] loss: 2.7230191230773926\n",
      "[step: 133] loss: 2.7211313247680664\n",
      "[step: 134] loss: 2.719248056411743\n",
      "[step: 135] loss: 2.717367649078369\n",
      "[step: 136] loss: 2.7154903411865234\n",
      "[step: 137] loss: 2.713616371154785\n",
      "[step: 138] loss: 2.7117443084716797\n",
      "[step: 139] loss: 2.7098748683929443\n",
      "[step: 140] loss: 2.708007335662842\n",
      "[step: 141] loss: 2.7061424255371094\n",
      "[step: 142] loss: 2.7042794227600098\n",
      "[step: 143] loss: 2.7024192810058594\n",
      "[step: 144] loss: 2.7005615234375\n",
      "[step: 145] loss: 2.6987059116363525\n",
      "[step: 146] loss: 2.6968531608581543\n",
      "[step: 147] loss: 2.695003032684326\n",
      "[step: 148] loss: 2.693155288696289\n",
      "[step: 149] loss: 2.6913094520568848\n",
      "[step: 150] loss: 2.6894659996032715\n",
      "[step: 151] loss: 2.687624454498291\n",
      "[step: 152] loss: 2.6857845783233643\n",
      "[step: 153] loss: 2.6839468479156494\n",
      "[step: 154] loss: 2.68211030960083\n",
      "[step: 155] loss: 2.6802759170532227\n",
      "[step: 156] loss: 2.6784424781799316\n",
      "[step: 157] loss: 2.676610231399536\n",
      "[step: 158] loss: 2.674779176712036\n",
      "[step: 159] loss: 2.672950029373169\n",
      "[step: 160] loss: 2.671121835708618\n",
      "[step: 161] loss: 2.6692943572998047\n",
      "[step: 162] loss: 2.667468547821045\n",
      "[step: 163] loss: 2.6656436920166016\n",
      "[step: 164] loss: 2.6638197898864746\n",
      "[step: 165] loss: 2.661996603012085\n",
      "[step: 166] loss: 2.6601743698120117\n",
      "[step: 167] loss: 2.658352851867676\n",
      "[step: 168] loss: 2.656531810760498\n",
      "[step: 169] loss: 2.6547112464904785\n",
      "[step: 170] loss: 2.652891159057617\n",
      "[step: 171] loss: 2.6510720252990723\n",
      "[step: 172] loss: 2.6492528915405273\n",
      "[step: 173] loss: 2.6474337577819824\n",
      "[step: 174] loss: 2.6456148624420166\n",
      "[step: 175] loss: 2.643795967102051\n",
      "[step: 176] loss: 2.6419777870178223\n",
      "[step: 177] loss: 2.6401586532592773\n",
      "[step: 178] loss: 2.638340473175049\n",
      "[step: 179] loss: 2.636521577835083\n",
      "[step: 180] loss: 2.634702205657959\n",
      "[step: 181] loss: 2.632883071899414\n",
      "[step: 182] loss: 2.63106369972229\n",
      "[step: 183] loss: 2.6292431354522705\n",
      "[step: 184] loss: 2.627422571182251\n",
      "[step: 185] loss: 2.625601291656494\n",
      "[step: 186] loss: 2.623779535293579\n",
      "[step: 187] loss: 2.6219568252563477\n",
      "[step: 188] loss: 2.620133638381958\n",
      "[step: 189] loss: 2.618309497833252\n",
      "[step: 190] loss: 2.616483688354492\n",
      "[step: 191] loss: 2.6146578788757324\n",
      "[step: 192] loss: 2.612830638885498\n",
      "[step: 193] loss: 2.611002206802368\n",
      "[step: 194] loss: 2.609172821044922\n",
      "[step: 195] loss: 2.607342004776001\n",
      "[step: 196] loss: 2.6055097579956055\n",
      "[step: 197] loss: 2.6036765575408936\n",
      "[step: 198] loss: 2.601841449737549\n",
      "[step: 199] loss: 2.6000051498413086\n",
      "[step: 200] loss: 2.5981669425964355\n",
      "[step: 201] loss: 2.596327543258667\n",
      "[step: 202] loss: 2.5944855213165283\n",
      "[step: 203] loss: 2.5926425457000732\n",
      "[step: 204] loss: 2.5907976627349854\n",
      "[step: 205] loss: 2.5889506340026855\n",
      "[step: 206] loss: 2.587101459503174\n",
      "[step: 207] loss: 2.5852508544921875\n",
      "[step: 208] loss: 2.58339786529541\n",
      "[step: 209] loss: 2.581542491912842\n",
      "[step: 210] loss: 2.5796849727630615\n",
      "[step: 211] loss: 2.5778253078460693\n",
      "[step: 212] loss: 2.575963020324707\n",
      "[step: 213] loss: 2.574098825454712\n",
      "[step: 214] loss: 2.5722317695617676\n",
      "[step: 215] loss: 2.5703623294830322\n",
      "[step: 216] loss: 2.568490505218506\n",
      "[step: 217] loss: 2.566615581512451\n",
      "[step: 218] loss: 2.5647380352020264\n",
      "[step: 219] loss: 2.5628576278686523\n",
      "[step: 220] loss: 2.56097412109375\n",
      "[step: 221] loss: 2.559088706970215\n",
      "[step: 222] loss: 2.557199478149414\n",
      "[step: 223] loss: 2.555307388305664\n",
      "[step: 224] loss: 2.553412437438965\n",
      "[step: 225] loss: 2.551513910293579\n",
      "[step: 226] loss: 2.549612283706665\n",
      "[step: 227] loss: 2.547708034515381\n",
      "[step: 228] loss: 2.545799732208252\n",
      "[step: 229] loss: 2.5438880920410156\n",
      "[step: 230] loss: 2.54197359085083\n",
      "[step: 231] loss: 2.5400550365448\n",
      "[step: 232] loss: 2.538132905960083\n",
      "[step: 233] loss: 2.5362071990966797\n",
      "[step: 234] loss: 2.534278392791748\n",
      "[step: 235] loss: 2.5323450565338135\n",
      "[step: 236] loss: 2.5304083824157715\n",
      "[step: 237] loss: 2.528467893600464\n",
      "[step: 238] loss: 2.5265231132507324\n",
      "[step: 239] loss: 2.5245747566223145\n",
      "[step: 240] loss: 2.5226221084594727\n",
      "[step: 241] loss: 2.5206658840179443\n",
      "[step: 242] loss: 2.518705129623413\n",
      "[step: 243] loss: 2.516740322113037\n",
      "[step: 244] loss: 2.5147714614868164\n",
      "[step: 245] loss: 2.5127980709075928\n",
      "[step: 246] loss: 2.5108203887939453\n",
      "[step: 247] loss: 2.508838653564453\n",
      "[step: 248] loss: 2.5068519115448\n",
      "[step: 249] loss: 2.5048608779907227\n",
      "[step: 250] loss: 2.502865791320801\n",
      "[step: 251] loss: 2.5008654594421387\n",
      "[step: 252] loss: 2.4988608360290527\n",
      "[step: 253] loss: 2.4968512058258057\n",
      "[step: 254] loss: 2.4948372840881348\n",
      "[step: 255] loss: 2.4928183555603027\n",
      "[step: 256] loss: 2.4907946586608887\n",
      "[step: 257] loss: 2.4887661933898926\n",
      "[step: 258] loss: 2.486732244491577\n",
      "[step: 259] loss: 2.484694242477417\n",
      "[step: 260] loss: 2.4826507568359375\n",
      "[step: 261] loss: 2.4806020259857178\n",
      "[step: 262] loss: 2.478548526763916\n",
      "[step: 263] loss: 2.476490020751953\n",
      "[step: 264] loss: 2.47442626953125\n",
      "[step: 265] loss: 2.4723567962646484\n",
      "[step: 266] loss: 2.4702823162078857\n",
      "[step: 267] loss: 2.468203067779541\n",
      "[step: 268] loss: 2.4661178588867188\n",
      "[step: 269] loss: 2.4640274047851562\n",
      "[step: 270] loss: 2.4619317054748535\n",
      "[step: 271] loss: 2.4598307609558105\n",
      "[step: 272] loss: 2.457724094390869\n",
      "[step: 273] loss: 2.4556117057800293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 274] loss: 2.453494071960449\n",
      "[step: 275] loss: 2.451371192932129\n",
      "[step: 276] loss: 2.44924259185791\n",
      "[step: 277] loss: 2.4471077919006348\n",
      "[step: 278] loss: 2.4449679851531982\n",
      "[step: 279] loss: 2.4428224563598633\n",
      "[step: 280] loss: 2.44067120552063\n",
      "[step: 281] loss: 2.438514232635498\n",
      "[step: 282] loss: 2.4363512992858887\n",
      "[step: 283] loss: 2.434183359146118\n",
      "[step: 284] loss: 2.432009220123291\n",
      "[step: 285] loss: 2.4298295974731445\n",
      "[step: 286] loss: 2.4276437759399414\n",
      "[step: 287] loss: 2.425452709197998\n",
      "[step: 288] loss: 2.423255681991577\n",
      "[step: 289] loss: 2.4210526943206787\n",
      "[step: 290] loss: 2.418844223022461\n",
      "[step: 291] loss: 2.4166297912597656\n",
      "[step: 292] loss: 2.414409875869751\n",
      "[step: 293] loss: 2.412184238433838\n",
      "[step: 294] loss: 2.4099531173706055\n",
      "[step: 295] loss: 2.4077160358428955\n",
      "[step: 296] loss: 2.405473232269287\n",
      "[step: 297] loss: 2.403224468231201\n",
      "[step: 298] loss: 2.400970458984375\n",
      "[step: 299] loss: 2.3987112045288086\n",
      "[step: 300] loss: 2.3964457511901855\n",
      "[step: 301] loss: 2.3941750526428223\n",
      "[step: 302] loss: 2.3918991088867188\n",
      "[step: 303] loss: 2.3896172046661377\n",
      "[step: 304] loss: 2.3873302936553955\n",
      "[step: 305] loss: 2.385037899017334\n",
      "[step: 306] loss: 2.382740020751953\n",
      "[step: 307] loss: 2.3804373741149902\n",
      "[step: 308] loss: 2.378129482269287\n",
      "[step: 309] loss: 2.3758161067962646\n",
      "[step: 310] loss: 2.37349796295166\n",
      "[step: 311] loss: 2.3711748123168945\n",
      "[step: 312] loss: 2.368846893310547\n",
      "[step: 313] loss: 2.3665144443511963\n",
      "[step: 314] loss: 2.3641772270202637\n",
      "[step: 315] loss: 2.361835479736328\n",
      "[step: 316] loss: 2.3594894409179688\n",
      "[step: 317] loss: 2.3571386337280273\n",
      "[step: 318] loss: 2.354783773422241\n",
      "[step: 319] loss: 2.3524246215820312\n",
      "[step: 320] loss: 2.3500618934631348\n",
      "[step: 321] loss: 2.3476953506469727\n",
      "[step: 322] loss: 2.3453245162963867\n",
      "[step: 323] loss: 2.3429508209228516\n",
      "[step: 324] loss: 2.340573310852051\n",
      "[step: 325] loss: 2.338192939758301\n",
      "[step: 326] loss: 2.3358089923858643\n",
      "[step: 327] loss: 2.3334226608276367\n",
      "[step: 328] loss: 2.33103346824646\n",
      "[step: 329] loss: 2.328641414642334\n",
      "[step: 330] loss: 2.326247453689575\n",
      "[step: 331] loss: 2.3238511085510254\n",
      "[step: 332] loss: 2.3214528560638428\n",
      "[step: 333] loss: 2.3190529346466064\n",
      "[step: 334] loss: 2.3166518211364746\n",
      "[step: 335] loss: 2.3142480850219727\n",
      "[step: 336] loss: 2.3118443489074707\n",
      "[step: 337] loss: 2.309439182281494\n",
      "[step: 338] loss: 2.3070335388183594\n",
      "[step: 339] loss: 2.3046278953552246\n",
      "[step: 340] loss: 2.3022217750549316\n",
      "[step: 341] loss: 2.2998154163360596\n",
      "[step: 342] loss: 2.2974095344543457\n",
      "[step: 343] loss: 2.295003890991211\n",
      "[step: 344] loss: 2.2925994396209717\n",
      "[step: 345] loss: 2.2901957035064697\n",
      "[step: 346] loss: 2.2877936363220215\n",
      "[step: 347] loss: 2.2853922843933105\n",
      "[step: 348] loss: 2.2829933166503906\n",
      "[step: 349] loss: 2.2805964946746826\n",
      "[step: 350] loss: 2.2782013416290283\n",
      "[step: 351] loss: 2.275808811187744\n",
      "[step: 352] loss: 2.2734193801879883\n",
      "[step: 353] loss: 2.2710320949554443\n",
      "[step: 354] loss: 2.268648147583008\n",
      "[step: 355] loss: 2.266267776489258\n",
      "[step: 356] loss: 2.2638912200927734\n",
      "[step: 357] loss: 2.2615180015563965\n",
      "[step: 358] loss: 2.259148597717285\n",
      "[step: 359] loss: 2.2567830085754395\n",
      "[step: 360] loss: 2.254422187805176\n",
      "[step: 361] loss: 2.252065658569336\n",
      "[step: 362] loss: 2.249713659286499\n",
      "[step: 363] loss: 2.247366189956665\n",
      "[step: 364] loss: 2.245023727416992\n",
      "[step: 365] loss: 2.2426857948303223\n",
      "[step: 366] loss: 2.2403526306152344\n",
      "[step: 367] loss: 2.238024950027466\n",
      "[step: 368] loss: 2.2357020378112793\n",
      "[step: 369] loss: 2.233383893966675\n",
      "[step: 370] loss: 2.2310709953308105\n",
      "[step: 371] loss: 2.228762626647949\n",
      "[step: 372] loss: 2.226459503173828\n",
      "[step: 373] loss: 2.224161148071289\n",
      "[step: 374] loss: 2.221867322921753\n",
      "[step: 375] loss: 2.219578266143799\n",
      "[step: 376] loss: 2.2172937393188477\n",
      "[step: 377] loss: 2.2150135040283203\n",
      "[step: 378] loss: 2.2127370834350586\n",
      "[step: 379] loss: 2.2104649543762207\n",
      "[step: 380] loss: 2.2081961631774902\n",
      "[step: 381] loss: 2.2059311866760254\n",
      "[step: 382] loss: 2.2036690711975098\n",
      "[step: 383] loss: 2.2014098167419434\n",
      "[step: 384] loss: 2.199153423309326\n",
      "[step: 385] loss: 2.1968994140625\n",
      "[step: 386] loss: 2.1946470737457275\n",
      "[step: 387] loss: 2.192396640777588\n",
      "[step: 388] loss: 2.1901471614837646\n",
      "[step: 389] loss: 2.187899112701416\n",
      "[step: 390] loss: 2.1856508255004883\n",
      "[step: 391] loss: 2.1834030151367188\n",
      "[step: 392] loss: 2.181155204772949\n",
      "[step: 393] loss: 2.1789069175720215\n",
      "[step: 394] loss: 2.1766574382781982\n",
      "[step: 395] loss: 2.1744065284729004\n",
      "[step: 396] loss: 2.172154188156128\n",
      "[step: 397] loss: 2.1698994636535645\n",
      "[step: 398] loss: 2.167642593383789\n",
      "[step: 399] loss: 2.1653823852539062\n",
      "[step: 400] loss: 2.1631195545196533\n",
      "[step: 401] loss: 2.1608526706695557\n",
      "[step: 402] loss: 2.1585822105407715\n",
      "[step: 403] loss: 2.156306743621826\n",
      "[step: 404] loss: 2.1540274620056152\n",
      "[step: 405] loss: 2.151742935180664\n",
      "[step: 406] loss: 2.1494529247283936\n",
      "[step: 407] loss: 2.147157669067383\n",
      "[step: 408] loss: 2.1448564529418945\n",
      "[step: 409] loss: 2.1425490379333496\n",
      "[step: 410] loss: 2.140235424041748\n",
      "[step: 411] loss: 2.1379151344299316\n",
      "[step: 412] loss: 2.135587692260742\n",
      "[step: 413] loss: 2.1332528591156006\n",
      "[step: 414] loss: 2.1309115886688232\n",
      "[step: 415] loss: 2.1285622119903564\n",
      "[step: 416] loss: 2.1262049674987793\n",
      "[step: 417] loss: 2.123840093612671\n",
      "[step: 418] loss: 2.121466636657715\n",
      "[step: 419] loss: 2.1190855503082275\n",
      "[step: 420] loss: 2.1166954040527344\n",
      "[step: 421] loss: 2.1142969131469727\n",
      "[step: 422] loss: 2.1118903160095215\n",
      "[step: 423] loss: 2.1094741821289062\n",
      "[step: 424] loss: 2.1070494651794434\n",
      "[step: 425] loss: 2.1046152114868164\n",
      "[step: 426] loss: 2.102172374725342\n",
      "[step: 427] loss: 2.0997202396392822\n",
      "[step: 428] loss: 2.0972585678100586\n",
      "[step: 429] loss: 2.09478759765625\n",
      "[step: 430] loss: 2.0923070907592773\n",
      "[step: 431] loss: 2.0898172855377197\n",
      "[step: 432] loss: 2.087317705154419\n",
      "[step: 433] loss: 2.084808588027954\n",
      "[step: 434] loss: 2.082289934158325\n",
      "[step: 435] loss: 2.079761505126953\n",
      "[step: 436] loss: 2.077223300933838\n",
      "[step: 437] loss: 2.0746750831604004\n",
      "[step: 438] loss: 2.072117805480957\n",
      "[step: 439] loss: 2.0695505142211914\n",
      "[step: 440] loss: 2.0669734477996826\n",
      "[step: 441] loss: 2.0643863677978516\n",
      "[step: 442] loss: 2.0617899894714355\n",
      "[step: 443] loss: 2.0591835975646973\n",
      "[step: 444] loss: 2.056567430496216\n",
      "[step: 445] loss: 2.0539417266845703\n",
      "[step: 446] loss: 2.0513062477111816\n",
      "[step: 447] loss: 2.048661231994629\n",
      "[step: 448] loss: 2.046006441116333\n",
      "[step: 449] loss: 2.043342351913452\n",
      "[step: 450] loss: 2.040668487548828\n",
      "[step: 451] loss: 2.0379858016967773\n",
      "[step: 452] loss: 2.035292625427246\n",
      "[step: 453] loss: 2.0325911045074463\n",
      "[step: 454] loss: 2.0298802852630615\n",
      "[step: 455] loss: 2.0271599292755127\n",
      "[step: 456] loss: 2.024430751800537\n",
      "[step: 457] loss: 2.0216922760009766\n",
      "[step: 458] loss: 2.0189449787139893\n",
      "[step: 459] loss: 2.0161890983581543\n",
      "[step: 460] loss: 2.0134243965148926\n",
      "[step: 461] loss: 2.010651111602783\n",
      "[step: 462] loss: 2.0078697204589844\n",
      "[step: 463] loss: 2.005079507827759\n",
      "[step: 464] loss: 2.0022811889648438\n",
      "[step: 465] loss: 1.9994746446609497\n",
      "[step: 466] loss: 1.9966607093811035\n",
      "[step: 467] loss: 1.9938386678695679\n",
      "[step: 468] loss: 1.99100923538208\n",
      "[step: 469] loss: 1.9881720542907715\n",
      "[step: 470] loss: 1.985327959060669\n",
      "[step: 471] loss: 1.9824764728546143\n",
      "[step: 472] loss: 1.9796181917190552\n",
      "[step: 473] loss: 1.9767529964447021\n",
      "[step: 474] loss: 1.9738816022872925\n",
      "[step: 475] loss: 1.971003532409668\n",
      "[step: 476] loss: 1.9681193828582764\n",
      "[step: 477] loss: 1.9652293920516968\n",
      "[step: 478] loss: 1.9623336791992188\n",
      "[step: 479] loss: 1.9594321250915527\n",
      "[step: 480] loss: 1.9565256834030151\n",
      "[step: 481] loss: 1.9536137580871582\n",
      "[step: 482] loss: 1.9506973028182983\n",
      "[step: 483] loss: 1.9477763175964355\n",
      "[step: 484] loss: 1.9448506832122803\n",
      "[step: 485] loss: 1.9419211149215698\n",
      "[step: 486] loss: 1.9389876127243042\n",
      "[step: 487] loss: 1.936050295829773\n",
      "[step: 488] loss: 1.933109998703003\n",
      "[step: 489] loss: 1.9301663637161255\n",
      "[step: 490] loss: 1.927220106124878\n",
      "[step: 491] loss: 1.9242711067199707\n",
      "[step: 492] loss: 1.9213197231292725\n",
      "[step: 493] loss: 1.9183669090270996\n",
      "[step: 494] loss: 1.9154114723205566\n",
      "[step: 495] loss: 1.9124549627304077\n",
      "[step: 496] loss: 1.9094971418380737\n",
      "[step: 497] loss: 1.9065386056900024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 498] loss: 1.9035794734954834\n",
      "[step: 499] loss: 1.9006195068359375\n",
      "[step: 500] loss: 1.8976597785949707\n",
      "[step: 501] loss: 1.894700050354004\n",
      "[step: 502] loss: 1.8917410373687744\n",
      "[step: 503] loss: 1.8887827396392822\n",
      "[step: 504] loss: 1.885825276374817\n",
      "[step: 505] loss: 1.8828693628311157\n",
      "[step: 506] loss: 1.8799147605895996\n",
      "[step: 507] loss: 1.876962423324585\n",
      "[step: 508] loss: 1.8740119934082031\n",
      "[step: 509] loss: 1.8710639476776123\n",
      "[step: 510] loss: 1.868118405342102\n",
      "[step: 511] loss: 1.8651762008666992\n",
      "[step: 512] loss: 1.8622368574142456\n",
      "[step: 513] loss: 1.8593010902404785\n",
      "[step: 514] loss: 1.8563690185546875\n",
      "[step: 515] loss: 1.8534414768218994\n",
      "[step: 516] loss: 1.850517749786377\n",
      "[step: 517] loss: 1.8475987911224365\n",
      "[step: 518] loss: 1.8446847200393677\n",
      "[step: 519] loss: 1.8417754173278809\n",
      "[step: 520] loss: 1.8388718366622925\n",
      "[step: 521] loss: 1.8359737396240234\n",
      "[step: 522] loss: 1.8330813646316528\n",
      "[step: 523] loss: 1.830195426940918\n",
      "[step: 524] loss: 1.8273155689239502\n",
      "[step: 525] loss: 1.8244422674179077\n",
      "[step: 526] loss: 1.8215761184692383\n",
      "[step: 527] loss: 1.8187172412872314\n",
      "[step: 528] loss: 1.8158652782440186\n",
      "[step: 529] loss: 1.8130215406417847\n",
      "[step: 530] loss: 1.810185194015503\n",
      "[step: 531] loss: 1.8073574304580688\n",
      "[step: 532] loss: 1.8045381307601929\n",
      "[step: 533] loss: 1.801727533340454\n",
      "[step: 534] loss: 1.7989258766174316\n",
      "[step: 535] loss: 1.7961336374282837\n",
      "[step: 536] loss: 1.7933506965637207\n",
      "[step: 537] loss: 1.7905782461166382\n",
      "[step: 538] loss: 1.7878155708312988\n",
      "[step: 539] loss: 1.7850635051727295\n",
      "[step: 540] loss: 1.7823221683502197\n",
      "[step: 541] loss: 1.7795920372009277\n",
      "[step: 542] loss: 1.7768734693527222\n",
      "[step: 543] loss: 1.774166464805603\n",
      "[step: 544] loss: 1.7714715003967285\n",
      "[step: 545] loss: 1.7687890529632568\n",
      "[step: 546] loss: 1.7661197185516357\n",
      "[step: 547] loss: 1.7634638547897339\n",
      "[step: 548] loss: 1.7608206272125244\n",
      "[step: 549] loss: 1.7581915855407715\n",
      "[step: 550] loss: 1.7555768489837646\n",
      "[step: 551] loss: 1.7529771327972412\n",
      "[step: 552] loss: 1.7503920793533325\n",
      "[step: 553] loss: 1.7478222846984863\n",
      "[step: 554] loss: 1.7452685832977295\n",
      "[step: 555] loss: 1.7427306175231934\n",
      "[step: 556] loss: 1.7402095794677734\n",
      "[step: 557] loss: 1.7377052307128906\n",
      "[step: 558] loss: 1.7352176904678345\n",
      "[step: 559] loss: 1.732748031616211\n",
      "[step: 560] loss: 1.73029625415802\n",
      "[step: 561] loss: 1.7278627157211304\n",
      "[step: 562] loss: 1.725447416305542\n",
      "[step: 563] loss: 1.7230511903762817\n",
      "[step: 564] loss: 1.7206741571426392\n",
      "[step: 565] loss: 1.7183159589767456\n",
      "[step: 566] loss: 1.7159780263900757\n",
      "[step: 567] loss: 1.7136595249176025\n",
      "[step: 568] loss: 1.7113609313964844\n",
      "[step: 569] loss: 1.7090829610824585\n",
      "[step: 570] loss: 1.7068250179290771\n",
      "[step: 571] loss: 1.704587697982788\n",
      "[step: 572] loss: 1.7023706436157227\n",
      "[step: 573] loss: 1.7001746892929077\n",
      "[step: 574] loss: 1.6979990005493164\n",
      "[step: 575] loss: 1.695844054222107\n",
      "[step: 576] loss: 1.693710207939148\n",
      "[step: 577] loss: 1.691596508026123\n",
      "[step: 578] loss: 1.6895036697387695\n",
      "[step: 579] loss: 1.6874314546585083\n",
      "[step: 580] loss: 1.6853795051574707\n",
      "[step: 581] loss: 1.6833479404449463\n",
      "[step: 582] loss: 1.6813364028930664\n",
      "[step: 583] loss: 1.6793447732925415\n",
      "[step: 584] loss: 1.6773731708526611\n",
      "[step: 585] loss: 1.675420880317688\n",
      "[step: 586] loss: 1.673487901687622\n",
      "[step: 587] loss: 1.671574354171753\n",
      "[step: 588] loss: 1.6696791648864746\n",
      "[step: 589] loss: 1.6678028106689453\n",
      "[step: 590] loss: 1.6659448146820068\n",
      "[step: 591] loss: 1.6641051769256592\n",
      "[step: 592] loss: 1.662282943725586\n",
      "[step: 593] loss: 1.6604783535003662\n",
      "[step: 594] loss: 1.65869140625\n",
      "[step: 595] loss: 1.6569210290908813\n",
      "[step: 596] loss: 1.655167579650879\n",
      "[step: 597] loss: 1.6534309387207031\n",
      "[step: 598] loss: 1.651710033416748\n",
      "[step: 599] loss: 1.6500053405761719\n",
      "[step: 600] loss: 1.6483163833618164\n",
      "[step: 601] loss: 1.6466425657272339\n",
      "[step: 602] loss: 1.6449840068817139\n",
      "[step: 603] loss: 1.6433405876159668\n",
      "[step: 604] loss: 1.641711711883545\n",
      "[step: 605] loss: 1.6400971412658691\n",
      "[step: 606] loss: 1.638497233390808\n",
      "[step: 607] loss: 1.6369105577468872\n",
      "[step: 608] loss: 1.6353377103805542\n",
      "[step: 609] loss: 1.6337782144546509\n",
      "[step: 610] loss: 1.6322318315505981\n",
      "[step: 611] loss: 1.6306982040405273\n",
      "[step: 612] loss: 1.6291776895523071\n",
      "[step: 613] loss: 1.6276689767837524\n",
      "[step: 614] loss: 1.626172661781311\n",
      "[step: 615] loss: 1.6246881484985352\n",
      "[step: 616] loss: 1.6232151985168457\n",
      "[step: 617] loss: 1.621753215789795\n",
      "[step: 618] loss: 1.6203022003173828\n",
      "[step: 619] loss: 1.6188626289367676\n",
      "[step: 620] loss: 1.617432951927185\n",
      "[step: 621] loss: 1.616014003753662\n",
      "[step: 622] loss: 1.6146049499511719\n",
      "[step: 623] loss: 1.6132051944732666\n",
      "[step: 624] loss: 1.6118156909942627\n",
      "[step: 625] loss: 1.610435128211975\n",
      "[step: 626] loss: 1.609063744544983\n",
      "[step: 627] loss: 1.6077008247375488\n",
      "[step: 628] loss: 1.6063469648361206\n",
      "[step: 629] loss: 1.6050009727478027\n",
      "[step: 630] loss: 1.6036633253097534\n",
      "[step: 631] loss: 1.602333426475525\n",
      "[step: 632] loss: 1.6010112762451172\n",
      "[step: 633] loss: 1.599696397781372\n",
      "[step: 634] loss: 1.598388910293579\n",
      "[step: 635] loss: 1.5970878601074219\n",
      "[step: 636] loss: 1.5957942008972168\n",
      "[step: 637] loss: 1.5945065021514893\n",
      "[step: 638] loss: 1.593225121498108\n",
      "[step: 639] loss: 1.5919502973556519\n",
      "[step: 640] loss: 1.5906808376312256\n",
      "[step: 641] loss: 1.5894174575805664\n",
      "[step: 642] loss: 1.5881589651107788\n",
      "[step: 643] loss: 1.5869057178497314\n",
      "[step: 644] loss: 1.5856575965881348\n",
      "[step: 645] loss: 1.5844144821166992\n",
      "[step: 646] loss: 1.5831758975982666\n",
      "[step: 647] loss: 1.5819414854049683\n",
      "[step: 648] loss: 1.5807111263275146\n",
      "[step: 649] loss: 1.5794848203659058\n",
      "[step: 650] loss: 1.5782625675201416\n",
      "[step: 651] loss: 1.5770436525344849\n",
      "[step: 652] loss: 1.5758283138275146\n",
      "[step: 653] loss: 1.5746161937713623\n",
      "[step: 654] loss: 1.5734070539474487\n",
      "[step: 655] loss: 1.5722006559371948\n",
      "[step: 656] loss: 1.5709967613220215\n",
      "[step: 657] loss: 1.569795846939087\n",
      "[step: 658] loss: 1.5685975551605225\n",
      "[step: 659] loss: 1.5674006938934326\n",
      "[step: 660] loss: 1.566206455230713\n",
      "[step: 661] loss: 1.5650138854980469\n",
      "[step: 662] loss: 1.5638231039047241\n",
      "[step: 663] loss: 1.5626338720321655\n",
      "[step: 664] loss: 1.561446189880371\n",
      "[step: 665] loss: 1.5602598190307617\n",
      "[step: 666] loss: 1.5590744018554688\n",
      "[step: 667] loss: 1.5578901767730713\n",
      "[step: 668] loss: 1.5567071437835693\n",
      "[step: 669] loss: 1.5555247068405151\n",
      "[step: 670] loss: 1.5543434619903564\n",
      "[step: 671] loss: 1.5531623363494873\n",
      "[step: 672] loss: 1.5519819259643555\n",
      "[step: 673] loss: 1.5508015155792236\n",
      "[step: 674] loss: 1.549621820449829\n",
      "[step: 675] loss: 1.5484421253204346\n",
      "[step: 676] loss: 1.5472626686096191\n",
      "[step: 677] loss: 1.5460832118988037\n",
      "[step: 678] loss: 1.5449037551879883\n",
      "[step: 679] loss: 1.5437242984771729\n",
      "[step: 680] loss: 1.5425446033477783\n",
      "[step: 681] loss: 1.5413646697998047\n",
      "[step: 682] loss: 1.540184497833252\n",
      "[step: 683] loss: 1.5390037298202515\n",
      "[step: 684] loss: 1.5378227233886719\n",
      "[step: 685] loss: 1.536641240119934\n",
      "[step: 686] loss: 1.535459280014038\n",
      "[step: 687] loss: 1.5342768430709839\n",
      "[step: 688] loss: 1.5330935716629028\n",
      "[step: 689] loss: 1.5319101810455322\n",
      "[step: 690] loss: 1.5307257175445557\n",
      "[step: 691] loss: 1.529540777206421\n",
      "[step: 692] loss: 1.528355360031128\n",
      "[step: 693] loss: 1.5271685123443604\n",
      "[step: 694] loss: 1.5259811878204346\n",
      "[step: 695] loss: 1.5247931480407715\n",
      "[step: 696] loss: 1.5236040353775024\n",
      "[step: 697] loss: 1.5224144458770752\n",
      "[step: 698] loss: 1.521224021911621\n",
      "[step: 699] loss: 1.5200326442718506\n",
      "[step: 700] loss: 1.5188400745391846\n",
      "[step: 701] loss: 1.5176466703414917\n",
      "[step: 702] loss: 1.5164523124694824\n",
      "[step: 703] loss: 1.5152573585510254\n",
      "[step: 704] loss: 1.5140609741210938\n",
      "[step: 705] loss: 1.5128637552261353\n",
      "[step: 706] loss: 1.51166570186615\n",
      "[step: 707] loss: 1.5104659795761108\n",
      "[step: 708] loss: 1.5092662572860718\n",
      "[step: 709] loss: 1.5080645084381104\n",
      "[step: 710] loss: 1.5068621635437012\n",
      "[step: 711] loss: 1.5056581497192383\n",
      "[step: 712] loss: 1.5044530630111694\n",
      "[step: 713] loss: 1.5032473802566528\n",
      "[step: 714] loss: 1.5020397901535034\n",
      "[step: 715] loss: 1.500831127166748\n",
      "[step: 716] loss: 1.4996213912963867\n",
      "[step: 717] loss: 1.4984102249145508\n",
      "[step: 718] loss: 1.4971976280212402\n",
      "[step: 719] loss: 1.4959838390350342\n",
      "[step: 720] loss: 1.4947681427001953\n",
      "[step: 721] loss: 1.4935510158538818\n",
      "[step: 722] loss: 1.4923322200775146\n",
      "[step: 723] loss: 1.491112470626831\n",
      "[step: 724] loss: 1.4898900985717773\n",
      "[step: 725] loss: 1.4886667728424072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 726] loss: 1.4874416589736938\n",
      "[step: 727] loss: 1.4862141609191895\n",
      "[step: 728] loss: 1.4849853515625\n",
      "[step: 729] loss: 1.4837543964385986\n",
      "[step: 730] loss: 1.4825211763381958\n",
      "[step: 731] loss: 1.4812861680984497\n",
      "[step: 732] loss: 1.4800487756729126\n",
      "[step: 733] loss: 1.478808879852295\n",
      "[step: 734] loss: 1.477567434310913\n",
      "[step: 735] loss: 1.476322889328003\n",
      "[step: 736] loss: 1.4750761985778809\n",
      "[step: 737] loss: 1.47382652759552\n",
      "[step: 738] loss: 1.4725745916366577\n",
      "[step: 739] loss: 1.4713201522827148\n",
      "[step: 740] loss: 1.4700627326965332\n",
      "[step: 741] loss: 1.4688019752502441\n",
      "[step: 742] loss: 1.4675381183624268\n",
      "[step: 743] loss: 1.4662714004516602\n",
      "[step: 744] loss: 1.4650019407272339\n",
      "[step: 745] loss: 1.463728904724121\n",
      "[step: 746] loss: 1.4624520540237427\n",
      "[step: 747] loss: 1.4611718654632568\n",
      "[step: 748] loss: 1.459888219833374\n",
      "[step: 749] loss: 1.4586011171340942\n",
      "[step: 750] loss: 1.4573099613189697\n",
      "[step: 751] loss: 1.4560147523880005\n",
      "[step: 752] loss: 1.4547152519226074\n",
      "[step: 753] loss: 1.4534122943878174\n",
      "[step: 754] loss: 1.4521042108535767\n",
      "[step: 755] loss: 1.4507920742034912\n",
      "[step: 756] loss: 1.4494755268096924\n",
      "[step: 757] loss: 1.4481544494628906\n",
      "[step: 758] loss: 1.446828007698059\n",
      "[step: 759] loss: 1.4454972743988037\n",
      "[step: 760] loss: 1.444161057472229\n",
      "[step: 761] loss: 1.4428200721740723\n",
      "[step: 762] loss: 1.4414740800857544\n",
      "[step: 763] loss: 1.4401218891143799\n",
      "[step: 764] loss: 1.4387643337249756\n",
      "[step: 765] loss: 1.4374020099639893\n",
      "[step: 766] loss: 1.436033010482788\n",
      "[step: 767] loss: 1.4346582889556885\n",
      "[step: 768] loss: 1.4332771301269531\n",
      "[step: 769] loss: 1.4318897724151611\n",
      "[step: 770] loss: 1.4304968118667603\n",
      "[step: 771] loss: 1.4290966987609863\n",
      "[step: 772] loss: 1.4276902675628662\n",
      "[step: 773] loss: 1.4262769222259521\n",
      "[step: 774] loss: 1.4248565435409546\n",
      "[step: 775] loss: 1.4234291315078735\n",
      "[step: 776] loss: 1.4219945669174194\n",
      "[step: 777] loss: 1.420552372932434\n",
      "[step: 778] loss: 1.4191029071807861\n",
      "[step: 779] loss: 1.4176456928253174\n",
      "[step: 780] loss: 1.4161806106567383\n",
      "[step: 781] loss: 1.4147076606750488\n",
      "[step: 782] loss: 1.4132258892059326\n",
      "[step: 783] loss: 1.411736011505127\n",
      "[step: 784] loss: 1.4102380275726318\n",
      "[step: 785] loss: 1.4087313413619995\n",
      "[step: 786] loss: 1.4072155952453613\n",
      "[step: 787] loss: 1.4056906700134277\n",
      "[step: 788] loss: 1.4041569232940674\n",
      "[step: 789] loss: 1.402613639831543\n",
      "[step: 790] loss: 1.401060938835144\n",
      "[step: 791] loss: 1.3994983434677124\n",
      "[step: 792] loss: 1.3979263305664062\n",
      "[step: 793] loss: 1.3963438272476196\n",
      "[step: 794] loss: 1.3947510719299316\n",
      "[step: 795] loss: 1.3931483030319214\n",
      "[step: 796] loss: 1.3915345668792725\n",
      "[step: 797] loss: 1.3899104595184326\n",
      "[step: 798] loss: 1.3882752656936646\n",
      "[step: 799] loss: 1.3866286277770996\n",
      "[step: 800] loss: 1.3849704265594482\n",
      "[step: 801] loss: 1.3833012580871582\n",
      "[step: 802] loss: 1.3816196918487549\n",
      "[step: 803] loss: 1.379926323890686\n",
      "[step: 804] loss: 1.3782212734222412\n",
      "[step: 805] loss: 1.3765032291412354\n",
      "[step: 806] loss: 1.3747730255126953\n",
      "[step: 807] loss: 1.3730297088623047\n",
      "[step: 808] loss: 1.3712736368179321\n",
      "[step: 809] loss: 1.3695038557052612\n",
      "[step: 810] loss: 1.367720127105713\n",
      "[step: 811] loss: 1.3659237623214722\n",
      "[step: 812] loss: 1.3641126155853271\n",
      "[step: 813] loss: 1.3622878789901733\n",
      "[step: 814] loss: 1.360447883605957\n",
      "[step: 815] loss: 1.3585939407348633\n",
      "[step: 816] loss: 1.3567239046096802\n",
      "[step: 817] loss: 1.354839563369751\n",
      "[step: 818] loss: 1.3529387712478638\n",
      "[step: 819] loss: 1.3510222434997559\n",
      "[step: 820] loss: 1.3490893840789795\n",
      "[step: 821] loss: 1.3471407890319824\n",
      "[step: 822] loss: 1.3451744318008423\n",
      "[step: 823] loss: 1.3431918621063232\n",
      "[step: 824] loss: 1.3411911725997925\n",
      "[step: 825] loss: 1.3391728401184082\n",
      "[step: 826] loss: 1.3371365070343018\n",
      "[step: 827] loss: 1.3350815773010254\n",
      "[step: 828] loss: 1.3330076932907104\n",
      "[step: 829] loss: 1.3309147357940674\n",
      "[step: 830] loss: 1.3288025856018066\n",
      "[step: 831] loss: 1.3266699314117432\n",
      "[step: 832] loss: 1.3245172500610352\n",
      "[step: 833] loss: 1.3223435878753662\n",
      "[step: 834] loss: 1.3201487064361572\n",
      "[step: 835] loss: 1.317932367324829\n",
      "[step: 836] loss: 1.3156940937042236\n",
      "[step: 837] loss: 1.313434362411499\n",
      "[step: 838] loss: 1.3111544847488403\n",
      "[step: 839] loss: 1.3088611364364624\n",
      "[step: 840] loss: 1.3065831661224365\n",
      "[step: 841] loss: 1.30442214012146\n",
      "[step: 842] loss: 1.3027608394622803\n",
      "[step: 843] loss: 1.302113652229309\n",
      "[step: 844] loss: 1.3024152517318726\n",
      "[step: 845] loss: 1.2995086908340454\n",
      "[step: 846] loss: 1.2935192584991455\n",
      "[step: 847] loss: 1.2908902168273926\n",
      "[step: 848] loss: 1.2912471294403076\n",
      "[step: 849] loss: 1.288240671157837\n",
      "[step: 850] loss: 1.28361976146698\n",
      "[step: 851] loss: 1.2825850248336792\n",
      "[step: 852] loss: 1.2813218832015991\n",
      "[step: 853] loss: 1.2771705389022827\n",
      "[step: 854] loss: 1.2747161388397217\n",
      "[step: 855] loss: 1.2737209796905518\n",
      "[step: 856] loss: 1.2705140113830566\n",
      "[step: 857] loss: 1.267317771911621\n",
      "[step: 858] loss: 1.265905737876892\n",
      "[step: 859] loss: 1.2634860277175903\n",
      "[step: 860] loss: 1.2601211071014404\n",
      "[step: 861] loss: 1.2580394744873047\n",
      "[step: 862] loss: 1.2560498714447021\n",
      "[step: 863] loss: 1.2529362440109253\n",
      "[step: 864] loss: 1.250225305557251\n",
      "[step: 865] loss: 1.2482192516326904\n",
      "[step: 866] loss: 1.2455612421035767\n",
      "[step: 867] loss: 1.2425286769866943\n",
      "[step: 868] loss: 1.240112066268921\n",
      "[step: 869] loss: 1.237770915031433\n",
      "[step: 870] loss: 1.2348685264587402\n",
      "[step: 871] loss: 1.231986165046692\n",
      "[step: 872] loss: 1.2295091152191162\n",
      "[step: 873] loss: 1.2269225120544434\n",
      "[step: 874] loss: 1.2239768505096436\n",
      "[step: 875] loss: 1.221088171005249\n",
      "[step: 876] loss: 1.218446969985962\n",
      "[step: 877] loss: 1.2157407999038696\n",
      "[step: 878] loss: 1.2127912044525146\n",
      "[step: 879] loss: 1.2098052501678467\n",
      "[step: 880] loss: 1.206965446472168\n",
      "[step: 881] loss: 1.2041654586791992\n",
      "[step: 882] loss: 1.2012362480163574\n",
      "[step: 883] loss: 1.1981831789016724\n",
      "[step: 884] loss: 1.195133924484253\n",
      "[step: 885] loss: 1.1921532154083252\n",
      "[step: 886] loss: 1.1891887187957764\n",
      "[step: 887] loss: 1.1861624717712402\n",
      "[step: 888] loss: 1.1830506324768066\n",
      "[step: 889] loss: 1.1798877716064453\n",
      "[step: 890] loss: 1.1767164468765259\n",
      "[step: 891] loss: 1.173555612564087\n",
      "[step: 892] loss: 1.170400619506836\n",
      "[step: 893] loss: 1.167232632637024\n",
      "[step: 894] loss: 1.1640408039093018\n",
      "[step: 895] loss: 1.1608210802078247\n",
      "[step: 896] loss: 1.157578945159912\n",
      "[step: 897] loss: 1.1543221473693848\n",
      "[step: 898] loss: 1.151066541671753\n",
      "[step: 899] loss: 1.1478312015533447\n",
      "[step: 900] loss: 1.1446588039398193\n",
      "[step: 901] loss: 1.1416094303131104\n",
      "[step: 902] loss: 1.1387991905212402\n",
      "[step: 903] loss: 1.1363332271575928\n",
      "[step: 904] loss: 1.134211540222168\n",
      "[step: 905] loss: 1.131885051727295\n",
      "[step: 906] loss: 1.1283466815948486\n",
      "[step: 907] loss: 1.1232091188430786\n",
      "[step: 908] loss: 1.1180088520050049\n",
      "[step: 909] loss: 1.1144905090332031\n",
      "[step: 910] loss: 1.1124709844589233\n",
      "[step: 911] loss: 1.110369086265564\n",
      "[step: 912] loss: 1.1068882942199707\n",
      "[step: 913] loss: 1.1023191213607788\n",
      "[step: 914] loss: 1.0981545448303223\n",
      "[step: 915] loss: 1.095224380493164\n",
      "[step: 916] loss: 1.0928995609283447\n",
      "[step: 917] loss: 1.0900793075561523\n",
      "[step: 918] loss: 1.086355447769165\n",
      "[step: 919] loss: 1.082289218902588\n",
      "[step: 920] loss: 1.0787065029144287\n",
      "[step: 921] loss: 1.0757989883422852\n",
      "[step: 922] loss: 1.0731290578842163\n",
      "[step: 923] loss: 1.0701837539672852\n",
      "[step: 924] loss: 1.066770076751709\n",
      "[step: 925] loss: 1.0631029605865479\n",
      "[step: 926] loss: 1.0595393180847168\n",
      "[step: 927] loss: 1.056291937828064\n",
      "[step: 928] loss: 1.053323745727539\n",
      "[step: 929] loss: 1.0504708290100098\n",
      "[step: 930] loss: 1.0475867986679077\n",
      "[step: 931] loss: 1.044584035873413\n",
      "[step: 932] loss: 1.0414512157440186\n",
      "[step: 933] loss: 1.0382170677185059\n",
      "[step: 934] loss: 1.0349466800689697\n",
      "[step: 935] loss: 1.0316945314407349\n",
      "[step: 936] loss: 1.028503656387329\n",
      "[step: 937] loss: 1.0253818035125732\n",
      "[step: 938] loss: 1.0223263502120972\n",
      "[step: 939] loss: 1.019322156906128\n",
      "[step: 940] loss: 1.0163593292236328\n",
      "[step: 941] loss: 1.0134320259094238\n",
      "[step: 942] loss: 1.010540246963501\n",
      "[step: 943] loss: 1.0076967477798462\n",
      "[step: 944] loss: 1.0049301385879517\n",
      "[step: 945] loss: 1.0023225545883179\n",
      "[step: 946] loss: 1.0000768899917603\n",
      "[step: 947] loss: 0.9987254738807678\n",
      "[step: 948] loss: 0.9994276762008667\n",
      "[step: 949] loss: 1.0035512447357178\n",
      "[step: 950] loss: 1.0084948539733887\n",
      "[step: 951] loss: 1.0027693510055542\n",
      "[step: 952] loss: 0.9872820973396301\n",
      "[step: 953] loss: 0.9830268621444702\n",
      "[step: 954] loss: 0.9899832010269165\n",
      "[step: 955] loss: 0.9868745803833008\n",
      "[step: 956] loss: 0.975874125957489\n",
      "[step: 957] loss: 0.9763329029083252\n",
      "[step: 958] loss: 0.9794288277626038\n",
      "[step: 959] loss: 0.9720176458358765\n",
      "[step: 960] loss: 0.9675639867782593\n",
      "[step: 961] loss: 0.970470130443573\n",
      "[step: 962] loss: 0.9672496914863586\n",
      "[step: 963] loss: 0.9614750742912292\n",
      "[step: 964] loss: 0.9622578024864197\n",
      "[step: 965] loss: 0.9616115093231201\n",
      "[step: 966] loss: 0.956545889377594\n",
      "[step: 967] loss: 0.9552206993103027\n",
      "[step: 968] loss: 0.9555562138557434\n",
      "[step: 969] loss: 0.9520385265350342\n",
      "[step: 970] loss: 0.9493129253387451\n",
      "[step: 971] loss: 0.9494215250015259\n",
      "[step: 972] loss: 0.9475240111351013\n",
      "[step: 973] loss: 0.9443737268447876\n",
      "[step: 974] loss: 0.9435446262359619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 975] loss: 0.942743182182312\n",
      "[step: 976] loss: 0.94010329246521\n",
      "[step: 977] loss: 0.9382714033126831\n",
      "[step: 978] loss: 0.9376769065856934\n",
      "[step: 979] loss: 0.9360369443893433\n",
      "[step: 980] loss: 0.9338064193725586\n",
      "[step: 981] loss: 0.9326558709144592\n",
      "[step: 982] loss: 0.9317359924316406\n",
      "[step: 983] loss: 0.9299378395080566\n",
      "[step: 984] loss: 0.9281895756721497\n",
      "[step: 985] loss: 0.9271885752677917\n",
      "[step: 986] loss: 0.9260748624801636\n",
      "[step: 987] loss: 0.9244130253791809\n",
      "[step: 988] loss: 0.9229297637939453\n",
      "[step: 989] loss: 0.9219143390655518\n",
      "[step: 990] loss: 0.9207836985588074\n",
      "[step: 991] loss: 0.9193106293678284\n",
      "[step: 992] loss: 0.917942464351654\n",
      "[step: 993] loss: 0.9168891906738281\n",
      "[step: 994] loss: 0.9158235788345337\n",
      "[step: 995] loss: 0.9145321249961853\n",
      "[step: 996] loss: 0.9132301807403564\n",
      "[step: 997] loss: 0.9121298789978027\n",
      "[step: 998] loss: 0.9111204743385315\n",
      "[step: 999] loss: 0.9100029468536377\n",
      "[step: 1000] loss: 0.9087965488433838\n",
      "[step: 1001] loss: 0.9076590538024902\n",
      "[step: 1002] loss: 0.906640887260437\n",
      "[step: 1003] loss: 0.9056466221809387\n",
      "[step: 1004] loss: 0.9045872688293457\n",
      "[step: 1005] loss: 0.903488278388977\n",
      "[step: 1006] loss: 0.902431845664978\n",
      "[step: 1007] loss: 0.9014467597007751\n",
      "[step: 1008] loss: 0.9004934430122375\n",
      "[step: 1009] loss: 0.8995190262794495\n",
      "[step: 1010] loss: 0.8985170722007751\n",
      "[step: 1011] loss: 0.8975176811218262\n",
      "[step: 1012] loss: 0.8965538144111633\n",
      "[step: 1013] loss: 0.8956267833709717\n",
      "[step: 1014] loss: 0.8947187066078186\n",
      "[step: 1015] loss: 0.8938086032867432\n",
      "[step: 1016] loss: 0.892888605594635\n",
      "[step: 1017] loss: 0.8919684886932373\n",
      "[step: 1018] loss: 0.8910589218139648\n",
      "[step: 1019] loss: 0.8901691436767578\n",
      "[step: 1020] loss: 0.8892995119094849\n",
      "[step: 1021] loss: 0.8884443044662476\n",
      "[step: 1022] loss: 0.8875969648361206\n",
      "[step: 1023] loss: 0.8867536783218384\n",
      "[step: 1024] loss: 0.8859130144119263\n",
      "[step: 1025] loss: 0.8850758075714111\n",
      "[step: 1026] loss: 0.8842453956604004\n",
      "[step: 1027] loss: 0.8834210634231567\n",
      "[step: 1028] loss: 0.8826051354408264\n",
      "[step: 1029] loss: 0.881798505783081\n",
      "[step: 1030] loss: 0.8810006380081177\n",
      "[step: 1031] loss: 0.8802103996276855\n",
      "[step: 1032] loss: 0.8794276714324951\n",
      "[step: 1033] loss: 0.8786523342132568\n",
      "[step: 1034] loss: 0.8778834342956543\n",
      "[step: 1035] loss: 0.8771218061447144\n",
      "[step: 1036] loss: 0.8763673305511475\n",
      "[step: 1037] loss: 0.8756194114685059\n",
      "[step: 1038] loss: 0.8748791217803955\n",
      "[step: 1039] loss: 0.8741484880447388\n",
      "[step: 1040] loss: 0.8734300136566162\n",
      "[step: 1041] loss: 0.872731626033783\n",
      "[step: 1042] loss: 0.8720648288726807\n",
      "[step: 1043] loss: 0.8714607954025269\n",
      "[step: 1044] loss: 0.8709812760353088\n",
      "[step: 1045] loss: 0.870756208896637\n",
      "[step: 1046] loss: 0.8710637092590332\n",
      "[step: 1047] loss: 0.872369647026062\n",
      "[step: 1048] loss: 0.8754063844680786\n",
      "[step: 1049] loss: 0.8799039721488953\n",
      "[step: 1050] loss: 0.8833879828453064\n",
      "[step: 1051] loss: 0.8795183897018433\n",
      "[step: 1052] loss: 0.8698739409446716\n",
      "[step: 1053] loss: 0.8645898103713989\n",
      "[step: 1054] loss: 0.868267834186554\n",
      "[step: 1055] loss: 0.8723784685134888\n",
      "[step: 1056] loss: 0.8686898350715637\n",
      "[step: 1057] loss: 0.8627327680587769\n",
      "[step: 1058] loss: 0.8630872964859009\n",
      "[step: 1059] loss: 0.8663372993469238\n",
      "[step: 1060] loss: 0.8647551536560059\n",
      "[step: 1061] loss: 0.8604236841201782\n",
      "[step: 1062] loss: 0.8602545261383057\n",
      "[step: 1063] loss: 0.8623265027999878\n",
      "[step: 1064] loss: 0.8609386682510376\n",
      "[step: 1065] loss: 0.8579486608505249\n",
      "[step: 1066] loss: 0.8579081296920776\n",
      "[step: 1067] loss: 0.8590238094329834\n",
      "[step: 1068] loss: 0.8577009439468384\n",
      "[step: 1069] loss: 0.8556302785873413\n",
      "[step: 1070] loss: 0.8556655645370483\n",
      "[step: 1071] loss: 0.856147289276123\n",
      "[step: 1072] loss: 0.8548915386199951\n",
      "[step: 1073] loss: 0.8534468412399292\n",
      "[step: 1074] loss: 0.8534398078918457\n",
      "[step: 1075] loss: 0.853500247001648\n",
      "[step: 1076] loss: 0.8524136543273926\n",
      "[step: 1077] loss: 0.8513469696044922\n",
      "[step: 1078] loss: 0.8512288331985474\n",
      "[step: 1079] loss: 0.8510653972625732\n",
      "[step: 1080] loss: 0.8501430749893188\n",
      "[step: 1081] loss: 0.849287748336792\n",
      "[step: 1082] loss: 0.8490524888038635\n",
      "[step: 1083] loss: 0.8487719297409058\n",
      "[step: 1084] loss: 0.8480013608932495\n",
      "[step: 1085] loss: 0.8472580909729004\n",
      "[step: 1086] loss: 0.8469204306602478\n",
      "[step: 1087] loss: 0.8465893864631653\n",
      "[step: 1088] loss: 0.8459364175796509\n",
      "[step: 1089] loss: 0.8452565670013428\n",
      "[step: 1090] loss: 0.8448383808135986\n",
      "[step: 1091] loss: 0.8444762229919434\n",
      "[step: 1092] loss: 0.8439159989356995\n",
      "[step: 1093] loss: 0.8432844281196594\n",
      "[step: 1094] loss: 0.8428053855895996\n",
      "[step: 1095] loss: 0.8424137234687805\n",
      "[step: 1096] loss: 0.8419201970100403\n",
      "[step: 1097] loss: 0.8413373231887817\n",
      "[step: 1098] loss: 0.8408188223838806\n",
      "[step: 1099] loss: 0.840390682220459\n",
      "[step: 1100] loss: 0.8399368524551392\n",
      "[step: 1101] loss: 0.8394052982330322\n",
      "[step: 1102] loss: 0.8388735055923462\n",
      "[step: 1103] loss: 0.8384050130844116\n",
      "[step: 1104] loss: 0.8379608392715454\n",
      "[step: 1105] loss: 0.8374752998352051\n",
      "[step: 1106] loss: 0.8369573354721069\n",
      "[step: 1107] loss: 0.8364599943161011\n",
      "[step: 1108] loss: 0.8359991908073425\n",
      "[step: 1109] loss: 0.8355386257171631\n",
      "[step: 1110] loss: 0.8350492715835571\n",
      "[step: 1111] loss: 0.8345490097999573\n",
      "[step: 1112] loss: 0.8340657353401184\n",
      "[step: 1113] loss: 0.833601713180542\n",
      "[step: 1114] loss: 0.833134651184082\n",
      "[step: 1115] loss: 0.8326511979103088\n",
      "[step: 1116] loss: 0.8321613073348999\n",
      "[step: 1117] loss: 0.831682562828064\n",
      "[step: 1118] loss: 0.831213653087616\n",
      "[step: 1119] loss: 0.8307457566261292\n",
      "[step: 1120] loss: 0.8302678465843201\n",
      "[step: 1121] loss: 0.8297855854034424\n",
      "[step: 1122] loss: 0.8293060660362244\n",
      "[step: 1123] loss: 0.8288344740867615\n",
      "[step: 1124] loss: 0.8283649682998657\n",
      "[step: 1125] loss: 0.8278920650482178\n",
      "[step: 1126] loss: 0.8274143934249878\n",
      "[step: 1127] loss: 0.8269363641738892\n",
      "[step: 1128] loss: 0.826461136341095\n",
      "[step: 1129] loss: 0.8259897232055664\n",
      "[step: 1130] loss: 0.8255175352096558\n",
      "[step: 1131] loss: 0.8250437378883362\n",
      "[step: 1132] loss: 0.8245670795440674\n",
      "[step: 1133] loss: 0.8240911364555359\n",
      "[step: 1134] loss: 0.8236159086227417\n",
      "[step: 1135] loss: 0.8231426477432251\n",
      "[step: 1136] loss: 0.8226691484451294\n",
      "[step: 1137] loss: 0.8221948146820068\n",
      "[step: 1138] loss: 0.8217191696166992\n",
      "[step: 1139] loss: 0.8212429285049438\n",
      "[step: 1140] loss: 0.8207666873931885\n",
      "[step: 1141] loss: 0.8202909827232361\n",
      "[step: 1142] loss: 0.8198162317276001\n",
      "[step: 1143] loss: 0.8193413019180298\n",
      "[step: 1144] loss: 0.8188650608062744\n",
      "[step: 1145] loss: 0.8183887600898743\n",
      "[step: 1146] loss: 0.8179111480712891\n",
      "[step: 1147] loss: 0.8174341917037964\n",
      "[step: 1148] loss: 0.816956639289856\n",
      "[step: 1149] loss: 0.8164793252944946\n",
      "[step: 1150] loss: 0.816002607345581\n",
      "[step: 1151] loss: 0.8155242800712585\n",
      "[step: 1152] loss: 0.8150462508201599\n",
      "[step: 1153] loss: 0.814568281173706\n",
      "[step: 1154] loss: 0.8140892386436462\n",
      "[step: 1155] loss: 0.8136101961135864\n",
      "[step: 1156] loss: 0.8131309747695923\n",
      "[step: 1157] loss: 0.8126514554023743\n",
      "[step: 1158] loss: 0.8121725916862488\n",
      "[step: 1159] loss: 0.8116929531097412\n",
      "[step: 1160] loss: 0.8112127780914307\n",
      "[step: 1161] loss: 0.8107329607009888\n",
      "[step: 1162] loss: 0.8102525472640991\n",
      "[step: 1163] loss: 0.8097725510597229\n",
      "[step: 1164] loss: 0.809291422367096\n",
      "[step: 1165] loss: 0.8088103532791138\n",
      "[step: 1166] loss: 0.8083293437957764\n",
      "[step: 1167] loss: 0.8078484535217285\n",
      "[step: 1168] loss: 0.8073670268058777\n",
      "[step: 1169] loss: 0.8068853616714478\n",
      "[step: 1170] loss: 0.8064044713973999\n",
      "[step: 1171] loss: 0.8059227466583252\n",
      "[step: 1172] loss: 0.8054410219192505\n",
      "[step: 1173] loss: 0.8049589395523071\n",
      "[step: 1174] loss: 0.8044775724411011\n",
      "[step: 1175] loss: 0.8039960861206055\n",
      "[step: 1176] loss: 0.8035141229629517\n",
      "[step: 1177] loss: 0.8030326962471008\n",
      "[step: 1178] loss: 0.8025517463684082\n",
      "[step: 1179] loss: 0.8020709753036499\n",
      "[step: 1180] loss: 0.8015909194946289\n",
      "[step: 1181] loss: 0.8011131882667542\n",
      "[step: 1182] loss: 0.8006374835968018\n",
      "[step: 1183] loss: 0.8001673221588135\n",
      "[step: 1184] loss: 0.7997068762779236\n",
      "[step: 1185] loss: 0.7992649078369141\n",
      "[step: 1186] loss: 0.7988603115081787\n",
      "[step: 1187] loss: 0.7985290884971619\n",
      "[step: 1188] loss: 0.798344612121582\n",
      "[step: 1189] loss: 0.7984510660171509\n",
      "[step: 1190] loss: 0.7991358041763306\n",
      "[step: 1191] loss: 0.800835371017456\n",
      "[step: 1192] loss: 0.8041929006576538\n",
      "[step: 1193] loss: 0.8088429570198059\n",
      "[step: 1194] loss: 0.8126657605171204\n",
      "[step: 1195] loss: 0.8098878860473633\n",
      "[step: 1196] loss: 0.8008628487586975\n",
      "[step: 1197] loss: 0.7938735485076904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1198] loss: 0.7956525683403015\n",
      "[step: 1199] loss: 0.8009317517280579\n",
      "[step: 1200] loss: 0.8004135489463806\n",
      "[step: 1201] loss: 0.794452428817749\n",
      "[step: 1202] loss: 0.7915626764297485\n",
      "[step: 1203] loss: 0.7944263815879822\n",
      "[step: 1204] loss: 0.7962478399276733\n",
      "[step: 1205] loss: 0.7929859757423401\n",
      "[step: 1206] loss: 0.7898433208465576\n",
      "[step: 1207] loss: 0.7909303903579712\n",
      "[step: 1208] loss: 0.7925117015838623\n",
      "[step: 1209] loss: 0.7907559871673584\n",
      "[step: 1210] loss: 0.7882549166679382\n",
      "[step: 1211] loss: 0.7885112762451172\n",
      "[step: 1212] loss: 0.7895941734313965\n",
      "[step: 1213] loss: 0.7884927988052368\n",
      "[step: 1214] loss: 0.7866191864013672\n",
      "[step: 1215] loss: 0.7864835262298584\n",
      "[step: 1216] loss: 0.7871063947677612\n",
      "[step: 1217] loss: 0.7863931059837341\n",
      "[step: 1218] loss: 0.7849750518798828\n",
      "[step: 1219] loss: 0.7846218943595886\n",
      "[step: 1220] loss: 0.7849221229553223\n",
      "[step: 1221] loss: 0.784419596195221\n",
      "[step: 1222] loss: 0.7833342552185059\n",
      "[step: 1223] loss: 0.7828474044799805\n",
      "[step: 1224] loss: 0.7829040288925171\n",
      "[step: 1225] loss: 0.7825478315353394\n",
      "[step: 1226] loss: 0.7817030549049377\n",
      "[step: 1227] loss: 0.7811362147331238\n",
      "[step: 1228] loss: 0.7810066938400269\n",
      "[step: 1229] loss: 0.7807250618934631\n",
      "[step: 1230] loss: 0.7800719141960144\n",
      "[step: 1231] loss: 0.7794746160507202\n",
      "[step: 1232] loss: 0.7791939973831177\n",
      "[step: 1233] loss: 0.7789349555969238\n",
      "[step: 1234] loss: 0.7784289717674255\n",
      "[step: 1235] loss: 0.7778500914573669\n",
      "[step: 1236] loss: 0.7774530649185181\n",
      "[step: 1237] loss: 0.7771660089492798\n",
      "[step: 1238] loss: 0.7767643928527832\n",
      "[step: 1239] loss: 0.7762423753738403\n",
      "[step: 1240] loss: 0.7757769823074341\n",
      "[step: 1241] loss: 0.7754271626472473\n",
      "[step: 1242] loss: 0.7750731110572815\n",
      "[step: 1243] loss: 0.7746251225471497\n",
      "[step: 1244] loss: 0.7741470336914062\n",
      "[step: 1245] loss: 0.773733377456665\n",
      "[step: 1246] loss: 0.773370087146759\n",
      "[step: 1247] loss: 0.7729774713516235\n",
      "[step: 1248] loss: 0.7725328207015991\n",
      "[step: 1249] loss: 0.7720871567726135\n",
      "[step: 1250] loss: 0.7716849446296692\n",
      "[step: 1251] loss: 0.7713030576705933\n",
      "[step: 1252] loss: 0.7708995342254639\n",
      "[step: 1253] loss: 0.7704662680625916\n",
      "[step: 1254] loss: 0.7700357437133789\n",
      "[step: 1255] loss: 0.7696316242218018\n",
      "[step: 1256] loss: 0.7692386507987976\n",
      "[step: 1257] loss: 0.768833577632904\n",
      "[step: 1258] loss: 0.7684107422828674\n",
      "[step: 1259] loss: 0.7679882645606995\n",
      "[step: 1260] loss: 0.7675783634185791\n",
      "[step: 1261] loss: 0.767177939414978\n",
      "[step: 1262] loss: 0.7667737007141113\n",
      "[step: 1263] loss: 0.7663585543632507\n",
      "[step: 1264] loss: 0.7659397125244141\n",
      "[step: 1265] loss: 0.7655261158943176\n",
      "[step: 1266] loss: 0.7651203870773315\n",
      "[step: 1267] loss: 0.764714241027832\n",
      "[step: 1268] loss: 0.7643052339553833\n",
      "[step: 1269] loss: 0.7638907432556152\n",
      "[step: 1270] loss: 0.7634758949279785\n",
      "[step: 1271] loss: 0.7630643248558044\n",
      "[step: 1272] loss: 0.7626566886901855\n",
      "[step: 1273] loss: 0.7622484564781189\n",
      "[step: 1274] loss: 0.7618375420570374\n",
      "[step: 1275] loss: 0.7614237070083618\n",
      "[step: 1276] loss: 0.7610106468200684\n",
      "[step: 1277] loss: 0.7605984807014465\n",
      "[step: 1278] loss: 0.7601881623268127\n",
      "[step: 1279] loss: 0.7597780227661133\n",
      "[step: 1280] loss: 0.7593661546707153\n",
      "[step: 1281] loss: 0.7589538097381592\n",
      "[step: 1282] loss: 0.7585405707359314\n",
      "[step: 1283] loss: 0.7581273317337036\n",
      "[step: 1284] loss: 0.7577143311500549\n",
      "[step: 1285] loss: 0.7573026418685913\n",
      "[step: 1286] loss: 0.7568906545639038\n",
      "[step: 1287] loss: 0.7564777135848999\n",
      "[step: 1288] loss: 0.75606369972229\n",
      "[step: 1289] loss: 0.7556493878364563\n",
      "[step: 1290] loss: 0.7552355527877808\n",
      "[step: 1291] loss: 0.7548213005065918\n",
      "[step: 1292] loss: 0.7544068694114685\n",
      "[step: 1293] loss: 0.753993034362793\n",
      "[step: 1294] loss: 0.753578245639801\n",
      "[step: 1295] loss: 0.7531632781028748\n",
      "[step: 1296] loss: 0.7527478337287903\n",
      "[step: 1297] loss: 0.7523318529129028\n",
      "[step: 1298] loss: 0.7519159317016602\n",
      "[step: 1299] loss: 0.7514989376068115\n",
      "[step: 1300] loss: 0.7510827779769897\n",
      "[step: 1301] loss: 0.7506662607192993\n",
      "[step: 1302] loss: 0.7502488493919373\n",
      "[step: 1303] loss: 0.7498317360877991\n",
      "[step: 1304] loss: 0.7494136691093445\n",
      "[step: 1305] loss: 0.748995840549469\n",
      "[step: 1306] loss: 0.7485776543617249\n",
      "[step: 1307] loss: 0.7481580972671509\n",
      "[step: 1308] loss: 0.7477393746376038\n",
      "[step: 1309] loss: 0.747319757938385\n",
      "[step: 1310] loss: 0.746900200843811\n",
      "[step: 1311] loss: 0.7464798092842102\n",
      "[step: 1312] loss: 0.7460594177246094\n",
      "[step: 1313] loss: 0.7456386089324951\n",
      "[step: 1314] loss: 0.7452173829078674\n",
      "[step: 1315] loss: 0.7447959184646606\n",
      "[step: 1316] loss: 0.7443742752075195\n",
      "[step: 1317] loss: 0.7439512610435486\n",
      "[step: 1318] loss: 0.7435287237167358\n",
      "[step: 1319] loss: 0.7431052923202515\n",
      "[step: 1320] loss: 0.7426818609237671\n",
      "[step: 1321] loss: 0.7422586679458618\n",
      "[step: 1322] loss: 0.7418340444564819\n",
      "[step: 1323] loss: 0.7414098381996155\n",
      "[step: 1324] loss: 0.7409846186637878\n",
      "[step: 1325] loss: 0.7405595183372498\n",
      "[step: 1326] loss: 0.7401338219642639\n",
      "[step: 1327] loss: 0.7397080063819885\n",
      "[step: 1328] loss: 0.7392815351486206\n",
      "[step: 1329] loss: 0.7388557195663452\n",
      "[step: 1330] loss: 0.738429605960846\n",
      "[step: 1331] loss: 0.7380034327507019\n",
      "[step: 1332] loss: 0.7375785708427429\n",
      "[step: 1333] loss: 0.7371553778648376\n",
      "[step: 1334] loss: 0.7367343902587891\n",
      "[step: 1335] loss: 0.7363200187683105\n",
      "[step: 1336] loss: 0.7359171509742737\n",
      "[step: 1337] loss: 0.7355331182479858\n",
      "[step: 1338] loss: 0.7351866960525513\n",
      "[step: 1339] loss: 0.7349072098731995\n",
      "[step: 1340] loss: 0.7347557544708252\n",
      "[step: 1341] loss: 0.7348337769508362\n",
      "[step: 1342] loss: 0.7353368401527405\n",
      "[step: 1343] loss: 0.7365281581878662\n",
      "[step: 1344] loss: 0.7388086318969727\n",
      "[step: 1345] loss: 0.7420729398727417\n",
      "[step: 1346] loss: 0.7455227971076965\n",
      "[step: 1347] loss: 0.7458667755126953\n",
      "[step: 1348] loss: 0.7413193583488464\n",
      "[step: 1349] loss: 0.7338370084762573\n",
      "[step: 1350] loss: 0.729951798915863\n",
      "[step: 1351] loss: 0.7319345474243164\n",
      "[step: 1352] loss: 0.7355347871780396\n",
      "[step: 1353] loss: 0.7354522943496704\n",
      "[step: 1354] loss: 0.7312228083610535\n",
      "[step: 1355] loss: 0.7280017137527466\n",
      "[step: 1356] loss: 0.7288346290588379\n",
      "[step: 1357] loss: 0.7309516668319702\n",
      "[step: 1358] loss: 0.7304186224937439\n",
      "[step: 1359] loss: 0.7275289297103882\n",
      "[step: 1360] loss: 0.7259529829025269\n",
      "[step: 1361] loss: 0.7268513441085815\n",
      "[step: 1362] loss: 0.7276750802993774\n",
      "[step: 1363] loss: 0.7264924645423889\n",
      "[step: 1364] loss: 0.7246019840240479\n",
      "[step: 1365] loss: 0.7241519093513489\n",
      "[step: 1366] loss: 0.7247970700263977\n",
      "[step: 1367] loss: 0.724667489528656\n",
      "[step: 1368] loss: 0.7234007120132446\n",
      "[step: 1369] loss: 0.7223609685897827\n",
      "[step: 1370] loss: 0.722346842288971\n",
      "[step: 1371] loss: 0.722520649433136\n",
      "[step: 1372] loss: 0.7219346761703491\n",
      "[step: 1373] loss: 0.7209339141845703\n",
      "[step: 1374] loss: 0.7203969955444336\n",
      "[step: 1375] loss: 0.7203776240348816\n",
      "[step: 1376] loss: 0.7201891541481018\n",
      "[step: 1377] loss: 0.719516396522522\n",
      "[step: 1378] loss: 0.7187970876693726\n",
      "[step: 1379] loss: 0.7184505462646484\n",
      "[step: 1380] loss: 0.7182999849319458\n",
      "[step: 1381] loss: 0.717936635017395\n",
      "[step: 1382] loss: 0.7173242568969727\n",
      "[step: 1383] loss: 0.716776967048645\n",
      "[step: 1384] loss: 0.716461181640625\n",
      "[step: 1385] loss: 0.7162023782730103\n",
      "[step: 1386] loss: 0.7157881259918213\n",
      "[step: 1387] loss: 0.7152519226074219\n",
      "[step: 1388] loss: 0.7147827744483948\n",
      "[step: 1389] loss: 0.7144466638565063\n",
      "[step: 1390] loss: 0.7141280174255371\n",
      "[step: 1391] loss: 0.7137126922607422\n",
      "[step: 1392] loss: 0.7132314443588257\n",
      "[step: 1393] loss: 0.7127922177314758\n",
      "[step: 1394] loss: 0.7124305367469788\n",
      "[step: 1395] loss: 0.7120806574821472\n",
      "[step: 1396] loss: 0.7116785049438477\n",
      "[step: 1397] loss: 0.7112334966659546\n",
      "[step: 1398] loss: 0.7108054161071777\n",
      "[step: 1399] loss: 0.7104225158691406\n",
      "[step: 1400] loss: 0.7100566625595093\n",
      "[step: 1401] loss: 0.7096667289733887\n",
      "[step: 1402] loss: 0.7092470526695251\n",
      "[step: 1403] loss: 0.7088270783424377\n",
      "[step: 1404] loss: 0.7084298133850098\n",
      "[step: 1405] loss: 0.7080509662628174\n",
      "[step: 1406] loss: 0.7076678276062012\n",
      "[step: 1407] loss: 0.7072668075561523\n",
      "[step: 1408] loss: 0.7068566679954529\n",
      "[step: 1409] loss: 0.7064534425735474\n",
      "[step: 1410] loss: 0.7060630321502686\n",
      "[step: 1411] loss: 0.7056798338890076\n",
      "[step: 1412] loss: 0.7052912712097168\n",
      "[step: 1413] loss: 0.7048927545547485\n",
      "[step: 1414] loss: 0.7044922113418579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1415] loss: 0.7040961384773254\n",
      "[step: 1416] loss: 0.7037073373794556\n",
      "[step: 1417] loss: 0.7033221125602722\n",
      "[step: 1418] loss: 0.7029337882995605\n",
      "[step: 1419] loss: 0.7025414109230042\n",
      "[step: 1420] loss: 0.7021474838256836\n",
      "[step: 1421] loss: 0.7017552852630615\n",
      "[step: 1422] loss: 0.7013676166534424\n",
      "[step: 1423] loss: 0.7009819746017456\n",
      "[step: 1424] loss: 0.70059734582901\n",
      "[step: 1425] loss: 0.7002103328704834\n",
      "[step: 1426] loss: 0.6998223066329956\n",
      "[step: 1427] loss: 0.6994338035583496\n",
      "[step: 1428] loss: 0.6990479230880737\n",
      "[step: 1429] loss: 0.6986646056175232\n",
      "[step: 1430] loss: 0.6982821822166443\n",
      "[step: 1431] loss: 0.6979002952575684\n",
      "[step: 1432] loss: 0.6975181102752686\n",
      "[step: 1433] loss: 0.697135329246521\n",
      "[step: 1434] loss: 0.6967534422874451\n",
      "[step: 1435] loss: 0.6963728070259094\n",
      "[step: 1436] loss: 0.695993185043335\n",
      "[step: 1437] loss: 0.6956149935722351\n",
      "[step: 1438] loss: 0.6952379941940308\n",
      "[step: 1439] loss: 0.6948606967926025\n",
      "[step: 1440] loss: 0.6944847106933594\n",
      "[step: 1441] loss: 0.6941086649894714\n",
      "[step: 1442] loss: 0.6937334537506104\n",
      "[step: 1443] loss: 0.6933589577674866\n",
      "[step: 1444] loss: 0.6929849982261658\n",
      "[step: 1445] loss: 0.6926137208938599\n",
      "[step: 1446] loss: 0.6922418475151062\n",
      "[step: 1447] loss: 0.6918721199035645\n",
      "[step: 1448] loss: 0.6915026903152466\n",
      "[step: 1449] loss: 0.6911340951919556\n",
      "[step: 1450] loss: 0.6907657384872437\n",
      "[step: 1451] loss: 0.6903989315032959\n",
      "[step: 1452] loss: 0.6900328993797302\n",
      "[step: 1453] loss: 0.6896680593490601\n",
      "[step: 1454] loss: 0.6893037557601929\n",
      "[step: 1455] loss: 0.6889405250549316\n",
      "[step: 1456] loss: 0.688578724861145\n",
      "[step: 1457] loss: 0.6882175207138062\n",
      "[step: 1458] loss: 0.6878578662872314\n",
      "[step: 1459] loss: 0.6874988079071045\n",
      "[step: 1460] loss: 0.6871404647827148\n",
      "[step: 1461] loss: 0.6867835521697998\n",
      "[step: 1462] loss: 0.6864275932312012\n",
      "[step: 1463] loss: 0.6860727071762085\n",
      "[step: 1464] loss: 0.6857186555862427\n",
      "[step: 1465] loss: 0.6853660345077515\n",
      "[step: 1466] loss: 0.6850133538246155\n",
      "[step: 1467] loss: 0.6846633553504944\n",
      "[step: 1468] loss: 0.6843134164810181\n",
      "[step: 1469] loss: 0.6839649081230164\n",
      "[step: 1470] loss: 0.6836179494857788\n",
      "[step: 1471] loss: 0.683271050453186\n",
      "[step: 1472] loss: 0.6829259991645813\n",
      "[step: 1473] loss: 0.6825820207595825\n",
      "[step: 1474] loss: 0.6822391748428345\n",
      "[step: 1475] loss: 0.6818966865539551\n",
      "[step: 1476] loss: 0.6815565228462219\n",
      "[step: 1477] loss: 0.681216835975647\n",
      "[step: 1478] loss: 0.6808789372444153\n",
      "[step: 1479] loss: 0.680543065071106\n",
      "[step: 1480] loss: 0.6802077889442444\n",
      "[step: 1481] loss: 0.6798758506774902\n",
      "[step: 1482] loss: 0.679546594619751\n",
      "[step: 1483] loss: 0.6792215704917908\n",
      "[step: 1484] loss: 0.6789041757583618\n",
      "[step: 1485] loss: 0.6785974502563477\n",
      "[step: 1486] loss: 0.6783090233802795\n",
      "[step: 1487] loss: 0.6780547499656677\n",
      "[step: 1488] loss: 0.6778584718704224\n",
      "[step: 1489] loss: 0.6777692437171936\n",
      "[step: 1490] loss: 0.6778665781021118\n",
      "[step: 1491] loss: 0.6783012747764587\n",
      "[step: 1492] loss: 0.6792809367179871\n",
      "[step: 1493] loss: 0.6811145544052124\n",
      "[step: 1494] loss: 0.683847188949585\n",
      "[step: 1495] loss: 0.6870734691619873\n",
      "[step: 1496] loss: 0.6886220574378967\n",
      "[step: 1497] loss: 0.6864872574806213\n",
      "[step: 1498] loss: 0.680416464805603\n",
      "[step: 1499] loss: 0.6750184297561646\n",
      "[step: 1500] loss: 0.6742538213729858\n",
      "[step: 1501] loss: 0.6772387027740479\n",
      "[step: 1502] loss: 0.679559588432312\n",
      "[step: 1503] loss: 0.6779829859733582\n",
      "[step: 1504] loss: 0.6742388010025024\n",
      "[step: 1505] loss: 0.6723683476448059\n",
      "[step: 1506] loss: 0.6736252307891846\n",
      "[step: 1507] loss: 0.675258994102478\n",
      "[step: 1508] loss: 0.6745060682296753\n",
      "[step: 1509] loss: 0.672166109085083\n",
      "[step: 1510] loss: 0.6709520816802979\n",
      "[step: 1511] loss: 0.6716818809509277\n",
      "[step: 1512] loss: 0.6724926233291626\n",
      "[step: 1513] loss: 0.6717513799667358\n",
      "[step: 1514] loss: 0.6702253222465515\n",
      "[step: 1515] loss: 0.6695971488952637\n",
      "[step: 1516] loss: 0.6700547933578491\n",
      "[step: 1517] loss: 0.6703053712844849\n",
      "[step: 1518] loss: 0.6695771217346191\n",
      "[step: 1519] loss: 0.668584942817688\n",
      "[step: 1520] loss: 0.6682713627815247\n",
      "[step: 1521] loss: 0.6684940457344055\n",
      "[step: 1522] loss: 0.6684260368347168\n",
      "[step: 1523] loss: 0.667789101600647\n",
      "[step: 1524] loss: 0.6671371459960938\n",
      "[step: 1525] loss: 0.6669410467147827\n",
      "[step: 1526] loss: 0.6669772267341614\n",
      "[step: 1527] loss: 0.6667629480361938\n",
      "[step: 1528] loss: 0.6662414073944092\n",
      "[step: 1529] loss: 0.6657809615135193\n",
      "[step: 1530] loss: 0.6656007766723633\n",
      "[step: 1531] loss: 0.6655135750770569\n",
      "[step: 1532] loss: 0.6652520895004272\n",
      "[step: 1533] loss: 0.664827287197113\n",
      "[step: 1534] loss: 0.6644635200500488\n",
      "[step: 1535] loss: 0.6642651557922363\n",
      "[step: 1536] loss: 0.664107084274292\n",
      "[step: 1537] loss: 0.6638416647911072\n",
      "[step: 1538] loss: 0.6634849905967712\n",
      "[step: 1539] loss: 0.6631667613983154\n",
      "[step: 1540] loss: 0.6629447937011719\n",
      "[step: 1541] loss: 0.6627496480941772\n",
      "[step: 1542] loss: 0.6624927520751953\n",
      "[step: 1543] loss: 0.6621808409690857\n",
      "[step: 1544] loss: 0.6618853807449341\n",
      "[step: 1545] loss: 0.6616472005844116\n",
      "[step: 1546] loss: 0.6614303588867188\n",
      "[step: 1547] loss: 0.6611827611923218\n",
      "[step: 1548] loss: 0.6608996391296387\n",
      "[step: 1549] loss: 0.6606189012527466\n",
      "[step: 1550] loss: 0.6603692770004272\n",
      "[step: 1551] loss: 0.6601393818855286\n",
      "[step: 1552] loss: 0.6598981618881226\n",
      "[step: 1553] loss: 0.6596341133117676\n",
      "[step: 1554] loss: 0.659364640712738\n",
      "[step: 1555] loss: 0.6591095328330994\n",
      "[step: 1556] loss: 0.6588701009750366\n",
      "[step: 1557] loss: 0.6586307287216187\n",
      "[step: 1558] loss: 0.6583789587020874\n",
      "[step: 1559] loss: 0.6581192016601562\n",
      "[step: 1560] loss: 0.6578634977340698\n",
      "[step: 1561] loss: 0.657618522644043\n",
      "[step: 1562] loss: 0.6573776006698608\n",
      "[step: 1563] loss: 0.6571332216262817\n",
      "[step: 1564] loss: 0.65688157081604\n",
      "[step: 1565] loss: 0.6566280126571655\n",
      "[step: 1566] loss: 0.6563796401023865\n",
      "[step: 1567] loss: 0.6561359167098999\n",
      "[step: 1568] loss: 0.6558932662010193\n",
      "[step: 1569] loss: 0.6556477546691895\n",
      "[step: 1570] loss: 0.6553982496261597\n",
      "[step: 1571] loss: 0.655149519443512\n",
      "[step: 1572] loss: 0.6549043655395508\n",
      "[step: 1573] loss: 0.6546610593795776\n",
      "[step: 1574] loss: 0.6544173955917358\n",
      "[step: 1575] loss: 0.6541731357574463\n",
      "[step: 1576] loss: 0.6539257764816284\n",
      "[step: 1577] loss: 0.6536796689033508\n",
      "[step: 1578] loss: 0.6534351706504822\n",
      "[step: 1579] loss: 0.653191328048706\n",
      "[step: 1580] loss: 0.6529472470283508\n",
      "[step: 1581] loss: 0.6527038812637329\n",
      "[step: 1582] loss: 0.6524584889411926\n",
      "[step: 1583] loss: 0.6522132158279419\n",
      "[step: 1584] loss: 0.6519689559936523\n",
      "[step: 1585] loss: 0.651725172996521\n",
      "[step: 1586] loss: 0.6514818668365479\n",
      "[step: 1587] loss: 0.6512373685836792\n",
      "[step: 1588] loss: 0.6509931087493896\n",
      "[step: 1589] loss: 0.6507483124732971\n",
      "[step: 1590] loss: 0.6505041122436523\n",
      "[step: 1591] loss: 0.6502594947814941\n",
      "[step: 1592] loss: 0.6500158905982971\n",
      "[step: 1593] loss: 0.6497715711593628\n",
      "[step: 1594] loss: 0.6495275497436523\n",
      "[step: 1595] loss: 0.6492828130722046\n",
      "[step: 1596] loss: 0.6490377187728882\n",
      "[step: 1597] loss: 0.6487932205200195\n",
      "[step: 1598] loss: 0.6485486030578613\n",
      "[step: 1599] loss: 0.6483035087585449\n",
      "[step: 1600] loss: 0.6480581760406494\n",
      "[step: 1601] loss: 0.6478133201599121\n",
      "[step: 1602] loss: 0.6475675106048584\n",
      "[step: 1603] loss: 0.6473225355148315\n",
      "[step: 1604] loss: 0.6470769643783569\n",
      "[step: 1605] loss: 0.6468310952186584\n",
      "[step: 1606] loss: 0.6465851068496704\n",
      "[step: 1607] loss: 0.646338939666748\n",
      "[step: 1608] loss: 0.6460926532745361\n",
      "[step: 1609] loss: 0.645845890045166\n",
      "[step: 1610] loss: 0.6455987691879272\n",
      "[step: 1611] loss: 0.6453518867492676\n",
      "[step: 1612] loss: 0.6451045870780945\n",
      "[step: 1613] loss: 0.6448569297790527\n",
      "[step: 1614] loss: 0.6446089744567871\n",
      "[step: 1615] loss: 0.6443613767623901\n",
      "[step: 1616] loss: 0.6441127061843872\n",
      "[step: 1617] loss: 0.6438639163970947\n",
      "[step: 1618] loss: 0.643614649772644\n",
      "[step: 1619] loss: 0.643365740776062\n",
      "[step: 1620] loss: 0.6431161165237427\n",
      "[step: 1621] loss: 0.6428663730621338\n",
      "[step: 1622] loss: 0.6426165103912354\n",
      "[step: 1623] loss: 0.642365574836731\n",
      "[step: 1624] loss: 0.6421145796775818\n",
      "[step: 1625] loss: 0.6418633460998535\n",
      "[step: 1626] loss: 0.6416112184524536\n",
      "[step: 1627] loss: 0.6413596272468567\n",
      "[step: 1628] loss: 0.641106903553009\n",
      "[step: 1629] loss: 0.6408538818359375\n",
      "[step: 1630] loss: 0.6406008005142212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1631] loss: 0.6403472423553467\n",
      "[step: 1632] loss: 0.6400931477546692\n",
      "[step: 1633] loss: 0.6398389339447021\n",
      "[step: 1634] loss: 0.6395837068557739\n",
      "[step: 1635] loss: 0.6393285989761353\n",
      "[step: 1636] loss: 0.6390729546546936\n",
      "[step: 1637] loss: 0.6388168931007385\n",
      "[step: 1638] loss: 0.6385596394538879\n",
      "[step: 1639] loss: 0.6383025646209717\n",
      "[step: 1640] loss: 0.6380449533462524\n",
      "[step: 1641] loss: 0.6377869844436646\n",
      "[step: 1642] loss: 0.6375284194946289\n",
      "[step: 1643] loss: 0.6372691988945007\n",
      "[step: 1644] loss: 0.6370092034339905\n",
      "[step: 1645] loss: 0.636749267578125\n",
      "[step: 1646] loss: 0.6364879012107849\n",
      "[step: 1647] loss: 0.6362276077270508\n",
      "[step: 1648] loss: 0.6359659433364868\n",
      "[step: 1649] loss: 0.6357032060623169\n",
      "[step: 1650] loss: 0.6354405283927917\n",
      "[step: 1651] loss: 0.6351765394210815\n",
      "[step: 1652] loss: 0.6349126100540161\n",
      "[step: 1653] loss: 0.6346484422683716\n",
      "[step: 1654] loss: 0.634382963180542\n",
      "[step: 1655] loss: 0.6341169476509094\n",
      "[step: 1656] loss: 0.6338506937026978\n",
      "[step: 1657] loss: 0.6335837841033936\n",
      "[step: 1658] loss: 0.6333160400390625\n",
      "[step: 1659] loss: 0.6330472230911255\n",
      "[step: 1660] loss: 0.6327786445617676\n",
      "[step: 1661] loss: 0.6325092315673828\n",
      "[step: 1662] loss: 0.6322388648986816\n",
      "[step: 1663] loss: 0.6319677233695984\n",
      "[step: 1664] loss: 0.6316967010498047\n",
      "[step: 1665] loss: 0.631424069404602\n",
      "[step: 1666] loss: 0.6311511993408203\n",
      "[step: 1667] loss: 0.630877673625946\n",
      "[step: 1668] loss: 0.6306031942367554\n",
      "[step: 1669] loss: 0.6303281784057617\n",
      "[step: 1670] loss: 0.6300522685050964\n",
      "[step: 1671] loss: 0.629776120185852\n",
      "[step: 1672] loss: 0.629499077796936\n",
      "[step: 1673] loss: 0.6292204856872559\n",
      "[step: 1674] loss: 0.6289420127868652\n",
      "[step: 1675] loss: 0.6286630630493164\n",
      "[step: 1676] loss: 0.6283826231956482\n",
      "[step: 1677] loss: 0.6281015872955322\n",
      "[step: 1678] loss: 0.6278200149536133\n",
      "[step: 1679] loss: 0.6275375485420227\n",
      "[step: 1680] loss: 0.6272542476654053\n",
      "[step: 1681] loss: 0.6269701719284058\n",
      "[step: 1682] loss: 0.6266849040985107\n",
      "[step: 1683] loss: 0.6263989210128784\n",
      "[step: 1684] loss: 0.626112699508667\n",
      "[step: 1685] loss: 0.6258252263069153\n",
      "[step: 1686] loss: 0.6255366206169128\n",
      "[step: 1687] loss: 0.625247597694397\n",
      "[step: 1688] loss: 0.624957263469696\n",
      "[step: 1689] loss: 0.6246670484542847\n",
      "[step: 1690] loss: 0.6243751049041748\n",
      "[step: 1691] loss: 0.6240825653076172\n",
      "[step: 1692] loss: 0.6237890124320984\n",
      "[step: 1693] loss: 0.6234946250915527\n",
      "[step: 1694] loss: 0.623199462890625\n",
      "[step: 1695] loss: 0.6229027509689331\n",
      "[step: 1696] loss: 0.6226061582565308\n",
      "[step: 1697] loss: 0.6223078370094299\n",
      "[step: 1698] loss: 0.6220090389251709\n",
      "[step: 1699] loss: 0.6217087507247925\n",
      "[step: 1700] loss: 0.6214081048965454\n",
      "[step: 1701] loss: 0.6211065053939819\n",
      "[step: 1702] loss: 0.6208032965660095\n",
      "[step: 1703] loss: 0.620499849319458\n",
      "[step: 1704] loss: 0.620195209980011\n",
      "[step: 1705] loss: 0.6198893785476685\n",
      "[step: 1706] loss: 0.619583010673523\n",
      "[step: 1707] loss: 0.6192757487297058\n",
      "[step: 1708] loss: 0.6189676523208618\n",
      "[step: 1709] loss: 0.6186598539352417\n",
      "[step: 1710] loss: 0.6183524131774902\n",
      "[step: 1711] loss: 0.618045449256897\n",
      "[step: 1712] loss: 0.6177418231964111\n",
      "[step: 1713] loss: 0.6174434423446655\n",
      "[step: 1714] loss: 0.6171560287475586\n",
      "[step: 1715] loss: 0.6168883442878723\n",
      "[step: 1716] loss: 0.6166567802429199\n",
      "[step: 1717] loss: 0.6164912581443787\n",
      "[step: 1718] loss: 0.6164438724517822\n",
      "[step: 1719] loss: 0.6166023015975952\n",
      "[step: 1720] loss: 0.6171107292175293\n",
      "[step: 1721] loss: 0.6181478500366211\n",
      "[step: 1722] loss: 0.6199065446853638\n",
      "[step: 1723] loss: 0.6221905946731567\n",
      "[step: 1724] loss: 0.6242861747741699\n",
      "[step: 1725] loss: 0.6242846250534058\n",
      "[step: 1726] loss: 0.6212356090545654\n",
      "[step: 1727] loss: 0.6162354946136475\n",
      "[step: 1728] loss: 0.612878143787384\n",
      "[step: 1729] loss: 0.6131005883216858\n",
      "[step: 1730] loss: 0.6154094934463501\n",
      "[step: 1731] loss: 0.6166762113571167\n",
      "[step: 1732] loss: 0.6151243448257446\n",
      "[step: 1733] loss: 0.6122527718544006\n",
      "[step: 1734] loss: 0.6107881665229797\n",
      "[step: 1735] loss: 0.6115248203277588\n",
      "[step: 1736] loss: 0.6126724481582642\n",
      "[step: 1737] loss: 0.6122817397117615\n",
      "[step: 1738] loss: 0.6105548143386841\n",
      "[step: 1739] loss: 0.6092382669448853\n",
      "[step: 1740] loss: 0.6093169450759888\n",
      "[step: 1741] loss: 0.6099281907081604\n",
      "[step: 1742] loss: 0.6097280979156494\n",
      "[step: 1743] loss: 0.6086225509643555\n",
      "[step: 1744] loss: 0.6076464653015137\n",
      "[step: 1745] loss: 0.6075042486190796\n",
      "[step: 1746] loss: 0.6077418923377991\n",
      "[step: 1747] loss: 0.6075230836868286\n",
      "[step: 1748] loss: 0.6067447066307068\n",
      "[step: 1749] loss: 0.6060175895690918\n",
      "[step: 1750] loss: 0.6057833433151245\n",
      "[step: 1751] loss: 0.605789065361023\n",
      "[step: 1752] loss: 0.6055357456207275\n",
      "[step: 1753] loss: 0.604944109916687\n",
      "[step: 1754] loss: 0.6043674349784851\n",
      "[step: 1755] loss: 0.6040765643119812\n",
      "[step: 1756] loss: 0.6039413213729858\n",
      "[step: 1757] loss: 0.6036673784255981\n",
      "[step: 1758] loss: 0.6031882166862488\n",
      "[step: 1759] loss: 0.6026963591575623\n",
      "[step: 1760] loss: 0.6023654341697693\n",
      "[step: 1761] loss: 0.602141797542572\n",
      "[step: 1762] loss: 0.6018551588058472\n",
      "[step: 1763] loss: 0.6014431118965149\n",
      "[step: 1764] loss: 0.6010024547576904\n",
      "[step: 1765] loss: 0.6006447076797485\n",
      "[step: 1766] loss: 0.6003630757331848\n",
      "[step: 1767] loss: 0.6000639200210571\n",
      "[step: 1768] loss: 0.5996904969215393\n",
      "[step: 1769] loss: 0.5992827415466309\n",
      "[step: 1770] loss: 0.598910391330719\n",
      "[step: 1771] loss: 0.5985887050628662\n",
      "[step: 1772] loss: 0.5982733964920044\n",
      "[step: 1773] loss: 0.5979197025299072\n",
      "[step: 1774] loss: 0.5975348949432373\n",
      "[step: 1775] loss: 0.5971565842628479\n",
      "[step: 1776] loss: 0.5968079566955566\n",
      "[step: 1777] loss: 0.5964744091033936\n",
      "[step: 1778] loss: 0.5961276292800903\n",
      "[step: 1779] loss: 0.5957574248313904\n",
      "[step: 1780] loss: 0.5953797101974487\n",
      "[step: 1781] loss: 0.5950140357017517\n",
      "[step: 1782] loss: 0.5946629047393799\n",
      "[step: 1783] loss: 0.5943118333816528\n",
      "[step: 1784] loss: 0.5939494371414185\n",
      "[step: 1785] loss: 0.5935755372047424\n",
      "[step: 1786] loss: 0.5932013988494873\n",
      "[step: 1787] loss: 0.5928353071212769\n",
      "[step: 1788] loss: 0.5924749970436096\n",
      "[step: 1789] loss: 0.5921115279197693\n",
      "[step: 1790] loss: 0.5917404294013977\n",
      "[step: 1791] loss: 0.5913649797439575\n",
      "[step: 1792] loss: 0.5909892916679382\n",
      "[step: 1793] loss: 0.5906177759170532\n",
      "[step: 1794] loss: 0.5902483463287354\n",
      "[step: 1795] loss: 0.5898761749267578\n",
      "[step: 1796] loss: 0.5895004868507385\n",
      "[step: 1797] loss: 0.589120626449585\n",
      "[step: 1798] loss: 0.5887409448623657\n",
      "[step: 1799] loss: 0.5883629322052002\n",
      "[step: 1800] loss: 0.5879857540130615\n",
      "[step: 1801] loss: 0.5876069068908691\n",
      "[step: 1802] loss: 0.5872251987457275\n",
      "[step: 1803] loss: 0.5868415832519531\n",
      "[step: 1804] loss: 0.5864571928977966\n",
      "[step: 1805] loss: 0.5860723257064819\n",
      "[step: 1806] loss: 0.5856875777244568\n",
      "[step: 1807] loss: 0.5853032469749451\n",
      "[step: 1808] loss: 0.5849159955978394\n",
      "[step: 1809] loss: 0.5845274925231934\n",
      "[step: 1810] loss: 0.5841375589370728\n",
      "[step: 1811] loss: 0.5837472677230835\n",
      "[step: 1812] loss: 0.583356499671936\n",
      "[step: 1813] loss: 0.5829651355743408\n",
      "[step: 1814] loss: 0.5825728178024292\n",
      "[step: 1815] loss: 0.5821794271469116\n",
      "[step: 1816] loss: 0.5817843675613403\n",
      "[step: 1817] loss: 0.5813882946968079\n",
      "[step: 1818] loss: 0.5809921622276306\n",
      "[step: 1819] loss: 0.580594539642334\n",
      "[step: 1820] loss: 0.580196738243103\n",
      "[step: 1821] loss: 0.5797985792160034\n",
      "[step: 1822] loss: 0.5793986320495605\n",
      "[step: 1823] loss: 0.578997790813446\n",
      "[step: 1824] loss: 0.5785952210426331\n",
      "[step: 1825] loss: 0.5781930088996887\n",
      "[step: 1826] loss: 0.5777899026870728\n",
      "[step: 1827] loss: 0.577385663986206\n",
      "[step: 1828] loss: 0.5769810080528259\n",
      "[step: 1829] loss: 0.5765759944915771\n",
      "[step: 1830] loss: 0.5761691331863403\n",
      "[step: 1831] loss: 0.5757619142532349\n",
      "[step: 1832] loss: 0.5753533840179443\n",
      "[step: 1833] loss: 0.5749443769454956\n",
      "[step: 1834] loss: 0.5745347738265991\n",
      "[step: 1835] loss: 0.5741240382194519\n",
      "[step: 1836] loss: 0.5737133026123047\n",
      "[step: 1837] loss: 0.5733011364936829\n",
      "[step: 1838] loss: 0.5728887319564819\n",
      "[step: 1839] loss: 0.5724755525588989\n",
      "[step: 1840] loss: 0.5720605850219727\n",
      "[step: 1841] loss: 0.5716457366943359\n",
      "[step: 1842] loss: 0.5712301731109619\n",
      "[step: 1843] loss: 0.5708140730857849\n",
      "[step: 1844] loss: 0.570396900177002\n",
      "[step: 1845] loss: 0.5699793100357056\n",
      "[step: 1846] loss: 0.5695605278015137\n",
      "[step: 1847] loss: 0.5691415071487427\n",
      "[step: 1848] loss: 0.5687220096588135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1849] loss: 0.5683013796806335\n",
      "[step: 1850] loss: 0.5678799748420715\n",
      "[step: 1851] loss: 0.5674588680267334\n",
      "[step: 1852] loss: 0.5670363306999207\n",
      "[step: 1853] loss: 0.566612958908081\n",
      "[step: 1854] loss: 0.566189169883728\n",
      "[step: 1855] loss: 0.565765380859375\n",
      "[step: 1856] loss: 0.5653402805328369\n",
      "[step: 1857] loss: 0.5649148225784302\n",
      "[step: 1858] loss: 0.5644888281822205\n",
      "[step: 1859] loss: 0.5640619993209839\n",
      "[step: 1860] loss: 0.5636351108551025\n",
      "[step: 1861] loss: 0.5632073879241943\n",
      "[step: 1862] loss: 0.562778651714325\n",
      "[step: 1863] loss: 0.5623498558998108\n",
      "[step: 1864] loss: 0.561920166015625\n",
      "[step: 1865] loss: 0.5614907145500183\n",
      "[step: 1866] loss: 0.5610597729682922\n",
      "[step: 1867] loss: 0.56062912940979\n",
      "[step: 1868] loss: 0.5601973533630371\n",
      "[step: 1869] loss: 0.5597649216651917\n",
      "[step: 1870] loss: 0.5593324899673462\n",
      "[step: 1871] loss: 0.5588998794555664\n",
      "[step: 1872] loss: 0.558465838432312\n",
      "[step: 1873] loss: 0.5580319166183472\n",
      "[step: 1874] loss: 0.5575973987579346\n",
      "[step: 1875] loss: 0.5571627020835876\n",
      "[step: 1876] loss: 0.5567277669906616\n",
      "[step: 1877] loss: 0.556291937828064\n",
      "[step: 1878] loss: 0.5558550357818604\n",
      "[step: 1879] loss: 0.5554181337356567\n",
      "[step: 1880] loss: 0.5549815893173218\n",
      "[step: 1881] loss: 0.5545436143875122\n",
      "[step: 1882] loss: 0.5541059970855713\n",
      "[step: 1883] loss: 0.5536675453186035\n",
      "[step: 1884] loss: 0.553228497505188\n",
      "[step: 1885] loss: 0.5527897477149963\n",
      "[step: 1886] loss: 0.5523499846458435\n",
      "[step: 1887] loss: 0.5519097447395325\n",
      "[step: 1888] loss: 0.5514694452285767\n",
      "[step: 1889] loss: 0.5510283708572388\n",
      "[step: 1890] loss: 0.5505883097648621\n",
      "[step: 1891] loss: 0.5501470565795898\n",
      "[step: 1892] loss: 0.549705982208252\n",
      "[step: 1893] loss: 0.5492647886276245\n",
      "[step: 1894] loss: 0.5488241910934448\n",
      "[step: 1895] loss: 0.5483840107917786\n",
      "[step: 1896] loss: 0.5479470491409302\n",
      "[step: 1897] loss: 0.5475134253501892\n",
      "[step: 1898] loss: 0.5470883250236511\n",
      "[step: 1899] loss: 0.5466787815093994\n",
      "[step: 1900] loss: 0.5463004112243652\n",
      "[step: 1901] loss: 0.5459802150726318\n",
      "[step: 1902] loss: 0.5457727909088135\n",
      "[step: 1903] loss: 0.5457834005355835\n",
      "[step: 1904] loss: 0.5461961627006531\n",
      "[step: 1905] loss: 0.5473408699035645\n",
      "[step: 1906] loss: 0.5495916604995728\n",
      "[step: 1907] loss: 0.5532581210136414\n",
      "[step: 1908] loss: 0.5572127103805542\n",
      "[step: 1909] loss: 0.5588564276695251\n",
      "[step: 1910] loss: 0.5543837547302246\n",
      "[step: 1911] loss: 0.5461339950561523\n",
      "[step: 1912] loss: 0.5410006642341614\n",
      "[step: 1913] loss: 0.542736291885376\n",
      "[step: 1914] loss: 0.5470223426818848\n",
      "[step: 1915] loss: 0.5471681356430054\n",
      "[step: 1916] loss: 0.5425726175308228\n",
      "[step: 1917] loss: 0.5388655662536621\n",
      "[step: 1918] loss: 0.5398582220077515\n",
      "[step: 1919] loss: 0.542312741279602\n",
      "[step: 1920] loss: 0.5415068864822388\n",
      "[step: 1921] loss: 0.5382275581359863\n",
      "[step: 1922] loss: 0.5367203950881958\n",
      "[step: 1923] loss: 0.5379550457000732\n",
      "[step: 1924] loss: 0.5387046337127686\n",
      "[step: 1925] loss: 0.5370402336120605\n",
      "[step: 1926] loss: 0.5350743532180786\n",
      "[step: 1927] loss: 0.5350000262260437\n",
      "[step: 1928] loss: 0.5357482433319092\n",
      "[step: 1929] loss: 0.5352187156677246\n",
      "[step: 1930] loss: 0.5336562395095825\n",
      "[step: 1931] loss: 0.5328839421272278\n",
      "[step: 1932] loss: 0.5331695079803467\n",
      "[step: 1933] loss: 0.5331125855445862\n",
      "[step: 1934] loss: 0.5321195721626282\n",
      "[step: 1935] loss: 0.5311485528945923\n",
      "[step: 1936] loss: 0.53095543384552\n",
      "[step: 1937] loss: 0.5309703350067139\n",
      "[step: 1938] loss: 0.5304169654846191\n",
      "[step: 1939] loss: 0.5295408964157104\n",
      "[step: 1940] loss: 0.5290305018424988\n",
      "[step: 1941] loss: 0.5288972854614258\n",
      "[step: 1942] loss: 0.5285861492156982\n",
      "[step: 1943] loss: 0.5279180407524109\n",
      "[step: 1944] loss: 0.5272852182388306\n",
      "[step: 1945] loss: 0.5269545316696167\n",
      "[step: 1946] loss: 0.5266960263252258\n",
      "[step: 1947] loss: 0.5262206792831421\n",
      "[step: 1948] loss: 0.5256137251853943\n",
      "[step: 1949] loss: 0.5251388549804688\n",
      "[step: 1950] loss: 0.524817943572998\n",
      "[step: 1951] loss: 0.5244463682174683\n",
      "[step: 1952] loss: 0.5239350199699402\n",
      "[step: 1953] loss: 0.5234097242355347\n",
      "[step: 1954] loss: 0.522994875907898\n",
      "[step: 1955] loss: 0.5226335525512695\n",
      "[step: 1956] loss: 0.5222055315971375\n",
      "[step: 1957] loss: 0.5217077136039734\n",
      "[step: 1958] loss: 0.521233320236206\n",
      "[step: 1959] loss: 0.5208266973495483\n",
      "[step: 1960] loss: 0.5204299688339233\n",
      "[step: 1961] loss: 0.5199825167655945\n",
      "[step: 1962] loss: 0.5195052623748779\n",
      "[step: 1963] loss: 0.5190539360046387\n",
      "[step: 1964] loss: 0.518639087677002\n",
      "[step: 1965] loss: 0.5182199478149414\n",
      "[step: 1966] loss: 0.5177697539329529\n",
      "[step: 1967] loss: 0.5173062682151794\n",
      "[step: 1968] loss: 0.516861081123352\n",
      "[step: 1969] loss: 0.516435444355011\n",
      "[step: 1970] loss: 0.5160046219825745\n",
      "[step: 1971] loss: 0.5155541896820068\n",
      "[step: 1972] loss: 0.5150980353355408\n",
      "[step: 1973] loss: 0.514652669429779\n",
      "[step: 1974] loss: 0.5142180919647217\n",
      "[step: 1975] loss: 0.5137808322906494\n",
      "[step: 1976] loss: 0.5133317708969116\n",
      "[step: 1977] loss: 0.5128771066665649\n",
      "[step: 1978] loss: 0.5124285817146301\n",
      "[step: 1979] loss: 0.511986494064331\n",
      "[step: 1980] loss: 0.5115432143211365\n",
      "[step: 1981] loss: 0.5110938549041748\n",
      "[step: 1982] loss: 0.5106393098831177\n",
      "[step: 1983] loss: 0.5101876854896545\n",
      "[step: 1984] loss: 0.5097389221191406\n",
      "[step: 1985] loss: 0.5092906951904297\n",
      "[step: 1986] loss: 0.5088391304016113\n",
      "[step: 1987] loss: 0.5083832740783691\n",
      "[step: 1988] loss: 0.507927656173706\n",
      "[step: 1989] loss: 0.507472813129425\n",
      "[step: 1990] loss: 0.5070202350616455\n",
      "[step: 1991] loss: 0.5065640211105347\n",
      "[step: 1992] loss: 0.506106436252594\n",
      "[step: 1993] loss: 0.5056463479995728\n",
      "[step: 1994] loss: 0.5051868557929993\n",
      "[step: 1995] loss: 0.5047276616096497\n",
      "[step: 1996] loss: 0.5042667388916016\n",
      "[step: 1997] loss: 0.5038056969642639\n",
      "[step: 1998] loss: 0.503341794013977\n",
      "[step: 1999] loss: 0.5028773546218872\n",
      "[step: 2000] loss: 0.5024126768112183\n",
      "[step: 2001] loss: 0.5019475221633911\n",
      "[step: 2002] loss: 0.501480221748352\n",
      "[step: 2003] loss: 0.5010124444961548\n",
      "[step: 2004] loss: 0.5005425810813904\n",
      "[step: 2005] loss: 0.5000720024108887\n",
      "[step: 2006] loss: 0.4996010959148407\n",
      "[step: 2007] loss: 0.4991289973258972\n",
      "[step: 2008] loss: 0.4986549913883209\n",
      "[step: 2009] loss: 0.49818024039268494\n",
      "[step: 2010] loss: 0.4977041482925415\n",
      "[step: 2011] loss: 0.4972270131111145\n",
      "[step: 2012] loss: 0.49674856662750244\n",
      "[step: 2013] loss: 0.4962688386440277\n",
      "[step: 2014] loss: 0.49578845500946045\n",
      "[step: 2015] loss: 0.4953056573867798\n",
      "[step: 2016] loss: 0.4948226809501648\n",
      "[step: 2017] loss: 0.49433767795562744\n",
      "[step: 2018] loss: 0.4938517212867737\n",
      "[step: 2019] loss: 0.4933648109436035\n",
      "[step: 2020] loss: 0.49287617206573486\n",
      "[step: 2021] loss: 0.492386132478714\n",
      "[step: 2022] loss: 0.49189484119415283\n",
      "[step: 2023] loss: 0.491402268409729\n",
      "[step: 2024] loss: 0.4909084439277649\n",
      "[step: 2025] loss: 0.49041277170181274\n",
      "[step: 2026] loss: 0.4899160861968994\n",
      "[step: 2027] loss: 0.489418089389801\n",
      "[step: 2028] loss: 0.48891836404800415\n",
      "[step: 2029] loss: 0.4884173274040222\n",
      "[step: 2030] loss: 0.4879148602485657\n",
      "[step: 2031] loss: 0.48741084337234497\n",
      "[step: 2032] loss: 0.4869052767753601\n",
      "[step: 2033] loss: 0.4863981008529663\n",
      "[step: 2034] loss: 0.48589032888412476\n",
      "[step: 2035] loss: 0.48538029193878174\n",
      "[step: 2036] loss: 0.4848690330982208\n",
      "[step: 2037] loss: 0.4843561053276062\n",
      "[step: 2038] loss: 0.4838418662548065\n",
      "[step: 2039] loss: 0.4833263158798218\n",
      "[step: 2040] loss: 0.48280900716781616\n",
      "[step: 2041] loss: 0.48228949308395386\n",
      "[step: 2042] loss: 0.48176929354667664\n",
      "[step: 2043] loss: 0.481247216463089\n",
      "[step: 2044] loss: 0.4807235598564148\n",
      "[step: 2045] loss: 0.480198472738266\n",
      "[step: 2046] loss: 0.47967150807380676\n",
      "[step: 2047] loss: 0.47914350032806396\n",
      "[step: 2048] loss: 0.4786137342453003\n",
      "[step: 2049] loss: 0.4780823588371277\n",
      "[step: 2050] loss: 0.4775492250919342\n",
      "[step: 2051] loss: 0.4770144820213318\n",
      "[step: 2052] loss: 0.4764782190322876\n",
      "[step: 2053] loss: 0.47593992948532104\n",
      "[step: 2054] loss: 0.4754006862640381\n",
      "[step: 2055] loss: 0.4748596251010895\n",
      "[step: 2056] loss: 0.47431665658950806\n",
      "[step: 2057] loss: 0.47377246618270874\n",
      "[step: 2058] loss: 0.47322604060173035\n",
      "[step: 2059] loss: 0.472678542137146\n",
      "[step: 2060] loss: 0.47212880849838257\n",
      "[step: 2061] loss: 0.47157761454582214\n",
      "[step: 2062] loss: 0.4710254669189453\n",
      "[step: 2063] loss: 0.4704713523387909\n",
      "[step: 2064] loss: 0.4699151813983917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2065] loss: 0.469356507062912\n",
      "[step: 2066] loss: 0.4687976837158203\n",
      "[step: 2067] loss: 0.46823668479919434\n",
      "[step: 2068] loss: 0.46767377853393555\n",
      "[step: 2069] loss: 0.4671088755130768\n",
      "[step: 2070] loss: 0.4665428102016449\n",
      "[step: 2071] loss: 0.4659745693206787\n",
      "[step: 2072] loss: 0.4654051661491394\n",
      "[step: 2073] loss: 0.4648336172103882\n",
      "[step: 2074] loss: 0.46426036953926086\n",
      "[step: 2075] loss: 0.4636850953102112\n",
      "[step: 2076] loss: 0.4631084203720093\n",
      "[step: 2077] loss: 0.46252965927124023\n",
      "[step: 2078] loss: 0.4619491994380951\n",
      "[step: 2079] loss: 0.4613667130470276\n",
      "[step: 2080] loss: 0.4607824683189392\n",
      "[step: 2081] loss: 0.4601965546607971\n",
      "[step: 2082] loss: 0.4596092402935028\n",
      "[step: 2083] loss: 0.459019273519516\n",
      "[step: 2084] loss: 0.4584282636642456\n",
      "[step: 2085] loss: 0.4578346610069275\n",
      "[step: 2086] loss: 0.45723971724510193\n",
      "[step: 2087] loss: 0.4566425681114197\n",
      "[step: 2088] loss: 0.4560432434082031\n",
      "[step: 2089] loss: 0.45544305443763733\n",
      "[step: 2090] loss: 0.45483946800231934\n",
      "[step: 2091] loss: 0.45423537492752075\n",
      "[step: 2092] loss: 0.45362839102745056\n",
      "[step: 2093] loss: 0.45302054286003113\n",
      "[step: 2094] loss: 0.452409952878952\n",
      "[step: 2095] loss: 0.45179781317710876\n",
      "[step: 2096] loss: 0.4511828124523163\n",
      "[step: 2097] loss: 0.4505675733089447\n",
      "[step: 2098] loss: 0.4499487578868866\n",
      "[step: 2099] loss: 0.4493292272090912\n",
      "[step: 2100] loss: 0.4487084746360779\n",
      "[step: 2101] loss: 0.4480847120285034\n",
      "[step: 2102] loss: 0.4474606513977051\n",
      "[step: 2103] loss: 0.44683653116226196\n",
      "[step: 2104] loss: 0.44621366262435913\n",
      "[step: 2105] loss: 0.44559600949287415\n",
      "[step: 2106] loss: 0.44498971104621887\n",
      "[step: 2107] loss: 0.4444091022014618\n",
      "[step: 2108] loss: 0.44388312101364136\n",
      "[step: 2109] loss: 0.4434688985347748\n",
      "[step: 2110] loss: 0.4432884454727173\n",
      "[step: 2111] loss: 0.4435873031616211\n",
      "[step: 2112] loss: 0.4448138177394867\n",
      "[step: 2113] loss: 0.4477212727069855\n",
      "[step: 2114] loss: 0.45288151502609253\n",
      "[step: 2115] loss: 0.45945507287979126\n",
      "[step: 2116] loss: 0.4619487524032593\n",
      "[step: 2117] loss: 0.45485973358154297\n",
      "[step: 2118] loss: 0.44190579652786255\n",
      "[step: 2119] loss: 0.4368913769721985\n",
      "[step: 2120] loss: 0.442626416683197\n",
      "[step: 2121] loss: 0.44738084077835083\n",
      "[step: 2122] loss: 0.44257211685180664\n",
      "[step: 2123] loss: 0.43497198820114136\n",
      "[step: 2124] loss: 0.4352492690086365\n",
      "[step: 2125] loss: 0.4396838843822479\n",
      "[step: 2126] loss: 0.4384136199951172\n",
      "[step: 2127] loss: 0.4328971803188324\n",
      "[step: 2128] loss: 0.4316698908805847\n",
      "[step: 2129] loss: 0.4344293475151062\n",
      "[step: 2130] loss: 0.43401840329170227\n",
      "[step: 2131] loss: 0.4301616847515106\n",
      "[step: 2132] loss: 0.4289020895957947\n",
      "[step: 2133] loss: 0.4305315613746643\n",
      "[step: 2134] loss: 0.4300755262374878\n",
      "[step: 2135] loss: 0.4273236393928528\n",
      "[step: 2136] loss: 0.42626771330833435\n",
      "[step: 2137] loss: 0.42713141441345215\n",
      "[step: 2138] loss: 0.42662400007247925\n",
      "[step: 2139] loss: 0.4245704412460327\n",
      "[step: 2140] loss: 0.42365217208862305\n",
      "[step: 2141] loss: 0.4240041971206665\n",
      "[step: 2142] loss: 0.4234512150287628\n",
      "[step: 2143] loss: 0.4218778610229492\n",
      "[step: 2144] loss: 0.4210295081138611\n",
      "[step: 2145] loss: 0.421032190322876\n",
      "[step: 2146] loss: 0.420482337474823\n",
      "[step: 2147] loss: 0.4192233681678772\n",
      "[step: 2148] loss: 0.4184006452560425\n",
      "[step: 2149] loss: 0.4181678295135498\n",
      "[step: 2150] loss: 0.41762423515319824\n",
      "[step: 2151] loss: 0.41658398509025574\n",
      "[step: 2152] loss: 0.4157651662826538\n",
      "[step: 2153] loss: 0.41536328196525574\n",
      "[step: 2154] loss: 0.4148325026035309\n",
      "[step: 2155] loss: 0.41394609212875366\n",
      "[step: 2156] loss: 0.4131290316581726\n",
      "[step: 2157] loss: 0.41260409355163574\n",
      "[step: 2158] loss: 0.41206952929496765\n",
      "[step: 2159] loss: 0.4112973213195801\n",
      "[step: 2160] loss: 0.4104939103126526\n",
      "[step: 2161] loss: 0.4098789095878601\n",
      "[step: 2162] loss: 0.40932339429855347\n",
      "[step: 2163] loss: 0.40862977504730225\n",
      "[step: 2164] loss: 0.407858669757843\n",
      "[step: 2165] loss: 0.40717828273773193\n",
      "[step: 2166] loss: 0.4065857231616974\n",
      "[step: 2167] loss: 0.405941367149353\n",
      "[step: 2168] loss: 0.4052126407623291\n",
      "[step: 2169] loss: 0.40450042486190796\n",
      "[step: 2170] loss: 0.4038606286048889\n",
      "[step: 2171] loss: 0.4032318592071533\n",
      "[step: 2172] loss: 0.40254688262939453\n",
      "[step: 2173] loss: 0.40183520317077637\n",
      "[step: 2174] loss: 0.40115588903427124\n",
      "[step: 2175] loss: 0.4005122184753418\n",
      "[step: 2176] loss: 0.3998561501502991\n",
      "[step: 2177] loss: 0.3991662561893463\n",
      "[step: 2178] loss: 0.39847180247306824\n",
      "[step: 2179] loss: 0.3978002965450287\n",
      "[step: 2180] loss: 0.3971465528011322\n",
      "[step: 2181] loss: 0.39647960662841797\n",
      "[step: 2182] loss: 0.39579397439956665\n",
      "[step: 2183] loss: 0.3951086699962616\n",
      "[step: 2184] loss: 0.39443671703338623\n",
      "[step: 2185] loss: 0.39377397298812866\n",
      "[step: 2186] loss: 0.39310556650161743\n",
      "[step: 2187] loss: 0.39242613315582275\n",
      "[step: 2188] loss: 0.3917442560195923\n",
      "[step: 2189] loss: 0.39107030630111694\n",
      "[step: 2190] loss: 0.39040297269821167\n",
      "[step: 2191] loss: 0.38973480463027954\n",
      "[step: 2192] loss: 0.3890601396560669\n",
      "[step: 2193] loss: 0.38838258385658264\n",
      "[step: 2194] loss: 0.3877071142196655\n",
      "[step: 2195] loss: 0.38703691959381104\n",
      "[step: 2196] loss: 0.38636794686317444\n",
      "[step: 2197] loss: 0.3856981694698334\n",
      "[step: 2198] loss: 0.3850250244140625\n",
      "[step: 2199] loss: 0.3843509554862976\n",
      "[step: 2200] loss: 0.3836798071861267\n",
      "[step: 2201] loss: 0.38301047682762146\n",
      "[step: 2202] loss: 0.38234269618988037\n",
      "[step: 2203] loss: 0.38167375326156616\n",
      "[step: 2204] loss: 0.38100457191467285\n",
      "[step: 2205] loss: 0.3803342282772064\n",
      "[step: 2206] loss: 0.3796656131744385\n",
      "[step: 2207] loss: 0.37899795174598694\n",
      "[step: 2208] loss: 0.37833184003829956\n",
      "[step: 2209] loss: 0.3776665925979614\n",
      "[step: 2210] loss: 0.3770015239715576\n",
      "[step: 2211] loss: 0.3763350248336792\n",
      "[step: 2212] loss: 0.37567031383514404\n",
      "[step: 2213] loss: 0.37500590085983276\n",
      "[step: 2214] loss: 0.37434250116348267\n",
      "[step: 2215] loss: 0.37368038296699524\n",
      "[step: 2216] loss: 0.3730185031890869\n",
      "[step: 2217] loss: 0.37235820293426514\n",
      "[step: 2218] loss: 0.37169724702835083\n",
      "[step: 2219] loss: 0.3710377514362335\n",
      "[step: 2220] loss: 0.37037861347198486\n",
      "[step: 2221] loss: 0.3697199523448944\n",
      "[step: 2222] loss: 0.36906248331069946\n",
      "[step: 2223] loss: 0.36840546131134033\n",
      "[step: 2224] loss: 0.36774927377700806\n",
      "[step: 2225] loss: 0.36709484457969666\n",
      "[step: 2226] loss: 0.3664399981498718\n",
      "[step: 2227] loss: 0.3657867908477783\n",
      "[step: 2228] loss: 0.365134060382843\n",
      "[step: 2229] loss: 0.3644828200340271\n",
      "[step: 2230] loss: 0.36383163928985596\n",
      "[step: 2231] loss: 0.3631819486618042\n",
      "[step: 2232] loss: 0.36253276467323303\n",
      "[step: 2233] loss: 0.3618840277194977\n",
      "[step: 2234] loss: 0.36123642325401306\n",
      "[step: 2235] loss: 0.36058977246284485\n",
      "[step: 2236] loss: 0.35994401574134827\n",
      "[step: 2237] loss: 0.3592991828918457\n",
      "[step: 2238] loss: 0.35865476727485657\n",
      "[step: 2239] loss: 0.35801300406455994\n",
      "[step: 2240] loss: 0.35737115144729614\n",
      "[step: 2241] loss: 0.3567306697368622\n",
      "[step: 2242] loss: 0.3560914397239685\n",
      "[step: 2243] loss: 0.3554550111293793\n",
      "[step: 2244] loss: 0.35482272505760193\n",
      "[step: 2245] loss: 0.35419556498527527\n",
      "[step: 2246] loss: 0.35357874631881714\n",
      "[step: 2247] loss: 0.35298049449920654\n",
      "[step: 2248] loss: 0.35241594910621643\n",
      "[step: 2249] loss: 0.3519201874732971\n",
      "[step: 2250] loss: 0.35155606269836426\n",
      "[step: 2251] loss: 0.35146307945251465\n",
      "[step: 2252] loss: 0.3518849015235901\n",
      "[step: 2253] loss: 0.3533363938331604\n",
      "[step: 2254] loss: 0.3564162254333496\n",
      "[step: 2255] loss: 0.3619261384010315\n",
      "[step: 2256] loss: 0.36805111169815063\n",
      "[step: 2257] loss: 0.37099456787109375\n",
      "[step: 2258] loss: 0.3637099862098694\n",
      "[step: 2259] loss: 0.3511815369129181\n",
      "[step: 2260] loss: 0.3450103998184204\n",
      "[step: 2261] loss: 0.3495861291885376\n",
      "[step: 2262] loss: 0.355692595243454\n",
      "[step: 2263] loss: 0.3531627953052521\n",
      "[step: 2264] loss: 0.3453295826911926\n",
      "[step: 2265] loss: 0.342423677444458\n",
      "[step: 2266] loss: 0.34624192118644714\n",
      "[step: 2267] loss: 0.34847456216812134\n",
      "[step: 2268] loss: 0.3443966507911682\n",
      "[step: 2269] loss: 0.3401286005973816\n",
      "[step: 2270] loss: 0.34095633029937744\n",
      "[step: 2271] loss: 0.34327441453933716\n",
      "[step: 2272] loss: 0.34189701080322266\n",
      "[step: 2273] loss: 0.3383922278881073\n",
      "[step: 2274] loss: 0.33760905265808105\n",
      "[step: 2275] loss: 0.33911651372909546\n",
      "[step: 2276] loss: 0.33888673782348633\n",
      "[step: 2277] loss: 0.33651769161224365\n",
      "[step: 2278] loss: 0.33509939908981323\n",
      "[step: 2279] loss: 0.33566606044769287\n",
      "[step: 2280] loss: 0.3359096050262451\n",
      "[step: 2281] loss: 0.3344547748565674\n",
      "[step: 2282] loss: 0.3329435884952545\n",
      "[step: 2283] loss: 0.3327923119068146\n",
      "[step: 2284] loss: 0.33300915360450745\n",
      "[step: 2285] loss: 0.3322407901287079\n",
      "[step: 2286] loss: 0.330928772687912\n",
      "[step: 2287] loss: 0.33029526472091675\n",
      "[step: 2288] loss: 0.3302852511405945\n",
      "[step: 2289] loss: 0.3299025595188141\n",
      "[step: 2290] loss: 0.32892492413520813\n",
      "[step: 2291] loss: 0.32806915044784546\n",
      "[step: 2292] loss: 0.3277396261692047\n",
      "[step: 2293] loss: 0.327487975358963\n",
      "[step: 2294] loss: 0.32684072852134705\n",
      "[step: 2295] loss: 0.32600051164627075\n",
      "[step: 2296] loss: 0.3254113793373108\n",
      "[step: 2297] loss: 0.3250759243965149\n",
      "[step: 2298] loss: 0.32464033365249634\n",
      "[step: 2299] loss: 0.3239629864692688\n",
      "[step: 2300] loss: 0.3232693076133728\n",
      "[step: 2301] loss: 0.32276391983032227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2302] loss: 0.3223589062690735\n",
      "[step: 2303] loss: 0.32185137271881104\n",
      "[step: 2304] loss: 0.3212162256240845\n",
      "[step: 2305] loss: 0.3206029534339905\n",
      "[step: 2306] loss: 0.32010364532470703\n",
      "[step: 2307] loss: 0.3196496367454529\n",
      "[step: 2308] loss: 0.3191307485103607\n",
      "[step: 2309] loss: 0.31854021549224854\n",
      "[step: 2310] loss: 0.3179621696472168\n",
      "[step: 2311] loss: 0.317448228597641\n",
      "[step: 2312] loss: 0.3169681429862976\n",
      "[step: 2313] loss: 0.3164569139480591\n",
      "[step: 2314] loss: 0.315901517868042\n",
      "[step: 2315] loss: 0.31534165143966675\n",
      "[step: 2316] loss: 0.31481462717056274\n",
      "[step: 2317] loss: 0.31431639194488525\n",
      "[step: 2318] loss: 0.31381291151046753\n",
      "[step: 2319] loss: 0.3132842779159546\n",
      "[step: 2320] loss: 0.31274113059043884\n",
      "[step: 2321] loss: 0.3122076988220215\n",
      "[step: 2322] loss: 0.31169354915618896\n",
      "[step: 2323] loss: 0.31118935346603394\n",
      "[step: 2324] loss: 0.3106783330440521\n",
      "[step: 2325] loss: 0.31015467643737793\n",
      "[step: 2326] loss: 0.30962538719177246\n",
      "[step: 2327] loss: 0.30910199880599976\n",
      "[step: 2328] loss: 0.3085881471633911\n",
      "[step: 2329] loss: 0.30808085203170776\n",
      "[step: 2330] loss: 0.307571142911911\n",
      "[step: 2331] loss: 0.3070544898509979\n",
      "[step: 2332] loss: 0.3065345287322998\n",
      "[step: 2333] loss: 0.30601584911346436\n",
      "[step: 2334] loss: 0.30550092458724976\n",
      "[step: 2335] loss: 0.3049907088279724\n",
      "[step: 2336] loss: 0.30448222160339355\n",
      "[step: 2337] loss: 0.3039718270301819\n",
      "[step: 2338] loss: 0.3034588098526001\n",
      "[step: 2339] loss: 0.3029446005821228\n",
      "[step: 2340] loss: 0.3024299144744873\n",
      "[step: 2341] loss: 0.30191779136657715\n",
      "[step: 2342] loss: 0.30140751600265503\n",
      "[step: 2343] loss: 0.30089864134788513\n",
      "[step: 2344] loss: 0.30039024353027344\n",
      "[step: 2345] loss: 0.2998809814453125\n",
      "[step: 2346] loss: 0.29937025904655457\n",
      "[step: 2347] loss: 0.29885944724082947\n",
      "[step: 2348] loss: 0.29834869503974915\n",
      "[step: 2349] loss: 0.2978386878967285\n",
      "[step: 2350] loss: 0.29732948541641235\n",
      "[step: 2351] loss: 0.29682084918022156\n",
      "[step: 2352] loss: 0.2963120937347412\n",
      "[step: 2353] loss: 0.2958051562309265\n",
      "[step: 2354] loss: 0.29529741406440735\n",
      "[step: 2355] loss: 0.2947898507118225\n",
      "[step: 2356] loss: 0.29428189992904663\n",
      "[step: 2357] loss: 0.2937746047973633\n",
      "[step: 2358] loss: 0.2932673692703247\n",
      "[step: 2359] loss: 0.29275959730148315\n",
      "[step: 2360] loss: 0.29225295782089233\n",
      "[step: 2361] loss: 0.2917459309101105\n",
      "[step: 2362] loss: 0.2912391424179077\n",
      "[step: 2363] loss: 0.29073286056518555\n",
      "[step: 2364] loss: 0.29022663831710815\n",
      "[step: 2365] loss: 0.28972068428993225\n",
      "[step: 2366] loss: 0.2892153859138489\n",
      "[step: 2367] loss: 0.2887107729911804\n",
      "[step: 2368] loss: 0.28820574283599854\n",
      "[step: 2369] loss: 0.2877015471458435\n",
      "[step: 2370] loss: 0.2871973514556885\n",
      "[step: 2371] loss: 0.28669458627700806\n",
      "[step: 2372] loss: 0.28619176149368286\n",
      "[step: 2373] loss: 0.28568971157073975\n",
      "[step: 2374] loss: 0.2851889133453369\n",
      "[step: 2375] loss: 0.2846892774105072\n",
      "[step: 2376] loss: 0.2841932475566864\n",
      "[step: 2377] loss: 0.2837008535861969\n",
      "[step: 2378] loss: 0.2832167148590088\n",
      "[step: 2379] loss: 0.2827475666999817\n",
      "[step: 2380] loss: 0.2823072075843811\n",
      "[step: 2381] loss: 0.2819253206253052\n",
      "[step: 2382] loss: 0.28165844082832336\n",
      "[step: 2383] loss: 0.28163060545921326\n",
      "[step: 2384] loss: 0.282074898481369\n",
      "[step: 2385] loss: 0.28349214792251587\n",
      "[step: 2386] loss: 0.28658533096313477\n",
      "[step: 2387] loss: 0.2925100326538086\n",
      "[step: 2388] loss: 0.30029231309890747\n",
      "[step: 2389] loss: 0.3067680597305298\n",
      "[step: 2390] loss: 0.30201250314712524\n",
      "[step: 2391] loss: 0.28797897696495056\n",
      "[step: 2392] loss: 0.2770569324493408\n",
      "[step: 2393] loss: 0.2797811031341553\n",
      "[step: 2394] loss: 0.28852465748786926\n",
      "[step: 2395] loss: 0.28856056928634644\n",
      "[step: 2396] loss: 0.27967369556427\n",
      "[step: 2397] loss: 0.2743792235851288\n",
      "[step: 2398] loss: 0.2784133553504944\n",
      "[step: 2399] loss: 0.28241872787475586\n",
      "[step: 2400] loss: 0.27852731943130493\n",
      "[step: 2401] loss: 0.2730792164802551\n",
      "[step: 2402] loss: 0.2737909257411957\n",
      "[step: 2403] loss: 0.27707529067993164\n",
      "[step: 2404] loss: 0.27584508061408997\n",
      "[step: 2405] loss: 0.2717357873916626\n",
      "[step: 2406] loss: 0.2711230516433716\n",
      "[step: 2407] loss: 0.273276686668396\n",
      "[step: 2408] loss: 0.2729611396789551\n",
      "[step: 2409] loss: 0.27019649744033813\n",
      "[step: 2410] loss: 0.26910191774368286\n",
      "[step: 2411] loss: 0.2702837586402893\n",
      "[step: 2412] loss: 0.2704234719276428\n",
      "[step: 2413] loss: 0.26856085658073425\n",
      "[step: 2414] loss: 0.2673599123954773\n",
      "[step: 2415] loss: 0.2678607702255249\n",
      "[step: 2416] loss: 0.26807209849357605\n",
      "[step: 2417] loss: 0.2669122815132141\n",
      "[step: 2418] loss: 0.2657659351825714\n",
      "[step: 2419] loss: 0.2657504081726074\n",
      "[step: 2420] loss: 0.26593658328056335\n",
      "[step: 2421] loss: 0.2652479410171509\n",
      "[step: 2422] loss: 0.2642412781715393\n",
      "[step: 2423] loss: 0.2638913094997406\n",
      "[step: 2424] loss: 0.26394280791282654\n",
      "[step: 2425] loss: 0.2635555565357208\n",
      "[step: 2426] loss: 0.26275432109832764\n",
      "[step: 2427] loss: 0.26221764087677\n",
      "[step: 2428] loss: 0.2620815634727478\n",
      "[step: 2429] loss: 0.26184600591659546\n",
      "[step: 2430] loss: 0.26126575469970703\n",
      "[step: 2431] loss: 0.2606693506240845\n",
      "[step: 2432] loss: 0.2603549361228943\n",
      "[step: 2433] loss: 0.26014241576194763\n",
      "[step: 2434] loss: 0.2597382664680481\n",
      "[step: 2435] loss: 0.2591956853866577\n",
      "[step: 2436] loss: 0.25876033306121826\n",
      "[step: 2437] loss: 0.2584778368473053\n",
      "[step: 2438] loss: 0.2581713795661926\n",
      "[step: 2439] loss: 0.25773555040359497\n",
      "[step: 2440] loss: 0.25726795196533203\n",
      "[step: 2441] loss: 0.25689512491226196\n",
      "[step: 2442] loss: 0.25659075379371643\n",
      "[step: 2443] loss: 0.2562437057495117\n",
      "[step: 2444] loss: 0.2558247447013855\n",
      "[step: 2445] loss: 0.2554069757461548\n",
      "[step: 2446] loss: 0.2550494074821472\n",
      "[step: 2447] loss: 0.25472527742385864\n",
      "[step: 2448] loss: 0.2543710470199585\n",
      "[step: 2449] loss: 0.25397586822509766\n",
      "[step: 2450] loss: 0.25358468294143677\n",
      "[step: 2451] loss: 0.25322967767715454\n",
      "[step: 2452] loss: 0.2528937757015228\n",
      "[step: 2453] loss: 0.2525433599948883\n",
      "[step: 2454] loss: 0.2521700859069824\n",
      "[step: 2455] loss: 0.2517949938774109\n",
      "[step: 2456] loss: 0.2514374256134033\n",
      "[step: 2457] loss: 0.2510964274406433\n",
      "[step: 2458] loss: 0.25075340270996094\n",
      "[step: 2459] loss: 0.25039640069007874\n",
      "[step: 2460] loss: 0.2500332295894623\n",
      "[step: 2461] loss: 0.24967706203460693\n",
      "[step: 2462] loss: 0.24933277070522308\n",
      "[step: 2463] loss: 0.2489931583404541\n",
      "[step: 2464] loss: 0.2486487776041031\n",
      "[step: 2465] loss: 0.24829797446727753\n",
      "[step: 2466] loss: 0.24794678390026093\n",
      "[step: 2467] loss: 0.24760107696056366\n",
      "[step: 2468] loss: 0.24726155400276184\n",
      "[step: 2469] loss: 0.24692371487617493\n",
      "[step: 2470] loss: 0.24658353626728058\n",
      "[step: 2471] loss: 0.24624116718769073\n",
      "[step: 2472] loss: 0.24589847028255463\n",
      "[step: 2473] loss: 0.24555814266204834\n",
      "[step: 2474] loss: 0.24522162973880768\n",
      "[step: 2475] loss: 0.24488742649555206\n",
      "[step: 2476] loss: 0.24455228447914124\n",
      "[step: 2477] loss: 0.24421623349189758\n",
      "[step: 2478] loss: 0.2438802272081375\n",
      "[step: 2479] loss: 0.24354469776153564\n",
      "[step: 2480] loss: 0.24321115016937256\n",
      "[step: 2481] loss: 0.2428794801235199\n",
      "[step: 2482] loss: 0.24254879355430603\n",
      "[step: 2483] loss: 0.24221797287464142\n",
      "[step: 2484] loss: 0.24188697338104248\n",
      "[step: 2485] loss: 0.2415565401315689\n",
      "[step: 2486] loss: 0.2412262260913849\n",
      "[step: 2487] loss: 0.2408970296382904\n",
      "[step: 2488] loss: 0.24056857824325562\n",
      "[step: 2489] loss: 0.24024099111557007\n",
      "[step: 2490] loss: 0.2399139702320099\n",
      "[step: 2491] loss: 0.23958729207515717\n",
      "[step: 2492] loss: 0.23926061391830444\n",
      "[step: 2493] loss: 0.23893386125564575\n",
      "[step: 2494] loss: 0.23860694468021393\n",
      "[step: 2495] loss: 0.23828157782554626\n",
      "[step: 2496] loss: 0.2379555106163025\n",
      "[step: 2497] loss: 0.2376316338777542\n",
      "[step: 2498] loss: 0.23730677366256714\n",
      "[step: 2499] loss: 0.2369825541973114\n",
      "[step: 2500] loss: 0.23665893077850342\n",
      "[step: 2501] loss: 0.23633505403995514\n",
      "[step: 2502] loss: 0.23601147532463074\n",
      "[step: 2503] loss: 0.23568812012672424\n",
      "[step: 2504] loss: 0.2353648841381073\n",
      "[step: 2505] loss: 0.23504063487052917\n",
      "[step: 2506] loss: 0.23471799492835999\n",
      "[step: 2507] loss: 0.23439455032348633\n",
      "[step: 2508] loss: 0.23407182097434998\n",
      "[step: 2509] loss: 0.2337486743927002\n",
      "[step: 2510] loss: 0.23342560231685638\n",
      "[step: 2511] loss: 0.23310282826423645\n",
      "[step: 2512] loss: 0.23278063535690308\n",
      "[step: 2513] loss: 0.23245765268802643\n",
      "[step: 2514] loss: 0.23213471472263336\n",
      "[step: 2515] loss: 0.2318117469549179\n",
      "[step: 2516] loss: 0.23148879408836365\n",
      "[step: 2517] loss: 0.23116566240787506\n",
      "[step: 2518] loss: 0.23084291815757751\n",
      "[step: 2519] loss: 0.2305196076631546\n",
      "[step: 2520] loss: 0.23019680380821228\n",
      "[step: 2521] loss: 0.22987286746501923\n",
      "[step: 2522] loss: 0.2295498549938202\n",
      "[step: 2523] loss: 0.2292262613773346\n",
      "[step: 2524] loss: 0.22890320420265198\n",
      "[step: 2525] loss: 0.22858011722564697\n",
      "[step: 2526] loss: 0.22825756669044495\n",
      "[step: 2527] loss: 0.22793635725975037\n",
      "[step: 2528] loss: 0.22761714458465576\n",
      "[step: 2529] loss: 0.2273021936416626\n",
      "[step: 2530] loss: 0.22699549794197083\n",
      "[step: 2531] loss: 0.22670383751392365\n",
      "[step: 2532] loss: 0.22644288837909698\n",
      "[step: 2533] loss: 0.22624000906944275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2534] loss: 0.22615227103233337\n",
      "[step: 2535] loss: 0.22629237174987793\n",
      "[step: 2536] loss: 0.22686690092086792\n",
      "[step: 2537] loss: 0.2282804250717163\n",
      "[step: 2538] loss: 0.23104889690876007\n",
      "[step: 2539] loss: 0.23584292829036713\n",
      "[step: 2540] loss: 0.24161499738693237\n",
      "[step: 2541] loss: 0.24571025371551514\n",
      "[step: 2542] loss: 0.24190036952495575\n",
      "[step: 2543] loss: 0.23196172714233398\n",
      "[step: 2544] loss: 0.22356638312339783\n",
      "[step: 2545] loss: 0.22386790812015533\n",
      "[step: 2546] loss: 0.22976206243038177\n",
      "[step: 2547] loss: 0.23226773738861084\n",
      "[step: 2548] loss: 0.22782811522483826\n",
      "[step: 2549] loss: 0.2218371331691742\n",
      "[step: 2550] loss: 0.2215876579284668\n",
      "[step: 2551] loss: 0.22535312175750732\n",
      "[step: 2552] loss: 0.22619223594665527\n",
      "[step: 2553] loss: 0.22268027067184448\n",
      "[step: 2554] loss: 0.21964703500270844\n",
      "[step: 2555] loss: 0.22049587965011597\n",
      "[step: 2556] loss: 0.22250181436538696\n",
      "[step: 2557] loss: 0.22180360555648804\n",
      "[step: 2558] loss: 0.2192051112651825\n",
      "[step: 2559] loss: 0.21810463070869446\n",
      "[step: 2560] loss: 0.21919748187065125\n",
      "[step: 2561] loss: 0.21983639895915985\n",
      "[step: 2562] loss: 0.21848738193511963\n",
      "[step: 2563] loss: 0.21691875159740448\n",
      "[step: 2564] loss: 0.21689125895500183\n",
      "[step: 2565] loss: 0.21755953133106232\n",
      "[step: 2566] loss: 0.21725422143936157\n",
      "[step: 2567] loss: 0.2160567194223404\n",
      "[step: 2568] loss: 0.21535024046897888\n",
      "[step: 2569] loss: 0.21553243696689606\n",
      "[step: 2570] loss: 0.21566170454025269\n",
      "[step: 2571] loss: 0.21506109833717346\n",
      "[step: 2572] loss: 0.21422196924686432\n",
      "[step: 2573] loss: 0.21390563249588013\n",
      "[step: 2574] loss: 0.21398994326591492\n",
      "[step: 2575] loss: 0.21380004286766052\n",
      "[step: 2576] loss: 0.2131912112236023\n",
      "[step: 2577] loss: 0.212638258934021\n",
      "[step: 2578] loss: 0.21244698762893677\n",
      "[step: 2579] loss: 0.21236318349838257\n",
      "[step: 2580] loss: 0.21204528212547302\n",
      "[step: 2581] loss: 0.21154065430164337\n",
      "[step: 2582] loss: 0.21113362908363342\n",
      "[step: 2583] loss: 0.21092693507671356\n",
      "[step: 2584] loss: 0.210743710398674\n",
      "[step: 2585] loss: 0.21041128039360046\n",
      "[step: 2586] loss: 0.2099817395210266\n",
      "[step: 2587] loss: 0.20962929725646973\n",
      "[step: 2588] loss: 0.20939743518829346\n",
      "[step: 2589] loss: 0.2091650515794754\n",
      "[step: 2590] loss: 0.20883838832378387\n",
      "[step: 2591] loss: 0.2084614783525467\n",
      "[step: 2592] loss: 0.20813268423080444\n",
      "[step: 2593] loss: 0.20787131786346436\n",
      "[step: 2594] loss: 0.20761315524578094\n",
      "[step: 2595] loss: 0.20730501413345337\n",
      "[step: 2596] loss: 0.206961989402771\n",
      "[step: 2597] loss: 0.20663832128047943\n",
      "[step: 2598] loss: 0.20635665953159332\n",
      "[step: 2599] loss: 0.20608699321746826\n",
      "[step: 2600] loss: 0.20579224824905396\n",
      "[step: 2601] loss: 0.20547178387641907\n",
      "[step: 2602] loss: 0.205154150724411\n",
      "[step: 2603] loss: 0.204858660697937\n",
      "[step: 2604] loss: 0.20457857847213745\n",
      "[step: 2605] loss: 0.20429182052612305\n",
      "[step: 2606] loss: 0.20399032533168793\n",
      "[step: 2607] loss: 0.20368093252182007\n",
      "[step: 2608] loss: 0.20337912440299988\n",
      "[step: 2609] loss: 0.20308935642242432\n",
      "[step: 2610] loss: 0.2028050720691681\n",
      "[step: 2611] loss: 0.202515110373497\n",
      "[step: 2612] loss: 0.2022169977426529\n",
      "[step: 2613] loss: 0.2019166350364685\n",
      "[step: 2614] loss: 0.20162227749824524\n",
      "[step: 2615] loss: 0.2013338804244995\n",
      "[step: 2616] loss: 0.20104794204235077\n",
      "[step: 2617] loss: 0.20076027512550354\n",
      "[step: 2618] loss: 0.20046821236610413\n",
      "[step: 2619] loss: 0.20017501711845398\n",
      "[step: 2620] loss: 0.19988462328910828\n",
      "[step: 2621] loss: 0.19959776103496552\n",
      "[step: 2622] loss: 0.19931361079216003\n",
      "[step: 2623] loss: 0.1990290880203247\n",
      "[step: 2624] loss: 0.1987433135509491\n",
      "[step: 2625] loss: 0.19845668971538544\n",
      "[step: 2626] loss: 0.1981705129146576\n",
      "[step: 2627] loss: 0.19788649678230286\n",
      "[step: 2628] loss: 0.19760406017303467\n",
      "[step: 2629] loss: 0.19732338190078735\n",
      "[step: 2630] loss: 0.19704347848892212\n",
      "[step: 2631] loss: 0.19676288962364197\n",
      "[step: 2632] loss: 0.19648276269435883\n",
      "[step: 2633] loss: 0.19620318710803986\n",
      "[step: 2634] loss: 0.19592449069023132\n",
      "[step: 2635] loss: 0.19564732909202576\n",
      "[step: 2636] loss: 0.1953713595867157\n",
      "[step: 2637] loss: 0.19509632885456085\n",
      "[step: 2638] loss: 0.19482237100601196\n",
      "[step: 2639] loss: 0.19454872608184814\n",
      "[step: 2640] loss: 0.19427597522735596\n",
      "[step: 2641] loss: 0.19400322437286377\n",
      "[step: 2642] loss: 0.19373206794261932\n",
      "[step: 2643] loss: 0.19346165657043457\n",
      "[step: 2644] loss: 0.19319243729114532\n",
      "[step: 2645] loss: 0.192924365401268\n",
      "[step: 2646] loss: 0.1926575005054474\n",
      "[step: 2647] loss: 0.192391037940979\n",
      "[step: 2648] loss: 0.19212590157985687\n",
      "[step: 2649] loss: 0.19186155498027802\n",
      "[step: 2650] loss: 0.19159787893295288\n",
      "[step: 2651] loss: 0.19133539497852325\n",
      "[step: 2652] loss: 0.19107398390769958\n",
      "[step: 2653] loss: 0.19081348180770874\n",
      "[step: 2654] loss: 0.19055381417274475\n",
      "[step: 2655] loss: 0.19029545783996582\n",
      "[step: 2656] loss: 0.1900378167629242\n",
      "[step: 2657] loss: 0.1897812783718109\n",
      "[step: 2658] loss: 0.1895258128643036\n",
      "[step: 2659] loss: 0.18927131593227386\n",
      "[step: 2660] loss: 0.1890178918838501\n",
      "[step: 2661] loss: 0.1887657344341278\n",
      "[step: 2662] loss: 0.18851464986801147\n",
      "[step: 2663] loss: 0.18826444447040558\n",
      "[step: 2664] loss: 0.1880148947238922\n",
      "[step: 2665] loss: 0.18776696920394897\n",
      "[step: 2666] loss: 0.18751966953277588\n",
      "[step: 2667] loss: 0.1872740387916565\n",
      "[step: 2668] loss: 0.18702872097492218\n",
      "[step: 2669] loss: 0.1867850422859192\n",
      "[step: 2670] loss: 0.1865425407886505\n",
      "[step: 2671] loss: 0.18630091845989227\n",
      "[step: 2672] loss: 0.18606102466583252\n",
      "[step: 2673] loss: 0.18582221865653992\n",
      "[step: 2674] loss: 0.18558576703071594\n",
      "[step: 2675] loss: 0.1853521168231964\n",
      "[step: 2676] loss: 0.1851218193769455\n",
      "[step: 2677] loss: 0.18489804863929749\n",
      "[step: 2678] loss: 0.1846851408481598\n",
      "[step: 2679] loss: 0.18449048697948456\n",
      "[step: 2680] loss: 0.1843307614326477\n",
      "[step: 2681] loss: 0.18423695862293243\n",
      "[step: 2682] loss: 0.1842680126428604\n",
      "[step: 2683] loss: 0.1845436543226242\n",
      "[step: 2684] loss: 0.18527165055274963\n",
      "[step: 2685] loss: 0.1868567168712616\n",
      "[step: 2686] loss: 0.1897825300693512\n",
      "[step: 2687] loss: 0.1946331262588501\n",
      "[step: 2688] loss: 0.20029190182685852\n",
      "[step: 2689] loss: 0.20411139726638794\n",
      "[step: 2690] loss: 0.20038743317127228\n",
      "[step: 2691] loss: 0.19093015789985657\n",
      "[step: 2692] loss: 0.18298883736133575\n",
      "[step: 2693] loss: 0.1829911470413208\n",
      "[step: 2694] loss: 0.18816286325454712\n",
      "[step: 2695] loss: 0.19094212353229523\n",
      "[step: 2696] loss: 0.18794310092926025\n",
      "[step: 2697] loss: 0.18252016603946686\n",
      "[step: 2698] loss: 0.18091313540935516\n",
      "[step: 2699] loss: 0.18367889523506165\n",
      "[step: 2700] loss: 0.18581202626228333\n",
      "[step: 2701] loss: 0.18393635749816895\n",
      "[step: 2702] loss: 0.1803913116455078\n",
      "[step: 2703] loss: 0.1797647774219513\n",
      "[step: 2704] loss: 0.18184694647789001\n",
      "[step: 2705] loss: 0.18263909220695496\n",
      "[step: 2706] loss: 0.18072226643562317\n",
      "[step: 2707] loss: 0.178769052028656\n",
      "[step: 2708] loss: 0.1790827512741089\n",
      "[step: 2709] loss: 0.1802835762500763\n",
      "[step: 2710] loss: 0.18005478382110596\n",
      "[step: 2711] loss: 0.17864128947257996\n",
      "[step: 2712] loss: 0.17783546447753906\n",
      "[step: 2713] loss: 0.17819881439208984\n",
      "[step: 2714] loss: 0.17863069474697113\n",
      "[step: 2715] loss: 0.17821654677391052\n",
      "[step: 2716] loss: 0.17736132442951202\n",
      "[step: 2717] loss: 0.1769336462020874\n",
      "[step: 2718] loss: 0.17710307240486145\n",
      "[step: 2719] loss: 0.17725883424282074\n",
      "[step: 2720] loss: 0.17690131068229675\n",
      "[step: 2721] loss: 0.17628790438175201\n",
      "[step: 2722] loss: 0.17598530650138855\n",
      "[step: 2723] loss: 0.17607851326465607\n",
      "[step: 2724] loss: 0.17611010372638702\n",
      "[step: 2725] loss: 0.17577196657657623\n",
      "[step: 2726] loss: 0.17531150579452515\n",
      "[step: 2727] loss: 0.1751002073287964\n",
      "[step: 2728] loss: 0.1751159131526947\n",
      "[step: 2729] loss: 0.17504823207855225\n",
      "[step: 2730] loss: 0.17476187646389008\n",
      "[step: 2731] loss: 0.17443163692951202\n",
      "[step: 2732] loss: 0.17424504458904266\n",
      "[step: 2733] loss: 0.17417296767234802\n",
      "[step: 2734] loss: 0.17406471073627472\n",
      "[step: 2735] loss: 0.17385002970695496\n",
      "[step: 2736] loss: 0.17359504103660583\n",
      "[step: 2737] loss: 0.17339523136615753\n",
      "[step: 2738] loss: 0.173267662525177\n",
      "[step: 2739] loss: 0.17315194010734558\n",
      "[step: 2740] loss: 0.1729850172996521\n",
      "[step: 2741] loss: 0.17277048528194427\n",
      "[step: 2742] loss: 0.17256547510623932\n",
      "[step: 2743] loss: 0.17241017520427704\n",
      "[step: 2744] loss: 0.17228420078754425\n",
      "[step: 2745] loss: 0.17213913798332214\n",
      "[step: 2746] loss: 0.1719556450843811\n",
      "[step: 2747] loss: 0.17176252603530884\n",
      "[step: 2748] loss: 0.17159277200698853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2749] loss: 0.17144893109798431\n",
      "[step: 2750] loss: 0.1713077276945114\n",
      "[step: 2751] loss: 0.17114874720573425\n",
      "[step: 2752] loss: 0.17097587883472443\n",
      "[step: 2753] loss: 0.17080453038215637\n",
      "[step: 2754] loss: 0.1706453561782837\n",
      "[step: 2755] loss: 0.1704963594675064\n",
      "[step: 2756] loss: 0.170348659157753\n",
      "[step: 2757] loss: 0.17019440233707428\n",
      "[step: 2758] loss: 0.17003335058689117\n",
      "[step: 2759] loss: 0.16987058520317078\n",
      "[step: 2760] loss: 0.16971302032470703\n",
      "[step: 2761] loss: 0.1695629209280014\n",
      "[step: 2762] loss: 0.1694156527519226\n",
      "[step: 2763] loss: 0.1692657768726349\n",
      "[step: 2764] loss: 0.16911156475543976\n",
      "[step: 2765] loss: 0.16895529627799988\n",
      "[step: 2766] loss: 0.16880100965499878\n",
      "[step: 2767] loss: 0.16865074634552002\n",
      "[step: 2768] loss: 0.16850337386131287\n",
      "[step: 2769] loss: 0.1683557629585266\n",
      "[step: 2770] loss: 0.1682078242301941\n",
      "[step: 2771] loss: 0.16805794835090637\n",
      "[step: 2772] loss: 0.1679076850414276\n",
      "[step: 2773] loss: 0.1677587330341339\n",
      "[step: 2774] loss: 0.16761097311973572\n",
      "[step: 2775] loss: 0.1674652397632599\n",
      "[step: 2776] loss: 0.16732004284858704\n",
      "[step: 2777] loss: 0.16717468202114105\n",
      "[step: 2778] loss: 0.16702961921691895\n",
      "[step: 2779] loss: 0.1668834388256073\n",
      "[step: 2780] loss: 0.16673770546913147\n",
      "[step: 2781] loss: 0.16659289598464966\n",
      "[step: 2782] loss: 0.16644956171512604\n",
      "[step: 2783] loss: 0.16630646586418152\n",
      "[step: 2784] loss: 0.16616331040859222\n",
      "[step: 2785] loss: 0.166021466255188\n",
      "[step: 2786] loss: 0.16587939858436584\n",
      "[step: 2787] loss: 0.16573746502399445\n",
      "[step: 2788] loss: 0.16559576988220215\n",
      "[step: 2789] loss: 0.16545423865318298\n",
      "[step: 2790] loss: 0.16531315445899963\n",
      "[step: 2791] loss: 0.16517293453216553\n",
      "[step: 2792] loss: 0.16503335535526276\n",
      "[step: 2793] loss: 0.16489413380622864\n",
      "[step: 2794] loss: 0.16475480794906616\n",
      "[step: 2795] loss: 0.16461658477783203\n",
      "[step: 2796] loss: 0.16447772085666656\n",
      "[step: 2797] loss: 0.16433969140052795\n",
      "[step: 2798] loss: 0.1642022281885147\n",
      "[step: 2799] loss: 0.16406480967998505\n",
      "[step: 2800] loss: 0.16392715275287628\n",
      "[step: 2801] loss: 0.1637904942035675\n",
      "[step: 2802] loss: 0.1636541485786438\n",
      "[step: 2803] loss: 0.16351813077926636\n",
      "[step: 2804] loss: 0.1633824110031128\n",
      "[step: 2805] loss: 0.163247212767601\n",
      "[step: 2806] loss: 0.1631125807762146\n",
      "[step: 2807] loss: 0.16297736763954163\n",
      "[step: 2808] loss: 0.16284319758415222\n",
      "[step: 2809] loss: 0.16270899772644043\n",
      "[step: 2810] loss: 0.1625756472349167\n",
      "[step: 2811] loss: 0.16244204342365265\n",
      "[step: 2812] loss: 0.16230875253677368\n",
      "[step: 2813] loss: 0.16217611730098724\n",
      "[step: 2814] loss: 0.16204392910003662\n",
      "[step: 2815] loss: 0.16191163659095764\n",
      "[step: 2816] loss: 0.16177955269813538\n",
      "[step: 2817] loss: 0.16164827346801758\n",
      "[step: 2818] loss: 0.16151756048202515\n",
      "[step: 2819] loss: 0.16138653457164764\n",
      "[step: 2820] loss: 0.1612561047077179\n",
      "[step: 2821] loss: 0.16112634539604187\n",
      "[step: 2822] loss: 0.16099725663661957\n",
      "[step: 2823] loss: 0.16086895763874054\n",
      "[step: 2824] loss: 0.16074198484420776\n",
      "[step: 2825] loss: 0.16061651706695557\n",
      "[step: 2826] loss: 0.1604938805103302\n",
      "[step: 2827] loss: 0.16037575900554657\n",
      "[step: 2828] loss: 0.16026510298252106\n",
      "[step: 2829] loss: 0.16016629338264465\n",
      "[step: 2830] loss: 0.16008982062339783\n",
      "[step: 2831] loss: 0.16005024313926697\n",
      "[step: 2832] loss: 0.16007892787456512\n",
      "[step: 2833] loss: 0.16022591292858124\n",
      "[step: 2834] loss: 0.16059066355228424\n",
      "[step: 2835] loss: 0.16132962703704834\n",
      "[step: 2836] loss: 0.16274034976959229\n",
      "[step: 2837] loss: 0.16517233848571777\n",
      "[step: 2838] loss: 0.1691654622554779\n",
      "[step: 2839] loss: 0.17441293597221375\n",
      "[step: 2840] loss: 0.17990832030773163\n",
      "[step: 2841] loss: 0.1812237799167633\n",
      "[step: 2842] loss: 0.17639262974262238\n",
      "[step: 2843] loss: 0.16677948832511902\n",
      "[step: 2844] loss: 0.1602427363395691\n",
      "[step: 2845] loss: 0.16057026386260986\n",
      "[step: 2846] loss: 0.1646871566772461\n",
      "[step: 2847] loss: 0.16698412597179413\n",
      "[step: 2848] loss: 0.16489605605602264\n",
      "[step: 2849] loss: 0.16139476001262665\n",
      "[step: 2850] loss: 0.15965679287910461\n",
      "[step: 2851] loss: 0.16015227138996124\n",
      "[step: 2852] loss: 0.16111110150814056\n",
      "[step: 2853] loss: 0.1611396074295044\n",
      "[step: 2854] loss: 0.16028650104999542\n",
      "[step: 2855] loss: 0.1589498370885849\n",
      "[step: 2856] loss: 0.15808628499507904\n",
      "[step: 2857] loss: 0.1583261489868164\n",
      "[step: 2858] loss: 0.1590760052204132\n",
      "[step: 2859] loss: 0.1589689552783966\n",
      "[step: 2860] loss: 0.15763330459594727\n",
      "[step: 2861] loss: 0.15665610134601593\n",
      "[step: 2862] loss: 0.1570987105369568\n",
      "[step: 2863] loss: 0.1578996777534485\n",
      "[step: 2864] loss: 0.15757039189338684\n",
      "[step: 2865] loss: 0.1563732922077179\n",
      "[step: 2866] loss: 0.15588457882404327\n",
      "[step: 2867] loss: 0.1564207226037979\n",
      "[step: 2868] loss: 0.15678510069847107\n",
      "[step: 2869] loss: 0.15628032386302948\n",
      "[step: 2870] loss: 0.1555606722831726\n",
      "[step: 2871] loss: 0.1554495096206665\n",
      "[step: 2872] loss: 0.15571723878383636\n",
      "[step: 2873] loss: 0.1556926816701889\n",
      "[step: 2874] loss: 0.15533024072647095\n",
      "[step: 2875] loss: 0.1550462543964386\n",
      "[step: 2876] loss: 0.15499141812324524\n",
      "[step: 2877] loss: 0.1549459844827652\n",
      "[step: 2878] loss: 0.15479148924350739\n",
      "[step: 2879] loss: 0.1546466052532196\n",
      "[step: 2880] loss: 0.15456251800060272\n",
      "[step: 2881] loss: 0.15443788468837738\n",
      "[step: 2882] loss: 0.1542419195175171\n",
      "[step: 2883] loss: 0.15409398078918457\n",
      "[step: 2884] loss: 0.1540561020374298\n",
      "[step: 2885] loss: 0.154013991355896\n",
      "[step: 2886] loss: 0.1538572758436203\n",
      "[step: 2887] loss: 0.15364639461040497\n",
      "[step: 2888] loss: 0.1535165011882782\n",
      "[step: 2889] loss: 0.15347948670387268\n",
      "[step: 2890] loss: 0.15342429280281067\n",
      "[step: 2891] loss: 0.153285413980484\n",
      "[step: 2892] loss: 0.15311576426029205\n",
      "[step: 2893] loss: 0.15299351513385773\n",
      "[step: 2894] loss: 0.15291857719421387\n",
      "[step: 2895] loss: 0.15283606946468353\n",
      "[step: 2896] loss: 0.15272094309329987\n",
      "[step: 2897] loss: 0.1525968313217163\n",
      "[step: 2898] loss: 0.15248709917068481\n",
      "[step: 2899] loss: 0.15238527953624725\n",
      "[step: 2900] loss: 0.15227802097797394\n",
      "[step: 2901] loss: 0.15216754376888275\n",
      "[step: 2902] loss: 0.15206605195999146\n",
      "[step: 2903] loss: 0.15197083353996277\n",
      "[step: 2904] loss: 0.15186847746372223\n",
      "[step: 2905] loss: 0.15175364911556244\n",
      "[step: 2906] loss: 0.15163838863372803\n",
      "[step: 2907] loss: 0.15153485536575317\n",
      "[step: 2908] loss: 0.15144182741641998\n",
      "[step: 2909] loss: 0.1513461172580719\n",
      "[step: 2910] loss: 0.1512400060892105\n",
      "[step: 2911] loss: 0.15112842619419098\n",
      "[step: 2912] loss: 0.15102097392082214\n",
      "[step: 2913] loss: 0.15092024207115173\n",
      "[step: 2914] loss: 0.15082120895385742\n",
      "[step: 2915] loss: 0.15072011947631836\n",
      "[step: 2916] loss: 0.15061655640602112\n",
      "[step: 2917] loss: 0.15051326155662537\n",
      "[step: 2918] loss: 0.15041175484657288\n",
      "[step: 2919] loss: 0.1503099501132965\n",
      "[step: 2920] loss: 0.15020686388015747\n",
      "[step: 2921] loss: 0.15010342001914978\n",
      "[step: 2922] loss: 0.15000101923942566\n",
      "[step: 2923] loss: 0.14990144968032837\n",
      "[step: 2924] loss: 0.1498020887374878\n",
      "[step: 2925] loss: 0.1497018039226532\n",
      "[step: 2926] loss: 0.14959944784641266\n",
      "[step: 2927] loss: 0.14949654042720795\n",
      "[step: 2928] loss: 0.1493949592113495\n",
      "[step: 2929] loss: 0.1492941528558731\n",
      "[step: 2930] loss: 0.1491941511631012\n",
      "[step: 2931] loss: 0.14909397065639496\n",
      "[step: 2932] loss: 0.14899331331253052\n",
      "[step: 2933] loss: 0.14889298379421234\n",
      "[step: 2934] loss: 0.1487928032875061\n",
      "[step: 2935] loss: 0.14869266748428345\n",
      "[step: 2936] loss: 0.1485927402973175\n",
      "[step: 2937] loss: 0.1484922468662262\n",
      "[step: 2938] loss: 0.1483917236328125\n",
      "[step: 2939] loss: 0.14829155802726746\n",
      "[step: 2940] loss: 0.1481919288635254\n",
      "[step: 2941] loss: 0.14809229969978333\n",
      "[step: 2942] loss: 0.14799295365810394\n",
      "[step: 2943] loss: 0.14789363741874695\n",
      "[step: 2944] loss: 0.14779403805732727\n",
      "[step: 2945] loss: 0.1476949155330658\n",
      "[step: 2946] loss: 0.14759525656700134\n",
      "[step: 2947] loss: 0.14749634265899658\n",
      "[step: 2948] loss: 0.1473972350358963\n",
      "[step: 2949] loss: 0.14729821681976318\n",
      "[step: 2950] loss: 0.14719897508621216\n",
      "[step: 2951] loss: 0.1471002697944641\n",
      "[step: 2952] loss: 0.14700116217136383\n",
      "[step: 2953] loss: 0.14690238237380981\n",
      "[step: 2954] loss: 0.14680364727973938\n",
      "[step: 2955] loss: 0.14670516550540924\n",
      "[step: 2956] loss: 0.14660689234733582\n",
      "[step: 2957] loss: 0.14650805294513702\n",
      "[step: 2958] loss: 0.14640939235687256\n",
      "[step: 2959] loss: 0.14631125330924988\n",
      "[step: 2960] loss: 0.14621290564537048\n",
      "[step: 2961] loss: 0.1461150050163269\n",
      "[step: 2962] loss: 0.1460164487361908\n",
      "[step: 2963] loss: 0.14591877162456512\n",
      "[step: 2964] loss: 0.14582066237926483\n",
      "[step: 2965] loss: 0.1457223743200302\n",
      "[step: 2966] loss: 0.14562487602233887\n",
      "[step: 2967] loss: 0.14552676677703857\n",
      "[step: 2968] loss: 0.14542916417121887\n",
      "[step: 2969] loss: 0.1453312784433365\n",
      "[step: 2970] loss: 0.14523394405841827\n",
      "[step: 2971] loss: 0.14513638615608215\n",
      "[step: 2972] loss: 0.145038902759552\n",
      "[step: 2973] loss: 0.14494125545024872\n",
      "[step: 2974] loss: 0.1448439359664917\n",
      "[step: 2975] loss: 0.14474649727344513\n",
      "[step: 2976] loss: 0.14464940130710602\n",
      "[step: 2977] loss: 0.14455240964889526\n",
      "[step: 2978] loss: 0.14445564150810242\n",
      "[step: 2979] loss: 0.14435911178588867\n",
      "[step: 2980] loss: 0.14426350593566895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2981] loss: 0.14416863024234772\n",
      "[step: 2982] loss: 0.14407587051391602\n",
      "[step: 2983] loss: 0.1439870446920395\n",
      "[step: 2984] loss: 0.14390501379966736\n",
      "[step: 2985] loss: 0.14383742213249207\n",
      "[step: 2986] loss: 0.14379635453224182\n",
      "[step: 2987] loss: 0.14380860328674316\n",
      "[step: 2988] loss: 0.143927663564682\n",
      "[step: 2989] loss: 0.1442575603723526\n",
      "[step: 2990] loss: 0.14501850306987762\n",
      "[step: 2991] loss: 0.14661440253257751\n",
      "[step: 2992] loss: 0.14985935389995575\n",
      "[step: 2993] loss: 0.15587207674980164\n",
      "[step: 2994] loss: 0.1661786437034607\n",
      "[step: 2995] loss: 0.17900000512599945\n",
      "[step: 2996] loss: 0.18825113773345947\n",
      "[step: 2997] loss: 0.17975935339927673\n",
      "[step: 2998] loss: 0.15800324082374573\n",
      "[step: 2999] loss: 0.1438138782978058\n",
      "[step: 3000] loss: 0.15036657452583313\n",
      "[step: 3001] loss: 0.16279292106628418\n",
      "[step: 3002] loss: 0.1606244593858719\n",
      "[step: 3003] loss: 0.14824005961418152\n",
      "[step: 3004] loss: 0.14423827826976776\n",
      "[step: 3005] loss: 0.15112853050231934\n",
      "[step: 3006] loss: 0.1538556069135666\n",
      "[step: 3007] loss: 0.1474360078573227\n",
      "[step: 3008] loss: 0.1437317132949829\n",
      "[step: 3009] loss: 0.14699393510818481\n",
      "[step: 3010] loss: 0.1484971046447754\n",
      "[step: 3011] loss: 0.1451558917760849\n",
      "[step: 3012] loss: 0.14333730936050415\n",
      "[step: 3013] loss: 0.1450146734714508\n",
      "[step: 3014] loss: 0.1452711522579193\n",
      "[step: 3015] loss: 0.14329323172569275\n",
      "[step: 3016] loss: 0.1428648829460144\n",
      "[step: 3017] loss: 0.14370721578598022\n",
      "[step: 3018] loss: 0.1431429237127304\n",
      "[step: 3019] loss: 0.14210224151611328\n",
      "[step: 3020] loss: 0.1423126459121704\n",
      "[step: 3021] loss: 0.1425492912530899\n",
      "[step: 3022] loss: 0.141754612326622\n",
      "[step: 3023] loss: 0.14131174981594086\n",
      "[step: 3024] loss: 0.1417526751756668\n",
      "[step: 3025] loss: 0.14162367582321167\n",
      "[step: 3026] loss: 0.14084070920944214\n",
      "[step: 3027] loss: 0.14076104760169983\n",
      "[step: 3028] loss: 0.14115852117538452\n",
      "[step: 3029] loss: 0.14084245264530182\n",
      "[step: 3030] loss: 0.14021357893943787\n",
      "[step: 3031] loss: 0.14029793441295624\n",
      "[step: 3032] loss: 0.14059041440486908\n",
      "[step: 3033] loss: 0.1402232050895691\n",
      "[step: 3034] loss: 0.13975133001804352\n",
      "[step: 3035] loss: 0.1398768424987793\n",
      "[step: 3036] loss: 0.1400512456893921\n",
      "[step: 3037] loss: 0.1397136002779007\n",
      "[step: 3038] loss: 0.13936838507652283\n",
      "[step: 3039] loss: 0.13946881890296936\n",
      "[step: 3040] loss: 0.13956135511398315\n",
      "[step: 3041] loss: 0.13928182423114777\n",
      "[step: 3042] loss: 0.13902300596237183\n",
      "[step: 3043] loss: 0.13907581567764282\n",
      "[step: 3044] loss: 0.13911114633083344\n",
      "[step: 3045] loss: 0.1388942003250122\n",
      "[step: 3046] loss: 0.13869236409664154\n",
      "[step: 3047] loss: 0.13869763910770416\n",
      "[step: 3048] loss: 0.13869664072990417\n",
      "[step: 3049] loss: 0.13853058218955994\n",
      "[step: 3050] loss: 0.13836684823036194\n",
      "[step: 3051] loss: 0.13833458721637726\n",
      "[step: 3052] loss: 0.13830754160881042\n",
      "[step: 3053] loss: 0.1381792277097702\n",
      "[step: 3054] loss: 0.13804249465465546\n",
      "[step: 3055] loss: 0.1379850208759308\n",
      "[step: 3056] loss: 0.1379389762878418\n",
      "[step: 3057] loss: 0.13783389329910278\n",
      "[step: 3058] loss: 0.13771705329418182\n",
      "[step: 3059] loss: 0.13764464855194092\n",
      "[step: 3060] loss: 0.13758471608161926\n",
      "[step: 3061] loss: 0.1374923437833786\n",
      "[step: 3062] loss: 0.13738933205604553\n",
      "[step: 3063] loss: 0.13731029629707336\n",
      "[step: 3064] loss: 0.13724195957183838\n",
      "[step: 3065] loss: 0.13715504109859467\n",
      "[step: 3066] loss: 0.1370595395565033\n",
      "[step: 3067] loss: 0.13697829842567444\n",
      "[step: 3068] loss: 0.13690507411956787\n",
      "[step: 3069] loss: 0.13682140409946442\n",
      "[step: 3070] loss: 0.1367301046848297\n",
      "[step: 3071] loss: 0.13664701581001282\n",
      "[step: 3072] loss: 0.13657128810882568\n",
      "[step: 3073] loss: 0.13649052381515503\n",
      "[step: 3074] loss: 0.13640210032463074\n",
      "[step: 3075] loss: 0.1363169550895691\n",
      "[step: 3076] loss: 0.13623932003974915\n",
      "[step: 3077] loss: 0.13616040349006653\n",
      "[step: 3078] loss: 0.13607513904571533\n",
      "[step: 3079] loss: 0.13598909974098206\n",
      "[step: 3080] loss: 0.1359085738658905\n",
      "[step: 3081] loss: 0.13583040237426758\n",
      "[step: 3082] loss: 0.1357479691505432\n",
      "[step: 3083] loss: 0.1356627345085144\n",
      "[step: 3084] loss: 0.13557955622673035\n",
      "[step: 3085] loss: 0.13549983501434326\n",
      "[step: 3086] loss: 0.13541953265666962\n",
      "[step: 3087] loss: 0.13533654808998108\n",
      "[step: 3088] loss: 0.13525287806987762\n",
      "[step: 3089] loss: 0.1351711004972458\n",
      "[step: 3090] loss: 0.1350906938314438\n",
      "[step: 3091] loss: 0.13500894606113434\n",
      "[step: 3092] loss: 0.13492661714553833\n",
      "[step: 3093] loss: 0.13484390079975128\n",
      "[step: 3094] loss: 0.13476255536079407\n",
      "[step: 3095] loss: 0.13468113541603088\n",
      "[step: 3096] loss: 0.1345996856689453\n",
      "[step: 3097] loss: 0.13451719284057617\n",
      "[step: 3098] loss: 0.1344350278377533\n",
      "[step: 3099] loss: 0.13435371220111847\n",
      "[step: 3100] loss: 0.13427208364009857\n",
      "[step: 3101] loss: 0.13419032096862793\n",
      "[step: 3102] loss: 0.13410833477973938\n",
      "[step: 3103] loss: 0.13402655720710754\n",
      "[step: 3104] loss: 0.1339450478553772\n",
      "[step: 3105] loss: 0.1338634043931961\n",
      "[step: 3106] loss: 0.13378170132637024\n",
      "[step: 3107] loss: 0.1336996853351593\n",
      "[step: 3108] loss: 0.1336180567741394\n",
      "[step: 3109] loss: 0.13353632390499115\n",
      "[step: 3110] loss: 0.13345474004745483\n",
      "[step: 3111] loss: 0.13337302207946777\n",
      "[step: 3112] loss: 0.13329118490219116\n",
      "[step: 3113] loss: 0.13320936262607574\n",
      "[step: 3114] loss: 0.13312780857086182\n",
      "[step: 3115] loss: 0.1330460011959076\n",
      "[step: 3116] loss: 0.13296419382095337\n",
      "[step: 3117] loss: 0.1328825205564499\n",
      "[step: 3118] loss: 0.13280099630355835\n",
      "[step: 3119] loss: 0.1327190399169922\n",
      "[step: 3120] loss: 0.1326373815536499\n",
      "[step: 3121] loss: 0.1325557976961136\n",
      "[step: 3122] loss: 0.1324739158153534\n",
      "[step: 3123] loss: 0.13239240646362305\n",
      "[step: 3124] loss: 0.13231071829795837\n",
      "[step: 3125] loss: 0.1322287619113922\n",
      "[step: 3126] loss: 0.13214726746082306\n",
      "[step: 3127] loss: 0.1320653259754181\n",
      "[step: 3128] loss: 0.13198357820510864\n",
      "[step: 3129] loss: 0.13190214335918427\n",
      "[step: 3130] loss: 0.13182029128074646\n",
      "[step: 3131] loss: 0.1317385882139206\n",
      "[step: 3132] loss: 0.13165707886219025\n",
      "[step: 3133] loss: 0.13157498836517334\n",
      "[step: 3134] loss: 0.13149358332157135\n",
      "[step: 3135] loss: 0.13141165673732758\n",
      "[step: 3136] loss: 0.13132984936237335\n",
      "[step: 3137] loss: 0.1312481164932251\n",
      "[step: 3138] loss: 0.1311665177345276\n",
      "[step: 3139] loss: 0.13108468055725098\n",
      "[step: 3140] loss: 0.1310030221939087\n",
      "[step: 3141] loss: 0.1309216320514679\n",
      "[step: 3142] loss: 0.13083937764167786\n",
      "[step: 3143] loss: 0.13075757026672363\n",
      "[step: 3144] loss: 0.1306760609149933\n",
      "[step: 3145] loss: 0.13059410452842712\n",
      "[step: 3146] loss: 0.1305125206708908\n",
      "[step: 3147] loss: 0.13043050467967987\n",
      "[step: 3148] loss: 0.1303490400314331\n",
      "[step: 3149] loss: 0.13026729226112366\n",
      "[step: 3150] loss: 0.13018546998500824\n",
      "[step: 3151] loss: 0.1301039308309555\n",
      "[step: 3152] loss: 0.1300221085548401\n",
      "[step: 3153] loss: 0.12994036078453064\n",
      "[step: 3154] loss: 0.12985850870609283\n",
      "[step: 3155] loss: 0.1297767162322998\n",
      "[step: 3156] loss: 0.12969496846199036\n",
      "[step: 3157] loss: 0.12961310148239136\n",
      "[step: 3158] loss: 0.12953153252601624\n",
      "[step: 3159] loss: 0.12944966554641724\n",
      "[step: 3160] loss: 0.1293678879737854\n",
      "[step: 3161] loss: 0.12928594648838043\n",
      "[step: 3162] loss: 0.12920449674129486\n",
      "[step: 3163] loss: 0.12912234663963318\n",
      "[step: 3164] loss: 0.12904079258441925\n",
      "[step: 3165] loss: 0.12895901501178741\n",
      "[step: 3166] loss: 0.12887702882289886\n",
      "[step: 3167] loss: 0.12879520654678345\n",
      "[step: 3168] loss: 0.12871360778808594\n",
      "[step: 3169] loss: 0.1286320835351944\n",
      "[step: 3170] loss: 0.12855003774166107\n",
      "[step: 3171] loss: 0.1284683346748352\n",
      "[step: 3172] loss: 0.12838667631149292\n",
      "[step: 3173] loss: 0.1283046305179596\n",
      "[step: 3174] loss: 0.12822282314300537\n",
      "[step: 3175] loss: 0.12814125418663025\n",
      "[step: 3176] loss: 0.1280594915151596\n",
      "[step: 3177] loss: 0.12797775864601135\n",
      "[step: 3178] loss: 0.12789605557918549\n",
      "[step: 3179] loss: 0.12781405448913574\n",
      "[step: 3180] loss: 0.12773196399211884\n",
      "[step: 3181] loss: 0.12765052914619446\n",
      "[step: 3182] loss: 0.12756867706775665\n",
      "[step: 3183] loss: 0.12748678028583527\n",
      "[step: 3184] loss: 0.12740491330623627\n",
      "[step: 3185] loss: 0.12732326984405518\n",
      "[step: 3186] loss: 0.1272413730621338\n",
      "[step: 3187] loss: 0.12715965509414673\n",
      "[step: 3188] loss: 0.12707801163196564\n",
      "[step: 3189] loss: 0.12699609994888306\n",
      "[step: 3190] loss: 0.12691429257392883\n",
      "[step: 3191] loss: 0.12683270871639252\n",
      "[step: 3192] loss: 0.12675072252750397\n",
      "[step: 3193] loss: 0.1266690194606781\n",
      "[step: 3194] loss: 0.126587375998497\n",
      "[step: 3195] loss: 0.12650547921657562\n",
      "[step: 3196] loss: 0.12642371654510498\n",
      "[step: 3197] loss: 0.1263422667980194\n",
      "[step: 3198] loss: 0.1262603998184204\n",
      "[step: 3199] loss: 0.12617915868759155\n",
      "[step: 3200] loss: 0.12609760463237762\n",
      "[step: 3201] loss: 0.126016765832901\n",
      "[step: 3202] loss: 0.12593653798103333\n",
      "[step: 3203] loss: 0.1258573681116104\n",
      "[step: 3204] loss: 0.1257806420326233\n",
      "[step: 3205] loss: 0.12570829689502716\n",
      "[step: 3206] loss: 0.12564446032047272\n",
      "[step: 3207] loss: 0.12559732794761658\n",
      "[step: 3208] loss: 0.12558318674564362\n",
      "[step: 3209] loss: 0.12563514709472656\n",
      "[step: 3210] loss: 0.12581999599933624\n",
      "[step: 3211] loss: 0.12627212703227997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3212] loss: 0.1272699236869812\n",
      "[step: 3213] loss: 0.12933538854122162\n",
      "[step: 3214] loss: 0.1334960013628006\n",
      "[step: 3215] loss: 0.14117009937763214\n",
      "[step: 3216] loss: 0.15400728583335876\n",
      "[step: 3217] loss: 0.16941064596176147\n",
      "[step: 3218] loss: 0.17848750948905945\n",
      "[step: 3219] loss: 0.16605249047279358\n",
      "[step: 3220] loss: 0.13981139659881592\n",
      "[step: 3221] loss: 0.12645648419857025\n",
      "[step: 3222] loss: 0.13657709956169128\n",
      "[step: 3223] loss: 0.14945131540298462\n",
      "[step: 3224] loss: 0.14429131150245667\n",
      "[step: 3225] loss: 0.13034740090370178\n",
      "[step: 3226] loss: 0.12840183079242706\n",
      "[step: 3227] loss: 0.13630345463752747\n",
      "[step: 3228] loss: 0.13714003562927246\n",
      "[step: 3229] loss: 0.1298636496067047\n",
      "[step: 3230] loss: 0.12723787128925323\n",
      "[step: 3231] loss: 0.13064076006412506\n",
      "[step: 3232] loss: 0.13102076947689056\n",
      "[step: 3233] loss: 0.12789946794509888\n",
      "[step: 3234] loss: 0.12675534188747406\n",
      "[step: 3235] loss: 0.1275542974472046\n",
      "[step: 3236] loss: 0.12740206718444824\n",
      "[step: 3237] loss: 0.12652352452278137\n",
      "[step: 3238] loss: 0.12599121034145355\n",
      "[step: 3239] loss: 0.12546972930431366\n",
      "[step: 3240] loss: 0.12536141276359558\n",
      "[step: 3241] loss: 0.12570329010486603\n",
      "[step: 3242] loss: 0.1250527799129486\n",
      "[step: 3243] loss: 0.1239364892244339\n",
      "[step: 3244] loss: 0.12426144629716873\n",
      "[step: 3245] loss: 0.12493529915809631\n",
      "[step: 3246] loss: 0.1240052729845047\n",
      "[step: 3247] loss: 0.12300056964159012\n",
      "[step: 3248] loss: 0.12366905808448792\n",
      "[step: 3249] loss: 0.12416261434555054\n",
      "[step: 3250] loss: 0.1231144368648529\n",
      "[step: 3251] loss: 0.12250812351703644\n",
      "[step: 3252] loss: 0.12323220074176788\n",
      "[step: 3253] loss: 0.12337255477905273\n",
      "[step: 3254] loss: 0.12247457355260849\n",
      "[step: 3255] loss: 0.12222634255886078\n",
      "[step: 3256] loss: 0.12275446951389313\n",
      "[step: 3257] loss: 0.12266217172145844\n",
      "[step: 3258] loss: 0.12204737961292267\n",
      "[step: 3259] loss: 0.12198818475008011\n",
      "[step: 3260] loss: 0.12225909531116486\n",
      "[step: 3261] loss: 0.12208621203899384\n",
      "[step: 3262] loss: 0.12173691391944885\n",
      "[step: 3263] loss: 0.12172391265630722\n",
      "[step: 3264] loss: 0.12179066240787506\n",
      "[step: 3265] loss: 0.1216321736574173\n",
      "[step: 3266] loss: 0.12146467715501785\n",
      "[step: 3267] loss: 0.12143145501613617\n",
      "[step: 3268] loss: 0.12137055397033691\n",
      "[step: 3269] loss: 0.1212562620639801\n",
      "[step: 3270] loss: 0.12118928879499435\n",
      "[step: 3271] loss: 0.12112286686897278\n",
      "[step: 3272] loss: 0.1210060715675354\n",
      "[step: 3273] loss: 0.12092585861682892\n",
      "[step: 3274] loss: 0.12089748680591583\n",
      "[step: 3275] loss: 0.12081500142812729\n",
      "[step: 3276] loss: 0.1206841766834259\n",
      "[step: 3277] loss: 0.12061664462089539\n",
      "[step: 3278] loss: 0.12059509009122849\n",
      "[step: 3279] loss: 0.12051209062337875\n",
      "[step: 3280] loss: 0.12038766592741013\n",
      "[step: 3281] loss: 0.12031926214694977\n",
      "[step: 3282] loss: 0.12028855830430984\n",
      "[step: 3283] loss: 0.12021198123693466\n",
      "[step: 3284] loss: 0.1201038807630539\n",
      "[step: 3285] loss: 0.1200309693813324\n",
      "[step: 3286] loss: 0.11998449265956879\n",
      "[step: 3287] loss: 0.11991433799266815\n",
      "[step: 3288] loss: 0.11982360482215881\n",
      "[step: 3289] loss: 0.11974892020225525\n",
      "[step: 3290] loss: 0.11968810856342316\n",
      "[step: 3291] loss: 0.11961811035871506\n",
      "[step: 3292] loss: 0.11954031884670258\n",
      "[step: 3293] loss: 0.11946840584278107\n",
      "[step: 3294] loss: 0.11939939856529236\n",
      "[step: 3295] loss: 0.11932671815156937\n",
      "[step: 3296] loss: 0.11925478279590607\n",
      "[step: 3297] loss: 0.1191866472363472\n",
      "[step: 3298] loss: 0.11911582946777344\n",
      "[step: 3299] loss: 0.11904066801071167\n",
      "[step: 3300] loss: 0.1189691424369812\n",
      "[step: 3301] loss: 0.11890225857496262\n",
      "[step: 3302] loss: 0.11883348226547241\n",
      "[step: 3303] loss: 0.11875879019498825\n",
      "[step: 3304] loss: 0.11868554353713989\n",
      "[step: 3305] loss: 0.11861728131771088\n",
      "[step: 3306] loss: 0.11854960769414902\n",
      "[step: 3307] loss: 0.11847780644893646\n",
      "[step: 3308] loss: 0.11840479075908661\n",
      "[step: 3309] loss: 0.11833450943231583\n",
      "[step: 3310] loss: 0.11826592683792114\n",
      "[step: 3311] loss: 0.11819569766521454\n",
      "[step: 3312] loss: 0.11812400817871094\n",
      "[step: 3313] loss: 0.11805335432291031\n",
      "[step: 3314] loss: 0.1179833635687828\n",
      "[step: 3315] loss: 0.11791317164897919\n",
      "[step: 3316] loss: 0.11784309148788452\n",
      "[step: 3317] loss: 0.11777272820472717\n",
      "[step: 3318] loss: 0.1177029088139534\n",
      "[step: 3319] loss: 0.11763228476047516\n",
      "[step: 3320] loss: 0.11756188422441483\n",
      "[step: 3321] loss: 0.11749158799648285\n",
      "[step: 3322] loss: 0.11742201447486877\n",
      "[step: 3323] loss: 0.11735203117132187\n",
      "[step: 3324] loss: 0.1172814816236496\n",
      "[step: 3325] loss: 0.11721101403236389\n",
      "[step: 3326] loss: 0.11714138090610504\n",
      "[step: 3327] loss: 0.11707168817520142\n",
      "[step: 3328] loss: 0.1170014962553978\n",
      "[step: 3329] loss: 0.11693158745765686\n",
      "[step: 3330] loss: 0.1168612390756607\n",
      "[step: 3331] loss: 0.11679168045520782\n",
      "[step: 3332] loss: 0.11672195047140121\n",
      "[step: 3333] loss: 0.11665192246437073\n",
      "[step: 3334] loss: 0.11658211052417755\n",
      "[step: 3335] loss: 0.11651232838630676\n",
      "[step: 3336] loss: 0.11644240468740463\n",
      "[step: 3337] loss: 0.11637253314256668\n",
      "[step: 3338] loss: 0.11630295217037201\n",
      "[step: 3339] loss: 0.1162332147359848\n",
      "[step: 3340] loss: 0.116163469851017\n",
      "[step: 3341] loss: 0.11609362065792084\n",
      "[step: 3342] loss: 0.11602413654327393\n",
      "[step: 3343] loss: 0.11595439165830612\n",
      "[step: 3344] loss: 0.11588473618030548\n",
      "[step: 3345] loss: 0.11581509560346603\n",
      "[step: 3346] loss: 0.11574582755565643\n",
      "[step: 3347] loss: 0.11567606776952744\n",
      "[step: 3348] loss: 0.11560671031475067\n",
      "[step: 3349] loss: 0.11553697288036346\n",
      "[step: 3350] loss: 0.11546747386455536\n",
      "[step: 3351] loss: 0.11539821326732635\n",
      "[step: 3352] loss: 0.11532864719629288\n",
      "[step: 3353] loss: 0.11525920033454895\n",
      "[step: 3354] loss: 0.11518987268209457\n",
      "[step: 3355] loss: 0.11512065678834915\n",
      "[step: 3356] loss: 0.1150510162115097\n",
      "[step: 3357] loss: 0.11498188972473145\n",
      "[step: 3358] loss: 0.11491265892982483\n",
      "[step: 3359] loss: 0.11484338343143463\n",
      "[step: 3360] loss: 0.1147741973400116\n",
      "[step: 3361] loss: 0.11470489203929901\n",
      "[step: 3362] loss: 0.11463581025600433\n",
      "[step: 3363] loss: 0.11456654965877533\n",
      "[step: 3364] loss: 0.1144973635673523\n",
      "[step: 3365] loss: 0.11442828178405762\n",
      "[step: 3366] loss: 0.11435931921005249\n",
      "[step: 3367] loss: 0.1142900362610817\n",
      "[step: 3368] loss: 0.11422107368707657\n",
      "[step: 3369] loss: 0.11415193229913712\n",
      "[step: 3370] loss: 0.11408326029777527\n",
      "[step: 3371] loss: 0.11401421576738358\n",
      "[step: 3372] loss: 0.11394499242305756\n",
      "[step: 3373] loss: 0.11387636512517929\n",
      "[step: 3374] loss: 0.11380742490291595\n",
      "[step: 3375] loss: 0.11373848468065262\n",
      "[step: 3376] loss: 0.1136694923043251\n",
      "[step: 3377] loss: 0.1136007308959961\n",
      "[step: 3378] loss: 0.11353202909231186\n",
      "[step: 3379] loss: 0.1134633719921112\n",
      "[step: 3380] loss: 0.11339455842971802\n",
      "[step: 3381] loss: 0.11332578957080841\n",
      "[step: 3382] loss: 0.1132572814822197\n",
      "[step: 3383] loss: 0.11318833380937576\n",
      "[step: 3384] loss: 0.11311991512775421\n",
      "[step: 3385] loss: 0.11305105686187744\n",
      "[step: 3386] loss: 0.11298251897096634\n",
      "[step: 3387] loss: 0.11291398108005524\n",
      "[step: 3388] loss: 0.11284536868333817\n",
      "[step: 3389] loss: 0.11277692019939423\n",
      "[step: 3390] loss: 0.11270837485790253\n",
      "[step: 3391] loss: 0.1126399040222168\n",
      "[step: 3392] loss: 0.1125713437795639\n",
      "[step: 3393] loss: 0.11250291019678116\n",
      "[step: 3394] loss: 0.11243465542793274\n",
      "[step: 3395] loss: 0.11236630380153656\n",
      "[step: 3396] loss: 0.11229786276817322\n",
      "[step: 3397] loss: 0.11222955584526062\n",
      "[step: 3398] loss: 0.11216121912002563\n",
      "[step: 3399] loss: 0.11209332197904587\n",
      "[step: 3400] loss: 0.1120249405503273\n",
      "[step: 3401] loss: 0.11195676773786545\n",
      "[step: 3402] loss: 0.11188866198062897\n",
      "[step: 3403] loss: 0.11182017624378204\n",
      "[step: 3404] loss: 0.11175217479467392\n",
      "[step: 3405] loss: 0.111684150993824\n",
      "[step: 3406] loss: 0.1116161197423935\n",
      "[step: 3407] loss: 0.11154778301715851\n",
      "[step: 3408] loss: 0.1114799752831459\n",
      "[step: 3409] loss: 0.1114119440317154\n",
      "[step: 3410] loss: 0.11134377866983414\n",
      "[step: 3411] loss: 0.11127610504627228\n",
      "[step: 3412] loss: 0.11120794713497162\n",
      "[step: 3413] loss: 0.11114040017127991\n",
      "[step: 3414] loss: 0.11107222735881805\n",
      "[step: 3415] loss: 0.11100418120622635\n",
      "[step: 3416] loss: 0.11093653738498688\n",
      "[step: 3417] loss: 0.11086881905794144\n",
      "[step: 3418] loss: 0.11080096662044525\n",
      "[step: 3419] loss: 0.1107332706451416\n",
      "[step: 3420] loss: 0.11066554486751556\n",
      "[step: 3421] loss: 0.11059791594743729\n",
      "[step: 3422] loss: 0.11053021252155304\n",
      "[step: 3423] loss: 0.11046238243579865\n",
      "[step: 3424] loss: 0.11039511114358902\n",
      "[step: 3425] loss: 0.11032745242118835\n",
      "[step: 3426] loss: 0.11026021838188171\n",
      "[step: 3427] loss: 0.11019309610128403\n",
      "[step: 3428] loss: 0.110126793384552\n",
      "[step: 3429] loss: 0.11006127297878265\n",
      "[step: 3430] loss: 0.10999739170074463\n",
      "[step: 3431] loss: 0.1099366620182991\n",
      "[step: 3432] loss: 0.10988247394561768\n",
      "[step: 3433] loss: 0.10984115302562714\n",
      "[step: 3434] loss: 0.10982488095760345\n",
      "[step: 3435] loss: 0.1098593920469284\n",
      "[step: 3436] loss: 0.1099969744682312\n",
      "[step: 3437] loss: 0.11034511774778366\n",
      "[step: 3438] loss: 0.11112045496702194\n",
      "[step: 3439] loss: 0.11276952922344208\n",
      "[step: 3440] loss: 0.11609815806150436\n",
      "[step: 3441] loss: 0.12257082015275955\n",
      "[step: 3442] loss: 0.13357427716255188\n",
      "[step: 3443] loss: 0.14901837706565857\n",
      "[step: 3444] loss: 0.16047000885009766\n",
      "[step: 3445] loss: 0.15507203340530396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3446] loss: 0.1316029280424118\n",
      "[step: 3447] loss: 0.11341086030006409\n",
      "[step: 3448] loss: 0.11671199649572372\n",
      "[step: 3449] loss: 0.12871313095092773\n",
      "[step: 3450] loss: 0.13050535321235657\n",
      "[step: 3451] loss: 0.12070943415164948\n",
      "[step: 3452] loss: 0.1145201027393341\n",
      "[step: 3453] loss: 0.11699294298887253\n",
      "[step: 3454] loss: 0.11951886862516403\n",
      "[step: 3455] loss: 0.11826644837856293\n",
      "[step: 3456] loss: 0.11603598296642303\n",
      "[step: 3457] loss: 0.11382468789815903\n",
      "[step: 3458] loss: 0.11240970343351364\n",
      "[step: 3459] loss: 0.11408174782991409\n",
      "[step: 3460] loss: 0.11503937095403671\n",
      "[step: 3461] loss: 0.1118273213505745\n",
      "[step: 3462] loss: 0.10932944715023041\n",
      "[step: 3463] loss: 0.1117386519908905\n",
      "[step: 3464] loss: 0.11339083313941956\n",
      "[step: 3465] loss: 0.11001421511173248\n",
      "[step: 3466] loss: 0.10808280110359192\n",
      "[step: 3467] loss: 0.11070731282234192\n",
      "[step: 3468] loss: 0.11132105439901352\n",
      "[step: 3469] loss: 0.10852941125631332\n",
      "[step: 3470] loss: 0.1080622673034668\n",
      "[step: 3471] loss: 0.10979761928319931\n",
      "[step: 3472] loss: 0.10944851487874985\n",
      "[step: 3473] loss: 0.10799111425876617\n",
      "[step: 3474] loss: 0.10819154977798462\n",
      "[step: 3475] loss: 0.10870254039764404\n",
      "[step: 3476] loss: 0.10814984142780304\n",
      "[step: 3477] loss: 0.1078416183590889\n",
      "[step: 3478] loss: 0.10804837942123413\n",
      "[step: 3479] loss: 0.10772933065891266\n",
      "[step: 3480] loss: 0.1074228584766388\n",
      "[step: 3481] loss: 0.10772550106048584\n",
      "[step: 3482] loss: 0.10763697326183319\n",
      "[step: 3483] loss: 0.107059046626091\n",
      "[step: 3484] loss: 0.1070760041475296\n",
      "[step: 3485] loss: 0.10741132497787476\n",
      "[step: 3486] loss: 0.10713164508342743\n",
      "[step: 3487] loss: 0.1067155972123146\n",
      "[step: 3488] loss: 0.10684272646903992\n",
      "[step: 3489] loss: 0.10699030756950378\n",
      "[step: 3490] loss: 0.10672630369663239\n",
      "[step: 3491] loss: 0.10652235150337219\n",
      "[step: 3492] loss: 0.10659044235944748\n",
      "[step: 3493] loss: 0.10656468570232391\n",
      "[step: 3494] loss: 0.10640546679496765\n",
      "[step: 3495] loss: 0.1063498854637146\n",
      "[step: 3496] loss: 0.10631942749023438\n",
      "[step: 3497] loss: 0.10620077699422836\n",
      "[step: 3498] loss: 0.106134332716465\n",
      "[step: 3499] loss: 0.1061379462480545\n",
      "[step: 3500] loss: 0.10605025291442871\n",
      "[step: 3501] loss: 0.10591490566730499\n",
      "[step: 3502] loss: 0.10588479042053223\n",
      "[step: 3503] loss: 0.10588641464710236\n",
      "[step: 3504] loss: 0.10579109936952591\n",
      "[step: 3505] loss: 0.10567861795425415\n",
      "[step: 3506] loss: 0.1056453287601471\n",
      "[step: 3507] loss: 0.10561994463205338\n",
      "[step: 3508] loss: 0.10553866624832153\n",
      "[step: 3509] loss: 0.10545963048934937\n",
      "[step: 3510] loss: 0.1054149717092514\n",
      "[step: 3511] loss: 0.10536044836044312\n",
      "[step: 3512] loss: 0.10529088973999023\n",
      "[step: 3513] loss: 0.10523705184459686\n",
      "[step: 3514] loss: 0.10518677532672882\n",
      "[step: 3515] loss: 0.105117566883564\n",
      "[step: 3516] loss: 0.10505126416683197\n",
      "[step: 3517] loss: 0.10500526428222656\n",
      "[step: 3518] loss: 0.1049555167555809\n",
      "[step: 3519] loss: 0.10488662868738174\n",
      "[step: 3520] loss: 0.10482066869735718\n",
      "[step: 3521] loss: 0.10477128624916077\n",
      "[step: 3522] loss: 0.1047198697924614\n",
      "[step: 3523] loss: 0.10465697199106216\n",
      "[step: 3524] loss: 0.10459601134061813\n",
      "[step: 3525] loss: 0.10454154759645462\n",
      "[step: 3526] loss: 0.1044854074716568\n",
      "[step: 3527] loss: 0.10442575812339783\n",
      "[step: 3528] loss: 0.10436958074569702\n",
      "[step: 3529] loss: 0.10431452840566635\n",
      "[step: 3530] loss: 0.10425566881895065\n",
      "[step: 3531] loss: 0.10419575870037079\n",
      "[step: 3532] loss: 0.10414087027311325\n",
      "[step: 3533] loss: 0.10408639162778854\n",
      "[step: 3534] loss: 0.10402840375900269\n",
      "[step: 3535] loss: 0.10396921634674072\n",
      "[step: 3536] loss: 0.10391253232955933\n",
      "[step: 3537] loss: 0.1038573682308197\n",
      "[step: 3538] loss: 0.10380033403635025\n",
      "[step: 3539] loss: 0.10374286770820618\n",
      "[step: 3540] loss: 0.1036861464381218\n",
      "[step: 3541] loss: 0.10362960398197174\n",
      "[step: 3542] loss: 0.10357221961021423\n",
      "[step: 3543] loss: 0.10351517796516418\n",
      "[step: 3544] loss: 0.10345923900604248\n",
      "[step: 3545] loss: 0.1034030094742775\n",
      "[step: 3546] loss: 0.10334548354148865\n",
      "[step: 3547] loss: 0.10328838974237442\n",
      "[step: 3548] loss: 0.10323189944028854\n",
      "[step: 3549] loss: 0.10317591577768326\n",
      "[step: 3550] loss: 0.10311901569366455\n",
      "[step: 3551] loss: 0.10306243598461151\n",
      "[step: 3552] loss: 0.10300570726394653\n",
      "[step: 3553] loss: 0.10294915735721588\n",
      "[step: 3554] loss: 0.10289254784584045\n",
      "[step: 3555] loss: 0.10283584892749786\n",
      "[step: 3556] loss: 0.10277938842773438\n",
      "[step: 3557] loss: 0.10272295773029327\n",
      "[step: 3558] loss: 0.10266633331775665\n",
      "[step: 3559] loss: 0.10260944068431854\n",
      "[step: 3560] loss: 0.10255315899848938\n",
      "[step: 3561] loss: 0.10249663889408112\n",
      "[step: 3562] loss: 0.102440245449543\n",
      "[step: 3563] loss: 0.1023833304643631\n",
      "[step: 3564] loss: 0.10232694447040558\n",
      "[step: 3565] loss: 0.10227058082818985\n",
      "[step: 3566] loss: 0.10221406817436218\n",
      "[step: 3567] loss: 0.10215747356414795\n",
      "[step: 3568] loss: 0.10210094600915909\n",
      "[step: 3569] loss: 0.10204452276229858\n",
      "[step: 3570] loss: 0.10198812931776047\n",
      "[step: 3571] loss: 0.10193178057670593\n",
      "[step: 3572] loss: 0.10187515616416931\n",
      "[step: 3573] loss: 0.10181868076324463\n",
      "[step: 3574] loss: 0.10176225751638412\n",
      "[step: 3575] loss: 0.1017056480050087\n",
      "[step: 3576] loss: 0.10164937376976013\n",
      "[step: 3577] loss: 0.10159312188625336\n",
      "[step: 3578] loss: 0.10153651237487793\n",
      "[step: 3579] loss: 0.10148010402917862\n",
      "[step: 3580] loss: 0.10142368823289871\n",
      "[step: 3581] loss: 0.10136736929416656\n",
      "[step: 3582] loss: 0.1013108640909195\n",
      "[step: 3583] loss: 0.1012544110417366\n",
      "[step: 3584] loss: 0.10119831562042236\n",
      "[step: 3585] loss: 0.10114189237356186\n",
      "[step: 3586] loss: 0.10108532011508942\n",
      "[step: 3587] loss: 0.10102909803390503\n",
      "[step: 3588] loss: 0.10097260773181915\n",
      "[step: 3589] loss: 0.1009165570139885\n",
      "[step: 3590] loss: 0.10085989534854889\n",
      "[step: 3591] loss: 0.10080364346504211\n",
      "[step: 3592] loss: 0.10074719041585922\n",
      "[step: 3593] loss: 0.10069093108177185\n",
      "[step: 3594] loss: 0.10063458234071732\n",
      "[step: 3595] loss: 0.10057830810546875\n",
      "[step: 3596] loss: 0.10052166879177094\n",
      "[step: 3597] loss: 0.1004655584692955\n",
      "[step: 3598] loss: 0.10040903091430664\n",
      "[step: 3599] loss: 0.10035279393196106\n",
      "[step: 3600] loss: 0.1002964973449707\n",
      "[step: 3601] loss: 0.10024036467075348\n",
      "[step: 3602] loss: 0.10018374025821686\n",
      "[step: 3603] loss: 0.10012765228748322\n",
      "[step: 3604] loss: 0.10007135570049286\n",
      "[step: 3605] loss: 0.1000150740146637\n",
      "[step: 3606] loss: 0.09995867311954498\n",
      "[step: 3607] loss: 0.09990236163139343\n",
      "[step: 3608] loss: 0.09984609484672546\n",
      "[step: 3609] loss: 0.09978999942541122\n",
      "[step: 3610] loss: 0.09973369538784027\n",
      "[step: 3611] loss: 0.09967754781246185\n",
      "[step: 3612] loss: 0.09962116181850433\n",
      "[step: 3613] loss: 0.09956459701061249\n",
      "[step: 3614] loss: 0.09950847923755646\n",
      "[step: 3615] loss: 0.09945233911275864\n",
      "[step: 3616] loss: 0.09939616918563843\n",
      "[step: 3617] loss: 0.0993398129940033\n",
      "[step: 3618] loss: 0.099283367395401\n",
      "[step: 3619] loss: 0.09922761470079422\n",
      "[step: 3620] loss: 0.09917102009057999\n",
      "[step: 3621] loss: 0.09911498427391052\n",
      "[step: 3622] loss: 0.09905865043401718\n",
      "[step: 3623] loss: 0.099002406001091\n",
      "[step: 3624] loss: 0.09894631057977676\n",
      "[step: 3625] loss: 0.09889006614685059\n",
      "[step: 3626] loss: 0.09883368760347366\n",
      "[step: 3627] loss: 0.09877728670835495\n",
      "[step: 3628] loss: 0.09872142970561981\n",
      "[step: 3629] loss: 0.09866508841514587\n",
      "[step: 3630] loss: 0.0986090898513794\n",
      "[step: 3631] loss: 0.09855285286903381\n",
      "[step: 3632] loss: 0.09849660098552704\n",
      "[step: 3633] loss: 0.0984404981136322\n",
      "[step: 3634] loss: 0.0983843207359314\n",
      "[step: 3635] loss: 0.09832833707332611\n",
      "[step: 3636] loss: 0.09827215224504471\n",
      "[step: 3637] loss: 0.0982159823179245\n",
      "[step: 3638] loss: 0.09815982729196548\n",
      "[step: 3639] loss: 0.09810414165258408\n",
      "[step: 3640] loss: 0.09804847091436386\n",
      "[step: 3641] loss: 0.09799313545227051\n",
      "[step: 3642] loss: 0.09793892502784729\n",
      "[step: 3643] loss: 0.09788508713245392\n",
      "[step: 3644] loss: 0.09783375263214111\n",
      "[step: 3645] loss: 0.09778547286987305\n",
      "[step: 3646] loss: 0.0977432131767273\n",
      "[step: 3647] loss: 0.09771211445331573\n",
      "[step: 3648] loss: 0.09770013391971588\n",
      "[step: 3649] loss: 0.09772610664367676\n",
      "[step: 3650] loss: 0.097817063331604\n",
      "[step: 3651] loss: 0.09803923964500427\n",
      "[step: 3652] loss: 0.09848465025424957\n",
      "[step: 3653] loss: 0.09939828515052795\n",
      "[step: 3654] loss: 0.10103348642587662\n",
      "[step: 3655] loss: 0.10423294454813004\n",
      "[step: 3656] loss: 0.10916854441165924\n",
      "[step: 3657] loss: 0.11768832802772522\n",
      "[step: 3658] loss: 0.12624891102313995\n",
      "[step: 3659] loss: 0.13431328535079956\n",
      "[step: 3660] loss: 0.13123588263988495\n",
      "[step: 3661] loss: 0.11851315200328827\n",
      "[step: 3662] loss: 0.10316199064254761\n",
      "[step: 3663] loss: 0.09728042781352997\n",
      "[step: 3664] loss: 0.10202036798000336\n",
      "[step: 3665] loss: 0.11014437675476074\n",
      "[step: 3666] loss: 0.11393493413925171\n",
      "[step: 3667] loss: 0.10637293010950089\n",
      "[step: 3668] loss: 0.0982915386557579\n",
      "[step: 3669] loss: 0.0974537581205368\n",
      "[step: 3670] loss: 0.10254815220832825\n",
      "[step: 3671] loss: 0.10704664885997772\n",
      "[step: 3672] loss: 0.10490180552005768\n",
      "[step: 3673] loss: 0.09891441464424133\n",
      "[step: 3674] loss: 0.09641391783952713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3675] loss: 0.09952906519174576\n",
      "[step: 3676] loss: 0.10281901806592941\n",
      "[step: 3677] loss: 0.10209129750728607\n",
      "[step: 3678] loss: 0.09841202199459076\n",
      "[step: 3679] loss: 0.09610218554735184\n",
      "[step: 3680] loss: 0.09794405102729797\n",
      "[step: 3681] loss: 0.10061605274677277\n",
      "[step: 3682] loss: 0.1002124473452568\n",
      "[step: 3683] loss: 0.09775963425636292\n",
      "[step: 3684] loss: 0.09595029801130295\n",
      "[step: 3685] loss: 0.09688256680965424\n",
      "[step: 3686] loss: 0.09872666001319885\n",
      "[step: 3687] loss: 0.0986805409193039\n",
      "[step: 3688] loss: 0.09704461693763733\n",
      "[step: 3689] loss: 0.09567906707525253\n",
      "[step: 3690] loss: 0.09610395133495331\n",
      "[step: 3691] loss: 0.09735462069511414\n",
      "[step: 3692] loss: 0.09740087389945984\n",
      "[step: 3693] loss: 0.09640178084373474\n",
      "[step: 3694] loss: 0.09547866135835648\n",
      "[step: 3695] loss: 0.09555104374885559\n",
      "[step: 3696] loss: 0.09634611010551453\n",
      "[step: 3697] loss: 0.0965077355504036\n",
      "[step: 3698] loss: 0.09589074552059174\n",
      "[step: 3699] loss: 0.09525618702173233\n",
      "[step: 3700] loss: 0.09513580799102783\n",
      "[step: 3701] loss: 0.0955263078212738\n",
      "[step: 3702] loss: 0.09576021134853363\n",
      "[step: 3703] loss: 0.09545028209686279\n",
      "[step: 3704] loss: 0.09500963240861893\n",
      "[step: 3705] loss: 0.0948130264878273\n",
      "[step: 3706] loss: 0.09495803713798523\n",
      "[step: 3707] loss: 0.09516213089227676\n",
      "[step: 3708] loss: 0.095065176486969\n",
      "[step: 3709] loss: 0.0947771817445755\n",
      "[step: 3710] loss: 0.09455841034650803\n",
      "[step: 3711] loss: 0.09454517066478729\n",
      "[step: 3712] loss: 0.09467347711324692\n",
      "[step: 3713] loss: 0.09468096494674683\n",
      "[step: 3714] loss: 0.09452246874570847\n",
      "[step: 3715] loss: 0.09434087574481964\n",
      "[step: 3716] loss: 0.09423723071813583\n",
      "[step: 3717] loss: 0.09426521509885788\n",
      "[step: 3718] loss: 0.09430224448442459\n",
      "[step: 3719] loss: 0.09423989802598953\n",
      "[step: 3720] loss: 0.09411931037902832\n",
      "[step: 3721] loss: 0.09399661421775818\n",
      "[step: 3722] loss: 0.09394226223230362\n",
      "[step: 3723] loss: 0.09394766390323639\n",
      "[step: 3724] loss: 0.09393168240785599\n",
      "[step: 3725] loss: 0.09387244284152985\n",
      "[step: 3726] loss: 0.09377536922693253\n",
      "[step: 3727] loss: 0.09368560463190079\n",
      "[step: 3728] loss: 0.09364190697669983\n",
      "[step: 3729] loss: 0.09361827373504639\n",
      "[step: 3730] loss: 0.09358924627304077\n",
      "[step: 3731] loss: 0.0935353934764862\n",
      "[step: 3732] loss: 0.09345628321170807\n",
      "[step: 3733] loss: 0.0933864414691925\n",
      "[step: 3734] loss: 0.09333539009094238\n",
      "[step: 3735] loss: 0.0932973176240921\n",
      "[step: 3736] loss: 0.09326212108135223\n",
      "[step: 3737] loss: 0.093211829662323\n",
      "[step: 3738] loss: 0.09315026551485062\n",
      "[step: 3739] loss: 0.09308788180351257\n",
      "[step: 3740] loss: 0.09303144365549088\n",
      "[step: 3741] loss: 0.09298580884933472\n",
      "[step: 3742] loss: 0.09294325113296509\n",
      "[step: 3743] loss: 0.0928972065448761\n",
      "[step: 3744] loss: 0.09284592419862747\n",
      "[step: 3745] loss: 0.09278914332389832\n",
      "[step: 3746] loss: 0.09273318946361542\n",
      "[step: 3747] loss: 0.09268123656511307\n",
      "[step: 3748] loss: 0.09263315796852112\n",
      "[step: 3749] loss: 0.09258697181940079\n",
      "[step: 3750] loss: 0.09253927320241928\n",
      "[step: 3751] loss: 0.09248866140842438\n",
      "[step: 3752] loss: 0.09243598580360413\n",
      "[step: 3753] loss: 0.09238245338201523\n",
      "[step: 3754] loss: 0.09233082830905914\n",
      "[step: 3755] loss: 0.09228111803531647\n",
      "[step: 3756] loss: 0.09223249554634094\n",
      "[step: 3757] loss: 0.0921841412782669\n",
      "[step: 3758] loss: 0.0921345129609108\n",
      "[step: 3759] loss: 0.09208385646343231\n",
      "[step: 3760] loss: 0.09203289449214935\n",
      "[step: 3761] loss: 0.09198136627674103\n",
      "[step: 3762] loss: 0.09193076938390732\n",
      "[step: 3763] loss: 0.09188055992126465\n",
      "[step: 3764] loss: 0.09183100610971451\n",
      "[step: 3765] loss: 0.09178195148706436\n",
      "[step: 3766] loss: 0.09173206984996796\n",
      "[step: 3767] loss: 0.09168210625648499\n",
      "[step: 3768] loss: 0.09163165092468262\n",
      "[step: 3769] loss: 0.09158112108707428\n",
      "[step: 3770] loss: 0.0915306881070137\n",
      "[step: 3771] loss: 0.09148028492927551\n",
      "[step: 3772] loss: 0.09143034368753433\n",
      "[step: 3773] loss: 0.09138022363185883\n",
      "[step: 3774] loss: 0.09133045375347137\n",
      "[step: 3775] loss: 0.09128084778785706\n",
      "[step: 3776] loss: 0.09123068302869797\n",
      "[step: 3777] loss: 0.09118078649044037\n",
      "[step: 3778] loss: 0.09113064408302307\n",
      "[step: 3779] loss: 0.09108059108257294\n",
      "[step: 3780] loss: 0.09103052318096161\n",
      "[step: 3781] loss: 0.09098024666309357\n",
      "[step: 3782] loss: 0.09093008190393448\n",
      "[step: 3783] loss: 0.09087996929883957\n",
      "[step: 3784] loss: 0.09082998335361481\n",
      "[step: 3785] loss: 0.09077996015548706\n",
      "[step: 3786] loss: 0.09073014557361603\n",
      "[step: 3787] loss: 0.09068004786968231\n",
      "[step: 3788] loss: 0.09063023328781128\n",
      "[step: 3789] loss: 0.09058009833097458\n",
      "[step: 3790] loss: 0.09053017944097519\n",
      "[step: 3791] loss: 0.09048010408878326\n",
      "[step: 3792] loss: 0.09043023735284805\n",
      "[step: 3793] loss: 0.09038008749485016\n",
      "[step: 3794] loss: 0.09033015370368958\n",
      "[step: 3795] loss: 0.09028026461601257\n",
      "[step: 3796] loss: 0.09023024886846542\n",
      "[step: 3797] loss: 0.0901801735162735\n",
      "[step: 3798] loss: 0.09013040363788605\n",
      "[step: 3799] loss: 0.09008031338453293\n",
      "[step: 3800] loss: 0.09003016352653503\n",
      "[step: 3801] loss: 0.0899800956249237\n",
      "[step: 3802] loss: 0.0899304747581482\n",
      "[step: 3803] loss: 0.08988048881292343\n",
      "[step: 3804] loss: 0.08983062207698822\n",
      "[step: 3805] loss: 0.08978061378002167\n",
      "[step: 3806] loss: 0.08973097056150436\n",
      "[step: 3807] loss: 0.08968129009008408\n",
      "[step: 3808] loss: 0.08963166922330856\n",
      "[step: 3809] loss: 0.08958231657743454\n",
      "[step: 3810] loss: 0.08953336626291275\n",
      "[step: 3811] loss: 0.08948475122451782\n",
      "[step: 3812] loss: 0.0894371047616005\n",
      "[step: 3813] loss: 0.0893905758857727\n",
      "[step: 3814] loss: 0.08934592455625534\n",
      "[step: 3815] loss: 0.08930470049381256\n",
      "[step: 3816] loss: 0.08926853537559509\n",
      "[step: 3817] loss: 0.08924210071563721\n",
      "[step: 3818] loss: 0.0892305001616478\n",
      "[step: 3819] loss: 0.08924715220928192\n",
      "[step: 3820] loss: 0.08930616080760956\n",
      "[step: 3821] loss: 0.08945177495479584\n",
      "[step: 3822] loss: 0.08972190320491791\n",
      "[step: 3823] loss: 0.09026527404785156\n",
      "[step: 3824] loss: 0.09114667773246765\n",
      "[step: 3825] loss: 0.09285575151443481\n",
      "[step: 3826] loss: 0.09523817896842957\n",
      "[step: 3827] loss: 0.09958867728710175\n",
      "[step: 3828] loss: 0.10389362275600433\n",
      "[step: 3829] loss: 0.11000040173530579\n",
      "[step: 3830] loss: 0.1117054894566536\n",
      "[step: 3831] loss: 0.11056186258792877\n",
      "[step: 3832] loss: 0.10319016873836517\n",
      "[step: 3833] loss: 0.09510929137468338\n",
      "[step: 3834] loss: 0.08954386413097382\n",
      "[step: 3835] loss: 0.08893024921417236\n",
      "[step: 3836] loss: 0.09197600185871124\n",
      "[step: 3837] loss: 0.09587191790342331\n",
      "[step: 3838] loss: 0.09852839261293411\n",
      "[step: 3839] loss: 0.09582085907459259\n",
      "[step: 3840] loss: 0.091817706823349\n",
      "[step: 3841] loss: 0.08878374099731445\n",
      "[step: 3842] loss: 0.08838365972042084\n",
      "[step: 3843] loss: 0.09032043814659119\n",
      "[step: 3844] loss: 0.0926092118024826\n",
      "[step: 3845] loss: 0.0930723249912262\n",
      "[step: 3846] loss: 0.09126774966716766\n",
      "[step: 3847] loss: 0.08917974680662155\n",
      "[step: 3848] loss: 0.0878947526216507\n",
      "[step: 3849] loss: 0.08806686848402023\n",
      "[step: 3850] loss: 0.08918225765228271\n",
      "[step: 3851] loss: 0.08991731703281403\n",
      "[step: 3852] loss: 0.08984033018350601\n",
      "[step: 3853] loss: 0.08890119940042496\n",
      "[step: 3854] loss: 0.08795051276683807\n",
      "[step: 3855] loss: 0.08748205006122589\n",
      "[step: 3856] loss: 0.08765974640846252\n",
      "[step: 3857] loss: 0.08816725015640259\n",
      "[step: 3858] loss: 0.08847463876008987\n",
      "[step: 3859] loss: 0.08837293088436127\n",
      "[step: 3860] loss: 0.08790569007396698\n",
      "[step: 3861] loss: 0.08740808069705963\n",
      "[step: 3862] loss: 0.08713431656360626\n",
      "[step: 3863] loss: 0.08719254285097122\n",
      "[step: 3864] loss: 0.08742360025644302\n",
      "[step: 3865] loss: 0.0875755250453949\n",
      "[step: 3866] loss: 0.0875442773103714\n",
      "[step: 3867] loss: 0.08731533586978912\n",
      "[step: 3868] loss: 0.08702950179576874\n",
      "[step: 3869] loss: 0.08682809770107269\n",
      "[step: 3870] loss: 0.08678082376718521\n",
      "[step: 3871] loss: 0.08683569729328156\n",
      "[step: 3872] loss: 0.08690140396356583\n",
      "[step: 3873] loss: 0.08691813051700592\n",
      "[step: 3874] loss: 0.08684080839157104\n",
      "[step: 3875] loss: 0.08670591562986374\n",
      "[step: 3876] loss: 0.08655962347984314\n",
      "[step: 3877] loss: 0.08645255863666534\n",
      "[step: 3878] loss: 0.08639867603778839\n",
      "[step: 3879] loss: 0.08638864010572433\n",
      "[step: 3880] loss: 0.08639666438102722\n",
      "[step: 3881] loss: 0.08638809621334076\n",
      "[step: 3882] loss: 0.08634430915117264\n",
      "[step: 3883] loss: 0.086270771920681\n",
      "[step: 3884] loss: 0.08618403971195221\n",
      "[step: 3885] loss: 0.0860978364944458\n",
      "[step: 3886] loss: 0.08602841198444366\n",
      "[step: 3887] loss: 0.08597961068153381\n",
      "[step: 3888] loss: 0.08594581484794617\n",
      "[step: 3889] loss: 0.08591786026954651\n",
      "[step: 3890] loss: 0.0858888328075409\n",
      "[step: 3891] loss: 0.0858524888753891\n",
      "[step: 3892] loss: 0.08580490201711655\n",
      "[step: 3893] loss: 0.08574949949979782\n",
      "[step: 3894] loss: 0.0856894999742508\n",
      "[step: 3895] loss: 0.0856291651725769\n",
      "[step: 3896] loss: 0.08557122200727463\n",
      "[step: 3897] loss: 0.08551840484142303\n",
      "[step: 3898] loss: 0.08547134697437286\n",
      "[step: 3899] loss: 0.08542774617671967\n",
      "[step: 3900] loss: 0.08538644760847092\n",
      "[step: 3901] loss: 0.08534619957208633\n",
      "[step: 3902] loss: 0.08530507236719131\n",
      "[step: 3903] loss: 0.08526244759559631\n",
      "[step: 3904] loss: 0.08521835505962372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3905] loss: 0.08517247438430786\n",
      "[step: 3906] loss: 0.08512528240680695\n",
      "[step: 3907] loss: 0.08507687598466873\n",
      "[step: 3908] loss: 0.08502821624279022\n",
      "[step: 3909] loss: 0.08497927337884903\n",
      "[step: 3910] loss: 0.08493049442768097\n",
      "[step: 3911] loss: 0.08488226681947708\n",
      "[step: 3912] loss: 0.0848342627286911\n",
      "[step: 3913] loss: 0.08478662371635437\n",
      "[step: 3914] loss: 0.0847393125295639\n",
      "[step: 3915] loss: 0.08469218015670776\n",
      "[step: 3916] loss: 0.08464574068784714\n",
      "[step: 3917] loss: 0.08459911495447159\n",
      "[step: 3918] loss: 0.08455298840999603\n",
      "[step: 3919] loss: 0.08450664579868317\n",
      "[step: 3920] loss: 0.08446062356233597\n",
      "[step: 3921] loss: 0.08441472053527832\n",
      "[step: 3922] loss: 0.08436885476112366\n",
      "[step: 3923] loss: 0.0843229591846466\n",
      "[step: 3924] loss: 0.08427734673023224\n",
      "[step: 3925] loss: 0.08423178642988205\n",
      "[step: 3926] loss: 0.08418633043766022\n",
      "[step: 3927] loss: 0.08414136618375778\n",
      "[step: 3928] loss: 0.08409658074378967\n",
      "[step: 3929] loss: 0.08405283838510513\n",
      "[step: 3930] loss: 0.08400994539260864\n",
      "[step: 3931] loss: 0.08396850526332855\n",
      "[step: 3932] loss: 0.08393006771802902\n",
      "[step: 3933] loss: 0.0838962197303772\n",
      "[step: 3934] loss: 0.08387009054422379\n",
      "[step: 3935] loss: 0.08385616540908813\n",
      "[step: 3936] loss: 0.08386455476284027\n",
      "[step: 3937] loss: 0.08390802890062332\n",
      "[step: 3938] loss: 0.08402015268802643\n",
      "[step: 3939] loss: 0.08423332870006561\n",
      "[step: 3940] loss: 0.0846623107790947\n",
      "[step: 3941] loss: 0.085371233522892\n",
      "[step: 3942] loss: 0.08674610406160355\n",
      "[step: 3943] loss: 0.0887451022863388\n",
      "[step: 3944] loss: 0.0924876481294632\n",
      "[step: 3945] loss: 0.09664101898670197\n",
      "[step: 3946] loss: 0.10321563482284546\n",
      "[step: 3947] loss: 0.10654254257678986\n",
      "[step: 3948] loss: 0.10833408683538437\n",
      "[step: 3949] loss: 0.10266542434692383\n",
      "[step: 3950] loss: 0.09441480040550232\n",
      "[step: 3951] loss: 0.08639506250619888\n",
      "[step: 3952] loss: 0.08333239704370499\n",
      "[step: 3953] loss: 0.08503267168998718\n",
      "[step: 3954] loss: 0.08920946717262268\n",
      "[step: 3955] loss: 0.09324991703033447\n",
      "[step: 3956] loss: 0.0922037661075592\n",
      "[step: 3957] loss: 0.08886060863733292\n",
      "[step: 3958] loss: 0.08479727804660797\n",
      "[step: 3959] loss: 0.0830029547214508\n",
      "[step: 3960] loss: 0.08397199958562851\n",
      "[step: 3961] loss: 0.08640129119157791\n",
      "[step: 3962] loss: 0.08798610419034958\n",
      "[step: 3963] loss: 0.08711440861225128\n",
      "[step: 3964] loss: 0.08509597182273865\n",
      "[step: 3965] loss: 0.08315308392047882\n",
      "[step: 3966] loss: 0.08256162703037262\n",
      "[step: 3967] loss: 0.08333533257246017\n",
      "[step: 3968] loss: 0.08443515002727509\n",
      "[step: 3969] loss: 0.08494199067354202\n",
      "[step: 3970] loss: 0.08427318930625916\n",
      "[step: 3971] loss: 0.08321649581193924\n",
      "[step: 3972] loss: 0.08238407969474792\n",
      "[step: 3973] loss: 0.08225613832473755\n",
      "[step: 3974] loss: 0.08272544294595718\n",
      "[step: 3975] loss: 0.08323128521442413\n",
      "[step: 3976] loss: 0.08335394412279129\n",
      "[step: 3977] loss: 0.08298244327306747\n",
      "[step: 3978] loss: 0.08243992179632187\n",
      "[step: 3979] loss: 0.08201532065868378\n",
      "[step: 3980] loss: 0.08194726705551147\n",
      "[step: 3981] loss: 0.08214376866817474\n",
      "[step: 3982] loss: 0.08235380798578262\n",
      "[step: 3983] loss: 0.08242012560367584\n",
      "[step: 3984] loss: 0.08226326107978821\n",
      "[step: 3985] loss: 0.08199120312929153\n",
      "[step: 3986] loss: 0.08174416422843933\n",
      "[step: 3987] loss: 0.08163096755743027\n",
      "[step: 3988] loss: 0.08164528012275696\n",
      "[step: 3989] loss: 0.0817156508564949\n",
      "[step: 3990] loss: 0.08177082985639572\n",
      "[step: 3991] loss: 0.08174320310354233\n",
      "[step: 3992] loss: 0.081639364361763\n",
      "[step: 3993] loss: 0.08149255067110062\n",
      "[step: 3994] loss: 0.08136263489723206\n",
      "[step: 3995] loss: 0.08128154277801514\n",
      "[step: 3996] loss: 0.0812557190656662\n",
      "[step: 3997] loss: 0.08126630634069443\n",
      "[step: 3998] loss: 0.08127503097057343\n",
      "[step: 3999] loss: 0.08125467598438263\n",
      "[step: 4000] loss: 0.08119846880435944\n",
      "[step: 4001] loss: 0.08111967146396637\n",
      "[step: 4002] loss: 0.08103179931640625\n",
      "[step: 4003] loss: 0.08095662295818329\n",
      "[step: 4004] loss: 0.08090264350175858\n",
      "[step: 4005] loss: 0.08086689561605453\n",
      "[step: 4006] loss: 0.08084180951118469\n",
      "[step: 4007] loss: 0.08081881701946259\n",
      "[step: 4008] loss: 0.08079062402248383\n",
      "[step: 4009] loss: 0.08075221627950668\n",
      "[step: 4010] loss: 0.08070442825555801\n",
      "[step: 4011] loss: 0.08065074682235718\n",
      "[step: 4012] loss: 0.08059404045343399\n",
      "[step: 4013] loss: 0.08053810149431229\n",
      "[step: 4014] loss: 0.08048705756664276\n",
      "[step: 4015] loss: 0.08044107258319855\n",
      "[step: 4016] loss: 0.08039993047714233\n",
      "[step: 4017] loss: 0.08036262542009354\n",
      "[step: 4018] loss: 0.08032675832509995\n",
      "[step: 4019] loss: 0.08029058575630188\n",
      "[step: 4020] loss: 0.08025304973125458\n",
      "[step: 4021] loss: 0.08021372556686401\n",
      "[step: 4022] loss: 0.0801725834608078\n",
      "[step: 4023] loss: 0.0801299512386322\n",
      "[step: 4024] loss: 0.08008617907762527\n",
      "[step: 4025] loss: 0.08004210889339447\n",
      "[step: 4026] loss: 0.07999736070632935\n",
      "[step: 4027] loss: 0.07995286583900452\n",
      "[step: 4028] loss: 0.07990825921297073\n",
      "[step: 4029] loss: 0.079864501953125\n",
      "[step: 4030] loss: 0.079820916056633\n",
      "[step: 4031] loss: 0.07977808266878128\n",
      "[step: 4032] loss: 0.07973524928092957\n",
      "[step: 4033] loss: 0.07969305664300919\n",
      "[step: 4034] loss: 0.0796508938074112\n",
      "[step: 4035] loss: 0.0796087458729744\n",
      "[step: 4036] loss: 0.07956712692975998\n",
      "[step: 4037] loss: 0.07952522486448288\n",
      "[step: 4038] loss: 0.07948368042707443\n",
      "[step: 4039] loss: 0.07944204658269882\n",
      "[step: 4040] loss: 0.07940076291561127\n",
      "[step: 4041] loss: 0.07935932278633118\n",
      "[step: 4042] loss: 0.07931762933731079\n",
      "[step: 4043] loss: 0.07927638292312622\n",
      "[step: 4044] loss: 0.07923495024442673\n",
      "[step: 4045] loss: 0.079193614423275\n",
      "[step: 4046] loss: 0.07915221154689789\n",
      "[step: 4047] loss: 0.0791110023856163\n",
      "[step: 4048] loss: 0.07906971871852875\n",
      "[step: 4049] loss: 0.07902869582176208\n",
      "[step: 4050] loss: 0.07898755371570587\n",
      "[step: 4051] loss: 0.07894669473171234\n",
      "[step: 4052] loss: 0.07890646904706955\n",
      "[step: 4053] loss: 0.07886646687984467\n",
      "[step: 4054] loss: 0.07882749289274216\n",
      "[step: 4055] loss: 0.07879006862640381\n",
      "[step: 4056] loss: 0.07875523716211319\n",
      "[step: 4057] loss: 0.07872530817985535\n",
      "[step: 4058] loss: 0.0787038505077362\n",
      "[step: 4059] loss: 0.07869814336299896\n",
      "[step: 4060] loss: 0.07871904224157333\n",
      "[step: 4061] loss: 0.078793004155159\n",
      "[step: 4062] loss: 0.07895517349243164\n",
      "[step: 4063] loss: 0.07930335402488708\n",
      "[step: 4064] loss: 0.07993540912866592\n",
      "[step: 4065] loss: 0.08122380822896957\n",
      "[step: 4066] loss: 0.0833125114440918\n",
      "[step: 4067] loss: 0.0874677300453186\n",
      "[step: 4068] loss: 0.09277356415987015\n",
      "[step: 4069] loss: 0.10192365944385529\n",
      "[step: 4070] loss: 0.10787980258464813\n",
      "[step: 4071] loss: 0.11238355189561844\n",
      "[step: 4072] loss: 0.10555478930473328\n",
      "[step: 4073] loss: 0.09359023720026016\n",
      "[step: 4074] loss: 0.08196703344583511\n",
      "[step: 4075] loss: 0.07855509221553802\n",
      "[step: 4076] loss: 0.08235104382038116\n",
      "[step: 4077] loss: 0.08864588290452957\n",
      "[step: 4078] loss: 0.09308865666389465\n",
      "[step: 4079] loss: 0.08804477751255035\n",
      "[step: 4080] loss: 0.0817374438047409\n",
      "[step: 4081] loss: 0.07857695966959\n",
      "[step: 4082] loss: 0.08004891872406006\n",
      "[step: 4083] loss: 0.08383263647556305\n",
      "[step: 4084] loss: 0.08578325808048248\n",
      "[step: 4085] loss: 0.08384759724140167\n",
      "[step: 4086] loss: 0.0799899622797966\n",
      "[step: 4087] loss: 0.07838951051235199\n",
      "[step: 4088] loss: 0.0792313665151596\n",
      "[step: 4089] loss: 0.08090182393789291\n",
      "[step: 4090] loss: 0.08166459202766418\n",
      "[step: 4091] loss: 0.08013585209846497\n",
      "[step: 4092] loss: 0.07858308404684067\n",
      "[step: 4093] loss: 0.07815548032522202\n",
      "[step: 4094] loss: 0.07858587056398392\n",
      "[step: 4095] loss: 0.0792609453201294\n",
      "[step: 4096] loss: 0.07928961515426636\n",
      "[step: 4097] loss: 0.07866248488426208\n",
      "[step: 4098] loss: 0.07803397625684738\n",
      "[step: 4099] loss: 0.07773903012275696\n",
      "[step: 4100] loss: 0.07776397466659546\n",
      "[step: 4101] loss: 0.07794605940580368\n",
      "[step: 4102] loss: 0.07802696526050568\n",
      "[step: 4103] loss: 0.07786700129508972\n",
      "[step: 4104] loss: 0.07757487893104553\n",
      "[step: 4105] loss: 0.07727435231208801\n",
      "[step: 4106] loss: 0.07714850455522537\n",
      "[step: 4107] loss: 0.07720287889242172\n",
      "[step: 4108] loss: 0.07733308523893356\n",
      "[step: 4109] loss: 0.07735995203256607\n",
      "[step: 4110] loss: 0.0771738737821579\n",
      "[step: 4111] loss: 0.07691700756549835\n",
      "[step: 4112] loss: 0.07675464451313019\n",
      "[step: 4113] loss: 0.07676561921834946\n",
      "[step: 4114] loss: 0.07688730955123901\n",
      "[step: 4115] loss: 0.07693520188331604\n",
      "[step: 4116] loss: 0.076835498213768\n",
      "[step: 4117] loss: 0.07664523273706436\n",
      "[step: 4118] loss: 0.07649682462215424\n",
      "[step: 4119] loss: 0.0764680951833725\n",
      "[step: 4120] loss: 0.07651587575674057\n",
      "[step: 4121] loss: 0.07655228674411774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4122] loss: 0.0765109658241272\n",
      "[step: 4123] loss: 0.07640296220779419\n",
      "[step: 4124] loss: 0.07629987597465515\n",
      "[step: 4125] loss: 0.07624159008264542\n",
      "[step: 4126] loss: 0.07622380554676056\n",
      "[step: 4127] loss: 0.07621952891349792\n",
      "[step: 4128] loss: 0.0761944055557251\n",
      "[step: 4129] loss: 0.07614555954933167\n",
      "[step: 4130] loss: 0.07609119266271591\n",
      "[step: 4131] loss: 0.07604165375232697\n",
      "[step: 4132] loss: 0.07600018382072449\n",
      "[step: 4133] loss: 0.07595972716808319\n",
      "[step: 4134] loss: 0.07591915875673294\n",
      "[step: 4135] loss: 0.07588011026382446\n",
      "[step: 4136] loss: 0.07584504783153534\n",
      "[step: 4137] loss: 0.07581599056720734\n",
      "[step: 4138] loss: 0.07578431069850922\n",
      "[step: 4139] loss: 0.07574481517076492\n",
      "[step: 4140] loss: 0.07569941878318787\n",
      "[step: 4141] loss: 0.07565107941627502\n",
      "[step: 4142] loss: 0.07560815662145615\n",
      "[step: 4143] loss: 0.07557294517755508\n",
      "[step: 4144] loss: 0.07554327696561813\n",
      "[step: 4145] loss: 0.07551375031471252\n",
      "[step: 4146] loss: 0.07547987252473831\n",
      "[step: 4147] loss: 0.07544127106666565\n",
      "[step: 4148] loss: 0.07539992034435272\n",
      "[step: 4149] loss: 0.07535897940397263\n",
      "[step: 4150] loss: 0.07532065361738205\n",
      "[step: 4151] loss: 0.07528422772884369\n",
      "[step: 4152] loss: 0.07524856925010681\n",
      "[step: 4153] loss: 0.07521241903305054\n",
      "[step: 4154] loss: 0.07517632097005844\n",
      "[step: 4155] loss: 0.07514006644487381\n",
      "[step: 4156] loss: 0.07510480284690857\n",
      "[step: 4157] loss: 0.07506964355707169\n",
      "[step: 4158] loss: 0.07503427565097809\n",
      "[step: 4159] loss: 0.07499821484088898\n",
      "[step: 4160] loss: 0.0749613344669342\n",
      "[step: 4161] loss: 0.0749237984418869\n",
      "[step: 4162] loss: 0.07488607615232468\n",
      "[step: 4163] loss: 0.07484938949346542\n",
      "[step: 4164] loss: 0.07481326162815094\n",
      "[step: 4165] loss: 0.07477767020463943\n",
      "[step: 4166] loss: 0.07474192976951599\n",
      "[step: 4167] loss: 0.07470598816871643\n",
      "[step: 4168] loss: 0.07467013597488403\n",
      "[step: 4169] loss: 0.0746341198682785\n",
      "[step: 4170] loss: 0.07459849864244461\n",
      "[step: 4171] loss: 0.07456281036138535\n",
      "[step: 4172] loss: 0.074527308344841\n",
      "[step: 4173] loss: 0.07449205964803696\n",
      "[step: 4174] loss: 0.07445670664310455\n",
      "[step: 4175] loss: 0.07442106306552887\n",
      "[step: 4176] loss: 0.07438542693853378\n",
      "[step: 4177] loss: 0.07434983551502228\n",
      "[step: 4178] loss: 0.07431440055370331\n",
      "[step: 4179] loss: 0.07427909970283508\n",
      "[step: 4180] loss: 0.07424406707286835\n",
      "[step: 4181] loss: 0.07420937716960907\n",
      "[step: 4182] loss: 0.07417458295822144\n",
      "[step: 4183] loss: 0.07414036244153976\n",
      "[step: 4184] loss: 0.0741066038608551\n",
      "[step: 4185] loss: 0.07407353818416595\n",
      "[step: 4186] loss: 0.07404150813817978\n",
      "[step: 4187] loss: 0.07401140034198761\n",
      "[step: 4188] loss: 0.0739838182926178\n",
      "[step: 4189] loss: 0.07395992428064346\n",
      "[step: 4190] loss: 0.07394236326217651\n",
      "[step: 4191] loss: 0.07393381744623184\n",
      "[step: 4192] loss: 0.07394097000360489\n",
      "[step: 4193] loss: 0.07396981120109558\n",
      "[step: 4194] loss: 0.0740404799580574\n",
      "[step: 4195] loss: 0.07416592538356781\n",
      "[step: 4196] loss: 0.07440493255853653\n",
      "[step: 4197] loss: 0.07478100061416626\n",
      "[step: 4198] loss: 0.0754707008600235\n",
      "[step: 4199] loss: 0.0764646828174591\n",
      "[step: 4200] loss: 0.07827293127775192\n",
      "[step: 4201] loss: 0.08051787316799164\n",
      "[step: 4202] loss: 0.08441473543643951\n",
      "[step: 4203] loss: 0.08780781924724579\n",
      "[step: 4204] loss: 0.09260611236095428\n",
      "[step: 4205] loss: 0.09338018298149109\n",
      "[step: 4206] loss: 0.09259305149316788\n",
      "[step: 4207] loss: 0.08664410561323166\n",
      "[step: 4208] loss: 0.08026573061943054\n",
      "[step: 4209] loss: 0.07501070946455002\n",
      "[step: 4210] loss: 0.0732940062880516\n",
      "[step: 4211] loss: 0.07473821938037872\n",
      "[step: 4212] loss: 0.07780583202838898\n",
      "[step: 4213] loss: 0.08092032372951508\n",
      "[step: 4214] loss: 0.08103565126657486\n",
      "[step: 4215] loss: 0.07947155833244324\n",
      "[step: 4216] loss: 0.07633131742477417\n",
      "[step: 4217] loss: 0.07383888959884644\n",
      "[step: 4218] loss: 0.07300068438053131\n",
      "[step: 4219] loss: 0.07381483912467957\n",
      "[step: 4220] loss: 0.07528021931648254\n",
      "[step: 4221] loss: 0.07623464614152908\n",
      "[step: 4222] loss: 0.07634913921356201\n",
      "[step: 4223] loss: 0.07519091665744781\n",
      "[step: 4224] loss: 0.07387951016426086\n",
      "[step: 4225] loss: 0.07295700162649155\n",
      "[step: 4226] loss: 0.07279836386442184\n",
      "[step: 4227] loss: 0.07326166331768036\n",
      "[step: 4228] loss: 0.07391585409641266\n",
      "[step: 4229] loss: 0.07433794438838959\n",
      "[step: 4230] loss: 0.07419899851083755\n",
      "[step: 4231] loss: 0.0737043023109436\n",
      "[step: 4232] loss: 0.07305935770273209\n",
      "[step: 4233] loss: 0.0726093128323555\n",
      "[step: 4234] loss: 0.07248131930828094\n",
      "[step: 4235] loss: 0.07264167815446854\n",
      "[step: 4236] loss: 0.07290979474782944\n",
      "[step: 4237] loss: 0.07308134436607361\n",
      "[step: 4238] loss: 0.07309303432703018\n",
      "[step: 4239] loss: 0.07290421426296234\n",
      "[step: 4240] loss: 0.07262736558914185\n",
      "[step: 4241] loss: 0.07237076014280319\n",
      "[step: 4242] loss: 0.07222366333007812\n",
      "[step: 4243] loss: 0.07219693809747696\n",
      "[step: 4244] loss: 0.07225924730300903\n",
      "[step: 4245] loss: 0.07235003262758255\n",
      "[step: 4246] loss: 0.07239748537540436\n",
      "[step: 4247] loss: 0.07238524407148361\n",
      "[step: 4248] loss: 0.07230353355407715\n",
      "[step: 4249] loss: 0.07218516618013382\n",
      "[step: 4250] loss: 0.07205765694379807\n",
      "[step: 4251] loss: 0.07195572555065155\n",
      "[step: 4252] loss: 0.071891188621521\n",
      "[step: 4253] loss: 0.07186280190944672\n",
      "[step: 4254] loss: 0.07186036556959152\n",
      "[step: 4255] loss: 0.07186824083328247\n",
      "[step: 4256] loss: 0.0718708336353302\n",
      "[step: 4257] loss: 0.0718567967414856\n",
      "[step: 4258] loss: 0.07182791084051132\n",
      "[step: 4259] loss: 0.07178178429603577\n",
      "[step: 4260] loss: 0.07172597199678421\n",
      "[step: 4261] loss: 0.07166580855846405\n",
      "[step: 4262] loss: 0.07160794734954834\n",
      "[step: 4263] loss: 0.07155494391918182\n",
      "[step: 4264] loss: 0.07150918990373611\n",
      "[step: 4265] loss: 0.0714712142944336\n",
      "[step: 4266] loss: 0.07143981754779816\n",
      "[step: 4267] loss: 0.07141237705945969\n",
      "[step: 4268] loss: 0.07138833403587341\n",
      "[step: 4269] loss: 0.0713651180267334\n",
      "[step: 4270] loss: 0.07134178280830383\n",
      "[step: 4271] loss: 0.07131768763065338\n",
      "[step: 4272] loss: 0.07129202783107758\n",
      "[step: 4273] loss: 0.07126491516828537\n",
      "[step: 4274] loss: 0.07123646140098572\n",
      "[step: 4275] loss: 0.07120762765407562\n",
      "[step: 4276] loss: 0.07117801904678345\n",
      "[step: 4277] loss: 0.07114895433187485\n",
      "[step: 4278] loss: 0.07111947983503342\n",
      "[step: 4279] loss: 0.0710921585559845\n",
      "[step: 4280] loss: 0.0710655078291893\n",
      "[step: 4281] loss: 0.07104163616895676\n",
      "[step: 4282] loss: 0.07101959735155106\n",
      "[step: 4283] loss: 0.07100231945514679\n",
      "[step: 4284] loss: 0.07098943740129471\n",
      "[step: 4285] loss: 0.07098500430583954\n",
      "[step: 4286] loss: 0.07098910957574844\n",
      "[step: 4287] loss: 0.07101038098335266\n",
      "[step: 4288] loss: 0.07104846835136414\n",
      "[step: 4289] loss: 0.07112229615449905\n",
      "[step: 4290] loss: 0.07123129069805145\n",
      "[step: 4291] loss: 0.07141716778278351\n",
      "[step: 4292] loss: 0.0716739296913147\n",
      "[step: 4293] loss: 0.07209919393062592\n",
      "[step: 4294] loss: 0.07265392690896988\n",
      "[step: 4295] loss: 0.07356616854667664\n",
      "[step: 4296] loss: 0.07465415447950363\n",
      "[step: 4297] loss: 0.07640065252780914\n",
      "[step: 4298] loss: 0.07813476026058197\n",
      "[step: 4299] loss: 0.08069845288991928\n",
      "[step: 4300] loss: 0.08227391541004181\n",
      "[step: 4301] loss: 0.08403182774782181\n",
      "[step: 4302] loss: 0.08322958648204803\n",
      "[step: 4303] loss: 0.08160567283630371\n",
      "[step: 4304] loss: 0.0778774619102478\n",
      "[step: 4305] loss: 0.07435072958469391\n",
      "[step: 4306] loss: 0.07149449735879898\n",
      "[step: 4307] loss: 0.0702594444155693\n",
      "[step: 4308] loss: 0.07054216414690018\n",
      "[step: 4309] loss: 0.07182732224464417\n",
      "[step: 4310] loss: 0.07346788048744202\n",
      "[step: 4311] loss: 0.07451111078262329\n",
      "[step: 4312] loss: 0.07494240254163742\n",
      "[step: 4313] loss: 0.07416846603155136\n",
      "[step: 4314] loss: 0.07295497506856918\n",
      "[step: 4315] loss: 0.07150843739509583\n",
      "[step: 4316] loss: 0.07048054039478302\n",
      "[step: 4317] loss: 0.07003229856491089\n",
      "[step: 4318] loss: 0.07015037536621094\n",
      "[step: 4319] loss: 0.0706191286444664\n",
      "[step: 4320] loss: 0.07113049924373627\n",
      "[step: 4321] loss: 0.0714992880821228\n",
      "[step: 4322] loss: 0.0714997947216034\n",
      "[step: 4323] loss: 0.07127147912979126\n",
      "[step: 4324] loss: 0.0708198994398117\n",
      "[step: 4325] loss: 0.07036688923835754\n",
      "[step: 4326] loss: 0.06998953223228455\n",
      "[step: 4327] loss: 0.0697680115699768\n",
      "[step: 4328] loss: 0.06969741731882095\n",
      "[step: 4329] loss: 0.06974753737449646\n",
      "[step: 4330] loss: 0.06986019760370255\n",
      "[step: 4331] loss: 0.06996621191501617\n",
      "[step: 4332] loss: 0.07003918290138245\n",
      "[step: 4333] loss: 0.07003532350063324\n",
      "[step: 4334] loss: 0.06997448951005936\n",
      "[step: 4335] loss: 0.06985221803188324\n",
      "[step: 4336] loss: 0.06971250474452972\n",
      "[step: 4337] loss: 0.06956139206886292\n",
      "[step: 4338] loss: 0.0694277435541153\n",
      "[step: 4339] loss: 0.06932379305362701\n",
      "[step: 4340] loss: 0.06925708055496216\n",
      "[step: 4341] loss: 0.06922359764575958\n",
      "[step: 4342] loss: 0.06921806931495667\n",
      "[step: 4343] loss: 0.06923139095306396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4344] loss: 0.06924986094236374\n",
      "[step: 4345] loss: 0.06926663964986801\n",
      "[step: 4346] loss: 0.06927177309989929\n",
      "[step: 4347] loss: 0.06926660984754562\n",
      "[step: 4348] loss: 0.06924229115247726\n",
      "[step: 4349] loss: 0.06920883059501648\n",
      "[step: 4350] loss: 0.0691634938120842\n",
      "[step: 4351] loss: 0.06911557912826538\n",
      "[step: 4352] loss: 0.06906372308731079\n",
      "[step: 4353] loss: 0.06901627779006958\n",
      "[step: 4354] loss: 0.06897002458572388\n",
      "[step: 4355] loss: 0.06892985105514526\n",
      "[step: 4356] loss: 0.06889290362596512\n",
      "[step: 4357] loss: 0.06886125355958939\n",
      "[step: 4358] loss: 0.06883203983306885\n",
      "[step: 4359] loss: 0.06880675256252289\n",
      "[step: 4360] loss: 0.06878366321325302\n",
      "[step: 4361] loss: 0.06876546889543533\n",
      "[step: 4362] loss: 0.06875075399875641\n",
      "[step: 4363] loss: 0.06874366104602814\n",
      "[step: 4364] loss: 0.06874418258666992\n",
      "[step: 4365] loss: 0.06875913590192795\n",
      "[step: 4366] loss: 0.06878934800624847\n",
      "[step: 4367] loss: 0.06884936988353729\n",
      "[step: 4368] loss: 0.06893956661224365\n",
      "[step: 4369] loss: 0.06909248232841492\n",
      "[step: 4370] loss: 0.06930536776781082\n",
      "[step: 4371] loss: 0.06965283304452896\n",
      "[step: 4372] loss: 0.07011515647172928\n",
      "[step: 4373] loss: 0.07086021453142166\n",
      "[step: 4374] loss: 0.07178252190351486\n",
      "[step: 4375] loss: 0.07325078547000885\n",
      "[step: 4376] loss: 0.07483513653278351\n",
      "[step: 4377] loss: 0.07721605151891708\n",
      "[step: 4378] loss: 0.07908935844898224\n",
      "[step: 4379] loss: 0.08142265677452087\n",
      "[step: 4380] loss: 0.08174602687358856\n",
      "[step: 4381] loss: 0.08140179514884949\n",
      "[step: 4382] loss: 0.07841210812330246\n",
      "[step: 4383] loss: 0.07493856549263\n",
      "[step: 4384] loss: 0.07118097692728043\n",
      "[step: 4385] loss: 0.06875407695770264\n",
      "[step: 4386] loss: 0.06795816868543625\n",
      "[step: 4387] loss: 0.06863485276699066\n",
      "[step: 4388] loss: 0.07018043845891953\n",
      "[step: 4389] loss: 0.07169312238693237\n",
      "[step: 4390] loss: 0.07278554886579514\n",
      "[step: 4391] loss: 0.0726233497262001\n",
      "[step: 4392] loss: 0.07175732403993607\n",
      "[step: 4393] loss: 0.0702224150300026\n",
      "[step: 4394] loss: 0.06883783638477325\n",
      "[step: 4395] loss: 0.06794437021017075\n",
      "[step: 4396] loss: 0.0677422434091568\n",
      "[step: 4397] loss: 0.06809785217046738\n",
      "[step: 4398] loss: 0.06870847195386887\n",
      "[step: 4399] loss: 0.06927800178527832\n",
      "[step: 4400] loss: 0.06948836147785187\n",
      "[step: 4401] loss: 0.06938594579696655\n",
      "[step: 4402] loss: 0.06893471628427505\n",
      "[step: 4403] loss: 0.06839451193809509\n",
      "[step: 4404] loss: 0.06789536774158478\n",
      "[step: 4405] loss: 0.06758724898099899\n",
      "[step: 4406] loss: 0.06749024242162704\n",
      "[step: 4407] loss: 0.06756702065467834\n",
      "[step: 4408] loss: 0.06773566454648972\n",
      "[step: 4409] loss: 0.06789703667163849\n",
      "[step: 4410] loss: 0.06799392402172089\n",
      "[step: 4411] loss: 0.06797915697097778\n",
      "[step: 4412] loss: 0.06788010895252228\n",
      "[step: 4413] loss: 0.06771153956651688\n",
      "[step: 4414] loss: 0.06753532588481903\n",
      "[step: 4415] loss: 0.0673767700791359\n",
      "[step: 4416] loss: 0.06726354360580444\n",
      "[step: 4417] loss: 0.06719791144132614\n",
      "[step: 4418] loss: 0.06717619299888611\n",
      "[step: 4419] loss: 0.06718182563781738\n",
      "[step: 4420] loss: 0.06719827651977539\n",
      "[step: 4421] loss: 0.06721406430006027\n",
      "[step: 4422] loss: 0.06721856445074081\n",
      "[step: 4423] loss: 0.06720878183841705\n",
      "[step: 4424] loss: 0.06718285381793976\n",
      "[step: 4425] loss: 0.06714751571416855\n",
      "[step: 4426] loss: 0.06710142642259598\n",
      "[step: 4427] loss: 0.06705297529697418\n",
      "[step: 4428] loss: 0.06700241565704346\n",
      "[step: 4429] loss: 0.06695519387722015\n",
      "[step: 4430] loss: 0.06690990924835205\n",
      "[step: 4431] loss: 0.06686939299106598\n",
      "[step: 4432] loss: 0.06683134287595749\n",
      "[step: 4433] loss: 0.06679673492908478\n",
      "[step: 4434] loss: 0.06676361709833145\n",
      "[step: 4435] loss: 0.06673268973827362\n",
      "[step: 4436] loss: 0.06670252978801727\n",
      "[step: 4437] loss: 0.06667283177375793\n",
      "[step: 4438] loss: 0.06664413958787918\n",
      "[step: 4439] loss: 0.06661554425954819\n",
      "[step: 4440] loss: 0.0665874257683754\n",
      "[step: 4441] loss: 0.06655967235565186\n",
      "[step: 4442] loss: 0.06653214991092682\n",
      "[step: 4443] loss: 0.06650546193122864\n",
      "[step: 4444] loss: 0.06647897511720657\n",
      "[step: 4445] loss: 0.06645354628562927\n",
      "[step: 4446] loss: 0.06642897427082062\n",
      "[step: 4447] loss: 0.06640525907278061\n",
      "[step: 4448] loss: 0.06638292968273163\n",
      "[step: 4449] loss: 0.06636183708906174\n",
      "[step: 4450] loss: 0.06634292006492615\n",
      "[step: 4451] loss: 0.06632647663354874\n",
      "[step: 4452] loss: 0.06631317734718323\n",
      "[step: 4453] loss: 0.06630408763885498\n",
      "[step: 4454] loss: 0.06630083918571472\n",
      "[step: 4455] loss: 0.06630541384220123\n",
      "[step: 4456] loss: 0.0663209855556488\n",
      "[step: 4457] loss: 0.06635120511054993\n",
      "[step: 4458] loss: 0.06640232354402542\n",
      "[step: 4459] loss: 0.06648193299770355\n",
      "[step: 4460] loss: 0.06660157442092896\n",
      "[step: 4461] loss: 0.0667765885591507\n",
      "[step: 4462] loss: 0.06702764332294464\n",
      "[step: 4463] loss: 0.06738312542438507\n",
      "[step: 4464] loss: 0.06788025796413422\n",
      "[step: 4465] loss: 0.06857190281152725\n",
      "[step: 4466] loss: 0.06952441483736038\n",
      "[step: 4467] loss: 0.07083822786808014\n",
      "[step: 4468] loss: 0.07268890738487244\n",
      "[step: 4469] loss: 0.07517101615667343\n",
      "[step: 4470] loss: 0.07882902771234512\n",
      "[step: 4471] loss: 0.08280479907989502\n",
      "[step: 4472] loss: 0.08828497678041458\n",
      "[step: 4473] loss: 0.0908462330698967\n",
      "[step: 4474] loss: 0.09247870743274689\n",
      "[step: 4475] loss: 0.08744527399539948\n",
      "[step: 4476] loss: 0.08016001433134079\n",
      "[step: 4477] loss: 0.0715131163597107\n",
      "[step: 4478] loss: 0.06686420738697052\n",
      "[step: 4479] loss: 0.06776212155818939\n",
      "[step: 4480] loss: 0.07252920418977737\n",
      "[step: 4481] loss: 0.07747730612754822\n",
      "[step: 4482] loss: 0.0765165388584137\n",
      "[step: 4483] loss: 0.07212179154157639\n",
      "[step: 4484] loss: 0.06718333065509796\n",
      "[step: 4485] loss: 0.06576600670814514\n",
      "[step: 4486] loss: 0.0677228793501854\n",
      "[step: 4487] loss: 0.07018766552209854\n",
      "[step: 4488] loss: 0.07082502543926239\n",
      "[step: 4489] loss: 0.06923834979534149\n",
      "[step: 4490] loss: 0.06760165095329285\n",
      "[step: 4491] loss: 0.06667162477970123\n",
      "[step: 4492] loss: 0.0664214938879013\n",
      "[step: 4493] loss: 0.06656648963689804\n",
      "[step: 4494] loss: 0.06694068014621735\n",
      "[step: 4495] loss: 0.06746939569711685\n",
      "[step: 4496] loss: 0.06753017753362656\n",
      "[step: 4497] loss: 0.06677316874265671\n",
      "[step: 4498] loss: 0.06569481641054153\n",
      "[step: 4499] loss: 0.0651908814907074\n",
      "[step: 4500] loss: 0.06556850671768188\n",
      "[step: 4501] loss: 0.0662606805562973\n",
      "[step: 4502] loss: 0.06648720800876617\n",
      "[step: 4503] loss: 0.06608226150274277\n",
      "[step: 4504] loss: 0.06552532315254211\n",
      "[step: 4505] loss: 0.0652388483285904\n",
      "[step: 4506] loss: 0.06524673849344254\n",
      "[step: 4507] loss: 0.06533271819353104\n",
      "[step: 4508] loss: 0.0653834342956543\n",
      "[step: 4509] loss: 0.06542103737592697\n",
      "[step: 4510] loss: 0.06543032079935074\n",
      "[step: 4511] loss: 0.065345898270607\n",
      "[step: 4512] loss: 0.06512074172496796\n",
      "[step: 4513] loss: 0.06490042060613632\n",
      "[step: 4514] loss: 0.06483781337738037\n",
      "[step: 4515] loss: 0.06493501365184784\n",
      "[step: 4516] loss: 0.0650554895401001\n",
      "[step: 4517] loss: 0.06506502628326416\n",
      "[step: 4518] loss: 0.06496548652648926\n",
      "[step: 4519] loss: 0.06484207510948181\n",
      "[step: 4520] loss: 0.0647638738155365\n",
      "[step: 4521] loss: 0.06472351402044296\n",
      "[step: 4522] loss: 0.064692422747612\n",
      "[step: 4523] loss: 0.06466913968324661\n",
      "[step: 4524] loss: 0.06466847658157349\n",
      "[step: 4525] loss: 0.06468433886766434\n",
      "[step: 4526] loss: 0.06468335539102554\n",
      "[step: 4527] loss: 0.06464090943336487\n",
      "[step: 4528] loss: 0.0645684227347374\n",
      "[step: 4529] loss: 0.06450232118368149\n",
      "[step: 4530] loss: 0.0644654929637909\n",
      "[step: 4531] loss: 0.06445207446813583\n",
      "[step: 4532] loss: 0.06444288790225983\n",
      "[step: 4533] loss: 0.06442715972661972\n",
      "[step: 4534] loss: 0.06440798938274384\n",
      "[step: 4535] loss: 0.0643908679485321\n",
      "[step: 4536] loss: 0.06437228620052338\n",
      "[step: 4537] loss: 0.06434465944766998\n",
      "[step: 4538] loss: 0.06430721282958984\n",
      "[step: 4539] loss: 0.06426767259836197\n",
      "[step: 4540] loss: 0.06423518806695938\n",
      "[step: 4541] loss: 0.06421215087175369\n",
      "[step: 4542] loss: 0.06419380009174347\n",
      "[step: 4543] loss: 0.06417502462863922\n",
      "[step: 4544] loss: 0.06415387243032455\n",
      "[step: 4545] loss: 0.06413286924362183\n",
      "[step: 4546] loss: 0.06411378085613251\n",
      "[step: 4547] loss: 0.06409439444541931\n",
      "[step: 4548] loss: 0.06407228112220764\n",
      "[step: 4549] loss: 0.06404668837785721\n",
      "[step: 4550] loss: 0.06401927769184113\n",
      "[step: 4551] loss: 0.06399296969175339\n",
      "[step: 4552] loss: 0.06396830081939697\n",
      "[step: 4553] loss: 0.06394483894109726\n",
      "[step: 4554] loss: 0.063920758664608\n",
      "[step: 4555] loss: 0.06389664113521576\n",
      "[step: 4556] loss: 0.06387265771627426\n",
      "[step: 4557] loss: 0.06385017186403275\n",
      "[step: 4558] loss: 0.06382877379655838\n",
      "[step: 4559] loss: 0.06380772590637207\n",
      "[step: 4560] loss: 0.06378622353076935\n",
      "[step: 4561] loss: 0.06376389414072037\n",
      "[step: 4562] loss: 0.06374204158782959\n",
      "[step: 4563] loss: 0.06372059881687164\n",
      "[step: 4564] loss: 0.06369954347610474\n",
      "[step: 4565] loss: 0.06367861479520798\n",
      "[step: 4566] loss: 0.06365740299224854\n",
      "[step: 4567] loss: 0.06363625824451447\n",
      "[step: 4568] loss: 0.06361526250839233\n",
      "[step: 4569] loss: 0.06359478086233139\n",
      "[step: 4570] loss: 0.06357544660568237\n",
      "[step: 4571] loss: 0.06355677545070648\n",
      "[step: 4572] loss: 0.06353915482759476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4573] loss: 0.06352321058511734\n",
      "[step: 4574] loss: 0.06350955367088318\n",
      "[step: 4575] loss: 0.06349974125623703\n",
      "[step: 4576] loss: 0.06349531561136246\n",
      "[step: 4577] loss: 0.0634986013174057\n",
      "[step: 4578] loss: 0.06351403146982193\n",
      "[step: 4579] loss: 0.06354677677154541\n",
      "[step: 4580] loss: 0.06360800564289093\n",
      "[step: 4581] loss: 0.06370917707681656\n",
      "[step: 4582] loss: 0.06388041377067566\n",
      "[step: 4583] loss: 0.06414584815502167\n",
      "[step: 4584] loss: 0.06458494812250137\n",
      "[step: 4585] loss: 0.06524135917425156\n",
      "[step: 4586] loss: 0.06632192432880402\n",
      "[step: 4587] loss: 0.06785079836845398\n",
      "[step: 4588] loss: 0.07030661404132843\n",
      "[step: 4589] loss: 0.07336223870515823\n",
      "[step: 4590] loss: 0.07779297232627869\n",
      "[step: 4591] loss: 0.08165833353996277\n",
      "[step: 4592] loss: 0.08549217879772186\n",
      "[step: 4593] loss: 0.08499699831008911\n",
      "[step: 4594] loss: 0.08152887970209122\n",
      "[step: 4595] loss: 0.07406030595302582\n",
      "[step: 4596] loss: 0.06725630164146423\n",
      "[step: 4597] loss: 0.0634530633687973\n",
      "[step: 4598] loss: 0.06375009566545486\n",
      "[step: 4599] loss: 0.06679549813270569\n",
      "[step: 4600] loss: 0.07007405906915665\n",
      "[step: 4601] loss: 0.07185467332601547\n",
      "[step: 4602] loss: 0.07032443583011627\n",
      "[step: 4603] loss: 0.0673488974571228\n",
      "[step: 4604] loss: 0.0643167495727539\n",
      "[step: 4605] loss: 0.06297913938760757\n",
      "[step: 4606] loss: 0.06358135491609573\n",
      "[step: 4607] loss: 0.06519737839698792\n",
      "[step: 4608] loss: 0.06656596064567566\n",
      "[step: 4609] loss: 0.0665355771780014\n",
      "[step: 4610] loss: 0.06545865535736084\n",
      "[step: 4611] loss: 0.06386171281337738\n",
      "[step: 4612] loss: 0.06285527348518372\n",
      "[step: 4613] loss: 0.06281860917806625\n",
      "[step: 4614] loss: 0.06350479274988174\n",
      "[step: 4615] loss: 0.06428749114274979\n",
      "[step: 4616] loss: 0.06454506516456604\n",
      "[step: 4617] loss: 0.06421779096126556\n",
      "[step: 4618] loss: 0.0634828507900238\n",
      "[step: 4619] loss: 0.06284765899181366\n",
      "[step: 4620] loss: 0.06258699297904968\n",
      "[step: 4621] loss: 0.0627257451415062\n",
      "[step: 4622] loss: 0.06305986642837524\n",
      "[step: 4623] loss: 0.06331419944763184\n",
      "[step: 4624] loss: 0.06334833800792694\n",
      "[step: 4625] loss: 0.06312967091798782\n",
      "[step: 4626] loss: 0.06280479580163956\n",
      "[step: 4627] loss: 0.0625278651714325\n",
      "[step: 4628] loss: 0.06240803375840187\n",
      "[step: 4629] loss: 0.06245441362261772\n",
      "[step: 4630] loss: 0.06259115040302277\n",
      "[step: 4631] loss: 0.06271222978830338\n",
      "[step: 4632] loss: 0.06273398548364639\n",
      "[step: 4633] loss: 0.06265247613191605\n",
      "[step: 4634] loss: 0.06250260770320892\n",
      "[step: 4635] loss: 0.06235437095165253\n",
      "[step: 4636] loss: 0.06225748732686043\n",
      "[step: 4637] loss: 0.06222835183143616\n",
      "[step: 4638] loss: 0.06224990636110306\n",
      "[step: 4639] loss: 0.06229105591773987\n",
      "[step: 4640] loss: 0.06232055649161339\n",
      "[step: 4641] loss: 0.06231691315770149\n",
      "[step: 4642] loss: 0.06228051334619522\n",
      "[step: 4643] loss: 0.0622175969183445\n",
      "[step: 4644] loss: 0.06214696168899536\n",
      "[step: 4645] loss: 0.0620846152305603\n",
      "[step: 4646] loss: 0.06204236298799515\n",
      "[step: 4647] loss: 0.06202135235071182\n",
      "[step: 4648] loss: 0.06201659515500069\n",
      "[step: 4649] loss: 0.062018703669309616\n",
      "[step: 4650] loss: 0.0620175376534462\n",
      "[step: 4651] loss: 0.06200790777802467\n",
      "[step: 4652] loss: 0.06198810786008835\n",
      "[step: 4653] loss: 0.06195983290672302\n",
      "[step: 4654] loss: 0.06192600727081299\n",
      "[step: 4655] loss: 0.06189058721065521\n",
      "[step: 4656] loss: 0.061856359243392944\n",
      "[step: 4657] loss: 0.0618254616856575\n",
      "[step: 4658] loss: 0.0617995411157608\n",
      "[step: 4659] loss: 0.06177815794944763\n",
      "[step: 4660] loss: 0.06176052242517471\n",
      "[step: 4661] loss: 0.06174527108669281\n",
      "[step: 4662] loss: 0.06173080950975418\n",
      "[step: 4663] loss: 0.06171587482094765\n",
      "[step: 4664] loss: 0.06169958412647247\n",
      "[step: 4665] loss: 0.06168195977807045\n",
      "[step: 4666] loss: 0.06166340410709381\n",
      "[step: 4667] loss: 0.061643414199352264\n",
      "[step: 4668] loss: 0.06162253022193909\n",
      "[step: 4669] loss: 0.061600979417562485\n",
      "[step: 4670] loss: 0.06157881021499634\n",
      "[step: 4671] loss: 0.061556704342365265\n",
      "[step: 4672] loss: 0.061534859240055084\n",
      "[step: 4673] loss: 0.06151311844587326\n",
      "[step: 4674] loss: 0.06149156391620636\n",
      "[step: 4675] loss: 0.06147044152021408\n",
      "[step: 4676] loss: 0.06144959107041359\n",
      "[step: 4677] loss: 0.06142925098538399\n",
      "[step: 4678] loss: 0.061409130692481995\n",
      "[step: 4679] loss: 0.06138933449983597\n",
      "[step: 4680] loss: 0.06137010455131531\n",
      "[step: 4681] loss: 0.061351194977760315\n",
      "[step: 4682] loss: 0.06133296713232994\n",
      "[step: 4683] loss: 0.06131570786237717\n",
      "[step: 4684] loss: 0.06129961088299751\n",
      "[step: 4685] loss: 0.06128482520580292\n",
      "[step: 4686] loss: 0.0612725168466568\n",
      "[step: 4687] loss: 0.0612633116543293\n",
      "[step: 4688] loss: 0.06125904247164726\n",
      "[step: 4689] loss: 0.0612618550658226\n",
      "[step: 4690] loss: 0.06127580255270004\n",
      "[step: 4691] loss: 0.06130518764257431\n",
      "[step: 4692] loss: 0.06136104464530945\n",
      "[step: 4693] loss: 0.06145407259464264\n",
      "[step: 4694] loss: 0.06161146238446236\n",
      "[step: 4695] loss: 0.061858706176280975\n",
      "[step: 4696] loss: 0.0622674934566021\n",
      "[step: 4697] loss: 0.06289013475179672\n",
      "[step: 4698] loss: 0.06391633301973343\n",
      "[step: 4699] loss: 0.06541092693805695\n",
      "[step: 4700] loss: 0.06782974302768707\n",
      "[step: 4701] loss: 0.0709984079003334\n",
      "[step: 4702] loss: 0.07570875436067581\n",
      "[step: 4703] loss: 0.08033251017332077\n",
      "[step: 4704] loss: 0.08537530899047852\n",
      "[step: 4705] loss: 0.08623600006103516\n",
      "[step: 4706] loss: 0.08367787301540375\n",
      "[step: 4707] loss: 0.07565657794475555\n",
      "[step: 4708] loss: 0.06729208678007126\n",
      "[step: 4709] loss: 0.061793308705091476\n",
      "[step: 4710] loss: 0.061319783329963684\n",
      "[step: 4711] loss: 0.06462321430444717\n",
      "[step: 4712] loss: 0.0686802938580513\n",
      "[step: 4713] loss: 0.0709773600101471\n",
      "[step: 4714] loss: 0.06928201019763947\n",
      "[step: 4715] loss: 0.0657322034239769\n",
      "[step: 4716] loss: 0.06226164475083351\n",
      "[step: 4717] loss: 0.060947485268116\n",
      "[step: 4718] loss: 0.061903342604637146\n",
      "[step: 4719] loss: 0.06379791349172592\n",
      "[step: 4720] loss: 0.06508258730173111\n",
      "[step: 4721] loss: 0.06467381864786148\n",
      "[step: 4722] loss: 0.0632241889834404\n",
      "[step: 4723] loss: 0.06154712289571762\n",
      "[step: 4724] loss: 0.06074952706694603\n",
      "[step: 4725] loss: 0.061035871505737305\n",
      "[step: 4726] loss: 0.06191667169332504\n",
      "[step: 4727] loss: 0.0626428946852684\n",
      "[step: 4728] loss: 0.0626048743724823\n",
      "[step: 4729] loss: 0.06193320453166962\n",
      "[step: 4730] loss: 0.06103484332561493\n",
      "[step: 4731] loss: 0.06050781160593033\n",
      "[step: 4732] loss: 0.0605643093585968\n",
      "[step: 4733] loss: 0.06100337207317352\n",
      "[step: 4734] loss: 0.06141972541809082\n",
      "[step: 4735] loss: 0.061462972313165665\n",
      "[step: 4736] loss: 0.06115621700882912\n",
      "[step: 4737] loss: 0.0606970377266407\n",
      "[step: 4738] loss: 0.06038191169500351\n",
      "[step: 4739] loss: 0.06033921241760254\n",
      "[step: 4740] loss: 0.060500092804431915\n",
      "[step: 4741] loss: 0.06069517135620117\n",
      "[step: 4742] loss: 0.06077093631029129\n",
      "[step: 4743] loss: 0.060695841908454895\n",
      "[step: 4744] loss: 0.06051313132047653\n",
      "[step: 4745] loss: 0.06033216789364815\n",
      "[step: 4746] loss: 0.060221221297979355\n",
      "[step: 4747] loss: 0.060200467705726624\n",
      "[step: 4748] loss: 0.06024332717061043\n",
      "[step: 4749] loss: 0.06030014157295227\n",
      "[step: 4750] loss: 0.06032983958721161\n",
      "[step: 4751] loss: 0.06030943989753723\n",
      "[step: 4752] loss: 0.06024542450904846\n",
      "[step: 4753] loss: 0.06015783175826073\n",
      "[step: 4754] loss: 0.060077086091041565\n",
      "[step: 4755] loss: 0.06002616509795189\n",
      "[step: 4756] loss: 0.06001193821430206\n",
      "[step: 4757] loss: 0.06002475693821907\n",
      "[step: 4758] loss: 0.06004394218325615\n",
      "[step: 4759] loss: 0.06004961580038071\n",
      "[step: 4760] loss: 0.06003177538514137\n",
      "[step: 4761] loss: 0.05999266356229782\n",
      "[step: 4762] loss: 0.05994359776377678\n",
      "[step: 4763] loss: 0.059897180646657944\n",
      "[step: 4764] loss: 0.05986212193965912\n",
      "[step: 4765] loss: 0.059840187430381775\n",
      "[step: 4766] loss: 0.05982840061187744\n",
      "[step: 4767] loss: 0.05982154980301857\n",
      "[step: 4768] loss: 0.059814102947711945\n",
      "[step: 4769] loss: 0.05980326235294342\n",
      "[step: 4770] loss: 0.059787817299366\n",
      "[step: 4771] loss: 0.059767305850982666\n",
      "[step: 4772] loss: 0.05974283441901207\n",
      "[step: 4773] loss: 0.059715889394283295\n",
      "[step: 4774] loss: 0.05968868359923363\n",
      "[step: 4775] loss: 0.059662818908691406\n",
      "[step: 4776] loss: 0.05964018031954765\n",
      "[step: 4777] loss: 0.05962072312831879\n",
      "[step: 4778] loss: 0.0596037358045578\n",
      "[step: 4779] loss: 0.05958874151110649\n",
      "[step: 4780] loss: 0.059574246406555176\n",
      "[step: 4781] loss: 0.05955919623374939\n",
      "[step: 4782] loss: 0.05954337120056152\n",
      "[step: 4783] loss: 0.0595267079770565\n",
      "[step: 4784] loss: 0.05950896441936493\n",
      "[step: 4785] loss: 0.059490982443094254\n",
      "[step: 4786] loss: 0.05947224423289299\n",
      "[step: 4787] loss: 0.05945345386862755\n",
      "[step: 4788] loss: 0.05943397805094719\n",
      "[step: 4789] loss: 0.05941455811262131\n",
      "[step: 4790] loss: 0.05939507484436035\n",
      "[step: 4791] loss: 0.05937584117054939\n",
      "[step: 4792] loss: 0.05935656279325485\n",
      "[step: 4793] loss: 0.05933763086795807\n",
      "[step: 4794] loss: 0.059318907558918\n",
      "[step: 4795] loss: 0.05930042266845703\n",
      "[step: 4796] loss: 0.059282246977090836\n",
      "[step: 4797] loss: 0.05926390364766121\n",
      "[step: 4798] loss: 0.05924580246210098\n",
      "[step: 4799] loss: 0.059227850288152695\n",
      "[step: 4800] loss: 0.05921001359820366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4801] loss: 0.059192366898059845\n",
      "[step: 4802] loss: 0.059175044298172\n",
      "[step: 4803] loss: 0.0591580867767334\n",
      "[step: 4804] loss: 0.05914140120148659\n",
      "[step: 4805] loss: 0.05912535637617111\n",
      "[step: 4806] loss: 0.059110209345817566\n",
      "[step: 4807] loss: 0.05909616872668266\n",
      "[step: 4808] loss: 0.05908413231372833\n",
      "[step: 4809] loss: 0.059074610471725464\n",
      "[step: 4810] loss: 0.0590696707367897\n",
      "[step: 4811] loss: 0.059071168303489685\n",
      "[step: 4812] loss: 0.05908270925283432\n",
      "[step: 4813] loss: 0.05911082774400711\n",
      "[step: 4814] loss: 0.05916385352611542\n",
      "[step: 4815] loss: 0.0592593252658844\n",
      "[step: 4816] loss: 0.059419941157102585\n",
      "[step: 4817] loss: 0.059693627059459686\n",
      "[step: 4818] loss: 0.06013764068484306\n",
      "[step: 4819] loss: 0.060886915773153305\n",
      "[step: 4820] loss: 0.062071364372968674\n",
      "[step: 4821] loss: 0.06405025720596313\n",
      "[step: 4822] loss: 0.06699776649475098\n",
      "[step: 4823] loss: 0.07166612148284912\n",
      "[step: 4824] loss: 0.07752031832933426\n",
      "[step: 4825] loss: 0.08509151637554169\n",
      "[step: 4826] loss: 0.09026405215263367\n",
      "[step: 4827] loss: 0.09192535281181335\n",
      "[step: 4828] loss: 0.08473506569862366\n",
      "[step: 4829] loss: 0.07326163351535797\n",
      "[step: 4830] loss: 0.06261171400547028\n",
      "[step: 4831] loss: 0.05904635787010193\n",
      "[step: 4832] loss: 0.062433771789073944\n",
      "[step: 4833] loss: 0.0684317797422409\n",
      "[step: 4834] loss: 0.07208970189094543\n",
      "[step: 4835] loss: 0.06971990317106247\n",
      "[step: 4836] loss: 0.06456739455461502\n",
      "[step: 4837] loss: 0.06034065783023834\n",
      "[step: 4838] loss: 0.0597563236951828\n",
      "[step: 4839] loss: 0.06190473958849907\n",
      "[step: 4840] loss: 0.06403540074825287\n",
      "[step: 4841] loss: 0.06420838087797165\n",
      "[step: 4842] loss: 0.062354717403650284\n",
      "[step: 4843] loss: 0.06044268235564232\n",
      "[step: 4844] loss: 0.05957688391208649\n",
      "[step: 4845] loss: 0.05989958718419075\n",
      "[step: 4846] loss: 0.06065550446510315\n",
      "[step: 4847] loss: 0.06101769581437111\n",
      "[step: 4848] loss: 0.06078799068927765\n",
      "[step: 4849] loss: 0.06005425006151199\n",
      "[step: 4850] loss: 0.0593065470457077\n",
      "[step: 4851] loss: 0.05894065275788307\n",
      "[step: 4852] loss: 0.059092819690704346\n",
      "[step: 4853] loss: 0.05953407287597656\n",
      "[step: 4854] loss: 0.05976928025484085\n",
      "[step: 4855] loss: 0.05954053997993469\n",
      "[step: 4856] loss: 0.05894836038351059\n",
      "[step: 4857] loss: 0.05847497656941414\n",
      "[step: 4858] loss: 0.05845964699983597\n",
      "[step: 4859] loss: 0.05880801007151604\n",
      "[step: 4860] loss: 0.05911068245768547\n",
      "[step: 4861] loss: 0.05903337150812149\n",
      "[step: 4862] loss: 0.05865168198943138\n",
      "[step: 4863] loss: 0.05829211696982384\n",
      "[step: 4864] loss: 0.05822081118822098\n",
      "[step: 4865] loss: 0.058403875678777695\n",
      "[step: 4866] loss: 0.058593686670064926\n",
      "[step: 4867] loss: 0.05859721451997757\n",
      "[step: 4868] loss: 0.05842014402151108\n",
      "[step: 4869] loss: 0.0582270547747612\n",
      "[step: 4870] loss: 0.05814460292458534\n",
      "[step: 4871] loss: 0.05817608907818794\n",
      "[step: 4872] loss: 0.058232974261045456\n",
      "[step: 4873] loss: 0.058241426944732666\n",
      "[step: 4874] loss: 0.058199264109134674\n",
      "[step: 4875] loss: 0.05814136937260628\n",
      "[step: 4876] loss: 0.05809684470295906\n",
      "[step: 4877] loss: 0.05806412547826767\n",
      "[step: 4878] loss: 0.058032650500535965\n",
      "[step: 4879] loss: 0.058003418147563934\n",
      "[step: 4880] loss: 0.05798855423927307\n",
      "[step: 4881] loss: 0.05798976123332977\n",
      "[step: 4882] loss: 0.0579918809235096\n",
      "[step: 4883] loss: 0.057974930852651596\n",
      "[step: 4884] loss: 0.05793352425098419\n",
      "[step: 4885] loss: 0.05788315460085869\n",
      "[step: 4886] loss: 0.05784699320793152\n",
      "[step: 4887] loss: 0.057835083454847336\n",
      "[step: 4888] loss: 0.05783921480178833\n",
      "[step: 4889] loss: 0.057840898633003235\n",
      "[step: 4890] loss: 0.057826876640319824\n",
      "[step: 4891] loss: 0.05779815465211868\n",
      "[step: 4892] loss: 0.05776519700884819\n",
      "[step: 4893] loss: 0.05773783475160599\n",
      "[step: 4894] loss: 0.05771961063146591\n",
      "[step: 4895] loss: 0.057706743478775024\n",
      "[step: 4896] loss: 0.05769416317343712\n",
      "[step: 4897] loss: 0.05767952650785446\n",
      "[step: 4898] loss: 0.05766335502266884\n",
      "[step: 4899] loss: 0.057647060602903366\n",
      "[step: 4900] loss: 0.0576310008764267\n",
      "[step: 4901] loss: 0.05761347711086273\n",
      "[step: 4902] loss: 0.057593852281570435\n",
      "[step: 4903] loss: 0.05757318437099457\n",
      "[step: 4904] loss: 0.057552993297576904\n",
      "[step: 4905] loss: 0.05753561109304428\n",
      "[step: 4906] loss: 0.057520732283592224\n",
      "[step: 4907] loss: 0.057507436722517014\n",
      "[step: 4908] loss: 0.057493530213832855\n",
      "[step: 4909] loss: 0.05747808888554573\n",
      "[step: 4910] loss: 0.057460885494947433\n",
      "[step: 4911] loss: 0.05744302645325661\n",
      "[step: 4912] loss: 0.05742514878511429\n",
      "[step: 4913] loss: 0.05740806460380554\n",
      "[step: 4914] loss: 0.05739137530326843\n",
      "[step: 4915] loss: 0.05737479031085968\n",
      "[step: 4916] loss: 0.057357966899871826\n",
      "[step: 4917] loss: 0.05734124779701233\n",
      "[step: 4918] loss: 0.057324692606925964\n",
      "[step: 4919] loss: 0.05730850249528885\n",
      "[step: 4920] loss: 0.05729280412197113\n",
      "[step: 4921] loss: 0.05727744475007057\n",
      "[step: 4922] loss: 0.05726179480552673\n",
      "[step: 4923] loss: 0.057245854288339615\n",
      "[step: 4924] loss: 0.057229623198509216\n",
      "[step: 4925] loss: 0.057213038206100464\n",
      "[step: 4926] loss: 0.05719665065407753\n",
      "[step: 4927] loss: 0.05718034505844116\n",
      "[step: 4928] loss: 0.057163968682289124\n",
      "[step: 4929] loss: 0.05714786797761917\n",
      "[step: 4930] loss: 0.05713169649243355\n",
      "[step: 4931] loss: 0.05711528658866882\n",
      "[step: 4932] loss: 0.057098764926195145\n",
      "[step: 4933] loss: 0.057082243263721466\n",
      "[step: 4934] loss: 0.05706619843840599\n",
      "[step: 4935] loss: 0.0570497028529644\n",
      "[step: 4936] loss: 0.057033658027648926\n",
      "[step: 4937] loss: 0.0570174977183342\n",
      "[step: 4938] loss: 0.05700157955288887\n",
      "[step: 4939] loss: 0.0569855272769928\n",
      "[step: 4940] loss: 0.05696947127580643\n",
      "[step: 4941] loss: 0.05695367231965065\n",
      "[step: 4942] loss: 0.056937843561172485\n",
      "[step: 4943] loss: 0.05692233890295029\n",
      "[step: 4944] loss: 0.05690719187259674\n",
      "[step: 4945] loss: 0.056892335414886475\n",
      "[step: 4946] loss: 0.056878287345170975\n",
      "[step: 4947] loss: 0.05686475709080696\n",
      "[step: 4948] loss: 0.056852638721466064\n",
      "[step: 4949] loss: 0.056842707097530365\n",
      "[step: 4950] loss: 0.05683580040931702\n",
      "[step: 4951] loss: 0.056834083050489426\n",
      "[step: 4952] loss: 0.056839995086193085\n",
      "[step: 4953] loss: 0.0568588450551033\n",
      "[step: 4954] loss: 0.05689757317304611\n",
      "[step: 4955] loss: 0.05697028711438179\n",
      "[step: 4956] loss: 0.05709657445549965\n",
      "[step: 4957] loss: 0.057315096259117126\n",
      "[step: 4958] loss: 0.05767980217933655\n",
      "[step: 4959] loss: 0.05830255150794983\n",
      "[step: 4960] loss: 0.059321824461221695\n",
      "[step: 4961] loss: 0.061049893498420715\n",
      "[step: 4962] loss: 0.06376517564058304\n",
      "[step: 4963] loss: 0.0681886225938797\n",
      "[step: 4964] loss: 0.07431944459676743\n",
      "[step: 4965] loss: 0.08283897489309311\n",
      "[step: 4966] loss: 0.0905906930565834\n",
      "[step: 4967] loss: 0.09570002555847168\n",
      "[step: 4968] loss: 0.09095560014247894\n",
      "[step: 4969] loss: 0.07872124016284943\n",
      "[step: 4970] loss: 0.06417372077703476\n",
      "[step: 4971] loss: 0.05706452950835228\n",
      "[step: 4972] loss: 0.05957508087158203\n",
      "[step: 4973] loss: 0.06704296171665192\n",
      "[step: 4974] loss: 0.07239462435245514\n",
      "[step: 4975] loss: 0.07010902464389801\n",
      "[step: 4976] loss: 0.06356573849916458\n",
      "[step: 4977] loss: 0.05817212164402008\n",
      "[step: 4978] loss: 0.05782822147011757\n",
      "[step: 4979] loss: 0.06103777140378952\n",
      "[step: 4980] loss: 0.06367610394954681\n",
      "[step: 4981] loss: 0.06317138671875\n",
      "[step: 4982] loss: 0.06015205383300781\n",
      "[step: 4983] loss: 0.0578107126057148\n",
      "[step: 4984] loss: 0.05756978318095207\n",
      "[step: 4985] loss: 0.05880075693130493\n",
      "[step: 4986] loss: 0.05986043065786362\n",
      "[step: 4987] loss: 0.05958675593137741\n",
      "[step: 4988] loss: 0.058446887880563736\n",
      "[step: 4989] loss: 0.05733355134725571\n",
      "[step: 4990] loss: 0.05698719620704651\n",
      "[step: 4991] loss: 0.0574113205075264\n",
      "[step: 4992] loss: 0.05798213928937912\n",
      "[step: 4993] loss: 0.058079902082681656\n",
      "[step: 4994] loss: 0.05750344693660736\n",
      "[step: 4995] loss: 0.056752294301986694\n",
      "[step: 4996] loss: 0.05642803758382797\n",
      "[step: 4997] loss: 0.05671171098947525\n",
      "[step: 4998] loss: 0.05718519166111946\n",
      "[step: 4999] loss: 0.057267025113105774\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEKCAYAAADJvIhZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuUXGWZ7/Hv0/d0upN0Q4BAJwYhOuJdw0UdPVEcjOgR\n1hFHPM4QPayFulDxqMNFzzme0eGA41kijOiaKCzgLEbE2xAdmJjhqqNcwkUYLkIMSpqES0hVLl1J\nV1+e88d+q3p3p7q7ktSu2pX9+6zVK7Xf/e6qt3aSfvbz7ne/r7k7IiIi9dDS6AaIiEh2KOiIiEjd\nKOiIiEjdKOiIiEjdKOiIiEjdKOiIiEjdKOiIiEjdKOiIiEjdKOiIiEjdtDW6AWlz6KGH+tKlSxvd\nDBGRpnL//fdvdfeFs9VT0Jli6dKlrF+/vtHNEBFpKmb2p2rqqXtNRETqRkFHRETqRkFHRETqRkFH\nRETqRkFHRETqRkFHRETqRkFHRETqRkEnw/7l4S1s3TXc6GaISIYo6GTUjj0jnPtPD3D93c80uiki\nkiEKOhm1bVcRgGe2FRrcEhHJEgWdjMoVoqCzKaegIyL1o6CTUfnCCACDynREpI4UdDIqvzvKdLbs\n2ENxdLzBrRGRrFDQyajcUJTpuMPm/O4Gt0ZEskJBJ6Py4Z4OwGBOQUdE6kNBJ6NyhRHMotcaTCAi\n9aKgk1G5QpHFfd20tRibNJhAROpEK4dmVL4wwiE9HQBsUveaiNSJgk5G5QpFDp/XRXdHqzIdEakb\nda9lVL4wwoLudhb3dTOoezoiUifKdDIqVyjS191B/9wOtu4qsrs4xpyO1kY3S0QOcsp0Mmh4dIxC\ncYy+7nYG+uYAKNsRkbpQ0Mmg0hQ487s7GOjrBjRsWkTqQ0Eng0pBp6+7ncX9UaazaZtGsIlI8hR0\nMqg0w3RfdwcLezrpam/RCDYRqQsFnQwqTYGzoLsdM2Ogr1vdayJSFwo6GZQrd69FD4cu7puj+ddE\npC4UdDIo3r0GRJmOutdEpA4aHnTMrNXMHjSzX4Tto83sHjN7ysx+aGYdobwzbG8I+5fG3uOiUP57\nM3tPrHxlKNtgZhfW+7ulVb4wQmdbS/m5nMX9c9ixZ5Ttu0ca3DIROdg1POgA5wGPx7a/Dlzm7suA\nHHB2KD8byLn7scBloR5mdhxwJvBqYCXwnRDIWoErgfcCxwEfCXUzLzdULGc5AItLw6aV7YhIwhoa\ndMxsAHgf8P2wbcC7gB+HKtcCp4fXp4Vtwv6TQ/3TgBvcfdjdnwY2ACeEnw3uvtHdi8ANoW7m5cIU\nOCWL+6OgowdERSRpjc50vgWcD5TWSz4EyLv7aNgeBI4Kr48CNgGE/dtD/XL5lGOmK9+LmZ1jZuvN\nbP2LL754oN8p9fKF6TIdDSYQkWQ1LOiY2fuBF9z9/nhxhao+y759Ld+70H21uy939+ULFy6codUH\nh1yhOCnTmd/dTm9XmzIdEUlcIyf8fBvwATM7FegC5hFlPgvMrC1kMwPA5lB/EFgMDJpZGzAf2BYr\nL4kfM115pm3fPcKCWKYDUbajdXVEJGkNy3Tc/SJ3H3D3pUQDAW5z948CtwNnhGqrgJvC6zVhm7D/\nNnf3UH5mGN12NLAMuBe4D1gWRsN1hM9YU4evlmruTr4wQl8s0wEY6JujgQQikrhG39Op5ALg82a2\ngeiezVWh/CrgkFD+eeBCAHd/FLgReAz4V+Bcdx8LmdKngbVEo+NuDHUzbefwKKPjPumeDkSDCQZz\nu4niuIhIMlKxno673wHcEV5vJBp5NrXOHuBD0xx/MXBxhfKbgZtr2NSmlx+KnsVZMCXTWdw3h90j\nY2zdVWRhb2cjmiYiGZDGTEcSNHU2gpLSsGnNwSYiSVLQyZhy0Jk7JdMpP6ujwQQikhwFnYwpraUz\ndfRaaQVRDSYQkSQp6GTMdN1r3R1tHDK3Q8/qiEiiFHQyprSswbyuvceQDPR3a1YCEUmUgk7G5AtF\n5nW10da691/94r45GkggIolS0MmYXGGEvrkdFfct7u9mc343Y+N6VkdEkqGgkzH5QnGvQQQli/u6\nGRlznt+xp86tEpGsUNDJmEpT4JQs7tcINhFJloJOxuSmLGsQV17iQM/qiEhCFHQyJj9lAbe4RQu6\nMFOmIyLJUdDJkOLoOLuGR6fNdDrbWjliXpdGsIlIYhR0MiS/u/RgaOVMB6IutkE9qyMiCVHQyZDp\npsCJG+jXszoikhwFnQzJDUWZznT3dCDKdJ7bsYfi6Hi9miUiGaKgkyGlKXCmu6cD0QOi7rA5ry42\nEak9BZ0MyRdmz3TKs02ri01EEqCgkyH53dVlOoAm/hSRRCjoZEiuUKSjtYXujtZp6xwxr4v2VlOm\nIyKJUNDJkPxQ9GComU1bp7XFOHLBHD0gKiKJUNDJkJmmwIlb3NetZatFJBEKOhky0xQ4cYv752gF\nURFJhIJOhlSb6Qz0dbN1V5FCcbQOrRKRLFHQyZBoAbfZM53SsGl1sYlIrSnoZIS7ky8UmT+nins6\n5WHT6mITkdpS0MmIXcOjjI77jJN9lpTX1VHQEZEaU9DJiHwVU+CUHNrTwZz2VnWviUjNKehkRK6K\nKXBKzIyBPs02LSK119boBkh9lDOdubNnOhDd11n/xxxf/NHvaDFoMcPMMKO83VLetkl1WoxyuUEo\nmzjWYscZlPeVjwVaWiaOnVqXUp3ycaGc+HtMfB7YlPZM8z5ExxI7fqa2xD93altK+y12DuKfRez4\nSp9dOk/xP+Pn3GLbpToizUBBJyNKmU4193QA3vPqw3ny+Z389g8vMe4efqIBCeMO4+74lD9LdcbH\nHWeirtRHPDARuziIB+uWKYEM4hcMsSDaMjn4lQPwlHrxi4yJwBm7eIhdUJQDMVZ+/4kAPHHMpEBd\nLtv7veJtmXxRMPEZExccky8kJt5778+IX3RMPW7iu02+iIpfVMUvviaf74myyX8XsYu6Sd9rmu86\n3Z/sfYE09aKqdMEy9e+g9G/hyAVddHckGxYaFnTMbDFwHXAEMA6sdvfLzawf+CGwFPgj8JfunrPo\nUu5y4FSgAHzM3R8I77UK+B/hrf/O3a8N5W8GrgHmADcD57l7Jn8NVrOAW9yHj1/Ch49fUpPP9niA\nYiJQ7VU2PrFdCljO5LqlY6P33bs+lD5r4tj455XfL9YOKAXUyZ/LlHql1/jeZaVjvUK7vML3CB8b\nqxfVGQ8bzkTwngj2UwL++MS5q1Sn9P7jsc8FGBv32Lmp/PfDpO0pdWJtnXz+Sudu8vceH4/VHwdn\nHB+b+t2nP1dMOT8T9Sb/m4j/HUxcDMX/TcQ+b7rX5b+DKZ9H6bse3K75+PGseOVhiX5GIzOdUeAL\n7v6AmfUC95vZOuBjwK3ufqmZXQhcCFwAvBdYFn5OBL4LnBiC1FeA5UT/Du83szXungt1zgHuJgo6\nK4Fb6vgdU6N8T2dOdZlOLZWvQFEXkDS/cnDdK8DFsv5YoI0HxnhvAEy9YNj7YqvSBdpYiIrxi4mp\nbYhfPEwE/skXWpUuzo5bNC/x89ewoOPuW4At4fVOM3scOAo4DVgRql0L3EEUdE4DrguZyt1mtsDM\nFoW669x9G0AIXCvN7A5gnrv/NpRfB5xORoNOvjBCb1cbba0aOyJyIMyM1tAdJfsuFb+BzGwp8Ebg\nHuDwEJBKgamU6x0FbIodNhjKZiofrFCeKo9v2cG51z/AlbdvSPRzqp0CR0QkSQ0fSGBmPcBPgM+5\n+44ZRuFU2uH7UV6pDecQdcOxZElt7mPM5umtQ1y27kl+/vBm3OFP24Y4953HJvZ5uSon+xQRSVJD\ng46ZtRMFnOvd/aeh+HkzW+TuW0L32QuhfBBYHDt8ANgcyldMKb8jlA9UqL8Xd18NrAZYvnx5orcL\nN+d3c8WtT/Gj+wfpaG3hk//pGJ7YsoOntw4l+bHkC8WqBxGIiCSlYd1rYTTaVcDj7v7N2K41wKrw\nehVwU6z8LIucBGwP3W9rgVPMrM/M+oBTgLVh304zOyl81lmx96q7rbuG+dufP8qKb9zBTx94lr8+\n6WXcef4KLlj5ZyxaMIddw8nO6Bx1rynTEZHGamSm8zbgr4FHzOyhUPYl4FLgRjM7G3gG+FDYdzPR\ncOkNREOmPw7g7tvM7GvAfaHeV0uDCoBPMTFk+hYaMIhg1/Ao37trI9/71Ub2jIxxxpsH+OzJyxgI\n85sB9Ha2sXNPskEnPzSiezoi0nCNHL32a6Yf/nFyhfoOnDvNe10NXF2hfD3wmgNo5n4bGRvnB/c+\nwxW3PsXWXUVOfe0RfOGUV3LMwp696vZ0tjE8Ok5xdJyOttonnyNj4+wcHtU9HRFpuIYPJDjYuDs3\nP/Ic31j7BH98qcAJR/fzvbP+jDcu6Zv2mJ6u6K9haHiUjrbaZyPbd1c/2aeISJIUdGrono0vcckt\nT/DQpjyvOLyHqz+2nHe+8rBZ58Xq7YoykF3Do1XPjbYv8vsw2aeISJIUdGrkqz9/jKv//WkWze/i\n7894HR980wCtLdU9PNbTGf01JHVfJ7cPyxqIiCRJQadG3vGKQ1nY28nH37aUrvbWfTq2N3SvJTWC\nLTdUmuxTQUdEGktBp0ZWvPKw/Z4or5Tp7BoeqWWTyiYm+1T3mog01qxDpcJzMX9lZv8rbC8xsxOS\nb1p2lAYSJNe9pns6IpIO1YzP/Q7wFuAjYXsncGViLcqg3s6Eu9cKI7S1WDmjEhFplGp+C53o7m8y\nswcBwto2ujlQQ0lnOqUpcLS6pIg0WjWZzoiZtRImyzSzhUSLrkmNzGlvpcVgV4Lda5oCR0TSoJqg\ncwXwM+AwM7sY+DXwfxJtVcaYRV1fSXavaeSaiKTBrN1r7n69md1PNDWNAae7++OJtyxjervaE+te\n214Y4WWHdM9eUUQkYdWMXjsJeNbdr3T3bwODZnZi8k3LlijTSWbItBZwE5G0qKZ77bvArtj2UCiT\nGurpSqZ7zd3JF0ZYMFf3dESk8aoJOhZmeAbA3cfRQ6U119PZlshAgkJxjOLYuDIdEUmFaoLORjP7\nrJm1h5/zgI1JNyxrerva2JlAplN+MHSOMh0Rabxqgs4ngbcCzxItAX0icE6Sjcqi3q5kMp2JKXCU\n6YhI41Uzeu0F4Mw6tCXTkhoyXcp09JyOiKTBtEHHzM539783s38gPBga5+6fTbRlGdPT2U6hOMbY\nuFe9JEI1yssaJLBOj4jIvpop0yk9i7O+Hg3Jup7Y8gbza3j/RQu4iUiaTBt03P3nYfqb17j739Sx\nTZkUn/SzlkEnNxTu6cxRpiMijTfjQAJ3HwPeXKe2ZFo506nxYIL87iI9nW10tFUzZkREJFnVPG/z\noJmtAX5E9GAoAO7+08RalUFJLeSWL4yoa01EUqOaoNMPvAS8K1bmgIJODZUynR01znQ0BY6IpEk1\nQedv3H1r4i3JuPI9nZoHHWU6IpIe03b0m9l/NrMXgYfNbNDM3lrHdmVOfPRaLZUWcBMRSYOZ7i5f\nDLzd3Y8EPghcUp8mZVNPUpnOkBZwE5H0mCnojLr7EwDufg/QW58mZdPcjrBkdQ0zndGxcXbsGVWm\nIyKpMdM9ncPM7PPTbbv7N5NrVva0tFjNZ5revjvMRqBMR0RSYqag8z0mZzdTt6XGertqu5BbeQoc\nZToikhIzzUjwt/VsiOzfpJ/bhooYledW0xQ4IpI2WowtRXq62ti5j91rf/mPv2Xji7tY/rJ+Tn7V\nYbz7uMM5ZmEPoExHRNLnoA86ZrYSuBxoBb7v7pc2uEnT2tdMx915ZluB446cx67hUS655QkuueUJ\nXn7oXE5+1WGMjUf1FHREJC1mDTpm1unuw1PK+t19W3LNqo0wYemVwF8QLUB3n5mtcffHGtuyynq7\n2nhu+56q6+8eGaM4Os77Xnskn1pxDIO5Arc98QLrHnuea37zR0bGohUpFsxV95qIpEM1mc5Pzex0\ndx8BMLNFwC9ojolATwA2uPtGADO7ATgNSGXQ2ddMZ6L7LAoqA33dnPWWpZz1lqXs3DPCXU9upVAc\nZV6Xgo6IpEM1QeefgR+Z2QeBxcAa4IuJtqp2jgI2xbZLy21PYmbnEJbgXrJkSX1aVkFPZ/s+DZnO\nDZUGCuzdfdbb1c77XreoZm0TEamFapar/p6ZdRAFn6XAJ9z9N0k3rEYqLcFZaRXU1cBqgOXLl++1\nv156utrYVRxlfNxpqWL10HxBz+GISHOZabnq+IOhRpTlPAScZGYnNcnDoYNE7S4ZADY3qC2z6u1s\nwx0KI2PlaXFmkgtDorUUtYg0i5l+s019EPRn05Sn2X3AMjM7GngWOBP4r41t0vRKk37u3DNSVdDR\nczgi0mwO6odD3X3UzD4NrCUaMn21uz/a4GZNa9Kkn/Nnr18aSKClqEWkWcy6hrGZrTOzBbHtPjNb\nm2yzasfdb3b3V7j7Me5+caPbM5NyplPlCLZtQ0V6tRS1iDSRan5bLXT3fGnD3XPAYck1Kbv2dSG3\nfKGoZ3BEpKlUE3TGzKw8jtjMXkaFEWBy4HrD8zTVPquTK4xotgERaSrVPKfzZeDXZnZn2H4H4ZkW\nqa3y6qH7kuko6IhIE6nmOZ1/NbM3ASeFov/u7luTbVY2lQYSVHtPJ1cYYemhc5NskohITVU74edb\niTKckl8k0JbM29clq3OForrXRKSpVDN67VLgPKL5yh4DzjOzS5JuWBa1thjdHa1VLeQ2OjbOzj2j\nekZHRJpKNZnOqcAb3H0cwMyuBR4ELkqyYVlV7aSf+d1aK0dEmk+1D3gsiL2u4rFF2V/VLuRWmuxT\nU+CISDOpJtO5BHjQzG4nmoPtHcCXEm1VhvVWmelMXdZARKQZVDN67QdmdgdwPFHQucDdn0u6YVnV\n09VW1UCC8mSf6l4TkSZSzUCCW919i7uvcfeb3P05M7u1Ho3Loqrv6WiyTxFpQjMtbdAFdAOHmlkf\nE2vTzAOOrEPbMqmns726ezoFDSQQkeYzU/faJ4DPEQWY+5kIOjuAKxNuV2b1drWxc8/sQ6ZzhSId\nrS10d7TWoVUiIrUx09IGlwOXm9ln3P0f6timTCt1r7k7ZtOvHpofGmFBd/uMdURE0mbaezpmdryZ\nHVEKOGZ2lpndZGZXmFl//ZqYLT1dbYw77B4Zm7GeZiMQkWY000CCfwSKAGb2DuBS4DpgO7A6+aZl\nU2+Vk37mCkUNIhCRpjNT0Gl1923h9YeB1e7+E3f/n8CxyTctm6qd9DNXGKFfD4aKSJOZMeiYWeme\nz8nAbbF91U4UKvuo2kxHyxqISDOaKXj8ALjTzLYCu4FfAZjZsURdbJKAns7ZF3Jzd/KFEc1GICJN\nZ6bRaxeHh0AXAb9099JqoS3AZ+rRuCwqd6/NkOnsHB5ldNw1kEBEms6M3WTufneFsieTa46Uu9dm\nyHTyQ9FzPBpIICLNptpZpqVOJhZym/4BUc27JiLNSkEnZeZ2zp7plIPOXGU6ItJcFHRSpqOthc62\nlhmHTOfKk30q0xGR5qKgk0K9syxvkAv3dPoVdESkySjopNBsyxvkC0XMYN4cda+JSHNR0Emh2RZy\nyxVGmD+nndYWTfYpIs1FQSeFejrbZnxOR5N9ikizUtBJod6u9hkHEuQLI3pGR0SakoJOCvV2trFr\neObndJTpiEgzakjQMbNvmNkTZvawmf3MzBbE9l1kZhvM7Pdm9p5Y+cpQtsHMLoyVH21m95jZU2b2\nQzPrCOWdYXtD2L+0nt/xQMx2T0eZjog0q0ZlOuuA17j764AngYsAzOw44Ezg1cBK4Dtm1mpmrURL\nZL8XOA74SKgL8HXgMndfBuSAs0P52UDO3Y8FLgv1mkJ89dBKtg0p0xGR5tSQoOPuv3T30qX83cBA\neH0acIO7D7v708AG4ITws8HdN7p7EbgBOM2itZrfBfw4HH8tcHrsva4Nr38MnGxNsrZzT1cbI2PO\n8Oj4Xvv2jIyxe2RMM0yLSFNKwz2d/wbcEl4fBWyK7RsMZdOVHwLkYwGsVD7pvcL+7aF+6vXOMBVO\nvhDd6+nTAm4i0oQSW4zNzP4NOKLCri+7+02hzpeBUeD60mEV6juVg6PPUH+m96rU1nOAcwCWLFlS\nqUpd9cQWcju0p3PSPk32KSLNLLGg4+7vnmm/ma0C3g+cHFurZxBYHKs2AGwOryuVbwUWmFlbyGbi\n9UvvNRhWQJ0PbKMCd18NrAZYvnx55RspdTTTQm4T866pe01Emk+jRq+tBC4APuDuhdiuNcCZYeTZ\n0cAy4F7gPmBZGKnWQTTYYE0IVrcDZ4TjVwE3xd5rVXh9BnCbT3dnPmVmWsit3L2mTEdEmlBimc4s\nvg10AuvCvf273f2T7v6omd0IPEbU7Xauu48BmNmngbVAK3C1uz8a3usC4AYz+zvgQeCqUH4V8P/M\nbANRhnNmfb7agZtpITd1r4lIM2tI0AnDmKfbdzFwcYXym4GbK5RvJBrdNrV8D/ChA2tpY5QXcqvw\ngGgp01H3mog0ozSMXpMp4gMJpsoNFZnT3kpXe2u9myUicsAUdFKofE+nQvfatkJRz+iISNNS0Emh\nzrYW2ltt2oEEWjFURJqVgk4KmRm9Xe2Vu9cKRfr1YKiINCkFnZSabvVQTfYpIs1MQSelplvITcsa\niEgzU9BJqZ6uvdfUGRt3tu8e0UACEWlaCjop1Vuhe23H7hHc0UACEWlaCjopVWkht/JsBHOV6YhI\nc1LQSalKAwkmJvtUpiMizUlBJ6V6uvYeSJAb0mSfItLcFHRSqrezjeHRcYqx1UNLmU6/go6INCkF\nnZQqTYUzFOtiK0/2qXs6ItKkFHRSqqdr74XccoUibS1WXs5aRKTZKOikVKWF3HJhNoKwBpGISNNR\n0EmpSgu55QtFjVwTkaamoJNSE0FnYlaCnJY1EJEmp6CTUhW714a0rIGINDcFnZQqrR46+Z6OMh0R\naW4KOinV2zl59Jq7ky+M6MFQEWlqCjop1dXeQmuLledfKxTHKI6N06cF3ESkiSnopJSZTZp/rTzZ\np7rXRKSJKeikWHwht/JsBOpeE5EmpqCTYr2xhdwmMh0FHRFpXgo6KTa5e600w7S610SkeSnopFh8\nIbfckNbSEZHmp6CTYj2dbeycMpBggTIdEWliCjop1hvLdPKFEXo722hv1V+ZiDQv/QZLsalDprWO\njog0OwWdFOvpbKdQHGNs3MkVRrRiqIg0PQWdFIsvb6BlDUTkYNDQoGNmXzQzN7NDw7aZ2RVmtsHM\nHjazN8XqrjKzp8LPqlj5m83skXDMFRZWODOzfjNbF+qvM7O++n/DA9MTCzqa7FNEDgYNCzpmthj4\nC+CZWPF7gWXh5xzgu6FuP/AV4ETgBOArsSDy3VC3dNzKUH4hcKu7LwNuDdtNpbe8vMEIeS1rICIH\ngUZmOpcB5wMeKzsNuM4jdwMLzGwR8B5gnbtvc/ccsA5YGfbNc/ffursD1wGnx97r2vD62lh50yhl\nOtuGiuwcHtVsBCLS9BoSdMzsA8Cz7v67KbuOAjbFtgdD2UzlgxXKAQ539y0A4c/DZmjPOWa23szW\nv/jii/vxjZJRWshtMLcbgD6NXhORJteW1Bub2b8BR1TY9WXgS8AplQ6rUOb7Ub5P3H01sBpg+fLl\n+3x8UkoDCQa3FQDNRiAizS+xoOPu765UbmavBY4Gfhfu+Q8AD5jZCUSZyuJY9QFgcyhfMaX8jlA+\nUKE+wPNmtsjdt4RuuBcO8CvVXU9YyG1TKdPRQAIRaXJ1715z90fc/TB3X+ruS4kCx5vc/TlgDXBW\nGMV2ErA9dI2tBU4xs74wgOAUYG3Yt9PMTgqj1s4CbgoftQYojXJbFStvGqV7OptCpqN7OiLS7BLL\ndPbTzcCpwAagAHwcwN23mdnXgPtCva+6+7bw+lPANcAc4JbwA3ApcKOZnU00Qu5D9fgCtdTd3ooZ\nbMqFoKNVQ0WkyTU86IRsp/TagXOnqXc1cHWF8vXAayqUvwScXLOGNkBLi9HT0cbzO4YBda+JSPPT\njAQpV+pi62hrYU57a4NbIyJyYBR0Uq40bLqvu50w8EJEpGkp6KRcKdPRIAIRORgo6KRcb1d0H0eL\nt4nIwUBBJ+V6O5XpiMjBQ0En5Ur3dDQbgYgcDBR0Uq50T6df866JyEFAQSfletS9JiIHEQWdlCtN\n+qnuNRE5GCjopFz8OR0RkWanoJNyPcp0ROQgoqCTcm8/diGfeMfLee1R8xvdFBGRA9bwCT9lZvO7\n27no1Fc1uhkiIjWhTEdEROpGQUdEROpGQUdEROpGQUdEROpGQUdEROpGQUdEROpGQUdEROpGQUdE\nROrG3L3RbUgVM3sR+NN+Hn4osLWGzTlY6TxVT+eqOjpP1UnyPL3M3RfOVklBp4bMbL27L290O9JO\n56l6OlfV0XmqThrOk7rXRESkbhR0RESkbhR0amt1oxvQJHSeqqdzVR2dp+o0/Dzpno6IiNSNMh0R\nEakbBZ0aMbOVZvZ7M9tgZhc2uj1pYWZXm9kLZvYfsbJ+M1tnZk+FP/sa2cY0MLPFZna7mT1uZo+a\n2XmhXOcqxsy6zOxeM/tdOE9/G8qPNrN7wnn6oZlpqV3AzFrN7EEz+0XYbvh5UtCpATNrBa4E3gsc\nB3zEzI5rbKtS4xpg5ZSyC4Fb3X0ZcGvYzrpR4Avu/irgJODc8G9I52qyYeBd7v564A3ASjM7Cfg6\ncFk4Tzng7Aa2MU3OAx6PbTf8PCno1MYJwAZ33+juReAG4LQGtykV3P0uYNuU4tOAa8Pra4HT69qo\nFHL3Le7+QHi9k+gXxVHoXE3ikV1hsz38OPAu4MehPPPnCcDMBoD3Ad8P20YKzpOCTm0cBWyKbQ+G\nMqnscHffAtEvW+CwBrcnVcxsKfBG4B50rvYSuoweAl4A1gF/APLuPhqq6P9f5FvA+cB42D6EFJwn\nBZ3asAplGhYo+8zMeoCfAJ9z9x2Nbk8aufuYu78BGCDqZXhVpWr1bVW6mNn7gRfc/f54cYWqdT9P\nbfX+wIPUILA4tj0AbG5QW5rB82a2yN23mNkioivWzDOzdqKAc727/zQU61xNw93zZnYH0T2wBWbW\nFq7i9f+yQyqxAAADWElEQVQP3gZ8wMxOBbqAeUSZT8PPkzKd2rgPWBZGhnQAZwJrGtymNFsDrAqv\nVwE3NbAtqRD6268CHnf3b8Z26VzFmNlCM1sQXs8B3k10/+t24IxQLfPnyd0vcvcBd19K9PvoNnf/\nKCk4T3o4tEbCFcW3gFbgane/uMFNSgUz+wGwgmh22+eBrwD/DNwILAGeAT7k7lMHG2SKmf058Cvg\nESb64L9EdF9H5yows9cR3QBvJbpovtHdv2pmLycawNMPPAj8lbsPN66l6WFmK4Avuvv703CeFHRE\nRKRu1L0mIiJ1o6AjIiJ1o6AjIiJ1o6AjIiJ1o6AjIiJ1o6Ajsh/M7BAzeyj8PGdmz8a2f5PA560w\ns+1hxuDHzewr+/Ee+9QuM7vGzM6YvaZI9TQjgch+cPeXiGY5xsz+N7DL3f9vwh/7q/CsxVzgITP7\nxZRpTioys9YwdcxbE26fyKyU6YjUmJntCn+uMLM7zexGM3vSzC41s4+G9WAeMbNjQr2FZvYTM7sv\n/Lxtpvd39yHgfuCYMPnlN8JxD5vZJ2KffbuZ/RPRA6fxdlk45j9COz4cK/+2mT1mZv+CJheVBCjT\nEUnW64kmpNwGbAS+7+4nhEXaPgN8DricaI2TX5vZEmAtlSexBKKuPaL5xr5GtB7Kdnc/3sw6gX83\ns1+GqicAr3H3p6e8xX8hytJeTzRTxH1mdhfwFuCVwGuBw4HHgKsP9ASIxCnoiCTrvtLSBGb2B6AU\nEB4B3hlevxs4Lpp+DYB5ZtYb1tWJe7uZPUg0Tc6l7l5aOfN1sXsv84FlQBG4t0LAAfhz4AfuPkY0\noeidwPHAO2Llm83stgP76iJ7U9ARSVZ8Xqvx2PY4E///WoC3uPvuWd7rV+7+/illBnzG3ddOKozm\n2xqa5n0qTXFfonmxJFG6pyPSeL8EPl3aMLM37MOxa4FPhWURMLNXhIEGM7kL+HC4H7SQKMO5N5Sf\nGcoXMZGJidSMMh2RxvsscKWZPUz0f/Iu4JNVHvt9YCnwQFge4UVmX4L4Z0T3b35HlNmc7+7PmdnP\niJYzfgR4ErhzH7+HyKw0y7SIiNSNutdERKRuFHRERKRuFHRERKRuFHRERKRuFHRERKRuFHRERKRu\nFHRERKRuFHRERKRu/j+WjZk0GEHGmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf3214e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer=LSTM(txs,1,'WeekNumber_Month_Season_Year' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict=answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real=answer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15927.03710938],\n",
       "       [ 17966.33203125],\n",
       "       [ 19928.07617188],\n",
       "       [-48421.41796875],\n",
       "       [  1887.68847656],\n",
       "       [   745.61981201],\n",
       "       [ 51638.796875  ],\n",
       "       [ 22342.67578125],\n",
       "       [ 22314.609375  ],\n",
       "       [ 22286.66601562],\n",
       "       [ 22262.30664062],\n",
       "       [ 22238.80859375],\n",
       "       [ 22216.96484375],\n",
       "       [ 22197.578125  ],\n",
       "       [ 22177.24609375],\n",
       "       [ 22158.73632812],\n",
       "       [ 22140.56445312],\n",
       "       [ 22118.75195312],\n",
       "       [ 22098.390625  ],\n",
       "       [ 22079.8828125 ],\n",
       "       [ 22061.6796875 ],\n",
       "       [ 22033.5625    ],\n",
       "       [ 22005.5546875 ],\n",
       "       [ 21981.08789062],\n",
       "       [ 21957.53125   ],\n",
       "       [ 21935.609375  ],\n",
       "       [ 21916.17773438],\n",
       "       [ 21895.78125   ],\n",
       "       [ 21877.2265625 ],\n",
       "       [ 21858.99609375],\n",
       "       [ 21837.12304688],\n",
       "       [ 21816.703125  ],\n",
       "       [ 21798.12695312],\n",
       "       [ 21779.88085938],\n",
       "       [ 21761.50195312],\n",
       "       [ 21735.74804688],\n",
       "       [ 21707.61328125],\n",
       "       [ 21683.09765625],\n",
       "       [ 21659.45703125],\n",
       "       [ 21634.00390625],\n",
       "       [ 21613.51953125],\n",
       "       [ 21594.90625   ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16894.39999997],\n",
       "       [ 18365.09999997],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ],\n",
       "       [     0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestPredictY=[item for sublist in predict for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15798.604,\n",
       " 14722.048,\n",
       " 15121.582,\n",
       " 14954.279,\n",
       " 17118.977,\n",
       " 15644.573,\n",
       " 15849.232,\n",
       " 15525.247,\n",
       " 14816.899,\n",
       " 14112.375,\n",
       " 14630.68,\n",
       " 16013.2,\n",
       " 17937.387,\n",
       " 19444.676,\n",
       " 19259.762,\n",
       " 18787.432,\n",
       " 24630.928,\n",
       " 23599.018,\n",
       " 24115.422,\n",
       " 18142.271,\n",
       " 61038.18,\n",
       " 28532.447,\n",
       " 15645.645,\n",
       " 19705.068,\n",
       " 21329.863,\n",
       " 30300.719,\n",
       " 40043.18,\n",
       " 36211.652,\n",
       " 22588.805]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denormalizedTestPredictY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestY=originalSales[train_size+seq_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc41d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sales=noOutlierSales(sales)\n",
    "tempxy=[list(txs['season']),list(txs['year']),list(txs['month']),list(txs['week_number']),sales]\n",
    "# tempxy=[list(txs['season']),list(txs['day_of_week01']),list(txs['week_number']),sales]\n",
    "xy=np.array(tempxy).transpose().astype(np.float)\n",
    "originalxy=np.array(tempxy).transpose().astype(np.float)\n",
    "xy=minMaxNormalizer(xy)\n",
    "\n",
    "#data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "data_dim=len(tempxy)\n",
    "#data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "seq_length=5\n",
    "#output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "\n",
    "output_dim=forecastDay\n",
    "#hidden_dim은 정말 임의로 설정\n",
    "hidden_dim=100\n",
    "#learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "learning_rate=0.01\n",
    "#iterations는 반복 횟수\n",
    "iterations=1000\n",
    "x=xy\n",
    "y=xy[:,[-1]]\n",
    "\n",
    "#build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0, len(y)-seq_length - forecastDay):\n",
    "    _x=x[i:i+seq_length]\n",
    "    _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "    _y = np.reshape(_y, (forecastDay))\n",
    "#     print(_x,\"->\",_y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    train_size = int(len(dataY) * 0.7)\n",
    "    \n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "X=tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y=tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn= None) \n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "denormalizedTestY=originalSales[train_size+seq_length:]\n",
    "# denormalizedTestY_original=sales[train_size+seq_length:]\n",
    "denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "with tf.Session() as sess:\n",
    "    #초기화\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalxy)\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(denormalizedTestY_feed) #실제 sales 파란색\n",
    "    plt.plot(test_predict)           #예측 sales 주황색\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x000000000CA1B1A8>\n"
     ]
    }
   ],
   "source": [
    "print(i for i in list(test_predict[-1]    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17721.039]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_predict[  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestPredictY=[item for sublist in test_predict for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a5cffacf1166>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrootMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdenormalizedTestY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdenormalizedTestPredictY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-a9533e569715>\u001b[0m in \u001b[0;36mrootMeanSquaredError\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0msum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0msum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rootMeanSquaredError(denormalizedTestY,denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
