{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walmart 주단위 데이터 139주로 향후 4주 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime  \n",
    "# tf.set_random_seed(77)\n",
    "columns=['date','sales']\n",
    "txs=pd.read_table('./lstmData/lstmPrac11.csv', sep=',',header=None,names=columns )\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-03-12</td>\n",
       "      <td>21043.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-03-19</td>\n",
       "      <td>22136.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-03-26</td>\n",
       "      <td>26229.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-04-02</td>\n",
       "      <td>57258.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-04-09</td>\n",
       "      <td>42960.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>17596.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2010-04-23</td>\n",
       "      <td>16145.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>16555.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010-05-07</td>\n",
       "      <td>17413.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2010-05-14</td>\n",
       "      <td>18926.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010-05-21</td>\n",
       "      <td>14773.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2010-05-28</td>\n",
       "      <td>15580.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>17558.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2010-06-11</td>\n",
       "      <td>16637.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2010-06-18</td>\n",
       "      <td>16216.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2010-06-25</td>\n",
       "      <td>16328.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2010-07-02</td>\n",
       "      <td>16333.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2010-07-09</td>\n",
       "      <td>17688.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2010-07-16</td>\n",
       "      <td>17150.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010-07-23</td>\n",
       "      <td>15360.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2010-07-30</td>\n",
       "      <td>15381.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2010-08-06</td>\n",
       "      <td>17508.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2010-08-13</td>\n",
       "      <td>15536.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-08-20</td>\n",
       "      <td>15740.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2010-08-27</td>\n",
       "      <td>15793.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>57592.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2012-04-13</td>\n",
       "      <td>34684.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2012-04-20</td>\n",
       "      <td>16976.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2012-04-27</td>\n",
       "      <td>16347.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2012-05-04</td>\n",
       "      <td>17147.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2012-05-11</td>\n",
       "      <td>18164.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2012-05-18</td>\n",
       "      <td>18517.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2012-05-25</td>\n",
       "      <td>16963.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>16065.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2012-06-08</td>\n",
       "      <td>17666.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2012-06-15</td>\n",
       "      <td>17558.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>16633.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2012-06-29</td>\n",
       "      <td>15722.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2012-07-06</td>\n",
       "      <td>17823.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2012-07-13</td>\n",
       "      <td>16566.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>16348.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2012-07-27</td>\n",
       "      <td>15731.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2012-08-03</td>\n",
       "      <td>16628.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>16119.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2012-08-17</td>\n",
       "      <td>17330.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>16286.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2012-08-31</td>\n",
       "      <td>16680.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2012-09-07</td>\n",
       "      <td>18322.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>19616.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2012-09-21</td>\n",
       "      <td>19251.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>18947.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>21904.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2012-10-12</td>\n",
       "      <td>22764.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>24185.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>27390.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     sales\n",
       "0    2010-02-05  24924.50\n",
       "1    2010-02-12  46039.49\n",
       "2    2010-02-19  41595.55\n",
       "3    2010-02-26  19403.54\n",
       "4    2010-03-05  21827.90\n",
       "5    2010-03-12  21043.39\n",
       "6    2010-03-19  22136.64\n",
       "7    2010-03-26  26229.21\n",
       "8    2010-04-02  57258.43\n",
       "9    2010-04-09  42960.91\n",
       "10   2010-04-16  17596.96\n",
       "11   2010-04-23  16145.35\n",
       "12   2010-04-30  16555.11\n",
       "13   2010-05-07  17413.94\n",
       "14   2010-05-14  18926.74\n",
       "15   2010-05-21  14773.04\n",
       "16   2010-05-28  15580.43\n",
       "17   2010-06-04  17558.09\n",
       "18   2010-06-11  16637.62\n",
       "19   2010-06-18  16216.27\n",
       "20   2010-06-25  16328.72\n",
       "21   2010-07-02  16333.14\n",
       "22   2010-07-09  17688.76\n",
       "23   2010-07-16  17150.84\n",
       "24   2010-07-23  15360.45\n",
       "25   2010-07-30  15381.82\n",
       "26   2010-08-06  17508.41\n",
       "27   2010-08-13  15536.40\n",
       "28   2010-08-20  15740.13\n",
       "29   2010-08-27  15793.87\n",
       "..          ...       ...\n",
       "113  2012-04-06  57592.12\n",
       "114  2012-04-13  34684.21\n",
       "115  2012-04-20  16976.19\n",
       "116  2012-04-27  16347.60\n",
       "117  2012-05-04  17147.44\n",
       "118  2012-05-11  18164.20\n",
       "119  2012-05-18  18517.79\n",
       "120  2012-05-25  16963.55\n",
       "121  2012-06-01  16065.49\n",
       "122  2012-06-08  17666.00\n",
       "123  2012-06-15  17558.82\n",
       "124  2012-06-22  16633.41\n",
       "125  2012-06-29  15722.82\n",
       "126  2012-07-06  17823.37\n",
       "127  2012-07-13  16566.18\n",
       "128  2012-07-20  16348.06\n",
       "129  2012-07-27  15731.18\n",
       "130  2012-08-03  16628.31\n",
       "131  2012-08-10  16119.92\n",
       "132  2012-08-17  17330.70\n",
       "133  2012-08-24  16286.40\n",
       "134  2012-08-31  16680.24\n",
       "135  2012-09-07  18322.37\n",
       "136  2012-09-14  19616.22\n",
       "137  2012-09-21  19251.50\n",
       "138  2012-09-28  18947.81\n",
       "139  2012-10-05  21904.47\n",
       "140  2012-10-12  22764.01\n",
       "141  2012-10-19  24185.27\n",
       "142  2012-10-26  27390.81\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noOutlierSales(sales):\n",
    "    mean=np.mean(sales)\n",
    "    std=np.std(sales)\n",
    "    for i in range(len(sales)):\n",
    "        if (sales[i]<mean-2*std or sales[i]>mean+2*std):\n",
    "             sales[i]=int(mean)\n",
    "    return sales\n",
    "def logSales(sales):\n",
    "    for i in range(len(sales)):\n",
    "        if sales[i] is 0:\n",
    "            sales[i]=1\n",
    "    return np.log(sales)\n",
    "def sqrtSales(sales):\n",
    "    return np.sqrt(sales)\n",
    "\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LSTM(txs, forecastDay, features):\n",
    "\n",
    "    #Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['date'].map(year)\n",
    "    txs['month'] = txs['date'].map(month)\n",
    "    txs['weekNumber'] = txs['date'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['date'].map(dayOfWeek)\n",
    "\n",
    "    #Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['date'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['date'].map(day_of_week01)\n",
    "\n",
    "    #Backup originalSales\n",
    "    originalSales = list(txs['sales'])\n",
    "    sales = list(txs['sales'])\n",
    "\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season' :\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']),list(txs['month']),list(txs['season']) , sales]\n",
    "    elif features is'DayOfWeek01_WeekNumber_Month_Season' :\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']),list(txs['month']),list(txs['season']) , sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year' :\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']),list(txs['year']), sales]\n",
    "\n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "\n",
    "    #Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    #TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 5\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 50\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.001\n",
    "    # iterations는 반복 횟수\n",
    "    iterations = 5000\n",
    "\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay+1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        print(_x,\"->\",_y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    print('data set length:',len(y) - seq_length - forecastDay+1)\n",
    "    \n",
    "    train_size = int(len(dataY) * 0.7)\n",
    "    test_size = len(dataY) - train_size\n",
    "    print('train size:' , train_size)\n",
    "    print('test size:' , test_size)\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "    print('trainX:', trainX)\n",
    "    print('testX:', testX)\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    denormalizedTestY=originalSales[train_size+seq_length:]\n",
    "#     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "    \n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        for i in range(iterations):\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "       \n",
    "\n",
    "        # Test step\n",
    "        # test_predict= sess.run(Y_pred, feed_dict={X: testX}\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalXY)\n",
    "        realSale= minMaxDeNormalizer(testY,originalXY)\n",
    "        # Plot predictions\n",
    "#         plt.plot(denormalizedTestY_feed) #실제 sales 파란색\n",
    "#         plt.plot(realSale)      #실제 sales 파란색\n",
    "        plt.plot(test_predict) #예측 sales 주황색\n",
    "               \n",
    "        plt.xlabel(\"Time Period\")\n",
    "        plt.ylabel(\"Stock Price\")\n",
    "        plt.show()\n",
    "        \n",
    "    return (test_predict), realSale\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    4.32776220e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.99406065e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.22243772e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    3.36913105e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]] -> [ 0.36538662]\n",
      "[[  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.99406065e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.22243772e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    3.36913105e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]] -> [ 0.38436925]\n",
      "[[  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.22243772e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    3.36913105e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]] -> [ 0.45543053]\n",
      "[[  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    3.36913105e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]] -> [ 0.99420598]\n",
      "[[  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]] -> [ 0.74595118]\n",
      "[[  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]] -> [ 0.30554458]\n",
      "[[  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]] -> [ 0.28033957]\n",
      "[[  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]] -> [ 0.28745443]\n",
      "[[  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]] -> [ 0.30236671]\n",
      "[[  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]] -> [ 0.3286342]\n",
      "[[  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]] -> [ 0.25651148]\n",
      "[[  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]] -> [ 0.27053059]\n",
      "[[  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]] -> [ 0.30486966]\n",
      "[[  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]] -> [ 0.28888709]\n",
      "[[  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]] -> [ 0.28157099]\n",
      "[[  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]] -> [ 0.28352351]\n",
      "[[  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]] -> [ 0.28360026]\n",
      "[[  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]] -> [ 0.30713855]\n",
      "[[  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]] -> [ 0.29779838]\n",
      "[[  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]] -> [ 0.26671097]\n",
      "[[  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]] -> [ 0.26708202]\n",
      "[[  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]] -> [ 0.30400704]\n",
      "[[  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]] -> [ 0.26976607]\n",
      "[[  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]] -> [ 0.27330354]\n",
      "[[  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]] -> [ 0.27423665]\n",
      "[[  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]] -> [ 0.28201393]\n",
      "[[  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]] -> [ 0.31592412]\n",
      "[[  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]] -> [ 0.33605691]\n",
      "[[  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]] -> [ 0.31467013]\n",
      "[[  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]] -> [ 0.3489052]\n",
      "[[  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]] -> [ 0.40609774]\n",
      "[[  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]] -> [ 0.46843804]\n",
      "[[  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]] -> [ 0.44351623]\n",
      "[[  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]] -> [ 0.67094127]\n",
      "[[  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    6.70941268e-01]] -> [ 0.59450633]\n",
      "[[  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]] -> [ 0.33944557]\n",
      "[[  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]] -> [ 0.33950547]\n",
      "[[  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]] -> [ 0.32678585]\n",
      "[[  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]] -> [ 0.39098335]\n",
      "[[  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]] -> [ 0.54690902]\n",
      "[[  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]] -> [ 0.77984384]\n",
      "[[  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]] -> [ 0.97116116]\n",
      "[[  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]] -> [ 0.33206939]\n",
      "[[  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]] -> [ 0.27754214]\n",
      "[[  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]] -> [ 0.30142492]\n",
      "[[  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]] -> [ 0.30110838]\n",
      "[[  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]] -> [ 0.32055045]\n",
      "[[  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]] -> [ 0.37619313]\n",
      "[[  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]] -> [ 0.65785337]\n",
      "[[  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]] -> [ 0.81340763]\n",
      "[[  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]] -> [ 0.3362236]\n",
      "[[  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]] -> [ 0.35295818]\n",
      "[[  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]] -> [ 0.36950194]\n",
      "[[  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]] -> [ 0.35307313]\n",
      "[[  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]] -> [ 0.3625687]\n",
      "[[  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]] -> [ 0.35418196]\n",
      "[[  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]] -> [ 0.41453223]\n",
      "[[  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]] -> [ 0.49941502]\n",
      "[[  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]] -> [ 0.87703509]\n",
      "[[  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]] -> [ 0.72079982]\n",
      "[[  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]] -> [ 0.34966919]\n",
      "[[  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]] -> [ 0.29926229]\n",
      "[[  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]] -> [ 0.26282728]\n",
      "[[  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]] -> [ 0.27332906]\n",
      "[[  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]] -> [ 0.28535414]\n",
      "[[  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]] -> [ 0.27579329]\n",
      "[[  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]] -> [ 0.26007186]\n",
      "[[  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]] -> [ 0.27230826]\n",
      "[[  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]] -> [ 0.26676393]\n",
      "[[  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]] -> [ 0.28040069]\n",
      "[[  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]] -> [ 0.27182278]\n",
      "[[  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]] -> [ 0.27376315]\n",
      "[[  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]] -> [ 0.27646855]\n",
      "[[  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]] -> [ 0.26558408]\n",
      "[[  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]] -> [ 0.25246145]\n",
      "[[  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]] -> [ 0.25505642]\n",
      "[[  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]] -> [ 0.25241943]\n",
      "[[  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]] -> [ 0.26526667]\n",
      "[[  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]] -> [ 0.30814424]\n",
      "[[  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]] -> [ 0.32184056]\n",
      "[[  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]] -> [ 0.31009972]\n",
      "[[  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]] -> [ 0.31840606]\n",
      "[[  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]] -> [ 0.3611185]\n",
      "[[  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]] -> [ 0.40070673]\n",
      "[[  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]] -> [ 0.40546866]\n",
      "[[  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]] -> [ 0.54833717]\n",
      "[[  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]] -> [ 0.69256107]\n",
      "[[  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]] -> [ 0.32451558]\n",
      "[[  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]] -> [ 0.33078588]\n",
      "[[  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]] -> [ 0.36309221]\n",
      "[[  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]] -> [ 0.43918317]\n",
      "[[  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]] -> [ 0.57830689]\n",
      "[[  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]] -> [ 0.79477939]\n",
      "[[  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]] -> [ 0.81241583]\n",
      "[[  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]] -> [ 0.40545269]\n",
      "[[  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]] -> [ 0.28767286]\n",
      "[[  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]] -> [ 0.29334569]\n",
      "[[  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]] -> [ 0.31888217]\n",
      "[[  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]] -> [ 0.31910893]\n",
      "[[  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]] -> [ 0.40822408]\n",
      "[[  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]] -> [ 0.64224915]\n",
      "[[  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]] -> [ 0.93867182]\n",
      "[[  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]] -> [ 0.34942662]\n",
      "[[  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]] -> [ 0.34923233]\n",
      "[[  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]] -> [ 0.36706532]\n",
      "[[  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]] -> [ 0.38836702]\n",
      "[[  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]] -> [ 0.38386675]\n",
      "[[  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]] -> [ 0.50272259]\n",
      "[[  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]] -> [ 1.]\n",
      "[[  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]] -> [ 0.60223881]\n",
      "[[  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]] -> [ 0.29476585]\n",
      "[[  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]] -> [ 0.28385133]\n",
      "[[  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]] -> [ 0.29773934]\n",
      "[[  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]] -> [ 0.31539384]\n",
      "[[  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]] -> [ 0.3215334]\n",
      "[[  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]] -> [ 0.29454637]\n",
      "[[  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]] -> [ 0.27895292]\n",
      "[[  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]] -> [ 0.30674335]\n",
      "[[  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]] -> [ 0.30488233]\n",
      "[[  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]] -> [ 0.28881399]\n",
      "[[  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]] -> [ 0.27300297]\n",
      "[[  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]] -> [ 0.30947584]\n",
      "[[  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]] -> [ 0.28764664]\n",
      "[[  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]] -> [ 0.28385932]\n",
      "[[  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]] -> [ 0.27314813]\n",
      "[[  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]] -> [ 0.28872544]\n",
      "[[  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]] -> [ 0.27989801]\n",
      "[[  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]] -> [ 0.30092138]\n",
      "[[  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]] -> [ 0.28278869]\n",
      "[[  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.82788687e-01]] -> [ 0.28962712]\n",
      "[[  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.82788687e-01]\n",
      " [  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.89627123e-01]] -> [ 0.31814022]\n",
      "[[  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.82788687e-01]\n",
      " [  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.89627123e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.18140225e-01]] -> [ 0.34060597]\n",
      "[[  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.82788687e-01]\n",
      " [  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.89627123e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.18140225e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.40605972e-01]] -> [ 0.33427316]\n",
      "[[  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.82788687e-01]\n",
      " [  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.89627123e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.18140225e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.40605972e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.34273161e-01]] -> [ 0.32900004]\n",
      "[[  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.89627123e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.18140225e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.40605972e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.34273161e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.29000044e-01]] -> [ 0.38033797]\n",
      "[[  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.18140225e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.40605972e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.34273161e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.29000044e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.80337970e-01]] -> [ 0.39526258]\n",
      "[[  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.40605972e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.34273161e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.29000044e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.80337970e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.95262581e-01]] -> [ 0.41994061]\n",
      "[[  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.34273161e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.29000044e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.80337970e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.95262581e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "    4.19940610e-01]] -> [ 0.47559996]\n",
      "data set length: 138\n",
      "train size: 96\n",
      "test size: 42\n",
      "trainX: [[[  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     4.32776220e-01]\n",
      "  [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.99406065e-01]\n",
      "  [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.22243772e-01]\n",
      "  [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     3.36913105e-01]\n",
      "  [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.79008448e-01]]\n",
      "\n",
      " [[  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.99406065e-01]\n",
      "  [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.22243772e-01]\n",
      "  [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     3.36913105e-01]\n",
      "  [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.79008448e-01]\n",
      "  [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.65386619e-01]]\n",
      "\n",
      " [[  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.22243772e-01]\n",
      "  [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     3.36913105e-01]\n",
      "  [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.79008448e-01]\n",
      "  [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.65386619e-01]\n",
      "  [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.84369251e-01]]\n",
      "\n",
      " ..., \n",
      " [[  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.30785878e-01]\n",
      "  [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.63092208e-01]\n",
      "  [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.39183173e-01]\n",
      "  [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     5.78306893e-01]\n",
      "  [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     7.94779390e-01]]\n",
      "\n",
      " [[  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.63092208e-01]\n",
      "  [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.39183173e-01]\n",
      "  [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     5.78306893e-01]\n",
      "  [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     7.94779390e-01]\n",
      "  [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     8.12415830e-01]]\n",
      "\n",
      " [[  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.39183173e-01]\n",
      "  [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     5.78306893e-01]\n",
      "  [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     7.94779390e-01]\n",
      "  [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     8.12415830e-01]\n",
      "  [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.05452690e-01]]]\n",
      "testX: [[[  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     5.78306893e-01]\n",
      "  [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     7.94779390e-01]\n",
      "  [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     8.12415830e-01]\n",
      "  [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.05452690e-01]\n",
      "  [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "     2.87672862e-01]]\n",
      "\n",
      " [[  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     7.94779390e-01]\n",
      "  [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     8.12415830e-01]\n",
      "  [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.05452690e-01]\n",
      "  [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "     2.87672862e-01]\n",
      "  [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "     2.93345687e-01]]\n",
      "\n",
      " [[  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     8.12415830e-01]\n",
      "  [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.05452690e-01]\n",
      "  [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "     2.87672862e-01]\n",
      "  [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "     2.93345687e-01]\n",
      "  [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "     3.18882167e-01]]\n",
      "\n",
      " ..., \n",
      " [[  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.18140225e-01]\n",
      "  [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.40605972e-01]\n",
      "  [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.34273161e-01]\n",
      "  [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.29000044e-01]\n",
      "  [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.80337970e-01]]\n",
      "\n",
      " [[  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.40605972e-01]\n",
      "  [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.34273161e-01]\n",
      "  [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.29000044e-01]\n",
      "  [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.80337970e-01]\n",
      "  [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.95262581e-01]]\n",
      "\n",
      " [[  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.34273161e-01]\n",
      "  [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.29000044e-01]\n",
      "  [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.80337970e-01]\n",
      "  [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.95262581e-01]\n",
      "  [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49353349e-02\n",
      "     4.19940610e-01]]]\n",
      "[step: 0] loss: 18.317203521728516\n",
      "[step: 1] loss: 16.987262725830078\n",
      "[step: 2] loss: 15.716808319091797\n",
      "[step: 3] loss: 14.504920959472656\n",
      "[step: 4] loss: 13.350564002990723\n",
      "[step: 5] loss: 12.252819061279297\n",
      "[step: 6] loss: 11.210796356201172\n",
      "[step: 7] loss: 10.223714828491211\n",
      "[step: 8] loss: 9.291183471679688\n",
      "[step: 9] loss: 8.413421630859375\n",
      "[step: 10] loss: 7.591348648071289\n",
      "[step: 11] loss: 6.826595306396484\n",
      "[step: 12] loss: 6.121459484100342\n",
      "[step: 13] loss: 5.478828430175781\n",
      "[step: 14] loss: 4.902072906494141\n",
      "[step: 15] loss: 4.394872665405273\n",
      "[step: 16] loss: 3.9609720706939697\n",
      "[step: 17] loss: 3.603804349899292\n",
      "[step: 18] loss: 3.3259634971618652\n",
      "[step: 19] loss: 3.1284751892089844\n",
      "[step: 20] loss: 3.0098602771759033\n",
      "[step: 21] loss: 2.9650514125823975\n",
      "[step: 22] loss: 2.9843637943267822\n",
      "[step: 23] loss: 3.0529680252075195\n",
      "[step: 24] loss: 3.1515488624572754\n",
      "[step: 25] loss: 3.2586264610290527\n",
      "[step: 26] loss: 3.3541407585144043\n",
      "[step: 27] loss: 3.422928810119629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 28] loss: 3.456716537475586\n",
      "[step: 29] loss: 3.454197883605957\n",
      "[step: 30] loss: 3.4196557998657227\n",
      "[step: 31] loss: 3.360903263092041\n",
      "[step: 32] loss: 3.287217140197754\n",
      "[step: 33] loss: 3.2076659202575684\n",
      "[step: 34] loss: 3.1299774646759033\n",
      "[step: 35] loss: 3.059938430786133\n",
      "[step: 36] loss: 3.0012316703796387\n",
      "[step: 37] loss: 2.95558500289917\n",
      "[step: 38] loss: 2.9231035709381104\n",
      "[step: 39] loss: 2.9026927947998047\n",
      "[step: 40] loss: 2.8924760818481445\n",
      "[step: 41] loss: 2.89017391204834\n",
      "[step: 42] loss: 2.893409252166748\n",
      "[step: 43] loss: 2.8999338150024414\n",
      "[step: 44] loss: 2.9077773094177246\n",
      "[step: 45] loss: 2.915335178375244\n",
      "[step: 46] loss: 2.9214043617248535\n",
      "[step: 47] loss: 2.9251790046691895\n",
      "[step: 48] loss: 2.926223039627075\n",
      "[step: 49] loss: 2.9244213104248047\n",
      "[step: 50] loss: 2.9199259281158447\n",
      "[step: 51] loss: 2.9130935668945312\n",
      "[step: 52] loss: 2.904423236846924\n",
      "[step: 53] loss: 2.8944969177246094\n",
      "[step: 54] loss: 2.8839223384857178\n",
      "[step: 55] loss: 2.8732855319976807\n",
      "[step: 56] loss: 2.8631057739257812\n",
      "[step: 57] loss: 2.85380482673645\n",
      "[step: 58] loss: 2.8456835746765137\n",
      "[step: 59] loss: 2.838911533355713\n",
      "[step: 60] loss: 2.8335275650024414\n",
      "[step: 61] loss: 2.829451084136963\n",
      "[step: 62] loss: 2.8265013694763184\n",
      "[step: 63] loss: 2.8244290351867676\n",
      "[step: 64] loss: 2.8229448795318604\n",
      "[step: 65] loss: 2.8217554092407227\n",
      "[step: 66] loss: 2.8205904960632324\n",
      "[step: 67] loss: 2.8192293643951416\n",
      "[step: 68] loss: 2.8175137042999268\n",
      "[step: 69] loss: 2.815356731414795\n",
      "[step: 70] loss: 2.812739372253418\n",
      "[step: 71] loss: 2.8097023963928223\n",
      "[step: 72] loss: 2.8063292503356934\n",
      "[step: 73] loss: 2.8027307987213135\n",
      "[step: 74] loss: 2.7990267276763916\n",
      "[step: 75] loss: 2.7953319549560547\n",
      "[step: 76] loss: 2.7917416095733643\n",
      "[step: 77] loss: 2.788325786590576\n",
      "[step: 78] loss: 2.785125970840454\n",
      "[step: 79] loss: 2.7821547985076904\n",
      "[step: 80] loss: 2.77940034866333\n",
      "[step: 81] loss: 2.776832342147827\n",
      "[step: 82] loss: 2.774406909942627\n",
      "[step: 83] loss: 2.7720754146575928\n",
      "[step: 84] loss: 2.769789934158325\n",
      "[step: 85] loss: 2.7675089836120605\n",
      "[step: 86] loss: 2.765198230743408\n",
      "[step: 87] loss: 2.762836217880249\n",
      "[step: 88] loss: 2.760411500930786\n",
      "[step: 89] loss: 2.7579236030578613\n",
      "[step: 90] loss: 2.755380153656006\n",
      "[step: 91] loss: 2.752796173095703\n",
      "[step: 92] loss: 2.7501885890960693\n",
      "[step: 93] loss: 2.7475762367248535\n",
      "[step: 94] loss: 2.744976043701172\n",
      "[step: 95] loss: 2.7424020767211914\n",
      "[step: 96] loss: 2.739863157272339\n",
      "[step: 97] loss: 2.7373647689819336\n",
      "[step: 98] loss: 2.734907388687134\n",
      "[step: 99] loss: 2.732487678527832\n",
      "[step: 100] loss: 2.730098009109497\n",
      "[step: 101] loss: 2.727731943130493\n",
      "[step: 102] loss: 2.725381374359131\n",
      "[step: 103] loss: 2.723036527633667\n",
      "[step: 104] loss: 2.7206926345825195\n",
      "[step: 105] loss: 2.7183444499969482\n",
      "[step: 106] loss: 2.715989112854004\n",
      "[step: 107] loss: 2.713627338409424\n",
      "[step: 108] loss: 2.7112600803375244\n",
      "[step: 109] loss: 2.7088894844055176\n",
      "[step: 110] loss: 2.70651912689209\n",
      "[step: 111] loss: 2.7041516304016113\n",
      "[step: 112] loss: 2.701791286468506\n",
      "[step: 113] loss: 2.699439525604248\n",
      "[step: 114] loss: 2.6970975399017334\n",
      "[step: 115] loss: 2.6947669982910156\n",
      "[step: 116] loss: 2.69244647026062\n",
      "[step: 117] loss: 2.6901350021362305\n",
      "[step: 118] loss: 2.6878318786621094\n",
      "[step: 119] loss: 2.685535430908203\n",
      "[step: 120] loss: 2.683243751525879\n",
      "[step: 121] loss: 2.680955648422241\n",
      "[step: 122] loss: 2.6786696910858154\n",
      "[step: 123] loss: 2.6763856410980225\n",
      "[step: 124] loss: 2.674103021621704\n",
      "[step: 125] loss: 2.6718220710754395\n",
      "[step: 126] loss: 2.6695430278778076\n",
      "[step: 127] loss: 2.667266607284546\n",
      "[step: 128] loss: 2.6649928092956543\n",
      "[step: 129] loss: 2.662722587585449\n",
      "[step: 130] loss: 2.6604561805725098\n",
      "[step: 131] loss: 2.6581931114196777\n",
      "[step: 132] loss: 2.6559348106384277\n",
      "[step: 133] loss: 2.653679847717285\n",
      "[step: 134] loss: 2.651428699493408\n",
      "[step: 135] loss: 2.6491804122924805\n",
      "[step: 136] loss: 2.646935224533081\n",
      "[step: 137] loss: 2.6446919441223145\n",
      "[step: 138] loss: 2.642451524734497\n",
      "[step: 139] loss: 2.640212297439575\n",
      "[step: 140] loss: 2.637974500656128\n",
      "[step: 141] loss: 2.6357383728027344\n",
      "[step: 142] loss: 2.6335036754608154\n",
      "[step: 143] loss: 2.631270408630371\n",
      "[step: 144] loss: 2.6290385723114014\n",
      "[step: 145] loss: 2.626807928085327\n",
      "[step: 146] loss: 2.6245789527893066\n",
      "[step: 147] loss: 2.6223511695861816\n",
      "[step: 148] loss: 2.6201248168945312\n",
      "[step: 149] loss: 2.6178996562957764\n",
      "[step: 150] loss: 2.615675449371338\n",
      "[step: 151] loss: 2.613452196121216\n",
      "[step: 152] loss: 2.611229658126831\n",
      "[step: 153] loss: 2.6090075969696045\n",
      "[step: 154] loss: 2.6067862510681152\n",
      "[step: 155] loss: 2.604564666748047\n",
      "[step: 156] loss: 2.6023433208465576\n",
      "[step: 157] loss: 2.6001219749450684\n",
      "[step: 158] loss: 2.597900390625\n",
      "[step: 159] loss: 2.5956788063049316\n",
      "[step: 160] loss: 2.593456268310547\n",
      "[step: 161] loss: 2.591233730316162\n",
      "[step: 162] loss: 2.589010715484619\n",
      "[step: 163] loss: 2.5867867469787598\n",
      "[step: 164] loss: 2.584562063217163\n",
      "[step: 165] loss: 2.582336664199829\n",
      "[step: 166] loss: 2.5801100730895996\n",
      "[step: 167] loss: 2.577882766723633\n",
      "[step: 168] loss: 2.5756540298461914\n",
      "[step: 169] loss: 2.5734236240386963\n",
      "[step: 170] loss: 2.5711917877197266\n",
      "[step: 171] loss: 2.568958044052124\n",
      "[step: 172] loss: 2.566723108291626\n",
      "[step: 173] loss: 2.564486026763916\n",
      "[step: 174] loss: 2.562246799468994\n",
      "[step: 175] loss: 2.5600056648254395\n",
      "[step: 176] loss: 2.5577621459960938\n",
      "[step: 177] loss: 2.555515766143799\n",
      "[step: 178] loss: 2.553267478942871\n",
      "[step: 179] loss: 2.5510165691375732\n",
      "[step: 180] loss: 2.5487632751464844\n",
      "[step: 181] loss: 2.546505928039551\n",
      "[step: 182] loss: 2.5442466735839844\n",
      "[step: 183] loss: 2.5419838428497314\n",
      "[step: 184] loss: 2.539717435836792\n",
      "[step: 185] loss: 2.537447929382324\n",
      "[step: 186] loss: 2.535175323486328\n",
      "[step: 187] loss: 2.532898426055908\n",
      "[step: 188] loss: 2.5306177139282227\n",
      "[step: 189] loss: 2.5283336639404297\n",
      "[step: 190] loss: 2.5260448455810547\n",
      "[step: 191] loss: 2.523752212524414\n",
      "[step: 192] loss: 2.5214552879333496\n",
      "[step: 193] loss: 2.519153594970703\n",
      "[step: 194] loss: 2.516847610473633\n",
      "[step: 195] loss: 2.5145370960235596\n",
      "[step: 196] loss: 2.512221336364746\n",
      "[step: 197] loss: 2.5099005699157715\n",
      "[step: 198] loss: 2.5075747966766357\n",
      "[step: 199] loss: 2.505244255065918\n",
      "[step: 200] loss: 2.5029077529907227\n",
      "[step: 201] loss: 2.500565528869629\n",
      "[step: 202] loss: 2.498218059539795\n",
      "[step: 203] loss: 2.4958651065826416\n",
      "[step: 204] loss: 2.4935054779052734\n",
      "[step: 205] loss: 2.491140365600586\n",
      "[step: 206] loss: 2.4887688159942627\n",
      "[step: 207] loss: 2.486391067504883\n",
      "[step: 208] loss: 2.484006643295288\n",
      "[step: 209] loss: 2.4816153049468994\n",
      "[step: 210] loss: 2.479218006134033\n",
      "[step: 211] loss: 2.476813316345215\n",
      "[step: 212] loss: 2.4744019508361816\n",
      "[step: 213] loss: 2.471982955932617\n",
      "[step: 214] loss: 2.469557046890259\n",
      "[step: 215] loss: 2.4671237468719482\n",
      "[step: 216] loss: 2.4646825790405273\n",
      "[step: 217] loss: 2.4622342586517334\n",
      "[step: 218] loss: 2.45977783203125\n",
      "[step: 219] loss: 2.457313299179077\n",
      "[step: 220] loss: 2.454840898513794\n",
      "[step: 221] loss: 2.4523606300354004\n",
      "[step: 222] loss: 2.44987154006958\n",
      "[step: 223] loss: 2.4473743438720703\n",
      "[step: 224] loss: 2.444868326187134\n",
      "[step: 225] loss: 2.4423534870147705\n",
      "[step: 226] loss: 2.4398303031921387\n",
      "[step: 227] loss: 2.437298059463501\n",
      "[step: 228] loss: 2.4347567558288574\n",
      "[step: 229] loss: 2.432206392288208\n",
      "[step: 230] loss: 2.4296464920043945\n",
      "[step: 231] loss: 2.427077293395996\n",
      "[step: 232] loss: 2.424499034881592\n",
      "[step: 233] loss: 2.4219112396240234\n",
      "[step: 234] loss: 2.419313430786133\n",
      "[step: 235] loss: 2.416705846786499\n",
      "[step: 236] loss: 2.414088726043701\n",
      "[step: 237] loss: 2.411461591720581\n",
      "[step: 238] loss: 2.4088246822357178\n",
      "[step: 239] loss: 2.406177520751953\n",
      "[step: 240] loss: 2.403520107269287\n",
      "[step: 241] loss: 2.400853157043457\n",
      "[step: 242] loss: 2.3981757164001465\n",
      "[step: 243] loss: 2.3954877853393555\n",
      "[step: 244] loss: 2.392789602279663\n",
      "[step: 245] loss: 2.3900814056396484\n",
      "[step: 246] loss: 2.3873627185821533\n",
      "[step: 247] loss: 2.384633779525757\n",
      "[step: 248] loss: 2.38189435005188\n",
      "[step: 249] loss: 2.3791444301605225\n",
      "[step: 250] loss: 2.376384735107422\n",
      "[step: 251] loss: 2.3736143112182617\n",
      "[step: 252] loss: 2.370833396911621\n",
      "[step: 253] loss: 2.368042469024658\n",
      "[step: 254] loss: 2.365241289138794\n",
      "[step: 255] loss: 2.3624300956726074\n",
      "[step: 256] loss: 2.3596086502075195\n",
      "[step: 257] loss: 2.3567776679992676\n",
      "[step: 258] loss: 2.3539366722106934\n",
      "[step: 259] loss: 2.351086139678955\n",
      "[step: 260] loss: 2.3482258319854736\n",
      "[step: 261] loss: 2.345355987548828\n",
      "[step: 262] loss: 2.342477321624756\n",
      "[step: 263] loss: 2.3395891189575195\n",
      "[step: 264] loss: 2.3366918563842773\n",
      "[step: 265] loss: 2.3337864875793457\n",
      "[step: 266] loss: 2.3308725357055664\n",
      "[step: 267] loss: 2.3279495239257812\n",
      "[step: 268] loss: 2.325019598007202\n",
      "[step: 269] loss: 2.3220815658569336\n",
      "[step: 270] loss: 2.319136619567871\n",
      "[step: 271] loss: 2.316183567047119\n",
      "[step: 272] loss: 2.3132243156433105\n",
      "[step: 273] loss: 2.310258388519287\n",
      "[step: 274] loss: 2.3072867393493652\n",
      "[step: 275] loss: 2.304309368133545\n",
      "[step: 276] loss: 2.3013265132904053\n",
      "[step: 277] loss: 2.2983391284942627\n",
      "[step: 278] loss: 2.295346975326538\n",
      "[step: 279] loss: 2.2923507690429688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 280] loss: 2.289350986480713\n",
      "[step: 281] loss: 2.286348342895508\n",
      "[step: 282] loss: 2.2833428382873535\n",
      "[step: 283] loss: 2.2803354263305664\n",
      "[step: 284] loss: 2.2773265838623047\n",
      "[step: 285] loss: 2.2743163108825684\n",
      "[step: 286] loss: 2.271306037902832\n",
      "[step: 287] loss: 2.2682957649230957\n",
      "[step: 288] loss: 2.2652854919433594\n",
      "[step: 289] loss: 2.2622766494750977\n",
      "[step: 290] loss: 2.2592697143554688\n",
      "[step: 291] loss: 2.256265163421631\n",
      "[step: 292] loss: 2.253262996673584\n",
      "[step: 293] loss: 2.250265121459961\n",
      "[step: 294] loss: 2.2472705841064453\n",
      "[step: 295] loss: 2.24428129196167\n",
      "[step: 296] loss: 2.2412967681884766\n",
      "[step: 297] loss: 2.2383179664611816\n",
      "[step: 298] loss: 2.2353458404541016\n",
      "[step: 299] loss: 2.2323803901672363\n",
      "[step: 300] loss: 2.2294225692749023\n",
      "[step: 301] loss: 2.2264723777770996\n",
      "[step: 302] loss: 2.2235310077667236\n",
      "[step: 303] loss: 2.2205986976623535\n",
      "[step: 304] loss: 2.2176756858825684\n",
      "[step: 305] loss: 2.2147622108459473\n",
      "[step: 306] loss: 2.2118592262268066\n",
      "[step: 307] loss: 2.2089672088623047\n",
      "[step: 308] loss: 2.2060859203338623\n",
      "[step: 309] loss: 2.2032155990600586\n",
      "[step: 310] loss: 2.200357437133789\n",
      "[step: 311] loss: 2.1975109577178955\n",
      "[step: 312] loss: 2.194676399230957\n",
      "[step: 313] loss: 2.191854476928711\n",
      "[step: 314] loss: 2.189044713973999\n",
      "[step: 315] loss: 2.1862475872039795\n",
      "[step: 316] loss: 2.183462619781494\n",
      "[step: 317] loss: 2.1806905269622803\n",
      "[step: 318] loss: 2.1779308319091797\n",
      "[step: 319] loss: 2.1751837730407715\n",
      "[step: 320] loss: 2.1724483966827393\n",
      "[step: 321] loss: 2.1697254180908203\n",
      "[step: 322] loss: 2.1670141220092773\n",
      "[step: 323] loss: 2.1643149852752686\n",
      "[step: 324] loss: 2.1616268157958984\n",
      "[step: 325] loss: 2.158949375152588\n",
      "[step: 326] loss: 2.156282424926758\n",
      "[step: 327] loss: 2.15362548828125\n",
      "[step: 328] loss: 2.1509780883789062\n",
      "[step: 329] loss: 2.1483397483825684\n",
      "[step: 330] loss: 2.1457102298736572\n",
      "[step: 331] loss: 2.1430881023406982\n",
      "[step: 332] loss: 2.1404736042022705\n",
      "[step: 333] loss: 2.1378657817840576\n",
      "[step: 334] loss: 2.1352639198303223\n",
      "[step: 335] loss: 2.132667303085327\n",
      "[step: 336] loss: 2.130075454711914\n",
      "[step: 337] loss: 2.1274871826171875\n",
      "[step: 338] loss: 2.1249022483825684\n",
      "[step: 339] loss: 2.1223199367523193\n",
      "[step: 340] loss: 2.119739532470703\n",
      "[step: 341] loss: 2.117159843444824\n",
      "[step: 342] loss: 2.1145806312561035\n",
      "[step: 343] loss: 2.1120004653930664\n",
      "[step: 344] loss: 2.109419822692871\n",
      "[step: 345] loss: 2.106837034225464\n",
      "[step: 346] loss: 2.1042516231536865\n",
      "[step: 347] loss: 2.101663112640381\n",
      "[step: 348] loss: 2.0990707874298096\n",
      "[step: 349] loss: 2.0964741706848145\n",
      "[step: 350] loss: 2.093872547149658\n",
      "[step: 351] loss: 2.0912647247314453\n",
      "[step: 352] loss: 2.088651180267334\n",
      "[step: 353] loss: 2.086030960083008\n",
      "[step: 354] loss: 2.083402633666992\n",
      "[step: 355] loss: 2.0807673931121826\n",
      "[step: 356] loss: 2.0781235694885254\n",
      "[step: 357] loss: 2.0754714012145996\n",
      "[step: 358] loss: 2.072809934616089\n",
      "[step: 359] loss: 2.070138692855835\n",
      "[step: 360] loss: 2.067458391189575\n",
      "[step: 361] loss: 2.064767360687256\n",
      "[step: 362] loss: 2.062066078186035\n",
      "[step: 363] loss: 2.059354305267334\n",
      "[step: 364] loss: 2.056631088256836\n",
      "[step: 365] loss: 2.053896903991699\n",
      "[step: 366] loss: 2.0511515140533447\n",
      "[step: 367] loss: 2.048394203186035\n",
      "[step: 368] loss: 2.0456252098083496\n",
      "[step: 369] loss: 2.042844295501709\n",
      "[step: 370] loss: 2.040050983428955\n",
      "[step: 371] loss: 2.0372462272644043\n",
      "[step: 372] loss: 2.034428596496582\n",
      "[step: 373] loss: 2.0315990447998047\n",
      "[step: 374] loss: 2.028756618499756\n",
      "[step: 375] loss: 2.025902509689331\n",
      "[step: 376] loss: 2.0230355262756348\n",
      "[step: 377] loss: 2.0201563835144043\n",
      "[step: 378] loss: 2.0172646045684814\n",
      "[step: 379] loss: 2.0143609046936035\n",
      "[step: 380] loss: 2.0114448070526123\n",
      "[step: 381] loss: 2.008516311645508\n",
      "[step: 382] loss: 2.0055758953094482\n",
      "[step: 383] loss: 2.0026235580444336\n",
      "[step: 384] loss: 1.9996592998504639\n",
      "[step: 385] loss: 1.9966838359832764\n",
      "[step: 386] loss: 1.993696689605713\n",
      "[step: 387] loss: 1.9906983375549316\n",
      "[step: 388] loss: 1.987688660621643\n",
      "[step: 389] loss: 1.9846683740615845\n",
      "[step: 390] loss: 1.9816374778747559\n",
      "[step: 391] loss: 1.9785964488983154\n",
      "[step: 392] loss: 1.9755454063415527\n",
      "[step: 393] loss: 1.9724843502044678\n",
      "[step: 394] loss: 1.969414234161377\n",
      "[step: 395] loss: 1.9663352966308594\n",
      "[step: 396] loss: 1.963247537612915\n",
      "[step: 397] loss: 1.9601515531539917\n",
      "[step: 398] loss: 1.957047939300537\n",
      "[step: 399] loss: 1.9539371728897095\n",
      "[step: 400] loss: 1.9508192539215088\n",
      "[step: 401] loss: 1.947695016860962\n",
      "[step: 402] loss: 1.9445648193359375\n",
      "[step: 403] loss: 1.9414290189743042\n",
      "[step: 404] loss: 1.938288927078247\n",
      "[step: 405] loss: 1.935144066810608\n",
      "[step: 406] loss: 1.9319953918457031\n",
      "[step: 407] loss: 1.92884361743927\n",
      "[step: 408] loss: 1.925689458847046\n",
      "[step: 409] loss: 1.922533392906189\n",
      "[step: 410] loss: 1.9193758964538574\n",
      "[step: 411] loss: 1.9162174463272095\n",
      "[step: 412] loss: 1.9130587577819824\n",
      "[step: 413] loss: 1.9099012613296509\n",
      "[step: 414] loss: 1.9067442417144775\n",
      "[step: 415] loss: 1.9035898447036743\n",
      "[step: 416] loss: 1.900437593460083\n",
      "[step: 417] loss: 1.8972886800765991\n",
      "[step: 418] loss: 1.8941435813903809\n",
      "[step: 419] loss: 1.8910030126571655\n",
      "[step: 420] loss: 1.8878679275512695\n",
      "[step: 421] loss: 1.8847382068634033\n",
      "[step: 422] loss: 1.8816156387329102\n",
      "[step: 423] loss: 1.8785003423690796\n",
      "[step: 424] loss: 1.8753927946090698\n",
      "[step: 425] loss: 1.872293472290039\n",
      "[step: 426] loss: 1.8692033290863037\n",
      "[step: 427] loss: 1.866123080253601\n",
      "[step: 428] loss: 1.8630530834197998\n",
      "[step: 429] loss: 1.8599942922592163\n",
      "[step: 430] loss: 1.8569467067718506\n",
      "[step: 431] loss: 1.85391104221344\n",
      "[step: 432] loss: 1.8508880138397217\n",
      "[step: 433] loss: 1.847878098487854\n",
      "[step: 434] loss: 1.8448817729949951\n",
      "[step: 435] loss: 1.8418993949890137\n",
      "[step: 436] loss: 1.8389308452606201\n",
      "[step: 437] loss: 1.8359780311584473\n",
      "[step: 438] loss: 1.8330397605895996\n",
      "[step: 439] loss: 1.8301172256469727\n",
      "[step: 440] loss: 1.8272106647491455\n",
      "[step: 441] loss: 1.824319839477539\n",
      "[step: 442] loss: 1.8214457035064697\n",
      "[step: 443] loss: 1.8185882568359375\n",
      "[step: 444] loss: 1.8157474994659424\n",
      "[step: 445] loss: 1.812923789024353\n",
      "[step: 446] loss: 1.8101171255111694\n",
      "[step: 447] loss: 1.8073281049728394\n",
      "[step: 448] loss: 1.8045567274093628\n",
      "[step: 449] loss: 1.8018027544021606\n",
      "[step: 450] loss: 1.7990666627883911\n",
      "[step: 451] loss: 1.7963480949401855\n",
      "[step: 452] loss: 1.793647289276123\n",
      "[step: 453] loss: 1.7909643650054932\n",
      "[step: 454] loss: 1.788299322128296\n",
      "[step: 455] loss: 1.7856520414352417\n",
      "[step: 456] loss: 1.7830228805541992\n",
      "[step: 457] loss: 1.7804113626480103\n",
      "[step: 458] loss: 1.777817964553833\n",
      "[step: 459] loss: 1.7752419710159302\n",
      "[step: 460] loss: 1.77268385887146\n",
      "[step: 461] loss: 1.770143747329712\n",
      "[step: 462] loss: 1.7676212787628174\n",
      "[step: 463] loss: 1.7651164531707764\n",
      "[step: 464] loss: 1.762629508972168\n",
      "[step: 465] loss: 1.7601597309112549\n",
      "[step: 466] loss: 1.7577078342437744\n",
      "[step: 467] loss: 1.7552740573883057\n",
      "[step: 468] loss: 1.7528579235076904\n",
      "[step: 469] loss: 1.75045907497406\n",
      "[step: 470] loss: 1.7480781078338623\n",
      "[step: 471] loss: 1.7457153797149658\n",
      "[step: 472] loss: 1.7433701753616333\n",
      "[step: 473] loss: 1.7410424947738647\n",
      "[step: 474] loss: 1.7387330532073975\n",
      "[step: 475] loss: 1.7364424467086792\n",
      "[step: 476] loss: 1.734169602394104\n",
      "[step: 477] loss: 1.7319152355194092\n",
      "[step: 478] loss: 1.7296793460845947\n",
      "[step: 479] loss: 1.7274622917175293\n",
      "[step: 480] loss: 1.7252650260925293\n",
      "[step: 481] loss: 1.7230861186981201\n",
      "[step: 482] loss: 1.7209268808364868\n",
      "[step: 483] loss: 1.718787670135498\n",
      "[step: 484] loss: 1.7166680097579956\n",
      "[step: 485] loss: 1.714568853378296\n",
      "[step: 486] loss: 1.7124903202056885\n",
      "[step: 487] loss: 1.710432529449463\n",
      "[step: 488] loss: 1.7083964347839355\n",
      "[step: 489] loss: 1.70638108253479\n",
      "[step: 490] loss: 1.704387903213501\n",
      "[step: 491] loss: 1.702417016029358\n",
      "[step: 492] loss: 1.7004687786102295\n",
      "[step: 493] loss: 1.6985429525375366\n",
      "[step: 494] loss: 1.6966404914855957\n",
      "[step: 495] loss: 1.6947612762451172\n",
      "[step: 496] loss: 1.6929060220718384\n",
      "[step: 497] loss: 1.6910744905471802\n",
      "[step: 498] loss: 1.6892675161361694\n",
      "[step: 499] loss: 1.6874850988388062\n",
      "[step: 500] loss: 1.6857271194458008\n",
      "[step: 501] loss: 1.683994174003601\n",
      "[step: 502] loss: 1.6822867393493652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 503] loss: 1.6806039810180664\n",
      "[step: 504] loss: 1.6789472103118896\n",
      "[step: 505] loss: 1.6773152351379395\n",
      "[step: 506] loss: 1.6757091283798218\n",
      "[step: 507] loss: 1.6741290092468262\n",
      "[step: 508] loss: 1.6725739240646362\n",
      "[step: 509] loss: 1.6710444688796997\n",
      "[step: 510] loss: 1.6695406436920166\n",
      "[step: 511] loss: 1.6680619716644287\n",
      "[step: 512] loss: 1.6666083335876465\n",
      "[step: 513] loss: 1.6651791334152222\n",
      "[step: 514] loss: 1.6637747287750244\n",
      "[step: 515] loss: 1.662394642829895\n",
      "[step: 516] loss: 1.6610386371612549\n",
      "[step: 517] loss: 1.6597058773040771\n",
      "[step: 518] loss: 1.6583961248397827\n",
      "[step: 519] loss: 1.657109022140503\n",
      "[step: 520] loss: 1.6558449268341064\n",
      "[step: 521] loss: 1.6546014547348022\n",
      "[step: 522] loss: 1.6533799171447754\n",
      "[step: 523] loss: 1.6521786451339722\n",
      "[step: 524] loss: 1.6509976387023926\n",
      "[step: 525] loss: 1.6498360633850098\n",
      "[step: 526] loss: 1.6486932039260864\n",
      "[step: 527] loss: 1.647569179534912\n",
      "[step: 528] loss: 1.6464625597000122\n",
      "[step: 529] loss: 1.6453731060028076\n",
      "[step: 530] loss: 1.644299864768982\n",
      "[step: 531] loss: 1.643242597579956\n",
      "[step: 532] loss: 1.6422004699707031\n",
      "[step: 533] loss: 1.6411733627319336\n",
      "[step: 534] loss: 1.6401593685150146\n",
      "[step: 535] loss: 1.6391587257385254\n",
      "[step: 536] loss: 1.6381707191467285\n",
      "[step: 537] loss: 1.6371949911117554\n",
      "[step: 538] loss: 1.636230230331421\n",
      "[step: 539] loss: 1.6352765560150146\n",
      "[step: 540] loss: 1.6343328952789307\n",
      "[step: 541] loss: 1.6333991289138794\n",
      "[step: 542] loss: 1.6324737071990967\n",
      "[step: 543] loss: 1.6315569877624512\n",
      "[step: 544] loss: 1.6306483745574951\n",
      "[step: 545] loss: 1.629746675491333\n",
      "[step: 546] loss: 1.6288517713546753\n",
      "[step: 547] loss: 1.6279635429382324\n",
      "[step: 548] loss: 1.6270806789398193\n",
      "[step: 549] loss: 1.6262035369873047\n",
      "[step: 550] loss: 1.6253306865692139\n",
      "[step: 551] loss: 1.6244630813598633\n",
      "[step: 552] loss: 1.6235988140106201\n",
      "[step: 553] loss: 1.6227385997772217\n",
      "[step: 554] loss: 1.6218814849853516\n",
      "[step: 555] loss: 1.6210269927978516\n",
      "[step: 556] loss: 1.6201748847961426\n",
      "[step: 557] loss: 1.619324803352356\n",
      "[step: 558] loss: 1.6184762716293335\n",
      "[step: 559] loss: 1.6176296472549438\n",
      "[step: 560] loss: 1.616783857345581\n",
      "[step: 561] loss: 1.6159389019012451\n",
      "[step: 562] loss: 1.615094542503357\n",
      "[step: 563] loss: 1.6142504215240479\n",
      "[step: 564] loss: 1.6134058237075806\n",
      "[step: 565] loss: 1.6125612258911133\n",
      "[step: 566] loss: 1.6117159128189087\n",
      "[step: 567] loss: 1.6108696460723877\n",
      "[step: 568] loss: 1.6100229024887085\n",
      "[step: 569] loss: 1.609174132347107\n",
      "[step: 570] loss: 1.6083252429962158\n",
      "[step: 571] loss: 1.6074737310409546\n",
      "[step: 572] loss: 1.6066203117370605\n",
      "[step: 573] loss: 1.6057658195495605\n",
      "[step: 574] loss: 1.6049084663391113\n",
      "[step: 575] loss: 1.6040489673614502\n",
      "[step: 576] loss: 1.6031872034072876\n",
      "[step: 577] loss: 1.6023228168487549\n",
      "[step: 578] loss: 1.6014554500579834\n",
      "[step: 579] loss: 1.600585699081421\n",
      "[step: 580] loss: 1.5997130870819092\n",
      "[step: 581] loss: 1.59883713722229\n",
      "[step: 582] loss: 1.5979578495025635\n",
      "[step: 583] loss: 1.597076177597046\n",
      "[step: 584] loss: 1.5961904525756836\n",
      "[step: 585] loss: 1.595301628112793\n",
      "[step: 586] loss: 1.5944087505340576\n",
      "[step: 587] loss: 1.5935128927230835\n",
      "[step: 588] loss: 1.5926129817962646\n",
      "[step: 589] loss: 1.5917099714279175\n",
      "[step: 590] loss: 1.5908026695251465\n",
      "[step: 591] loss: 1.5898916721343994\n",
      "[step: 592] loss: 1.588977336883545\n",
      "[step: 593] loss: 1.588058352470398\n",
      "[step: 594] loss: 1.5871360301971436\n",
      "[step: 595] loss: 1.5862090587615967\n",
      "[step: 596] loss: 1.5852782726287842\n",
      "[step: 597] loss: 1.5843433141708374\n",
      "[step: 598] loss: 1.5834040641784668\n",
      "[step: 599] loss: 1.582460641860962\n",
      "[step: 600] loss: 1.5815134048461914\n",
      "[step: 601] loss: 1.580561637878418\n",
      "[step: 602] loss: 1.5796058177947998\n",
      "[step: 603] loss: 1.5786452293395996\n",
      "[step: 604] loss: 1.5776801109313965\n",
      "[step: 605] loss: 1.576710820198059\n",
      "[step: 606] loss: 1.5757372379302979\n",
      "[step: 607] loss: 1.5747590065002441\n",
      "[step: 608] loss: 1.5737758874893188\n",
      "[step: 609] loss: 1.5727887153625488\n",
      "[step: 610] loss: 1.5717968940734863\n",
      "[step: 611] loss: 1.5708001852035522\n",
      "[step: 612] loss: 1.5697990655899048\n",
      "[step: 613] loss: 1.5687932968139648\n",
      "[step: 614] loss: 1.567782998085022\n",
      "[step: 615] loss: 1.5667672157287598\n",
      "[step: 616] loss: 1.5657472610473633\n",
      "[step: 617] loss: 1.5647220611572266\n",
      "[step: 618] loss: 1.563692331314087\n",
      "[step: 619] loss: 1.5626577138900757\n",
      "[step: 620] loss: 1.561618447303772\n",
      "[step: 621] loss: 1.5605733394622803\n",
      "[step: 622] loss: 1.559523582458496\n",
      "[step: 623] loss: 1.5584686994552612\n",
      "[step: 624] loss: 1.557409405708313\n",
      "[step: 625] loss: 1.556344747543335\n",
      "[step: 626] loss: 1.555274486541748\n",
      "[step: 627] loss: 1.554199457168579\n",
      "[step: 628] loss: 1.5531187057495117\n",
      "[step: 629] loss: 1.5520331859588623\n",
      "[step: 630] loss: 1.5509421825408936\n",
      "[step: 631] loss: 1.5498462915420532\n",
      "[step: 632] loss: 1.5487444400787354\n",
      "[step: 633] loss: 1.547637701034546\n",
      "[step: 634] loss: 1.5465253591537476\n",
      "[step: 635] loss: 1.545407772064209\n",
      "[step: 636] loss: 1.5442842245101929\n",
      "[step: 637] loss: 1.5431556701660156\n",
      "[step: 638] loss: 1.5420212745666504\n",
      "[step: 639] loss: 1.540881633758545\n",
      "[step: 640] loss: 1.539736032485962\n",
      "[step: 641] loss: 1.5385853052139282\n",
      "[step: 642] loss: 1.5374282598495483\n",
      "[step: 643] loss: 1.5362659692764282\n",
      "[step: 644] loss: 1.535097360610962\n",
      "[step: 645] loss: 1.5339237451553345\n",
      "[step: 646] loss: 1.5327441692352295\n",
      "[step: 647] loss: 1.5315592288970947\n",
      "[step: 648] loss: 1.5303679704666138\n",
      "[step: 649] loss: 1.5291707515716553\n",
      "[step: 650] loss: 1.5279678106307983\n",
      "[step: 651] loss: 1.5267590284347534\n",
      "[step: 652] loss: 1.5255448818206787\n",
      "[step: 653] loss: 1.524324655532837\n",
      "[step: 654] loss: 1.523098111152649\n",
      "[step: 655] loss: 1.5218658447265625\n",
      "[step: 656] loss: 1.5206270217895508\n",
      "[step: 657] loss: 1.5193829536437988\n",
      "[step: 658] loss: 1.5181331634521484\n",
      "[step: 659] loss: 1.5168769359588623\n",
      "[step: 660] loss: 1.5156149864196777\n",
      "[step: 661] loss: 1.5143464803695679\n",
      "[step: 662] loss: 1.5130720138549805\n",
      "[step: 663] loss: 1.5117919445037842\n",
      "[step: 664] loss: 1.5105056762695312\n",
      "[step: 665] loss: 1.509212851524353\n",
      "[step: 666] loss: 1.5079141855239868\n",
      "[step: 667] loss: 1.5066092014312744\n",
      "[step: 668] loss: 1.5052984952926636\n",
      "[step: 669] loss: 1.5039809942245483\n",
      "[step: 670] loss: 1.5026578903198242\n",
      "[step: 671] loss: 1.5013285875320435\n",
      "[step: 672] loss: 1.4999923706054688\n",
      "[step: 673] loss: 1.4986504316329956\n",
      "[step: 674] loss: 1.4973020553588867\n",
      "[step: 675] loss: 1.4959473609924316\n",
      "[step: 676] loss: 1.4945868253707886\n",
      "[step: 677] loss: 1.493219017982483\n",
      "[step: 678] loss: 1.491845726966858\n",
      "[step: 679] loss: 1.4904659986495972\n",
      "[step: 680] loss: 1.4890791177749634\n",
      "[step: 681] loss: 1.4876856803894043\n",
      "[step: 682] loss: 1.4862864017486572\n",
      "[step: 683] loss: 1.4848800897598267\n",
      "[step: 684] loss: 1.4834675788879395\n",
      "[step: 685] loss: 1.4820482730865479\n",
      "[step: 686] loss: 1.4806222915649414\n",
      "[step: 687] loss: 1.4791892766952515\n",
      "[step: 688] loss: 1.4777501821517944\n",
      "[step: 689] loss: 1.476303219795227\n",
      "[step: 690] loss: 1.4748502969741821\n",
      "[step: 691] loss: 1.4733901023864746\n",
      "[step: 692] loss: 1.4719231128692627\n",
      "[step: 693] loss: 1.4704492092132568\n",
      "[step: 694] loss: 1.4689677953720093\n",
      "[step: 695] loss: 1.4674795866012573\n",
      "[step: 696] loss: 1.4659843444824219\n",
      "[step: 697] loss: 1.4644818305969238\n",
      "[step: 698] loss: 1.4629721641540527\n",
      "[step: 699] loss: 1.4614546298980713\n",
      "[step: 700] loss: 1.4599297046661377\n",
      "[step: 701] loss: 1.4583982229232788\n",
      "[step: 702] loss: 1.4568581581115723\n",
      "[step: 703] loss: 1.455310344696045\n",
      "[step: 704] loss: 1.4537553787231445\n",
      "[step: 705] loss: 1.4521926641464233\n",
      "[step: 706] loss: 1.4506220817565918\n",
      "[step: 707] loss: 1.4490433931350708\n",
      "[step: 708] loss: 1.447456955909729\n",
      "[step: 709] loss: 1.4458622932434082\n",
      "[step: 710] loss: 1.4442596435546875\n",
      "[step: 711] loss: 1.4426480531692505\n",
      "[step: 712] loss: 1.4410288333892822\n",
      "[step: 713] loss: 1.4394011497497559\n",
      "[step: 714] loss: 1.437765121459961\n",
      "[step: 715] loss: 1.4361202716827393\n",
      "[step: 716] loss: 1.43446683883667\n",
      "[step: 717] loss: 1.4328047037124634\n",
      "[step: 718] loss: 1.431133508682251\n",
      "[step: 719] loss: 1.4294533729553223\n",
      "[step: 720] loss: 1.4277645349502563\n",
      "[step: 721] loss: 1.4260663986206055\n",
      "[step: 722] loss: 1.4243587255477905\n",
      "[step: 723] loss: 1.4226417541503906\n",
      "[step: 724] loss: 1.4209153652191162\n",
      "[step: 725] loss: 1.4191805124282837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 726] loss: 1.4174376726150513\n",
      "[step: 727] loss: 1.415691614151001\n",
      "[step: 728] loss: 1.413956880569458\n",
      "[step: 729] loss: 1.4122815132141113\n",
      "[step: 730] loss: 1.4108034372329712\n",
      "[step: 731] loss: 1.4099016189575195\n",
      "[step: 732] loss: 1.4098515510559082\n",
      "[step: 733] loss: 1.4099128246307373\n",
      "[step: 734] loss: 1.4067940711975098\n",
      "[step: 735] loss: 1.4020700454711914\n",
      "[step: 736] loss: 1.4007110595703125\n",
      "[step: 737] loss: 1.401014804840088\n",
      "[step: 738] loss: 1.3982787132263184\n",
      "[step: 739] loss: 1.3949060440063477\n",
      "[step: 740] loss: 1.3944447040557861\n",
      "[step: 741] loss: 1.3932632207870483\n",
      "[step: 742] loss: 1.3900192975997925\n",
      "[step: 743] loss: 1.3885204792022705\n",
      "[step: 744] loss: 1.3877116441726685\n",
      "[step: 745] loss: 1.3850510120391846\n",
      "[step: 746] loss: 1.3830058574676514\n",
      "[step: 747] loss: 1.3820655345916748\n",
      "[step: 748] loss: 1.379883885383606\n",
      "[step: 749] loss: 1.3776352405548096\n",
      "[step: 750] loss: 1.3764303922653198\n",
      "[step: 751] loss: 1.3745533227920532\n",
      "[step: 752] loss: 1.372278094291687\n",
      "[step: 753] loss: 1.3708055019378662\n",
      "[step: 754] loss: 1.369089961051941\n",
      "[step: 755] loss: 1.366870403289795\n",
      "[step: 756] loss: 1.3651684522628784\n",
      "[step: 757] loss: 1.363509178161621\n",
      "[step: 758] loss: 1.3613791465759277\n",
      "[step: 759] loss: 1.3595004081726074\n",
      "[step: 760] loss: 1.357817530632019\n",
      "[step: 761] loss: 1.3557816743850708\n",
      "[step: 762] loss: 1.3537824153900146\n",
      "[step: 763] loss: 1.3520216941833496\n",
      "[step: 764] loss: 1.3500608205795288\n",
      "[step: 765] loss: 1.3479973077774048\n",
      "[step: 766] loss: 1.3461289405822754\n",
      "[step: 767] loss: 1.3442046642303467\n",
      "[step: 768] loss: 1.3421244621276855\n",
      "[step: 769] loss: 1.340144395828247\n",
      "[step: 770] loss: 1.338210105895996\n",
      "[step: 771] loss: 1.3361414670944214\n",
      "[step: 772] loss: 1.3340733051300049\n",
      "[step: 773] loss: 1.3320837020874023\n",
      "[step: 774] loss: 1.3300271034240723\n",
      "[step: 775] loss: 1.3279070854187012\n",
      "[step: 776] loss: 1.3258388042449951\n",
      "[step: 777] loss: 1.3237700462341309\n",
      "[step: 778] loss: 1.3216297626495361\n",
      "[step: 779] loss: 1.3194869756698608\n",
      "[step: 780] loss: 1.3173727989196777\n",
      "[step: 781] loss: 1.3152203559875488\n",
      "[step: 782] loss: 1.3130271434783936\n",
      "[step: 783] loss: 1.3108479976654053\n",
      "[step: 784] loss: 1.3086671829223633\n",
      "[step: 785] loss: 1.3064472675323486\n",
      "[step: 786] loss: 1.3042081594467163\n",
      "[step: 787] loss: 1.3019760847091675\n",
      "[step: 788] loss: 1.2997286319732666\n",
      "[step: 789] loss: 1.2974498271942139\n",
      "[step: 790] loss: 1.2951598167419434\n",
      "[step: 791] loss: 1.2928694486618042\n",
      "[step: 792] loss: 1.2905590534210205\n",
      "[step: 793] loss: 1.2882243394851685\n",
      "[step: 794] loss: 1.2858800888061523\n",
      "[step: 795] loss: 1.28352952003479\n",
      "[step: 796] loss: 1.2811596393585205\n",
      "[step: 797] loss: 1.2787697315216064\n",
      "[step: 798] loss: 1.2763694524765015\n",
      "[step: 799] loss: 1.2739603519439697\n",
      "[step: 800] loss: 1.2715342044830322\n",
      "[step: 801] loss: 1.2690902948379517\n",
      "[step: 802] loss: 1.2666348218917847\n",
      "[step: 803] loss: 1.2641687393188477\n",
      "[step: 804] loss: 1.2616889476776123\n",
      "[step: 805] loss: 1.259192943572998\n",
      "[step: 806] loss: 1.2566845417022705\n",
      "[step: 807] loss: 1.2541658878326416\n",
      "[step: 808] loss: 1.2516350746154785\n",
      "[step: 809] loss: 1.2490906715393066\n",
      "[step: 810] loss: 1.2465335130691528\n",
      "[step: 811] loss: 1.2439665794372559\n",
      "[step: 812] loss: 1.241389274597168\n",
      "[step: 813] loss: 1.2388012409210205\n",
      "[step: 814] loss: 1.2362008094787598\n",
      "[step: 815] loss: 1.2335913181304932\n",
      "[step: 816] loss: 1.2309736013412476\n",
      "[step: 817] loss: 1.2283467054367065\n",
      "[step: 818] loss: 1.2257106304168701\n",
      "[step: 819] loss: 1.2230666875839233\n",
      "[step: 820] loss: 1.2204148769378662\n",
      "[step: 821] loss: 1.2177575826644897\n",
      "[step: 822] loss: 1.2150936126708984\n",
      "[step: 823] loss: 1.2124242782592773\n",
      "[step: 824] loss: 1.2097500562667847\n",
      "[step: 825] loss: 1.2070705890655518\n",
      "[step: 826] loss: 1.2043887376785278\n",
      "[step: 827] loss: 1.2017035484313965\n",
      "[step: 828] loss: 1.199016809463501\n",
      "[step: 829] loss: 1.196328043937683\n",
      "[step: 830] loss: 1.193638801574707\n",
      "[step: 831] loss: 1.1909503936767578\n",
      "[step: 832] loss: 1.1882625818252563\n",
      "[step: 833] loss: 1.1855769157409668\n",
      "[step: 834] loss: 1.182893991470337\n",
      "[step: 835] loss: 1.1802144050598145\n",
      "[step: 836] loss: 1.1775392293930054\n",
      "[step: 837] loss: 1.1748696565628052\n",
      "[step: 838] loss: 1.172206163406372\n",
      "[step: 839] loss: 1.1695499420166016\n",
      "[step: 840] loss: 1.166901707649231\n",
      "[step: 841] loss: 1.1642625331878662\n",
      "[step: 842] loss: 1.1616326570510864\n",
      "[step: 843] loss: 1.159012794494629\n",
      "[step: 844] loss: 1.156404733657837\n",
      "[step: 845] loss: 1.1538093090057373\n",
      "[step: 846] loss: 1.1512267589569092\n",
      "[step: 847] loss: 1.1486577987670898\n",
      "[step: 848] loss: 1.1461024284362793\n",
      "[step: 849] loss: 1.143563151359558\n",
      "[step: 850] loss: 1.1410400867462158\n",
      "[step: 851] loss: 1.1385326385498047\n",
      "[step: 852] loss: 1.1360421180725098\n",
      "[step: 853] loss: 1.1335694789886475\n",
      "[step: 854] loss: 1.1311149597167969\n",
      "[step: 855] loss: 1.128678798675537\n",
      "[step: 856] loss: 1.1262613534927368\n",
      "[step: 857] loss: 1.123862624168396\n",
      "[step: 858] loss: 1.1214845180511475\n",
      "[step: 859] loss: 1.119125247001648\n",
      "[step: 860] loss: 1.1167864799499512\n",
      "[step: 861] loss: 1.1144671440124512\n",
      "[step: 862] loss: 1.112168312072754\n",
      "[step: 863] loss: 1.1098895072937012\n",
      "[step: 864] loss: 1.107630729675293\n",
      "[step: 865] loss: 1.1053919792175293\n",
      "[step: 866] loss: 1.1031732559204102\n",
      "[step: 867] loss: 1.1009745597839355\n",
      "[step: 868] loss: 1.09879469871521\n",
      "[step: 869] loss: 1.0966349840164185\n",
      "[step: 870] loss: 1.0944947004318237\n",
      "[step: 871] loss: 1.0923734903335571\n",
      "[step: 872] loss: 1.0902721881866455\n",
      "[step: 873] loss: 1.0881932973861694\n",
      "[step: 874] loss: 1.0861413478851318\n",
      "[step: 875] loss: 1.0841237306594849\n",
      "[step: 876] loss: 1.082166314125061\n",
      "[step: 877] loss: 1.0803172588348389\n",
      "[step: 878] loss: 1.0786921977996826\n",
      "[step: 879] loss: 1.0774778127670288\n",
      "[step: 880] loss: 1.0768556594848633\n",
      "[step: 881] loss: 1.0764520168304443\n",
      "[step: 882] loss: 1.0748298168182373\n",
      "[step: 883] loss: 1.0709080696105957\n",
      "[step: 884] loss: 1.0668411254882812\n",
      "[step: 885] loss: 1.0652492046356201\n",
      "[step: 886] loss: 1.0651202201843262\n",
      "[step: 887] loss: 1.0635647773742676\n",
      "[step: 888] loss: 1.0602067708969116\n",
      "[step: 889] loss: 1.0577082633972168\n",
      "[step: 890] loss: 1.0569515228271484\n",
      "[step: 891] loss: 1.0558249950408936\n",
      "[step: 892] loss: 1.0532450675964355\n",
      "[step: 893] loss: 1.050813913345337\n",
      "[step: 894] loss: 1.0496528148651123\n",
      "[step: 895] loss: 1.048508882522583\n",
      "[step: 896] loss: 1.0463417768478394\n",
      "[step: 897] loss: 1.0441265106201172\n",
      "[step: 898] loss: 1.0427627563476562\n",
      "[step: 899] loss: 1.0415215492248535\n",
      "[step: 900] loss: 1.039626121520996\n",
      "[step: 901] loss: 1.037588357925415\n",
      "[step: 902] loss: 1.0360960960388184\n",
      "[step: 903] loss: 1.0347905158996582\n",
      "[step: 904] loss: 1.0330750942230225\n",
      "[step: 905] loss: 1.0311803817749023\n",
      "[step: 906] loss: 1.0296015739440918\n",
      "[step: 907] loss: 1.028231143951416\n",
      "[step: 908] loss: 1.0266597270965576\n",
      "[step: 909] loss: 1.0248867273330688\n",
      "[step: 910] loss: 1.0232501029968262\n",
      "[step: 911] loss: 1.021815538406372\n",
      "[step: 912] loss: 1.0203315019607544\n",
      "[step: 913] loss: 1.018687129020691\n",
      "[step: 914] loss: 1.017034888267517\n",
      "[step: 915] loss: 1.0155220031738281\n",
      "[step: 916] loss: 1.0140721797943115\n",
      "[step: 917] loss: 1.0125389099121094\n",
      "[step: 918] loss: 1.010934829711914\n",
      "[step: 919] loss: 1.009366750717163\n",
      "[step: 920] loss: 1.0078814029693604\n",
      "[step: 921] loss: 1.0064142942428589\n",
      "[step: 922] loss: 1.004894733428955\n",
      "[step: 923] loss: 1.003340482711792\n",
      "[step: 924] loss: 1.0018086433410645\n",
      "[step: 925] loss: 1.0003244876861572\n",
      "[step: 926] loss: 0.9988598823547363\n",
      "[step: 927] loss: 0.9973714351654053\n",
      "[step: 928] loss: 0.9958602786064148\n",
      "[step: 929] loss: 0.9943490624427795\n",
      "[step: 930] loss: 0.9928616881370544\n",
      "[step: 931] loss: 0.9913982152938843\n",
      "[step: 932] loss: 0.9899368286132812\n",
      "[step: 933] loss: 0.9884660243988037\n",
      "[step: 934] loss: 0.9869842529296875\n",
      "[step: 935] loss: 0.9855034351348877\n",
      "[step: 936] loss: 0.9840346574783325\n",
      "[step: 937] loss: 0.9825785756111145\n",
      "[step: 938] loss: 0.9811316132545471\n",
      "[step: 939] loss: 0.9796836972236633\n",
      "[step: 940] loss: 0.9782343506813049\n",
      "[step: 941] loss: 0.9767813682556152\n",
      "[step: 942] loss: 0.9753296971321106\n",
      "[step: 943] loss: 0.9738823175430298\n",
      "[step: 944] loss: 0.9724403619766235\n",
      "[step: 945] loss: 0.9710043668746948\n",
      "[step: 946] loss: 0.9695738554000854\n",
      "[step: 947] loss: 0.9681470990180969\n",
      "[step: 948] loss: 0.9667227864265442\n",
      "[step: 949] loss: 0.9653018712997437\n",
      "[step: 950] loss: 0.9638822674751282\n",
      "[step: 951] loss: 0.9624659419059753\n",
      "[step: 952] loss: 0.961052417755127\n",
      "[step: 953] loss: 0.9596430659294128\n",
      "[step: 954] loss: 0.9582377672195435\n",
      "[step: 955] loss: 0.9568396806716919\n",
      "[step: 956] loss: 0.9554500579833984\n",
      "[step: 957] loss: 0.9540755152702332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 958] loss: 0.9527233242988586\n",
      "[step: 959] loss: 0.9514130353927612\n",
      "[step: 960] loss: 0.9501715302467346\n",
      "[step: 961] loss: 0.9490633010864258\n",
      "[step: 962] loss: 0.9481696486473083\n",
      "[step: 963] loss: 0.9476224184036255\n",
      "[step: 964] loss: 0.9474292397499084\n",
      "[step: 965] loss: 0.9473075270652771\n",
      "[step: 966] loss: 0.946334719657898\n",
      "[step: 967] loss: 0.9437573552131653\n",
      "[step: 968] loss: 0.9402550458908081\n",
      "[step: 969] loss: 0.9377907514572144\n",
      "[step: 970] loss: 0.9371656179428101\n",
      "[step: 971] loss: 0.9372320175170898\n",
      "[step: 972] loss: 0.9363466501235962\n",
      "[step: 973] loss: 0.9340647459030151\n",
      "[step: 974] loss: 0.9316485524177551\n",
      "[step: 975] loss: 0.9303560256958008\n",
      "[step: 976] loss: 0.9299294948577881\n",
      "[step: 977] loss: 0.9291821718215942\n",
      "[step: 978] loss: 0.9274991750717163\n",
      "[step: 979] loss: 0.9254999160766602\n",
      "[step: 980] loss: 0.9240767359733582\n",
      "[step: 981] loss: 0.9232934713363647\n",
      "[step: 982] loss: 0.9224787950515747\n",
      "[step: 983] loss: 0.9211249351501465\n",
      "[step: 984] loss: 0.9194462895393372\n",
      "[step: 985] loss: 0.9179798364639282\n",
      "[step: 986] loss: 0.9169356226921082\n",
      "[step: 987] loss: 0.9160329699516296\n",
      "[step: 988] loss: 0.9149047136306763\n",
      "[step: 989] loss: 0.9134998321533203\n",
      "[step: 990] loss: 0.9120520353317261\n",
      "[step: 991] loss: 0.9107991456985474\n",
      "[step: 992] loss: 0.909744918346405\n",
      "[step: 993] loss: 0.9087138175964355\n",
      "[step: 994] loss: 0.9075568914413452\n",
      "[step: 995] loss: 0.9062597751617432\n",
      "[step: 996] loss: 0.9049372673034668\n",
      "[step: 997] loss: 0.9036953449249268\n",
      "[step: 998] loss: 0.9025589227676392\n",
      "[step: 999] loss: 0.901475191116333\n",
      "[step: 1000] loss: 0.900366485118866\n",
      "[step: 1001] loss: 0.8991982340812683\n",
      "[step: 1002] loss: 0.8979740142822266\n",
      "[step: 1003] loss: 0.8967368006706238\n",
      "[step: 1004] loss: 0.8955219388008118\n",
      "[step: 1005] loss: 0.8943477869033813\n",
      "[step: 1006] loss: 0.8932093381881714\n",
      "[step: 1007] loss: 0.8920881748199463\n",
      "[step: 1008] loss: 0.8909695148468018\n",
      "[step: 1009] loss: 0.8898394107818604\n",
      "[step: 1010] loss: 0.8886969089508057\n",
      "[step: 1011] loss: 0.8875405788421631\n",
      "[step: 1012] loss: 0.8863792419433594\n",
      "[step: 1013] loss: 0.8852146863937378\n",
      "[step: 1014] loss: 0.8840529918670654\n",
      "[step: 1015] loss: 0.8828955888748169\n",
      "[step: 1016] loss: 0.8817436695098877\n",
      "[step: 1017] loss: 0.8805973529815674\n",
      "[step: 1018] loss: 0.8794581890106201\n",
      "[step: 1019] loss: 0.8783261179924011\n",
      "[step: 1020] loss: 0.8772051334381104\n",
      "[step: 1021] loss: 0.8760977983474731\n",
      "[step: 1022] loss: 0.8750163912773132\n",
      "[step: 1023] loss: 0.8739771246910095\n",
      "[step: 1024] loss: 0.8730200529098511\n",
      "[step: 1025] loss: 0.8722090721130371\n",
      "[step: 1026] loss: 0.8716768622398376\n",
      "[step: 1027] loss: 0.8716204762458801\n",
      "[step: 1028] loss: 0.8722633123397827\n",
      "[step: 1029] loss: 0.8735190629959106\n",
      "[step: 1030] loss: 0.8743112087249756\n",
      "[step: 1031] loss: 0.8725516200065613\n",
      "[step: 1032] loss: 0.8677953481674194\n",
      "[step: 1033] loss: 0.8633008003234863\n",
      "[step: 1034] loss: 0.86236572265625\n",
      "[step: 1035] loss: 0.863892674446106\n",
      "[step: 1036] loss: 0.8641576766967773\n",
      "[step: 1037] loss: 0.8614429235458374\n",
      "[step: 1038] loss: 0.8581045269966125\n",
      "[step: 1039] loss: 0.8571071028709412\n",
      "[step: 1040] loss: 0.8577797412872314\n",
      "[step: 1041] loss: 0.8573435544967651\n",
      "[step: 1042] loss: 0.8550207018852234\n",
      "[step: 1043] loss: 0.8528035283088684\n",
      "[step: 1044] loss: 0.8522258996963501\n",
      "[step: 1045] loss: 0.8522714376449585\n",
      "[step: 1046] loss: 0.8512653708457947\n",
      "[step: 1047] loss: 0.8493051528930664\n",
      "[step: 1048] loss: 0.8477967977523804\n",
      "[step: 1049] loss: 0.847278892993927\n",
      "[step: 1050] loss: 0.8468581438064575\n",
      "[step: 1051] loss: 0.8456933498382568\n",
      "[step: 1052] loss: 0.8440945148468018\n",
      "[step: 1053] loss: 0.8428825736045837\n",
      "[step: 1054] loss: 0.842228889465332\n",
      "[step: 1055] loss: 0.8415751457214355\n",
      "[step: 1056] loss: 0.8404790163040161\n",
      "[step: 1057] loss: 0.8391287326812744\n",
      "[step: 1058] loss: 0.8379777669906616\n",
      "[step: 1059] loss: 0.8371601104736328\n",
      "[step: 1060] loss: 0.8364093899726868\n",
      "[step: 1061] loss: 0.8354434967041016\n",
      "[step: 1062] loss: 0.8342733383178711\n",
      "[step: 1063] loss: 0.8331215381622314\n",
      "[step: 1064] loss: 0.8321477770805359\n",
      "[step: 1065] loss: 0.8313040733337402\n",
      "[step: 1066] loss: 0.8304315209388733\n",
      "[step: 1067] loss: 0.8294364809989929\n",
      "[step: 1068] loss: 0.8283520340919495\n",
      "[step: 1069] loss: 0.8272819519042969\n",
      "[step: 1070] loss: 0.826295018196106\n",
      "[step: 1071] loss: 0.8253812789916992\n",
      "[step: 1072] loss: 0.8244807720184326\n",
      "[step: 1073] loss: 0.823538064956665\n",
      "[step: 1074] loss: 0.8225418329238892\n",
      "[step: 1075] loss: 0.821516215801239\n",
      "[step: 1076] loss: 0.8204969763755798\n",
      "[step: 1077] loss: 0.8195068836212158\n",
      "[step: 1078] loss: 0.8185489773750305\n",
      "[step: 1079] loss: 0.8176112174987793\n",
      "[step: 1080] loss: 0.8166762590408325\n",
      "[step: 1081] loss: 0.8157333135604858\n",
      "[step: 1082] loss: 0.8147757649421692\n",
      "[step: 1083] loss: 0.8138054609298706\n",
      "[step: 1084] loss: 0.8128281831741333\n",
      "[step: 1085] loss: 0.8118473291397095\n",
      "[step: 1086] loss: 0.8108678460121155\n",
      "[step: 1087] loss: 0.8098917603492737\n",
      "[step: 1088] loss: 0.8089198470115662\n",
      "[step: 1089] loss: 0.8079526424407959\n",
      "[step: 1090] loss: 0.8069876432418823\n",
      "[step: 1091] loss: 0.8060263395309448\n",
      "[step: 1092] loss: 0.8050662875175476\n",
      "[step: 1093] loss: 0.8041084408760071\n",
      "[step: 1094] loss: 0.803152859210968\n",
      "[step: 1095] loss: 0.8021992444992065\n",
      "[step: 1096] loss: 0.8012481927871704\n",
      "[step: 1097] loss: 0.8003020286560059\n",
      "[step: 1098] loss: 0.7993627786636353\n",
      "[step: 1099] loss: 0.7984382510185242\n",
      "[step: 1100] loss: 0.7975425720214844\n",
      "[step: 1101] loss: 0.7967097163200378\n",
      "[step: 1102] loss: 0.7960145473480225\n",
      "[step: 1103] loss: 0.7956286668777466\n",
      "[step: 1104] loss: 0.7959355115890503\n",
      "[step: 1105] loss: 0.7976735234260559\n",
      "[step: 1106] loss: 0.8018693327903748\n",
      "[step: 1107] loss: 0.8081076145172119\n",
      "[step: 1108] loss: 0.8107784986495972\n",
      "[step: 1109] loss: 0.8024603128433228\n",
      "[step: 1110] loss: 0.7901194095611572\n",
      "[step: 1111] loss: 0.7893118858337402\n",
      "[step: 1112] loss: 0.796479344367981\n",
      "[step: 1113] loss: 0.7956268191337585\n",
      "[step: 1114] loss: 0.7868908643722534\n",
      "[step: 1115] loss: 0.7852106094360352\n",
      "[step: 1116] loss: 0.7899594306945801\n",
      "[step: 1117] loss: 0.7882509231567383\n",
      "[step: 1118] loss: 0.7822378277778625\n",
      "[step: 1119] loss: 0.7825942039489746\n",
      "[step: 1120] loss: 0.7851209044456482\n",
      "[step: 1121] loss: 0.7819435000419617\n",
      "[step: 1122] loss: 0.7785909175872803\n",
      "[step: 1123] loss: 0.7800328731536865\n",
      "[step: 1124] loss: 0.7802720069885254\n",
      "[step: 1125] loss: 0.7770096063613892\n",
      "[step: 1126] loss: 0.7758468389511108\n",
      "[step: 1127] loss: 0.7769471406936646\n",
      "[step: 1128] loss: 0.7756972312927246\n",
      "[step: 1129] loss: 0.773345410823822\n",
      "[step: 1130] loss: 0.7732726335525513\n",
      "[step: 1131] loss: 0.7734739184379578\n",
      "[step: 1132] loss: 0.7717784643173218\n",
      "[step: 1133] loss: 0.7704075574874878\n",
      "[step: 1134] loss: 0.7705216407775879\n",
      "[step: 1135] loss: 0.7700316905975342\n",
      "[step: 1136] loss: 0.7685091495513916\n",
      "[step: 1137] loss: 0.7677356004714966\n",
      "[step: 1138] loss: 0.7676671147346497\n",
      "[step: 1139] loss: 0.7668807506561279\n",
      "[step: 1140] loss: 0.7656773328781128\n",
      "[step: 1141] loss: 0.7651464939117432\n",
      "[step: 1142] loss: 0.76487797498703\n",
      "[step: 1143] loss: 0.7640554308891296\n",
      "[step: 1144] loss: 0.7631033658981323\n",
      "[step: 1145] loss: 0.7626333832740784\n",
      "[step: 1146] loss: 0.76225346326828\n",
      "[step: 1147] loss: 0.7615004777908325\n",
      "[step: 1148] loss: 0.7607084512710571\n",
      "[step: 1149] loss: 0.7602369785308838\n",
      "[step: 1150] loss: 0.7598177194595337\n",
      "[step: 1151] loss: 0.759162425994873\n",
      "[step: 1152] loss: 0.758468508720398\n",
      "[step: 1153] loss: 0.7579817771911621\n",
      "[step: 1154] loss: 0.7575610876083374\n",
      "[step: 1155] loss: 0.7569990158081055\n",
      "[step: 1156] loss: 0.7563806176185608\n",
      "[step: 1157] loss: 0.7558813095092773\n",
      "[step: 1158] loss: 0.7554640769958496\n",
      "[step: 1159] loss: 0.7549844980239868\n",
      "[step: 1160] loss: 0.7544357180595398\n",
      "[step: 1161] loss: 0.7539343237876892\n",
      "[step: 1162] loss: 0.7535125017166138\n",
      "[step: 1163] loss: 0.7530906200408936\n",
      "[step: 1164] loss: 0.7526169419288635\n",
      "[step: 1165] loss: 0.7521339654922485\n",
      "[step: 1166] loss: 0.7516998648643494\n",
      "[step: 1167] loss: 0.7513028383255005\n",
      "[step: 1168] loss: 0.7508926391601562\n",
      "[step: 1169] loss: 0.7504557371139526\n",
      "[step: 1170] loss: 0.7500230073928833\n",
      "[step: 1171] loss: 0.7496224641799927\n",
      "[step: 1172] loss: 0.7492423057556152\n",
      "[step: 1173] loss: 0.7488548755645752\n",
      "[step: 1174] loss: 0.748454749584198\n",
      "[step: 1175] loss: 0.7480559349060059\n",
      "[step: 1176] loss: 0.7476732134819031\n",
      "[step: 1177] loss: 0.7473070621490479\n",
      "[step: 1178] loss: 0.7469428181648254\n",
      "[step: 1179] loss: 0.7465729117393494\n",
      "[step: 1180] loss: 0.7461992502212524\n",
      "[step: 1181] loss: 0.7458304166793823\n",
      "[step: 1182] loss: 0.7454718351364136\n",
      "[step: 1183] loss: 0.7451215982437134\n",
      "[step: 1184] loss: 0.7447726726531982\n",
      "[step: 1185] loss: 0.744421124458313\n",
      "[step: 1186] loss: 0.7440685033798218\n",
      "[step: 1187] loss: 0.7437171936035156\n",
      "[step: 1188] loss: 0.7433695197105408\n",
      "[step: 1189] loss: 0.7430284023284912\n",
      "[step: 1190] loss: 0.7426886558532715\n",
      "[step: 1191] loss: 0.742351770401001\n",
      "[step: 1192] loss: 0.742013692855835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1193] loss: 0.7416762113571167\n",
      "[step: 1194] loss: 0.7413373589515686\n",
      "[step: 1195] loss: 0.7410008907318115\n",
      "[step: 1196] loss: 0.7406646609306335\n",
      "[step: 1197] loss: 0.7403303980827332\n",
      "[step: 1198] loss: 0.7399981021881104\n",
      "[step: 1199] loss: 0.7396664619445801\n",
      "[step: 1200] loss: 0.7393362522125244\n",
      "[step: 1201] loss: 0.7390071153640747\n",
      "[step: 1202] loss: 0.7386781573295593\n",
      "[step: 1203] loss: 0.7383503317832947\n",
      "[step: 1204] loss: 0.7380235195159912\n",
      "[step: 1205] loss: 0.737697958946228\n",
      "[step: 1206] loss: 0.7373761534690857\n",
      "[step: 1207] loss: 0.737057089805603\n",
      "[step: 1208] loss: 0.7367479801177979\n",
      "[step: 1209] loss: 0.7364542484283447\n",
      "[step: 1210] loss: 0.736194908618927\n",
      "[step: 1211] loss: 0.7360062003135681\n",
      "[step: 1212] loss: 0.7359730005264282\n",
      "[step: 1213] loss: 0.7362819910049438\n",
      "[step: 1214] loss: 0.7373072504997253\n",
      "[step: 1215] loss: 0.7397899627685547\n",
      "[step: 1216] loss: 0.7443736791610718\n",
      "[step: 1217] loss: 0.7505396604537964\n",
      "[step: 1218] loss: 0.7534111738204956\n",
      "[step: 1219] loss: 0.7472675442695618\n",
      "[step: 1220] loss: 0.7363269925117493\n",
      "[step: 1221] loss: 0.7330777645111084\n",
      "[step: 1222] loss: 0.73897784948349\n",
      "[step: 1223] loss: 0.7427083253860474\n",
      "[step: 1224] loss: 0.737617015838623\n",
      "[step: 1225] loss: 0.7318300604820251\n",
      "[step: 1226] loss: 0.7336830496788025\n",
      "[step: 1227] loss: 0.7375081181526184\n",
      "[step: 1228] loss: 0.7353092432022095\n",
      "[step: 1229] loss: 0.7309117913246155\n",
      "[step: 1230] loss: 0.7314830422401428\n",
      "[step: 1231] loss: 0.7341102361679077\n",
      "[step: 1232] loss: 0.7327643036842346\n",
      "[step: 1233] loss: 0.7297265529632568\n",
      "[step: 1234] loss: 0.7300136089324951\n",
      "[step: 1235] loss: 0.7316689491271973\n",
      "[step: 1236] loss: 0.7306205034255981\n",
      "[step: 1237] loss: 0.7285367250442505\n",
      "[step: 1238] loss: 0.7287125587463379\n",
      "[step: 1239] loss: 0.7297011017799377\n",
      "[step: 1240] loss: 0.7288614511489868\n",
      "[step: 1241] loss: 0.727400541305542\n",
      "[step: 1242] loss: 0.7274454832077026\n",
      "[step: 1243] loss: 0.7280085682868958\n",
      "[step: 1244] loss: 0.7273705005645752\n",
      "[step: 1245] loss: 0.7262985706329346\n",
      "[step: 1246] loss: 0.7261967658996582\n",
      "[step: 1247] loss: 0.7264957427978516\n",
      "[step: 1248] loss: 0.7260416746139526\n",
      "[step: 1249] loss: 0.7252178192138672\n",
      "[step: 1250] loss: 0.7249775528907776\n",
      "[step: 1251] loss: 0.7250973582267761\n",
      "[step: 1252] loss: 0.7247958183288574\n",
      "[step: 1253] loss: 0.7241498231887817\n",
      "[step: 1254] loss: 0.7237986326217651\n",
      "[step: 1255] loss: 0.7237746119499207\n",
      "[step: 1256] loss: 0.723577618598938\n",
      "[step: 1257] loss: 0.7230843305587769\n",
      "[step: 1258] loss: 0.7226673364639282\n",
      "[step: 1259] loss: 0.722508430480957\n",
      "[step: 1260] loss: 0.7223564386367798\n",
      "[step: 1261] loss: 0.7220006585121155\n",
      "[step: 1262] loss: 0.7215802073478699\n",
      "[step: 1263] loss: 0.7213022112846375\n",
      "[step: 1264] loss: 0.7211257815361023\n",
      "[step: 1265] loss: 0.7208722233772278\n",
      "[step: 1266] loss: 0.7205085754394531\n",
      "[step: 1267] loss: 0.7201623320579529\n",
      "[step: 1268] loss: 0.7199118137359619\n",
      "[step: 1269] loss: 0.7196895480155945\n",
      "[step: 1270] loss: 0.7194046974182129\n",
      "[step: 1271] loss: 0.7190657258033752\n",
      "[step: 1272] loss: 0.7187499403953552\n",
      "[step: 1273] loss: 0.7184882164001465\n",
      "[step: 1274] loss: 0.7182387113571167\n",
      "[step: 1275] loss: 0.7179527878761292\n",
      "[step: 1276] loss: 0.7176333665847778\n",
      "[step: 1277] loss: 0.7173235416412354\n",
      "[step: 1278] loss: 0.7170435190200806\n",
      "[step: 1279] loss: 0.7167770862579346\n",
      "[step: 1280] loss: 0.7164960503578186\n",
      "[step: 1281] loss: 0.7161911725997925\n",
      "[step: 1282] loss: 0.715881884098053\n",
      "[step: 1283] loss: 0.7155855894088745\n",
      "[step: 1284] loss: 0.7153028249740601\n",
      "[step: 1285] loss: 0.7150200605392456\n",
      "[step: 1286] loss: 0.7147262096405029\n",
      "[step: 1287] loss: 0.7144216299057007\n",
      "[step: 1288] loss: 0.7141152620315552\n",
      "[step: 1289] loss: 0.7138152122497559\n",
      "[step: 1290] loss: 0.7135217189788818\n",
      "[step: 1291] loss: 0.7132279872894287\n",
      "[step: 1292] loss: 0.7129294276237488\n",
      "[step: 1293] loss: 0.7126240730285645\n",
      "[step: 1294] loss: 0.7123153209686279\n",
      "[step: 1295] loss: 0.7120072841644287\n",
      "[step: 1296] loss: 0.711701512336731\n",
      "[step: 1297] loss: 0.7113978862762451\n",
      "[step: 1298] loss: 0.7110926508903503\n",
      "[step: 1299] loss: 0.7107855081558228\n",
      "[step: 1300] loss: 0.7104744911193848\n",
      "[step: 1301] loss: 0.7101607918739319\n",
      "[step: 1302] loss: 0.709845781326294\n",
      "[step: 1303] loss: 0.7095298767089844\n",
      "[step: 1304] loss: 0.7092146873474121\n",
      "[step: 1305] loss: 0.7088989019393921\n",
      "[step: 1306] loss: 0.7085821628570557\n",
      "[step: 1307] loss: 0.7082648873329163\n",
      "[step: 1308] loss: 0.7079460620880127\n",
      "[step: 1309] loss: 0.7076253294944763\n",
      "[step: 1310] loss: 0.7073030471801758\n",
      "[step: 1311] loss: 0.7069805264472961\n",
      "[step: 1312] loss: 0.7066552639007568\n",
      "[step: 1313] loss: 0.7063305974006653\n",
      "[step: 1314] loss: 0.7060043811798096\n",
      "[step: 1315] loss: 0.7056785821914673\n",
      "[step: 1316] loss: 0.7053515911102295\n",
      "[step: 1317] loss: 0.7050246000289917\n",
      "[step: 1318] loss: 0.70469731092453\n",
      "[step: 1319] loss: 0.7043691873550415\n",
      "[step: 1320] loss: 0.7040417790412903\n",
      "[step: 1321] loss: 0.7037137150764465\n",
      "[step: 1322] loss: 0.7033858299255371\n",
      "[step: 1323] loss: 0.703058123588562\n",
      "[step: 1324] loss: 0.7027308940887451\n",
      "[step: 1325] loss: 0.7024035453796387\n",
      "[step: 1326] loss: 0.7020770311355591\n",
      "[step: 1327] loss: 0.7017509937286377\n",
      "[step: 1328] loss: 0.7014274597167969\n",
      "[step: 1329] loss: 0.7011063098907471\n",
      "[step: 1330] loss: 0.7007904648780823\n",
      "[step: 1331] loss: 0.7004850506782532\n",
      "[step: 1332] loss: 0.700204074382782\n",
      "[step: 1333] loss: 0.6999779939651489\n",
      "[step: 1334] loss: 0.6998773813247681\n",
      "[step: 1335] loss: 0.7000719308853149\n",
      "[step: 1336] loss: 0.7009613513946533\n",
      "[step: 1337] loss: 0.7033208608627319\n",
      "[step: 1338] loss: 0.7085861563682556\n",
      "[step: 1339] loss: 0.7174693942070007\n",
      "[step: 1340] loss: 0.7267419099807739\n",
      "[step: 1341] loss: 0.7252001166343689\n",
      "[step: 1342] loss: 0.7091759443283081\n",
      "[step: 1343] loss: 0.6969797611236572\n",
      "[step: 1344] loss: 0.7028647065162659\n",
      "[step: 1345] loss: 0.7122316956520081\n",
      "[step: 1346] loss: 0.706895649433136\n",
      "[step: 1347] loss: 0.6964815855026245\n",
      "[step: 1348] loss: 0.6987472772598267\n",
      "[step: 1349] loss: 0.7052786350250244\n",
      "[step: 1350] loss: 0.7011619210243225\n",
      "[step: 1351] loss: 0.6947485208511353\n",
      "[step: 1352] loss: 0.6977115869522095\n",
      "[step: 1353] loss: 0.7009283304214478\n",
      "[step: 1354] loss: 0.6964632272720337\n",
      "[step: 1355] loss: 0.6936766505241394\n",
      "[step: 1356] loss: 0.6968322396278381\n",
      "[step: 1357] loss: 0.6971151828765869\n",
      "[step: 1358] loss: 0.6933989524841309\n",
      "[step: 1359] loss: 0.6932158470153809\n",
      "[step: 1360] loss: 0.6953110694885254\n",
      "[step: 1361] loss: 0.6939008235931396\n",
      "[step: 1362] loss: 0.6917182207107544\n",
      "[step: 1363] loss: 0.6926605701446533\n",
      "[step: 1364] loss: 0.6933134198188782\n",
      "[step: 1365] loss: 0.6915380954742432\n",
      "[step: 1366] loss: 0.6907560229301453\n",
      "[step: 1367] loss: 0.6916706562042236\n",
      "[step: 1368] loss: 0.6912981271743774\n",
      "[step: 1369] loss: 0.6899433135986328\n",
      "[step: 1370] loss: 0.6899271011352539\n",
      "[step: 1371] loss: 0.6903489828109741\n",
      "[step: 1372] loss: 0.6895806789398193\n",
      "[step: 1373] loss: 0.688784122467041\n",
      "[step: 1374] loss: 0.6889703273773193\n",
      "[step: 1375] loss: 0.6889456510543823\n",
      "[step: 1376] loss: 0.6881833672523499\n",
      "[step: 1377] loss: 0.6877678632736206\n",
      "[step: 1378] loss: 0.6878805160522461\n",
      "[step: 1379] loss: 0.6876100301742554\n",
      "[step: 1380] loss: 0.6869927644729614\n",
      "[step: 1381] loss: 0.6867554187774658\n",
      "[step: 1382] loss: 0.6867375373840332\n",
      "[step: 1383] loss: 0.686383068561554\n",
      "[step: 1384] loss: 0.6859036087989807\n",
      "[step: 1385] loss: 0.6857173442840576\n",
      "[step: 1386] loss: 0.6855987310409546\n",
      "[step: 1387] loss: 0.6852401494979858\n",
      "[step: 1388] loss: 0.6848520040512085\n",
      "[step: 1389] loss: 0.6846630573272705\n",
      "[step: 1390] loss: 0.6844857931137085\n",
      "[step: 1391] loss: 0.6841491460800171\n",
      "[step: 1392] loss: 0.6838124990463257\n",
      "[step: 1393] loss: 0.6836059093475342\n",
      "[step: 1394] loss: 0.6833997964859009\n",
      "[step: 1395] loss: 0.6830900311470032\n",
      "[step: 1396] loss: 0.6827802658081055\n",
      "[step: 1397] loss: 0.6825555562973022\n",
      "[step: 1398] loss: 0.6823347806930542\n",
      "[step: 1399] loss: 0.6820471286773682\n",
      "[step: 1400] loss: 0.6817532777786255\n",
      "[step: 1401] loss: 0.6815139055252075\n",
      "[step: 1402] loss: 0.6812857389450073\n",
      "[step: 1403] loss: 0.6810157299041748\n",
      "[step: 1404] loss: 0.6807330846786499\n",
      "[step: 1405] loss: 0.680482804775238\n",
      "[step: 1406] loss: 0.6802487373352051\n",
      "[step: 1407] loss: 0.6799913644790649\n",
      "[step: 1408] loss: 0.6797181367874146\n",
      "[step: 1409] loss: 0.6794615983963013\n",
      "[step: 1410] loss: 0.6792212724685669\n",
      "[step: 1411] loss: 0.6789725422859192\n",
      "[step: 1412] loss: 0.6787087917327881\n",
      "[step: 1413] loss: 0.6784493923187256\n",
      "[step: 1414] loss: 0.6782031059265137\n",
      "[step: 1415] loss: 0.6779582500457764\n",
      "[step: 1416] loss: 0.6777031421661377\n",
      "[step: 1417] loss: 0.6774449944496155\n",
      "[step: 1418] loss: 0.6771939396858215\n",
      "[step: 1419] loss: 0.6769490242004395\n",
      "[step: 1420] loss: 0.6767003536224365\n",
      "[step: 1421] loss: 0.6764464378356934\n",
      "[step: 1422] loss: 0.6761936545372009\n",
      "[step: 1423] loss: 0.6759457588195801\n",
      "[step: 1424] loss: 0.6757005453109741\n",
      "[step: 1425] loss: 0.6754510402679443\n",
      "[step: 1426] loss: 0.6752001047134399\n",
      "[step: 1427] loss: 0.6749510169029236\n",
      "[step: 1428] loss: 0.6747043132781982\n",
      "[step: 1429] loss: 0.6744589805603027\n",
      "[step: 1430] loss: 0.6742110252380371\n",
      "[step: 1431] loss: 0.6739618182182312\n",
      "[step: 1432] loss: 0.6737149953842163\n",
      "[step: 1433] loss: 0.6734697818756104\n",
      "[step: 1434] loss: 0.6732244491577148\n",
      "[step: 1435] loss: 0.6729785203933716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1436] loss: 0.672731339931488\n",
      "[step: 1437] loss: 0.6724864840507507\n",
      "[step: 1438] loss: 0.6722416877746582\n",
      "[step: 1439] loss: 0.6719974875450134\n",
      "[step: 1440] loss: 0.6717526912689209\n",
      "[step: 1441] loss: 0.6715080738067627\n",
      "[step: 1442] loss: 0.6712632775306702\n",
      "[step: 1443] loss: 0.6710196137428284\n",
      "[step: 1444] loss: 0.6707764863967896\n",
      "[step: 1445] loss: 0.670533299446106\n",
      "[step: 1446] loss: 0.6702896952629089\n",
      "[step: 1447] loss: 0.6700466871261597\n",
      "[step: 1448] loss: 0.6698040962219238\n",
      "[step: 1449] loss: 0.6695616245269775\n",
      "[step: 1450] loss: 0.6693195104598999\n",
      "[step: 1451] loss: 0.6690776348114014\n",
      "[step: 1452] loss: 0.6688356399536133\n",
      "[step: 1453] loss: 0.6685936450958252\n",
      "[step: 1454] loss: 0.6683522462844849\n",
      "[step: 1455] loss: 0.6681112051010132\n",
      "[step: 1456] loss: 0.6678699254989624\n",
      "[step: 1457] loss: 0.6676291227340698\n",
      "[step: 1458] loss: 0.6673880815505981\n",
      "[step: 1459] loss: 0.667147159576416\n",
      "[step: 1460] loss: 0.6669074296951294\n",
      "[step: 1461] loss: 0.6666672229766846\n",
      "[step: 1462] loss: 0.6664267778396606\n",
      "[step: 1463] loss: 0.6661872863769531\n",
      "[step: 1464] loss: 0.665947437286377\n",
      "[step: 1465] loss: 0.6657078266143799\n",
      "[step: 1466] loss: 0.6654676198959351\n",
      "[step: 1467] loss: 0.6652288436889648\n",
      "[step: 1468] loss: 0.6649892330169678\n",
      "[step: 1469] loss: 0.664750337600708\n",
      "[step: 1470] loss: 0.6645106673240662\n",
      "[step: 1471] loss: 0.6642724275588989\n",
      "[step: 1472] loss: 0.6640334129333496\n",
      "[step: 1473] loss: 0.6637952327728271\n",
      "[step: 1474] loss: 0.6635564565658569\n",
      "[step: 1475] loss: 0.6633181571960449\n",
      "[step: 1476] loss: 0.6630799174308777\n",
      "[step: 1477] loss: 0.6628420352935791\n",
      "[step: 1478] loss: 0.6626032590866089\n",
      "[step: 1479] loss: 0.6623652577400208\n",
      "[step: 1480] loss: 0.6621269583702087\n",
      "[step: 1481] loss: 0.6618894338607788\n",
      "[step: 1482] loss: 0.6616511940956116\n",
      "[step: 1483] loss: 0.661413311958313\n",
      "[step: 1484] loss: 0.6611759662628174\n",
      "[step: 1485] loss: 0.6609384417533875\n",
      "[step: 1486] loss: 0.660700798034668\n",
      "[step: 1487] loss: 0.660462498664856\n",
      "[step: 1488] loss: 0.6602253913879395\n",
      "[step: 1489] loss: 0.6599875688552856\n",
      "[step: 1490] loss: 0.6597498655319214\n",
      "[step: 1491] loss: 0.6595126390457153\n",
      "[step: 1492] loss: 0.6592749357223511\n",
      "[step: 1493] loss: 0.6590374112129211\n",
      "[step: 1494] loss: 0.6588002443313599\n",
      "[step: 1495] loss: 0.6585627198219299\n",
      "[step: 1496] loss: 0.6583254337310791\n",
      "[step: 1497] loss: 0.6580872535705566\n",
      "[step: 1498] loss: 0.6578502655029297\n",
      "[step: 1499] loss: 0.6576124429702759\n",
      "[step: 1500] loss: 0.6573750972747803\n",
      "[step: 1501] loss: 0.6571377515792847\n",
      "[step: 1502] loss: 0.6568998098373413\n",
      "[step: 1503] loss: 0.6566628813743591\n",
      "[step: 1504] loss: 0.6564246416091919\n",
      "[step: 1505] loss: 0.6561875343322754\n",
      "[step: 1506] loss: 0.6559492349624634\n",
      "[step: 1507] loss: 0.6557116508483887\n",
      "[step: 1508] loss: 0.6554735898971558\n",
      "[step: 1509] loss: 0.6552351713180542\n",
      "[step: 1510] loss: 0.6549969911575317\n",
      "[step: 1511] loss: 0.6547592878341675\n",
      "[step: 1512] loss: 0.6545212268829346\n",
      "[step: 1513] loss: 0.6542830467224121\n",
      "[step: 1514] loss: 0.6540447473526001\n",
      "[step: 1515] loss: 0.6538053750991821\n",
      "[step: 1516] loss: 0.6535675525665283\n",
      "[step: 1517] loss: 0.653328537940979\n",
      "[step: 1518] loss: 0.6530897617340088\n",
      "[step: 1519] loss: 0.6528505086898804\n",
      "[step: 1520] loss: 0.6526113748550415\n",
      "[step: 1521] loss: 0.6523725390434265\n",
      "[step: 1522] loss: 0.6521331667900085\n",
      "[step: 1523] loss: 0.6518934965133667\n",
      "[step: 1524] loss: 0.6516538858413696\n",
      "[step: 1525] loss: 0.6514145731925964\n",
      "[step: 1526] loss: 0.6511741876602173\n",
      "[step: 1527] loss: 0.6509341597557068\n",
      "[step: 1528] loss: 0.6506944298744202\n",
      "[step: 1529] loss: 0.6504535675048828\n",
      "[step: 1530] loss: 0.6502137184143066\n",
      "[step: 1531] loss: 0.6499724984169006\n",
      "[step: 1532] loss: 0.6497318148612976\n",
      "[step: 1533] loss: 0.649491012096405\n",
      "[step: 1534] loss: 0.649249792098999\n",
      "[step: 1535] loss: 0.6490098237991333\n",
      "[step: 1536] loss: 0.6487697958946228\n",
      "[step: 1537] loss: 0.6485317945480347\n",
      "[step: 1538] loss: 0.6482982635498047\n",
      "[step: 1539] loss: 0.6480730772018433\n",
      "[step: 1540] loss: 0.6478656530380249\n",
      "[step: 1541] loss: 0.6476963758468628\n",
      "[step: 1542] loss: 0.6476089954376221\n",
      "[step: 1543] loss: 0.6476984024047852\n",
      "[step: 1544] loss: 0.6481657028198242\n",
      "[step: 1545] loss: 0.6494377851486206\n",
      "[step: 1546] loss: 0.6523009538650513\n",
      "[step: 1547] loss: 0.6580655574798584\n",
      "[step: 1548] loss: 0.6673515439033508\n",
      "[step: 1549] loss: 0.6774100065231323\n",
      "[step: 1550] loss: 0.6774241924285889\n",
      "[step: 1551] loss: 0.6620301008224487\n",
      "[step: 1552] loss: 0.6463102698326111\n",
      "[step: 1553] loss: 0.6486711502075195\n",
      "[step: 1554] loss: 0.6600792407989502\n",
      "[step: 1555] loss: 0.6592147946357727\n",
      "[step: 1556] loss: 0.6474635601043701\n",
      "[step: 1557] loss: 0.644834041595459\n",
      "[step: 1558] loss: 0.6524759531021118\n",
      "[step: 1559] loss: 0.6530408263206482\n",
      "[step: 1560] loss: 0.6451637744903564\n",
      "[step: 1561] loss: 0.6439206600189209\n",
      "[step: 1562] loss: 0.6490911245346069\n",
      "[step: 1563] loss: 0.6481292843818665\n",
      "[step: 1564] loss: 0.6428930759429932\n",
      "[step: 1565] loss: 0.6434592008590698\n",
      "[step: 1566] loss: 0.6465505957603455\n",
      "[step: 1567] loss: 0.6444922089576721\n",
      "[step: 1568] loss: 0.6414620876312256\n",
      "[step: 1569] loss: 0.6429153680801392\n",
      "[step: 1570] loss: 0.6441894173622131\n",
      "[step: 1571] loss: 0.6418841481208801\n",
      "[step: 1572] loss: 0.6406185626983643\n",
      "[step: 1573] loss: 0.6420385837554932\n",
      "[step: 1574] loss: 0.6419875621795654\n",
      "[step: 1575] loss: 0.6401481628417969\n",
      "[step: 1576] loss: 0.6399630308151245\n",
      "[step: 1577] loss: 0.6408455967903137\n",
      "[step: 1578] loss: 0.6401469111442566\n",
      "[step: 1579] loss: 0.6389769911766052\n",
      "[step: 1580] loss: 0.6392212510108948\n",
      "[step: 1581] loss: 0.639525294303894\n",
      "[step: 1582] loss: 0.6386803984642029\n",
      "[step: 1583] loss: 0.6380562782287598\n",
      "[step: 1584] loss: 0.6383225917816162\n",
      "[step: 1585] loss: 0.6382307410240173\n",
      "[step: 1586] loss: 0.6374990940093994\n",
      "[step: 1587] loss: 0.6371883153915405\n",
      "[step: 1588] loss: 0.6373234987258911\n",
      "[step: 1589] loss: 0.6370442509651184\n",
      "[step: 1590] loss: 0.6364782452583313\n",
      "[step: 1591] loss: 0.6362981796264648\n",
      "[step: 1592] loss: 0.6362942457199097\n",
      "[step: 1593] loss: 0.6359547972679138\n",
      "[step: 1594] loss: 0.6355218887329102\n",
      "[step: 1595] loss: 0.6353746056556702\n",
      "[step: 1596] loss: 0.6352704763412476\n",
      "[step: 1597] loss: 0.634933590888977\n",
      "[step: 1598] loss: 0.6345863342285156\n",
      "[step: 1599] loss: 0.6344306468963623\n",
      "[step: 1600] loss: 0.6342663764953613\n",
      "[step: 1601] loss: 0.633949875831604\n",
      "[step: 1602] loss: 0.6336509585380554\n",
      "[step: 1603] loss: 0.6334744095802307\n",
      "[step: 1604] loss: 0.6332783699035645\n",
      "[step: 1605] loss: 0.6329835057258606\n",
      "[step: 1606] loss: 0.6327086687088013\n",
      "[step: 1607] loss: 0.6325135231018066\n",
      "[step: 1608] loss: 0.6323004961013794\n",
      "[step: 1609] loss: 0.6320231556892395\n",
      "[step: 1610] loss: 0.6317603588104248\n",
      "[step: 1611] loss: 0.6315486431121826\n",
      "[step: 1612] loss: 0.63132643699646\n",
      "[step: 1613] loss: 0.6310626268386841\n",
      "[step: 1614] loss: 0.6308044195175171\n",
      "[step: 1615] loss: 0.6305802464485168\n",
      "[step: 1616] loss: 0.6303527355194092\n",
      "[step: 1617] loss: 0.6300972700119019\n",
      "[step: 1618] loss: 0.6298410892486572\n",
      "[step: 1619] loss: 0.6296079158782959\n",
      "[step: 1620] loss: 0.6293755173683167\n",
      "[step: 1621] loss: 0.6291254758834839\n",
      "[step: 1622] loss: 0.6288705468177795\n",
      "[step: 1623] loss: 0.6286301016807556\n",
      "[step: 1624] loss: 0.6283930540084839\n",
      "[step: 1625] loss: 0.6281453371047974\n",
      "[step: 1626] loss: 0.6278917789459229\n",
      "[step: 1627] loss: 0.6276460886001587\n",
      "[step: 1628] loss: 0.6274048089981079\n",
      "[step: 1629] loss: 0.6271578669548035\n",
      "[step: 1630] loss: 0.6269047260284424\n",
      "[step: 1631] loss: 0.6266545057296753\n",
      "[step: 1632] loss: 0.6264092922210693\n",
      "[step: 1633] loss: 0.6261613965034485\n",
      "[step: 1634] loss: 0.6259095668792725\n",
      "[step: 1635] loss: 0.6256558299064636\n",
      "[step: 1636] loss: 0.62540602684021\n",
      "[step: 1637] loss: 0.6251559853553772\n",
      "[step: 1638] loss: 0.6249032020568848\n",
      "[step: 1639] loss: 0.6246484518051147\n",
      "[step: 1640] loss: 0.624394416809082\n",
      "[step: 1641] loss: 0.6241421699523926\n",
      "[step: 1642] loss: 0.6238874197006226\n",
      "[step: 1643] loss: 0.6236313581466675\n",
      "[step: 1644] loss: 0.623374342918396\n",
      "[step: 1645] loss: 0.6231184005737305\n",
      "[step: 1646] loss: 0.6228612661361694\n",
      "[step: 1647] loss: 0.6226037740707397\n",
      "[step: 1648] loss: 0.6223443746566772\n",
      "[step: 1649] loss: 0.6220851540565491\n",
      "[step: 1650] loss: 0.621825098991394\n",
      "[step: 1651] loss: 0.6215652227401733\n",
      "[step: 1652] loss: 0.6213036179542542\n",
      "[step: 1653] loss: 0.6210412979125977\n",
      "[step: 1654] loss: 0.6207786798477173\n",
      "[step: 1655] loss: 0.6205154657363892\n",
      "[step: 1656] loss: 0.6202520132064819\n",
      "[step: 1657] loss: 0.6199870705604553\n",
      "[step: 1658] loss: 0.6197217702865601\n",
      "[step: 1659] loss: 0.6194556951522827\n",
      "[step: 1660] loss: 0.6191889047622681\n",
      "[step: 1661] loss: 0.6189211010932922\n",
      "[step: 1662] loss: 0.6186528205871582\n",
      "[step: 1663] loss: 0.6183835864067078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1664] loss: 0.618114173412323\n",
      "[step: 1665] loss: 0.617843508720398\n",
      "[step: 1666] loss: 0.6175721883773804\n",
      "[step: 1667] loss: 0.6173000335693359\n",
      "[step: 1668] loss: 0.6170268654823303\n",
      "[step: 1669] loss: 0.6167535185813904\n",
      "[step: 1670] loss: 0.6164788007736206\n",
      "[step: 1671] loss: 0.6162031888961792\n",
      "[step: 1672] loss: 0.6159278154373169\n",
      "[step: 1673] loss: 0.6156502366065979\n",
      "[step: 1674] loss: 0.6153731346130371\n",
      "[step: 1675] loss: 0.615094006061554\n",
      "[step: 1676] loss: 0.6148146986961365\n",
      "[step: 1677] loss: 0.6145342588424683\n",
      "[step: 1678] loss: 0.6142536401748657\n",
      "[step: 1679] loss: 0.613971471786499\n",
      "[step: 1680] loss: 0.6136882305145264\n",
      "[step: 1681] loss: 0.6134049892425537\n",
      "[step: 1682] loss: 0.6131199598312378\n",
      "[step: 1683] loss: 0.6128346920013428\n",
      "[step: 1684] loss: 0.6125482320785522\n",
      "[step: 1685] loss: 0.6122608184814453\n",
      "[step: 1686] loss: 0.6119725108146667\n",
      "[step: 1687] loss: 0.6116830110549927\n",
      "[step: 1688] loss: 0.6113928556442261\n",
      "[step: 1689] loss: 0.6111027002334595\n",
      "[step: 1690] loss: 0.6108101606369019\n",
      "[step: 1691] loss: 0.6105166673660278\n",
      "[step: 1692] loss: 0.6102229356765747\n",
      "[step: 1693] loss: 0.609927773475647\n",
      "[step: 1694] loss: 0.6096318960189819\n",
      "[step: 1695] loss: 0.6093350052833557\n",
      "[step: 1696] loss: 0.6090366244316101\n",
      "[step: 1697] loss: 0.6087378263473511\n",
      "[step: 1698] loss: 0.6084375977516174\n",
      "[step: 1699] loss: 0.6081366539001465\n",
      "[step: 1700] loss: 0.6078343391418457\n",
      "[step: 1701] loss: 0.6075313091278076\n",
      "[step: 1702] loss: 0.607227087020874\n",
      "[step: 1703] loss: 0.6069214344024658\n",
      "[step: 1704] loss: 0.6066145896911621\n",
      "[step: 1705] loss: 0.6063079237937927\n",
      "[step: 1706] loss: 0.6059988737106323\n",
      "[step: 1707] loss: 0.6056890487670898\n",
      "[step: 1708] loss: 0.6053780317306519\n",
      "[step: 1709] loss: 0.6050660610198975\n",
      "[step: 1710] loss: 0.6047528982162476\n",
      "[step: 1711] loss: 0.6044383645057678\n",
      "[step: 1712] loss: 0.6041228771209717\n",
      "[step: 1713] loss: 0.6038058996200562\n",
      "[step: 1714] loss: 0.6034883856773376\n",
      "[step: 1715] loss: 0.6031690239906311\n",
      "[step: 1716] loss: 0.6028485298156738\n",
      "[step: 1717] loss: 0.6025272011756897\n",
      "[step: 1718] loss: 0.6022043228149414\n",
      "[step: 1719] loss: 0.6018800735473633\n",
      "[step: 1720] loss: 0.601554274559021\n",
      "[step: 1721] loss: 0.6012282371520996\n",
      "[step: 1722] loss: 0.6009002327919006\n",
      "[step: 1723] loss: 0.6005704402923584\n",
      "[step: 1724] loss: 0.6002402305603027\n",
      "[step: 1725] loss: 0.599908709526062\n",
      "[step: 1726] loss: 0.5995754599571228\n",
      "[step: 1727] loss: 0.599240779876709\n",
      "[step: 1728] loss: 0.5989046096801758\n",
      "[step: 1729] loss: 0.59856778383255\n",
      "[step: 1730] loss: 0.5982285737991333\n",
      "[step: 1731] loss: 0.5978891849517822\n",
      "[step: 1732] loss: 0.5975472927093506\n",
      "[step: 1733] loss: 0.5972040891647339\n",
      "[step: 1734] loss: 0.5968602895736694\n",
      "[step: 1735] loss: 0.5965143442153931\n",
      "[step: 1736] loss: 0.5961669683456421\n",
      "[step: 1737] loss: 0.5958183407783508\n",
      "[step: 1738] loss: 0.5954679250717163\n",
      "[step: 1739] loss: 0.5951167345046997\n",
      "[step: 1740] loss: 0.594763457775116\n",
      "[step: 1741] loss: 0.5944089889526367\n",
      "[step: 1742] loss: 0.5940523147583008\n",
      "[step: 1743] loss: 0.5936943888664246\n",
      "[step: 1744] loss: 0.5933355093002319\n",
      "[step: 1745] loss: 0.5929741859436035\n",
      "[step: 1746] loss: 0.5926116108894348\n",
      "[step: 1747] loss: 0.5922473669052124\n",
      "[step: 1748] loss: 0.5918813943862915\n",
      "[step: 1749] loss: 0.5915140509605408\n",
      "[step: 1750] loss: 0.5911452174186707\n",
      "[step: 1751] loss: 0.5907739996910095\n",
      "[step: 1752] loss: 0.5904013514518738\n",
      "[step: 1753] loss: 0.5900278091430664\n",
      "[step: 1754] loss: 0.5896519422531128\n",
      "[step: 1755] loss: 0.589274525642395\n",
      "[step: 1756] loss: 0.5888946652412415\n",
      "[step: 1757] loss: 0.5885130167007446\n",
      "[step: 1758] loss: 0.5881303548812866\n",
      "[step: 1759] loss: 0.5877460837364197\n",
      "[step: 1760] loss: 0.5873596668243408\n",
      "[step: 1761] loss: 0.5869714021682739\n",
      "[step: 1762] loss: 0.5865811109542847\n",
      "[step: 1763] loss: 0.5861891508102417\n",
      "[step: 1764] loss: 0.5857957005500793\n",
      "[step: 1765] loss: 0.5854001641273499\n",
      "[step: 1766] loss: 0.5850018262863159\n",
      "[step: 1767] loss: 0.5846024751663208\n",
      "[step: 1768] loss: 0.5842007398605347\n",
      "[step: 1769] loss: 0.5837973356246948\n",
      "[step: 1770] loss: 0.5833922624588013\n",
      "[step: 1771] loss: 0.5829848647117615\n",
      "[step: 1772] loss: 0.5825753211975098\n",
      "[step: 1773] loss: 0.58216392993927\n",
      "[step: 1774] loss: 0.5817511677742004\n",
      "[step: 1775] loss: 0.5813352465629578\n",
      "[step: 1776] loss: 0.5809180736541748\n",
      "[step: 1777] loss: 0.5804983377456665\n",
      "[step: 1778] loss: 0.580075740814209\n",
      "[step: 1779] loss: 0.5796529054641724\n",
      "[step: 1780] loss: 0.5792277455329895\n",
      "[step: 1781] loss: 0.5787990689277649\n",
      "[step: 1782] loss: 0.578369140625\n",
      "[step: 1783] loss: 0.5779374837875366\n",
      "[step: 1784] loss: 0.5775026082992554\n",
      "[step: 1785] loss: 0.5770659446716309\n",
      "[step: 1786] loss: 0.5766274333000183\n",
      "[step: 1787] loss: 0.576185941696167\n",
      "[step: 1788] loss: 0.5757430791854858\n",
      "[step: 1789] loss: 0.575297474861145\n",
      "[step: 1790] loss: 0.5748504400253296\n",
      "[step: 1791] loss: 0.5744011998176575\n",
      "[step: 1792] loss: 0.5739503502845764\n",
      "[step: 1793] loss: 0.5734986066818237\n",
      "[step: 1794] loss: 0.5730465650558472\n",
      "[step: 1795] loss: 0.5725974440574646\n",
      "[step: 1796] loss: 0.5721551179885864\n",
      "[step: 1797] loss: 0.57172691822052\n",
      "[step: 1798] loss: 0.5713326930999756\n",
      "[step: 1799] loss: 0.5710055232048035\n",
      "[step: 1800] loss: 0.5708147287368774\n",
      "[step: 1801] loss: 0.5709024667739868\n",
      "[step: 1802] loss: 0.5715335607528687\n",
      "[step: 1803] loss: 0.5732262134552002\n",
      "[step: 1804] loss: 0.5767088532447815\n",
      "[step: 1805] loss: 0.5828529596328735\n",
      "[step: 1806] loss: 0.590569257736206\n",
      "[step: 1807] loss: 0.5954945087432861\n",
      "[step: 1808] loss: 0.5894231200218201\n",
      "[step: 1809] loss: 0.5747976303100586\n",
      "[step: 1810] loss: 0.5656364560127258\n",
      "[step: 1811] loss: 0.5699558258056641\n",
      "[step: 1812] loss: 0.5778551697731018\n",
      "[step: 1813] loss: 0.575954020023346\n",
      "[step: 1814] loss: 0.5667521953582764\n",
      "[step: 1815] loss: 0.5634958744049072\n",
      "[step: 1816] loss: 0.5684347748756409\n",
      "[step: 1817] loss: 0.570666491985321\n",
      "[step: 1818] loss: 0.565433144569397\n",
      "[step: 1819] loss: 0.561256468296051\n",
      "[step: 1820] loss: 0.5634613037109375\n",
      "[step: 1821] loss: 0.565704345703125\n",
      "[step: 1822] loss: 0.5627988576889038\n",
      "[step: 1823] loss: 0.5593734979629517\n",
      "[step: 1824] loss: 0.5602192282676697\n",
      "[step: 1825] loss: 0.5618018507957458\n",
      "[step: 1826] loss: 0.5599597096443176\n",
      "[step: 1827] loss: 0.5574005842208862\n",
      "[step: 1828] loss: 0.5576239228248596\n",
      "[step: 1829] loss: 0.5585532188415527\n",
      "[step: 1830] loss: 0.5572732090950012\n",
      "[step: 1831] loss: 0.5553654432296753\n",
      "[step: 1832] loss: 0.5552669763565063\n",
      "[step: 1833] loss: 0.5557315349578857\n",
      "[step: 1834] loss: 0.5547535419464111\n",
      "[step: 1835] loss: 0.553289532661438\n",
      "[step: 1836] loss: 0.552983283996582\n",
      "[step: 1837] loss: 0.5531256198883057\n",
      "[step: 1838] loss: 0.5523607134819031\n",
      "[step: 1839] loss: 0.5511875152587891\n",
      "[step: 1840] loss: 0.5507314801216125\n",
      "[step: 1841] loss: 0.5506548881530762\n",
      "[step: 1842] loss: 0.5500255227088928\n",
      "[step: 1843] loss: 0.549049973487854\n",
      "[step: 1844] loss: 0.5484931468963623\n",
      "[step: 1845] loss: 0.5482540130615234\n",
      "[step: 1846] loss: 0.5477098822593689\n",
      "[step: 1847] loss: 0.5468751788139343\n",
      "[step: 1848] loss: 0.5462550520896912\n",
      "[step: 1849] loss: 0.5458937883377075\n",
      "[step: 1850] loss: 0.5453909635543823\n",
      "[step: 1851] loss: 0.5446575284004211\n",
      "[step: 1852] loss: 0.5440049767494202\n",
      "[step: 1853] loss: 0.5435475707054138\n",
      "[step: 1854] loss: 0.5430542230606079\n",
      "[step: 1855] loss: 0.5423951148986816\n",
      "[step: 1856] loss: 0.5417315363883972\n",
      "[step: 1857] loss: 0.5412000417709351\n",
      "[step: 1858] loss: 0.5406935214996338\n",
      "[step: 1859] loss: 0.5400834679603577\n",
      "[step: 1860] loss: 0.5394282341003418\n",
      "[step: 1861] loss: 0.5388410091400146\n",
      "[step: 1862] loss: 0.5383051633834839\n",
      "[step: 1863] loss: 0.5377228260040283\n",
      "[step: 1864] loss: 0.5370837450027466\n",
      "[step: 1865] loss: 0.5364636182785034\n",
      "[step: 1866] loss: 0.5358908772468567\n",
      "[step: 1867] loss: 0.5353125929832458\n",
      "[step: 1868] loss: 0.5346915125846863\n",
      "[step: 1869] loss: 0.5340582728385925\n",
      "[step: 1870] loss: 0.533451497554779\n",
      "[step: 1871] loss: 0.5328606367111206\n",
      "[step: 1872] loss: 0.5322490334510803\n",
      "[step: 1873] loss: 0.5316149592399597\n",
      "[step: 1874] loss: 0.5309845805168152\n",
      "[step: 1875] loss: 0.5303711295127869\n",
      "[step: 1876] loss: 0.5297567844390869\n",
      "[step: 1877] loss: 0.529125452041626\n",
      "[step: 1878] loss: 0.5284842848777771\n",
      "[step: 1879] loss: 0.5278488397598267\n",
      "[step: 1880] loss: 0.5272208452224731\n",
      "[step: 1881] loss: 0.5265872478485107\n",
      "[step: 1882] loss: 0.5259419679641724\n",
      "[step: 1883] loss: 0.5252927541732788\n",
      "[step: 1884] loss: 0.5246465802192688\n",
      "[step: 1885] loss: 0.5240026116371155\n",
      "[step: 1886] loss: 0.523352861404419\n",
      "[step: 1887] loss: 0.5226962566375732\n",
      "[step: 1888] loss: 0.5220357179641724\n",
      "[step: 1889] loss: 0.5213766694068909\n",
      "[step: 1890] loss: 0.5207175612449646\n",
      "[step: 1891] loss: 0.5200538635253906\n",
      "[step: 1892] loss: 0.519384503364563\n",
      "[step: 1893] loss: 0.5187118053436279\n",
      "[step: 1894] loss: 0.5180395245552063\n",
      "[step: 1895] loss: 0.5173659920692444\n",
      "[step: 1896] loss: 0.5166888236999512\n",
      "[step: 1897] loss: 0.5160066485404968\n",
      "[step: 1898] loss: 0.5153223872184753\n",
      "[step: 1899] loss: 0.5146358013153076\n",
      "[step: 1900] loss: 0.5139474868774414\n",
      "[step: 1901] loss: 0.5132571458816528\n",
      "[step: 1902] loss: 0.5125625133514404\n",
      "[step: 1903] loss: 0.5118650197982788\n",
      "[step: 1904] loss: 0.5111649632453918\n",
      "[step: 1905] loss: 0.5104631185531616\n",
      "[step: 1906] loss: 0.5097583532333374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1907] loss: 0.5090505480766296\n",
      "[step: 1908] loss: 0.508340060710907\n",
      "[step: 1909] loss: 0.5076262354850769\n",
      "[step: 1910] loss: 0.5069105625152588\n",
      "[step: 1911] loss: 0.5061922073364258\n",
      "[step: 1912] loss: 0.5054718255996704\n",
      "[step: 1913] loss: 0.504747748374939\n",
      "[step: 1914] loss: 0.5040209889411926\n",
      "[step: 1915] loss: 0.5032916069030762\n",
      "[step: 1916] loss: 0.5025599002838135\n",
      "[step: 1917] loss: 0.5018253326416016\n",
      "[step: 1918] loss: 0.5010887384414673\n",
      "[step: 1919] loss: 0.5003486275672913\n",
      "[step: 1920] loss: 0.4996058940887451\n",
      "[step: 1921] loss: 0.4988608658313751\n",
      "[step: 1922] loss: 0.49811261892318726\n",
      "[step: 1923] loss: 0.4973630905151367\n",
      "[step: 1924] loss: 0.49660950899124146\n",
      "[step: 1925] loss: 0.49585413932800293\n",
      "[step: 1926] loss: 0.49509570002555847\n",
      "[step: 1927] loss: 0.4943341910839081\n",
      "[step: 1928] loss: 0.4935709238052368\n",
      "[step: 1929] loss: 0.49280428886413574\n",
      "[step: 1930] loss: 0.49203598499298096\n",
      "[step: 1931] loss: 0.4912645220756531\n",
      "[step: 1932] loss: 0.49049049615859985\n",
      "[step: 1933] loss: 0.4897131621837616\n",
      "[step: 1934] loss: 0.48893439769744873\n",
      "[step: 1935] loss: 0.4881519079208374\n",
      "[step: 1936] loss: 0.4873678982257843\n",
      "[step: 1937] loss: 0.48658043146133423\n",
      "[step: 1938] loss: 0.48579084873199463\n",
      "[step: 1939] loss: 0.4849987328052521\n",
      "[step: 1940] loss: 0.4842037856578827\n",
      "[step: 1941] loss: 0.48340684175491333\n",
      "[step: 1942] loss: 0.4826068580150604\n",
      "[step: 1943] loss: 0.48180338740348816\n",
      "[step: 1944] loss: 0.4809986352920532\n",
      "[step: 1945] loss: 0.4801907241344452\n",
      "[step: 1946] loss: 0.47938036918640137\n",
      "[step: 1947] loss: 0.47856706380844116\n",
      "[step: 1948] loss: 0.47775134444236755\n",
      "[step: 1949] loss: 0.4769321382045746\n",
      "[step: 1950] loss: 0.4761104881763458\n",
      "[step: 1951] loss: 0.4752863645553589\n",
      "[step: 1952] loss: 0.47445929050445557\n",
      "[step: 1953] loss: 0.4736294448375702\n",
      "[step: 1954] loss: 0.47279638051986694\n",
      "[step: 1955] loss: 0.4719602167606354\n",
      "[step: 1956] loss: 0.4711209535598755\n",
      "[step: 1957] loss: 0.4702793061733246\n",
      "[step: 1958] loss: 0.46943315863609314\n",
      "[step: 1959] loss: 0.4685847759246826\n",
      "[step: 1960] loss: 0.46773168444633484\n",
      "[step: 1961] loss: 0.4668760895729065\n",
      "[step: 1962] loss: 0.4660159647464752\n",
      "[step: 1963] loss: 0.4651528000831604\n",
      "[step: 1964] loss: 0.46428489685058594\n",
      "[step: 1965] loss: 0.4634135067462921\n",
      "[step: 1966] loss: 0.46253758668899536\n",
      "[step: 1967] loss: 0.46165627241134644\n",
      "[step: 1968] loss: 0.4607714116573334\n",
      "[step: 1969] loss: 0.45988091826438904\n",
      "[step: 1970] loss: 0.45898476243019104\n",
      "[step: 1971] loss: 0.4580835700035095\n",
      "[step: 1972] loss: 0.4571762681007385\n",
      "[step: 1973] loss: 0.4562634825706482\n",
      "[step: 1974] loss: 0.45534443855285645\n",
      "[step: 1975] loss: 0.4544186592102051\n",
      "[step: 1976] loss: 0.4534853398799896\n",
      "[step: 1977] loss: 0.4525454640388489\n",
      "[step: 1978] loss: 0.4515969455242157\n",
      "[step: 1979] loss: 0.45064103603363037\n",
      "[step: 1980] loss: 0.4496762752532959\n",
      "[step: 1981] loss: 0.44870296120643616\n",
      "[step: 1982] loss: 0.447720468044281\n",
      "[step: 1983] loss: 0.4467282295227051\n",
      "[step: 1984] loss: 0.4457259774208069\n",
      "[step: 1985] loss: 0.4447137117385864\n",
      "[step: 1986] loss: 0.44369006156921387\n",
      "[step: 1987] loss: 0.44265568256378174\n",
      "[step: 1988] loss: 0.44160938262939453\n",
      "[step: 1989] loss: 0.4405516982078552\n",
      "[step: 1990] loss: 0.43948134779930115\n",
      "[step: 1991] loss: 0.4383995831012726\n",
      "[step: 1992] loss: 0.4373047351837158\n",
      "[step: 1993] loss: 0.43619710206985474\n",
      "[step: 1994] loss: 0.4350760579109192\n",
      "[step: 1995] loss: 0.43394234776496887\n",
      "[step: 1996] loss: 0.43279457092285156\n",
      "[step: 1997] loss: 0.43163472414016724\n",
      "[step: 1998] loss: 0.4304613471031189\n",
      "[step: 1999] loss: 0.4292747378349304\n",
      "[step: 2000] loss: 0.42807501554489136\n",
      "[step: 2001] loss: 0.42686283588409424\n",
      "[step: 2002] loss: 0.42563849687576294\n",
      "[step: 2003] loss: 0.42440158128738403\n",
      "[step: 2004] loss: 0.42315298318862915\n",
      "[step: 2005] loss: 0.42189523577690125\n",
      "[step: 2006] loss: 0.42062807083129883\n",
      "[step: 2007] loss: 0.4193536043167114\n",
      "[step: 2008] loss: 0.4180768132209778\n",
      "[step: 2009] loss: 0.41680261492729187\n",
      "[step: 2010] loss: 0.4155438244342804\n",
      "[step: 2011] loss: 0.41432470083236694\n",
      "[step: 2012] loss: 0.413191020488739\n",
      "[step: 2013] loss: 0.41223376989364624\n",
      "[step: 2014] loss: 0.4116186499595642\n",
      "[step: 2015] loss: 0.4116578698158264\n",
      "[step: 2016] loss: 0.4127461314201355\n",
      "[step: 2017] loss: 0.4153289794921875\n",
      "[step: 2018] loss: 0.41871166229248047\n",
      "[step: 2019] loss: 0.420762300491333\n",
      "[step: 2020] loss: 0.4172212779521942\n",
      "[step: 2021] loss: 0.40847116708755493\n",
      "[step: 2022] loss: 0.4005073308944702\n",
      "[step: 2023] loss: 0.3992440700531006\n",
      "[step: 2024] loss: 0.4027419686317444\n",
      "[step: 2025] loss: 0.4041900038719177\n",
      "[step: 2026] loss: 0.4001695513725281\n",
      "[step: 2027] loss: 0.3942875862121582\n",
      "[step: 2028] loss: 0.39229661226272583\n",
      "[step: 2029] loss: 0.393969863653183\n",
      "[step: 2030] loss: 0.394375741481781\n",
      "[step: 2031] loss: 0.391185462474823\n",
      "[step: 2032] loss: 0.3873342275619507\n",
      "[step: 2033] loss: 0.38627538084983826\n",
      "[step: 2034] loss: 0.38700070977211\n",
      "[step: 2035] loss: 0.3862497806549072\n",
      "[step: 2036] loss: 0.3834606409072876\n",
      "[step: 2037] loss: 0.381039559841156\n",
      "[step: 2038] loss: 0.3804874122142792\n",
      "[step: 2039] loss: 0.38044649362564087\n",
      "[step: 2040] loss: 0.3791329860687256\n",
      "[step: 2041] loss: 0.37689629197120667\n",
      "[step: 2042] loss: 0.37531787157058716\n",
      "[step: 2043] loss: 0.37480628490448\n",
      "[step: 2044] loss: 0.3742654621601105\n",
      "[step: 2045] loss: 0.3728734254837036\n",
      "[step: 2046] loss: 0.37114274501800537\n",
      "[step: 2047] loss: 0.3699623942375183\n",
      "[step: 2048] loss: 0.369347482919693\n",
      "[step: 2049] loss: 0.36859241127967834\n",
      "[step: 2050] loss: 0.3673376441001892\n",
      "[step: 2051] loss: 0.3659582734107971\n",
      "[step: 2052] loss: 0.36493635177612305\n",
      "[step: 2053] loss: 0.36423352360725403\n",
      "[step: 2054] loss: 0.3634483516216278\n",
      "[step: 2055] loss: 0.36238211393356323\n",
      "[step: 2056] loss: 0.36123397946357727\n",
      "[step: 2057] loss: 0.3602829873561859\n",
      "[step: 2058] loss: 0.3595415949821472\n",
      "[step: 2059] loss: 0.3587995767593384\n",
      "[step: 2060] loss: 0.3579089641571045\n",
      "[step: 2061] loss: 0.3569377660751343\n",
      "[step: 2062] loss: 0.35604578256607056\n",
      "[step: 2063] loss: 0.3552943170070648\n",
      "[step: 2064] loss: 0.3546009957790375\n",
      "[step: 2065] loss: 0.35385721921920776\n",
      "[step: 2066] loss: 0.3530454635620117\n",
      "[step: 2067] loss: 0.3522355258464813\n",
      "[step: 2068] loss: 0.3514955937862396\n",
      "[step: 2069] loss: 0.3508285880088806\n",
      "[step: 2070] loss: 0.3501834273338318\n",
      "[step: 2071] loss: 0.3495121896266937\n",
      "[step: 2072] loss: 0.34881681203842163\n",
      "[step: 2073] loss: 0.34812816977500916\n",
      "[step: 2074] loss: 0.34748077392578125\n",
      "[step: 2075] loss: 0.346876859664917\n",
      "[step: 2076] loss: 0.3462950885295868\n",
      "[step: 2077] loss: 0.3457128405570984\n",
      "[step: 2078] loss: 0.34512078762054443\n",
      "[step: 2079] loss: 0.34453022480010986\n",
      "[step: 2080] loss: 0.34395602345466614\n",
      "[step: 2081] loss: 0.343407541513443\n",
      "[step: 2082] loss: 0.3428827226161957\n",
      "[step: 2083] loss: 0.34237149357795715\n",
      "[step: 2084] loss: 0.341865599155426\n",
      "[step: 2085] loss: 0.34136056900024414\n",
      "[step: 2086] loss: 0.34085965156555176\n",
      "[step: 2087] loss: 0.3403671085834503\n",
      "[step: 2088] loss: 0.33988767862319946\n",
      "[step: 2089] loss: 0.339422345161438\n",
      "[step: 2090] loss: 0.3389705419540405\n",
      "[step: 2091] loss: 0.338528573513031\n",
      "[step: 2092] loss: 0.33809366822242737\n",
      "[step: 2093] loss: 0.33766332268714905\n",
      "[step: 2094] loss: 0.3372379541397095\n",
      "[step: 2095] loss: 0.3368179500102997\n",
      "[step: 2096] loss: 0.33640384674072266\n",
      "[step: 2097] loss: 0.33599674701690674\n",
      "[step: 2098] loss: 0.3355966806411743\n",
      "[step: 2099] loss: 0.33520448207855225\n",
      "[step: 2100] loss: 0.33481842279434204\n",
      "[step: 2101] loss: 0.3344395160675049\n",
      "[step: 2102] loss: 0.33406609296798706\n",
      "[step: 2103] loss: 0.3336975574493408\n",
      "[step: 2104] loss: 0.33333396911621094\n",
      "[step: 2105] loss: 0.3329755365848541\n",
      "[step: 2106] loss: 0.3326203227043152\n",
      "[step: 2107] loss: 0.3322699964046478\n",
      "[step: 2108] loss: 0.3319239616394043\n",
      "[step: 2109] loss: 0.33158212900161743\n",
      "[step: 2110] loss: 0.3312445282936096\n",
      "[step: 2111] loss: 0.33091241121292114\n",
      "[step: 2112] loss: 0.3305848240852356\n",
      "[step: 2113] loss: 0.33026444911956787\n",
      "[step: 2114] loss: 0.32995131611824036\n",
      "[step: 2115] loss: 0.32964977622032166\n",
      "[step: 2116] loss: 0.329365611076355\n",
      "[step: 2117] loss: 0.32910749316215515\n",
      "[step: 2118] loss: 0.3288898766040802\n",
      "[step: 2119] loss: 0.32874390482902527\n",
      "[step: 2120] loss: 0.32870912551879883\n",
      "[step: 2121] loss: 0.32887735962867737\n",
      "[step: 2122] loss: 0.3293410539627075\n",
      "[step: 2123] loss: 0.33032357692718506\n",
      "[step: 2124] loss: 0.3318544626235962\n",
      "[step: 2125] loss: 0.3341407775878906\n",
      "[step: 2126] loss: 0.3362670838832855\n",
      "[step: 2127] loss: 0.3375830352306366\n",
      "[step: 2128] loss: 0.3360445499420166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2129] loss: 0.3320741653442383\n",
      "[step: 2130] loss: 0.32737278938293457\n",
      "[step: 2131] loss: 0.3248581886291504\n",
      "[step: 2132] loss: 0.325431764125824\n",
      "[step: 2133] loss: 0.32760417461395264\n",
      "[step: 2134] loss: 0.32904940843582153\n",
      "[step: 2135] loss: 0.328183114528656\n",
      "[step: 2136] loss: 0.3257169723510742\n",
      "[step: 2137] loss: 0.3235265016555786\n",
      "[step: 2138] loss: 0.3231009244918823\n",
      "[step: 2139] loss: 0.3240784704685211\n",
      "[step: 2140] loss: 0.3249535858631134\n",
      "[step: 2141] loss: 0.3246327340602875\n",
      "[step: 2142] loss: 0.3232154846191406\n",
      "[step: 2143] loss: 0.32186219096183777\n",
      "[step: 2144] loss: 0.3214489817619324\n",
      "[step: 2145] loss: 0.32184433937072754\n",
      "[step: 2146] loss: 0.32224103808403015\n",
      "[step: 2147] loss: 0.3219754993915558\n",
      "[step: 2148] loss: 0.32112520933151245\n",
      "[step: 2149] loss: 0.32025647163391113\n",
      "[step: 2150] loss: 0.3198741674423218\n",
      "[step: 2151] loss: 0.31994861364364624\n",
      "[step: 2152] loss: 0.3200658857822418\n",
      "[step: 2153] loss: 0.3198692798614502\n",
      "[step: 2154] loss: 0.31933051347732544\n",
      "[step: 2155] loss: 0.3187289237976074\n",
      "[step: 2156] loss: 0.31834131479263306\n",
      "[step: 2157] loss: 0.3182147741317749\n",
      "[step: 2158] loss: 0.3181753158569336\n",
      "[step: 2159] loss: 0.3180226683616638\n",
      "[step: 2160] loss: 0.3176867663860321\n",
      "[step: 2161] loss: 0.3172496557235718\n",
      "[step: 2162] loss: 0.31686338782310486\n",
      "[step: 2163] loss: 0.31660765409469604\n",
      "[step: 2164] loss: 0.3164515495300293\n",
      "[step: 2165] loss: 0.3163010776042938\n",
      "[step: 2166] loss: 0.316076397895813\n",
      "[step: 2167] loss: 0.31577014923095703\n",
      "[step: 2168] loss: 0.31542932987213135\n",
      "[step: 2169] loss: 0.3151177763938904\n",
      "[step: 2170] loss: 0.3148649334907532\n",
      "[step: 2171] loss: 0.31465858221054077\n",
      "[step: 2172] loss: 0.3144611716270447\n",
      "[step: 2173] loss: 0.31423789262771606\n",
      "[step: 2174] loss: 0.3139772117137909\n",
      "[step: 2175] loss: 0.31369125843048096\n",
      "[step: 2176] loss: 0.31340494751930237\n",
      "[step: 2177] loss: 0.31313782930374146\n",
      "[step: 2178] loss: 0.31289374828338623\n",
      "[step: 2179] loss: 0.3126668334007263\n",
      "[step: 2180] loss: 0.3124425709247589\n",
      "[step: 2181] loss: 0.3122099041938782\n",
      "[step: 2182] loss: 0.3119637966156006\n",
      "[step: 2183] loss: 0.3117052912712097\n",
      "[step: 2184] loss: 0.311443030834198\n",
      "[step: 2185] loss: 0.31118282675743103\n",
      "[step: 2186] loss: 0.3109293580055237\n",
      "[step: 2187] loss: 0.3106841444969177\n",
      "[step: 2188] loss: 0.31044501066207886\n",
      "[step: 2189] loss: 0.31020796298980713\n",
      "[step: 2190] loss: 0.30997100472450256\n",
      "[step: 2191] loss: 0.30973076820373535\n",
      "[step: 2192] loss: 0.3094877004623413\n",
      "[step: 2193] loss: 0.3092409670352936\n",
      "[step: 2194] loss: 0.3089924454689026\n",
      "[step: 2195] loss: 0.30874311923980713\n",
      "[step: 2196] loss: 0.30849406123161316\n",
      "[step: 2197] loss: 0.30824553966522217\n",
      "[step: 2198] loss: 0.30799803137779236\n",
      "[step: 2199] loss: 0.3077527582645416\n",
      "[step: 2200] loss: 0.307508260011673\n",
      "[step: 2201] loss: 0.30726388096809387\n",
      "[step: 2202] loss: 0.30702123045921326\n",
      "[step: 2203] loss: 0.30677860975265503\n",
      "[step: 2204] loss: 0.30653584003448486\n",
      "[step: 2205] loss: 0.30629438161849976\n",
      "[step: 2206] loss: 0.30605340003967285\n",
      "[step: 2207] loss: 0.30581164360046387\n",
      "[step: 2208] loss: 0.305571049451828\n",
      "[step: 2209] loss: 0.30533191561698914\n",
      "[step: 2210] loss: 0.30509471893310547\n",
      "[step: 2211] loss: 0.3048602044582367\n",
      "[step: 2212] loss: 0.30462929606437683\n",
      "[step: 2213] loss: 0.3044049143791199\n",
      "[step: 2214] loss: 0.30419203639030457\n",
      "[step: 2215] loss: 0.30399733781814575\n",
      "[step: 2216] loss: 0.303830087184906\n",
      "[step: 2217] loss: 0.30371391773223877\n",
      "[step: 2218] loss: 0.3036739230155945\n",
      "[step: 2219] loss: 0.3037780821323395\n",
      "[step: 2220] loss: 0.30409061908721924\n",
      "[step: 2221] loss: 0.3047932982444763\n",
      "[step: 2222] loss: 0.30595529079437256\n",
      "[step: 2223] loss: 0.3079212009906769\n",
      "[step: 2224] loss: 0.310219943523407\n",
      "[step: 2225] loss: 0.3128424286842346\n",
      "[step: 2226] loss: 0.31353098154067993\n",
      "[step: 2227] loss: 0.3119848668575287\n",
      "[step: 2228] loss: 0.3074645698070526\n",
      "[step: 2229] loss: 0.3027495741844177\n",
      "[step: 2230] loss: 0.30035707354545593\n",
      "[step: 2231] loss: 0.30112138390541077\n",
      "[step: 2232] loss: 0.30347517132759094\n",
      "[step: 2233] loss: 0.30494752526283264\n",
      "[step: 2234] loss: 0.3042929172515869\n",
      "[step: 2235] loss: 0.3017641305923462\n",
      "[step: 2236] loss: 0.2994624674320221\n",
      "[step: 2237] loss: 0.2988535761833191\n",
      "[step: 2238] loss: 0.29978591203689575\n",
      "[step: 2239] loss: 0.30088046193122864\n",
      "[step: 2240] loss: 0.3008202314376831\n",
      "[step: 2241] loss: 0.2996135950088501\n",
      "[step: 2242] loss: 0.2981431484222412\n",
      "[step: 2243] loss: 0.29744580388069153\n",
      "[step: 2244] loss: 0.29769468307495117\n",
      "[step: 2245] loss: 0.29822811484336853\n",
      "[step: 2246] loss: 0.2983015179634094\n",
      "[step: 2247] loss: 0.29767870903015137\n",
      "[step: 2248] loss: 0.29678356647491455\n",
      "[step: 2249] loss: 0.2961496114730835\n",
      "[step: 2250] loss: 0.29602712392807007\n",
      "[step: 2251] loss: 0.2962026596069336\n",
      "[step: 2252] loss: 0.2962622046470642\n",
      "[step: 2253] loss: 0.2959868311882019\n",
      "[step: 2254] loss: 0.295444130897522\n",
      "[step: 2255] loss: 0.29491037130355835\n",
      "[step: 2256] loss: 0.29459095001220703\n",
      "[step: 2257] loss: 0.29449912905693054\n",
      "[step: 2258] loss: 0.29448193311691284\n",
      "[step: 2259] loss: 0.29436206817626953\n",
      "[step: 2260] loss: 0.29408204555511475\n",
      "[step: 2261] loss: 0.29369670152664185\n",
      "[step: 2262] loss: 0.293326735496521\n",
      "[step: 2263] loss: 0.29305338859558105\n",
      "[step: 2264] loss: 0.2928849458694458\n",
      "[step: 2265] loss: 0.2927618622779846\n",
      "[step: 2266] loss: 0.2926090359687805\n",
      "[step: 2267] loss: 0.29239165782928467\n",
      "[step: 2268] loss: 0.2921132743358612\n",
      "[step: 2269] loss: 0.2918139100074768\n",
      "[step: 2270] loss: 0.2915353775024414\n",
      "[step: 2271] loss: 0.29130011796951294\n",
      "[step: 2272] loss: 0.2911039888858795\n",
      "[step: 2273] loss: 0.2909243106842041\n",
      "[step: 2274] loss: 0.2907395660877228\n",
      "[step: 2275] loss: 0.29053324460983276\n",
      "[step: 2276] loss: 0.29030340909957886\n",
      "[step: 2277] loss: 0.2900567650794983\n",
      "[step: 2278] loss: 0.2898063659667969\n",
      "[step: 2279] loss: 0.2895618677139282\n",
      "[step: 2280] loss: 0.28933024406433105\n",
      "[step: 2281] loss: 0.2891122102737427\n",
      "[step: 2282] loss: 0.28890281915664673\n",
      "[step: 2283] loss: 0.2886977195739746\n",
      "[step: 2284] loss: 0.2884925901889801\n",
      "[step: 2285] loss: 0.28828442096710205\n",
      "[step: 2286] loss: 0.28806960582733154\n",
      "[step: 2287] loss: 0.2878512144088745\n",
      "[step: 2288] loss: 0.28762897849082947\n",
      "[step: 2289] loss: 0.2874039113521576\n",
      "[step: 2290] loss: 0.2871786653995514\n",
      "[step: 2291] loss: 0.286953330039978\n",
      "[step: 2292] loss: 0.2867284417152405\n",
      "[step: 2293] loss: 0.28650590777397156\n",
      "[step: 2294] loss: 0.2862829566001892\n",
      "[step: 2295] loss: 0.28606170415878296\n",
      "[step: 2296] loss: 0.2858412265777588\n",
      "[step: 2297] loss: 0.28562265634536743\n",
      "[step: 2298] loss: 0.2854030132293701\n",
      "[step: 2299] loss: 0.28518444299697876\n",
      "[step: 2300] loss: 0.28496673703193665\n",
      "[step: 2301] loss: 0.28474926948547363\n",
      "[step: 2302] loss: 0.2845327854156494\n",
      "[step: 2303] loss: 0.2843170166015625\n",
      "[step: 2304] loss: 0.2841026484966278\n",
      "[step: 2305] loss: 0.28388985991477966\n",
      "[step: 2306] loss: 0.28368061780929565\n",
      "[step: 2307] loss: 0.2834775149822235\n",
      "[step: 2308] loss: 0.28328409790992737\n",
      "[step: 2309] loss: 0.28311097621917725\n",
      "[step: 2310] loss: 0.2829698920249939\n",
      "[step: 2311] loss: 0.2828930914402008\n",
      "[step: 2312] loss: 0.2829296588897705\n",
      "[step: 2313] loss: 0.28319576382637024\n",
      "[step: 2314] loss: 0.28384941816329956\n",
      "[step: 2315] loss: 0.28529372811317444\n",
      "[step: 2316] loss: 0.28783106803894043\n",
      "[step: 2317] loss: 0.292367547750473\n",
      "[step: 2318] loss: 0.2976679801940918\n",
      "[step: 2319] loss: 0.30320799350738525\n",
      "[step: 2320] loss: 0.30220603942871094\n",
      "[step: 2321] loss: 0.29522955417633057\n",
      "[step: 2322] loss: 0.28504350781440735\n",
      "[step: 2323] loss: 0.280163049697876\n",
      "[step: 2324] loss: 0.28316110372543335\n",
      "[step: 2325] loss: 0.2887861132621765\n",
      "[step: 2326] loss: 0.2901899814605713\n",
      "[step: 2327] loss: 0.2853960394859314\n",
      "[step: 2328] loss: 0.28017523884773254\n",
      "[step: 2329] loss: 0.27949899435043335\n",
      "[step: 2330] loss: 0.28263455629348755\n",
      "[step: 2331] loss: 0.2847905158996582\n",
      "[step: 2332] loss: 0.28273463249206543\n",
      "[step: 2333] loss: 0.2792345881462097\n",
      "[step: 2334] loss: 0.2781675159931183\n",
      "[step: 2335] loss: 0.2798280417919159\n",
      "[step: 2336] loss: 0.2812612056732178\n",
      "[step: 2337] loss: 0.2802557945251465\n",
      "[step: 2338] loss: 0.27808934450149536\n",
      "[step: 2339] loss: 0.27714118361473083\n",
      "[step: 2340] loss: 0.27795934677124023\n",
      "[step: 2341] loss: 0.27884775400161743\n",
      "[step: 2342] loss: 0.2783108353614807\n",
      "[step: 2343] loss: 0.27695775032043457\n",
      "[step: 2344] loss: 0.2762012481689453\n",
      "[step: 2345] loss: 0.2764909565448761\n",
      "[step: 2346] loss: 0.27699005603790283\n",
      "[step: 2347] loss: 0.27677422761917114\n",
      "[step: 2348] loss: 0.27594101428985596\n",
      "[step: 2349] loss: 0.2752721905708313\n",
      "[step: 2350] loss: 0.2752434015274048\n",
      "[step: 2351] loss: 0.2754988670349121\n",
      "[step: 2352] loss: 0.2754365801811218\n",
      "[step: 2353] loss: 0.27495455741882324\n",
      "[step: 2354] loss: 0.27440470457077026\n",
      "[step: 2355] loss: 0.27415865659713745\n",
      "[step: 2356] loss: 0.2741965055465698\n",
      "[step: 2357] loss: 0.2742011845111847\n",
      "[step: 2358] loss: 0.27395617961883545\n",
      "[step: 2359] loss: 0.27354907989501953\n",
      "[step: 2360] loss: 0.27321577072143555\n",
      "[step: 2361] loss: 0.2730644643306732\n",
      "[step: 2362] loss: 0.2730132043361664\n",
      "[step: 2363] loss: 0.27290505170822144\n",
      "[step: 2364] loss: 0.2726617753505707\n",
      "[step: 2365] loss: 0.2723531126976013\n",
      "[step: 2366] loss: 0.2720925807952881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2367] loss: 0.2719283103942871\n",
      "[step: 2368] loss: 0.2718147039413452\n",
      "[step: 2369] loss: 0.27167463302612305\n",
      "[step: 2370] loss: 0.271471232175827\n",
      "[step: 2371] loss: 0.2712228298187256\n",
      "[step: 2372] loss: 0.27098485827445984\n",
      "[step: 2373] loss: 0.27079230546951294\n",
      "[step: 2374] loss: 0.2706364393234253\n",
      "[step: 2375] loss: 0.2704851031303406\n",
      "[step: 2376] loss: 0.2703114449977875\n",
      "[step: 2377] loss: 0.270109087228775\n",
      "[step: 2378] loss: 0.2698909044265747\n",
      "[step: 2379] loss: 0.269681453704834\n",
      "[step: 2380] loss: 0.26949259638786316\n",
      "[step: 2381] loss: 0.26931947469711304\n",
      "[step: 2382] loss: 0.2691519856452942\n",
      "[step: 2383] loss: 0.2689768075942993\n",
      "[step: 2384] loss: 0.2687894105911255\n",
      "[step: 2385] loss: 0.2685920000076294\n",
      "[step: 2386] loss: 0.2683927118778229\n",
      "[step: 2387] loss: 0.2681981027126312\n",
      "[step: 2388] loss: 0.26801061630249023\n",
      "[step: 2389] loss: 0.267830491065979\n",
      "[step: 2390] loss: 0.26765239238739014\n",
      "[step: 2391] loss: 0.2674739956855774\n",
      "[step: 2392] loss: 0.26729243993759155\n",
      "[step: 2393] loss: 0.2671061158180237\n",
      "[step: 2394] loss: 0.26691752672195435\n",
      "[step: 2395] loss: 0.26672717928886414\n",
      "[step: 2396] loss: 0.26653817296028137\n",
      "[step: 2397] loss: 0.26634955406188965\n",
      "[step: 2398] loss: 0.2661641836166382\n",
      "[step: 2399] loss: 0.26598024368286133\n",
      "[step: 2400] loss: 0.26579707860946655\n",
      "[step: 2401] loss: 0.2656148076057434\n",
      "[step: 2402] loss: 0.26543253660202026\n",
      "[step: 2403] loss: 0.2652503252029419\n",
      "[step: 2404] loss: 0.2650677561759949\n",
      "[step: 2405] loss: 0.2648846507072449\n",
      "[step: 2406] loss: 0.2647020220756531\n",
      "[step: 2407] loss: 0.26451826095581055\n",
      "[step: 2408] loss: 0.26433494687080383\n",
      "[step: 2409] loss: 0.2641524076461792\n",
      "[step: 2410] loss: 0.26397013664245605\n",
      "[step: 2411] loss: 0.2637891173362732\n",
      "[step: 2412] loss: 0.26360854506492615\n",
      "[step: 2413] loss: 0.2634304165840149\n",
      "[step: 2414] loss: 0.26325491070747375\n",
      "[step: 2415] loss: 0.26308417320251465\n",
      "[step: 2416] loss: 0.26292091608047485\n",
      "[step: 2417] loss: 0.2627682685852051\n",
      "[step: 2418] loss: 0.2626368999481201\n",
      "[step: 2419] loss: 0.262535035610199\n",
      "[step: 2420] loss: 0.2624916136264801\n",
      "[step: 2421] loss: 0.2625397741794586\n",
      "[step: 2422] loss: 0.26276153326034546\n",
      "[step: 2423] loss: 0.26325076818466187\n",
      "[step: 2424] loss: 0.2642592191696167\n",
      "[step: 2425] loss: 0.26595982909202576\n",
      "[step: 2426] loss: 0.2689473628997803\n",
      "[step: 2427] loss: 0.27281349897384644\n",
      "[step: 2428] loss: 0.27785807847976685\n",
      "[step: 2429] loss: 0.2802157402038574\n",
      "[step: 2430] loss: 0.27913808822631836\n",
      "[step: 2431] loss: 0.2718941569328308\n",
      "[step: 2432] loss: 0.2638590931892395\n",
      "[step: 2433] loss: 0.25990909337997437\n",
      "[step: 2434] loss: 0.2618452310562134\n",
      "[step: 2435] loss: 0.26641300320625305\n",
      "[step: 2436] loss: 0.26848334074020386\n",
      "[step: 2437] loss: 0.26631882786750793\n",
      "[step: 2438] loss: 0.26165491342544556\n",
      "[step: 2439] loss: 0.25895166397094727\n",
      "[step: 2440] loss: 0.25985991954803467\n",
      "[step: 2441] loss: 0.2623688578605652\n",
      "[step: 2442] loss: 0.2634272575378418\n",
      "[step: 2443] loss: 0.26167982816696167\n",
      "[step: 2444] loss: 0.2590934634208679\n",
      "[step: 2445] loss: 0.25793129205703735\n",
      "[step: 2446] loss: 0.25872594118118286\n",
      "[step: 2447] loss: 0.2600405216217041\n",
      "[step: 2448] loss: 0.2601437568664551\n",
      "[step: 2449] loss: 0.2589038908481598\n",
      "[step: 2450] loss: 0.25745880603790283\n",
      "[step: 2451] loss: 0.2569928765296936\n",
      "[step: 2452] loss: 0.25749722123146057\n",
      "[step: 2453] loss: 0.2580678462982178\n",
      "[step: 2454] loss: 0.2579491138458252\n",
      "[step: 2455] loss: 0.2571227550506592\n",
      "[step: 2456] loss: 0.2562934160232544\n",
      "[step: 2457] loss: 0.2560076117515564\n",
      "[step: 2458] loss: 0.25621843338012695\n",
      "[step: 2459] loss: 0.2564615309238434\n",
      "[step: 2460] loss: 0.2563329339027405\n",
      "[step: 2461] loss: 0.25584572553634644\n",
      "[step: 2462] loss: 0.2553008794784546\n",
      "[step: 2463] loss: 0.25500166416168213\n",
      "[step: 2464] loss: 0.2549845576286316\n",
      "[step: 2465] loss: 0.25505340099334717\n",
      "[step: 2466] loss: 0.2549973428249359\n",
      "[step: 2467] loss: 0.2547376751899719\n",
      "[step: 2468] loss: 0.2543724477291107\n",
      "[step: 2469] loss: 0.2540517747402191\n",
      "[step: 2470] loss: 0.2538685202598572\n",
      "[step: 2471] loss: 0.253797322511673\n",
      "[step: 2472] loss: 0.25374501943588257\n",
      "[step: 2473] loss: 0.25362834334373474\n",
      "[step: 2474] loss: 0.25342071056365967\n",
      "[step: 2475] loss: 0.2531620264053345\n",
      "[step: 2476] loss: 0.2529119551181793\n",
      "[step: 2477] loss: 0.2527148425579071\n",
      "[step: 2478] loss: 0.25257229804992676\n",
      "[step: 2479] loss: 0.25245991349220276\n",
      "[step: 2480] loss: 0.2523411512374878\n",
      "[step: 2481] loss: 0.2521907389163971\n",
      "[step: 2482] loss: 0.2520080804824829\n",
      "[step: 2483] loss: 0.2518048882484436\n",
      "[step: 2484] loss: 0.2516002357006073\n",
      "[step: 2485] loss: 0.25140976905822754\n",
      "[step: 2486] loss: 0.2512400150299072\n",
      "[step: 2487] loss: 0.2510868012905121\n",
      "[step: 2488] loss: 0.2509424388408661\n",
      "[step: 2489] loss: 0.2507967948913574\n",
      "[step: 2490] loss: 0.25064459443092346\n",
      "[step: 2491] loss: 0.25048258900642395\n",
      "[step: 2492] loss: 0.2503119707107544\n",
      "[step: 2493] loss: 0.25013628602027893\n",
      "[step: 2494] loss: 0.24995791912078857\n",
      "[step: 2495] loss: 0.24978101253509521\n",
      "[step: 2496] loss: 0.24960605800151825\n",
      "[step: 2497] loss: 0.24943527579307556\n",
      "[step: 2498] loss: 0.2492673546075821\n",
      "[step: 2499] loss: 0.24910354614257812\n",
      "[step: 2500] loss: 0.2489408254623413\n",
      "[step: 2501] loss: 0.24877947568893433\n",
      "[step: 2502] loss: 0.2486189901828766\n",
      "[step: 2503] loss: 0.24845993518829346\n",
      "[step: 2504] loss: 0.24830132722854614\n",
      "[step: 2505] loss: 0.2481432855129242\n",
      "[step: 2506] loss: 0.24798694252967834\n",
      "[step: 2507] loss: 0.24783319234848022\n",
      "[step: 2508] loss: 0.24768367409706116\n",
      "[step: 2509] loss: 0.24753984808921814\n",
      "[step: 2510] loss: 0.2474062591791153\n",
      "[step: 2511] loss: 0.24728696048259735\n",
      "[step: 2512] loss: 0.24719536304473877\n",
      "[step: 2513] loss: 0.24714331328868866\n",
      "[step: 2514] loss: 0.24716100096702576\n",
      "[step: 2515] loss: 0.24728785455226898\n",
      "[step: 2516] loss: 0.24761275947093964\n",
      "[step: 2517] loss: 0.24823163449764252\n",
      "[step: 2518] loss: 0.24939963221549988\n",
      "[step: 2519] loss: 0.2512674927711487\n",
      "[step: 2520] loss: 0.25439298152923584\n",
      "[step: 2521] loss: 0.25836750864982605\n",
      "[step: 2522] loss: 0.26345154643058777\n",
      "[step: 2523] loss: 0.26614123582839966\n",
      "[step: 2524] loss: 0.26558834314346313\n",
      "[step: 2525] loss: 0.25868678092956543\n",
      "[step: 2526] loss: 0.25021445751190186\n",
      "[step: 2527] loss: 0.24497033655643463\n",
      "[step: 2528] loss: 0.2457352727651596\n",
      "[step: 2529] loss: 0.25019532442092896\n",
      "[step: 2530] loss: 0.25344935059547424\n",
      "[step: 2531] loss: 0.2527150511741638\n",
      "[step: 2532] loss: 0.24830412864685059\n",
      "[step: 2533] loss: 0.24442924559116364\n",
      "[step: 2534] loss: 0.24387922883033752\n",
      "[step: 2535] loss: 0.24609586596488953\n",
      "[step: 2536] loss: 0.24822074174880981\n",
      "[step: 2537] loss: 0.24783353507518768\n",
      "[step: 2538] loss: 0.2454945296049118\n",
      "[step: 2539] loss: 0.24326522648334503\n",
      "[step: 2540] loss: 0.24288500845432281\n",
      "[step: 2541] loss: 0.24405157566070557\n",
      "[step: 2542] loss: 0.24511265754699707\n",
      "[step: 2543] loss: 0.24489480257034302\n",
      "[step: 2544] loss: 0.24352431297302246\n",
      "[step: 2545] loss: 0.24224096536636353\n",
      "[step: 2546] loss: 0.2419222742319107\n",
      "[step: 2547] loss: 0.24245195090770721\n",
      "[step: 2548] loss: 0.24301114678382874\n",
      "[step: 2549] loss: 0.24289283156394958\n",
      "[step: 2550] loss: 0.24216045439243317\n",
      "[step: 2551] loss: 0.24135079979896545\n",
      "[step: 2552] loss: 0.24098283052444458\n",
      "[step: 2553] loss: 0.24110642075538635\n",
      "[step: 2554] loss: 0.24137164652347565\n",
      "[step: 2555] loss: 0.24139901995658875\n",
      "[step: 2556] loss: 0.2410605102777481\n",
      "[step: 2557] loss: 0.24055181443691254\n",
      "[step: 2558] loss: 0.24014054238796234\n",
      "[step: 2559] loss: 0.2399764060974121\n",
      "[step: 2560] loss: 0.2400049865245819\n",
      "[step: 2561] loss: 0.2400514781475067\n",
      "[step: 2562] loss: 0.23996973037719727\n",
      "[step: 2563] loss: 0.2397252917289734\n",
      "[step: 2564] loss: 0.23940438032150269\n",
      "[step: 2565] loss: 0.2391137182712555\n",
      "[step: 2566] loss: 0.23892506957054138\n",
      "[step: 2567] loss: 0.2388339340686798\n",
      "[step: 2568] loss: 0.23878225684165955\n",
      "[step: 2569] loss: 0.23870527744293213\n",
      "[step: 2570] loss: 0.23856398463249207\n",
      "[step: 2571] loss: 0.23836749792099\n",
      "[step: 2572] loss: 0.2381422370672226\n",
      "[step: 2573] loss: 0.23792827129364014\n",
      "[step: 2574] loss: 0.2377493679523468\n",
      "[step: 2575] loss: 0.2376086711883545\n",
      "[step: 2576] loss: 0.237493097782135\n",
      "[step: 2577] loss: 0.2373834252357483\n",
      "[step: 2578] loss: 0.2372639924287796\n",
      "[step: 2579] loss: 0.2371254414319992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2580] loss: 0.2369678020477295\n",
      "[step: 2581] loss: 0.23679739236831665\n",
      "[step: 2582] loss: 0.23662233352661133\n",
      "[step: 2583] loss: 0.23644976317882538\n",
      "[step: 2584] loss: 0.2362852543592453\n",
      "[step: 2585] loss: 0.2361292988061905\n",
      "[step: 2586] loss: 0.23598140478134155\n",
      "[step: 2587] loss: 0.23584002256393433\n",
      "[step: 2588] loss: 0.235702782869339\n",
      "[step: 2589] loss: 0.2355654239654541\n",
      "[step: 2590] loss: 0.2354293167591095\n",
      "[step: 2591] loss: 0.23529204726219177\n",
      "[step: 2592] loss: 0.23515450954437256\n",
      "[step: 2593] loss: 0.23501697182655334\n",
      "[step: 2594] loss: 0.2348783016204834\n",
      "[step: 2595] loss: 0.23474177718162537\n",
      "[step: 2596] loss: 0.23460733890533447\n",
      "[step: 2597] loss: 0.23447749018669128\n",
      "[step: 2598] loss: 0.23435421288013458\n",
      "[step: 2599] loss: 0.23424120247364044\n",
      "[step: 2600] loss: 0.23414331674575806\n",
      "[step: 2601] loss: 0.23406989872455597\n",
      "[step: 2602] loss: 0.23403142392635345\n",
      "[step: 2603] loss: 0.23405325412750244\n",
      "[step: 2604] loss: 0.2341606318950653\n",
      "[step: 2605] loss: 0.23442213237285614\n",
      "[step: 2606] loss: 0.2348976880311966\n",
      "[step: 2607] loss: 0.23576213419437408\n",
      "[step: 2608] loss: 0.23712018132209778\n",
      "[step: 2609] loss: 0.2393568754196167\n",
      "[step: 2610] loss: 0.2423660010099411\n",
      "[step: 2611] loss: 0.2465662956237793\n",
      "[step: 2612] loss: 0.2502292990684509\n",
      "[step: 2613] loss: 0.2528255879878998\n",
      "[step: 2614] loss: 0.25049465894699097\n",
      "[step: 2615] loss: 0.24453401565551758\n",
      "[step: 2616] loss: 0.2367064654827118\n",
      "[step: 2617] loss: 0.23196738958358765\n",
      "[step: 2618] loss: 0.23224252462387085\n",
      "[step: 2619] loss: 0.23587772250175476\n",
      "[step: 2620] loss: 0.2393674999475479\n",
      "[step: 2621] loss: 0.2395363301038742\n",
      "[step: 2622] loss: 0.236594557762146\n",
      "[step: 2623] loss: 0.23261040449142456\n",
      "[step: 2624] loss: 0.2306365668773651\n",
      "[step: 2625] loss: 0.23145848512649536\n",
      "[step: 2626] loss: 0.23355039954185486\n",
      "[step: 2627] loss: 0.23478707671165466\n",
      "[step: 2628] loss: 0.23394247889518738\n",
      "[step: 2629] loss: 0.2318740040063858\n",
      "[step: 2630] loss: 0.230124831199646\n",
      "[step: 2631] loss: 0.22980359196662903\n",
      "[step: 2632] loss: 0.2306579053401947\n",
      "[step: 2633] loss: 0.23157232999801636\n",
      "[step: 2634] loss: 0.23165646195411682\n",
      "[step: 2635] loss: 0.2307615578174591\n",
      "[step: 2636] loss: 0.229605033993721\n",
      "[step: 2637] loss: 0.22891803085803986\n",
      "[step: 2638] loss: 0.22896713018417358\n",
      "[step: 2639] loss: 0.22942057251930237\n",
      "[step: 2640] loss: 0.2297232449054718\n",
      "[step: 2641] loss: 0.2295697033405304\n",
      "[step: 2642] loss: 0.22900158166885376\n",
      "[step: 2643] loss: 0.22836849093437195\n",
      "[step: 2644] loss: 0.22797337174415588\n",
      "[step: 2645] loss: 0.2279128134250641\n",
      "[step: 2646] loss: 0.22804906964302063\n",
      "[step: 2647] loss: 0.22815507650375366\n",
      "[step: 2648] loss: 0.22808098793029785\n",
      "[step: 2649] loss: 0.22780205309391022\n",
      "[step: 2650] loss: 0.227430060505867\n",
      "[step: 2651] loss: 0.22709570825099945\n",
      "[step: 2652] loss: 0.22688636183738708\n",
      "[step: 2653] loss: 0.226805180311203\n",
      "[step: 2654] loss: 0.22678950428962708\n",
      "[step: 2655] loss: 0.22676359117031097\n",
      "[step: 2656] loss: 0.2266710251569748\n",
      "[step: 2657] loss: 0.22650310397148132\n",
      "[step: 2658] loss: 0.22628048062324524\n",
      "[step: 2659] loss: 0.22604665160179138\n",
      "[step: 2660] loss: 0.22583653032779694\n",
      "[step: 2661] loss: 0.22566793859004974\n",
      "[step: 2662] loss: 0.22554215788841248\n",
      "[step: 2663] loss: 0.2254430502653122\n",
      "[step: 2664] loss: 0.22535234689712524\n",
      "[step: 2665] loss: 0.22525417804718018\n",
      "[step: 2666] loss: 0.22513899207115173\n",
      "[step: 2667] loss: 0.22500264644622803\n",
      "[step: 2668] loss: 0.2248508632183075\n",
      "[step: 2669] loss: 0.22468803822994232\n",
      "[step: 2670] loss: 0.22452175617218018\n",
      "[step: 2671] loss: 0.22435703873634338\n",
      "[step: 2672] loss: 0.22419685125350952\n",
      "[step: 2673] loss: 0.2240433394908905\n",
      "[step: 2674] loss: 0.22389653325080872\n",
      "[step: 2675] loss: 0.2237548828125\n",
      "[step: 2676] loss: 0.22361811995506287\n",
      "[step: 2677] loss: 0.22348354756832123\n",
      "[step: 2678] loss: 0.22335194051265717\n",
      "[step: 2679] loss: 0.22322173416614532\n",
      "[step: 2680] loss: 0.2230931669473648\n",
      "[step: 2681] loss: 0.22296717762947083\n",
      "[step: 2682] loss: 0.2228439748287201\n",
      "[step: 2683] loss: 0.2227254956960678\n",
      "[step: 2684] loss: 0.22261403501033783\n",
      "[step: 2685] loss: 0.2225143015384674\n",
      "[step: 2686] loss: 0.2224312126636505\n",
      "[step: 2687] loss: 0.2223777323961258\n",
      "[step: 2688] loss: 0.22236880660057068\n",
      "[step: 2689] loss: 0.22243675589561462\n",
      "[step: 2690] loss: 0.22262242436408997\n",
      "[step: 2691] loss: 0.22301629185676575\n",
      "[step: 2692] loss: 0.22372181713581085\n",
      "[step: 2693] loss: 0.2249944806098938\n",
      "[step: 2694] loss: 0.22701866924762726\n",
      "[step: 2695] loss: 0.2303716093301773\n",
      "[step: 2696] loss: 0.2348613142967224\n",
      "[step: 2697] loss: 0.24092598259449005\n",
      "[step: 2698] loss: 0.2454593926668167\n",
      "[step: 2699] loss: 0.24720074236392975\n",
      "[step: 2700] loss: 0.2411688268184662\n",
      "[step: 2701] loss: 0.23114128410816193\n",
      "[step: 2702] loss: 0.2222776710987091\n",
      "[step: 2703] loss: 0.22028908133506775\n",
      "[step: 2704] loss: 0.22460898756980896\n",
      "[step: 2705] loss: 0.22992508113384247\n",
      "[step: 2706] loss: 0.23137129843235016\n",
      "[step: 2707] loss: 0.2272358238697052\n",
      "[step: 2708] loss: 0.22173480689525604\n",
      "[step: 2709] loss: 0.21927091479301453\n",
      "[step: 2710] loss: 0.22102954983711243\n",
      "[step: 2711] loss: 0.22419142723083496\n",
      "[step: 2712] loss: 0.22506539523601532\n",
      "[step: 2713] loss: 0.22297018766403198\n",
      "[step: 2714] loss: 0.21985743939876556\n",
      "[step: 2715] loss: 0.21852564811706543\n",
      "[step: 2716] loss: 0.21952268481254578\n",
      "[step: 2717] loss: 0.22116123139858246\n",
      "[step: 2718] loss: 0.22154992818832397\n",
      "[step: 2719] loss: 0.22021660208702087\n",
      "[step: 2720] loss: 0.21849828958511353\n",
      "[step: 2721] loss: 0.21777069568634033\n",
      "[step: 2722] loss: 0.21827787160873413\n",
      "[step: 2723] loss: 0.2191198766231537\n",
      "[step: 2724] loss: 0.21926546096801758\n",
      "[step: 2725] loss: 0.21854904294013977\n",
      "[step: 2726] loss: 0.217542827129364\n",
      "[step: 2727] loss: 0.2169998735189438\n",
      "[step: 2728] loss: 0.21711398661136627\n",
      "[step: 2729] loss: 0.2175072580575943\n",
      "[step: 2730] loss: 0.21766602993011475\n",
      "[step: 2731] loss: 0.21735377609729767\n",
      "[step: 2732] loss: 0.2167755663394928\n",
      "[step: 2733] loss: 0.21628792583942413\n",
      "[step: 2734] loss: 0.21611422300338745\n",
      "[step: 2735] loss: 0.2162003219127655\n",
      "[step: 2736] loss: 0.21631550788879395\n",
      "[step: 2737] loss: 0.21626418828964233\n",
      "[step: 2738] loss: 0.2160005122423172\n",
      "[step: 2739] loss: 0.21564355492591858\n",
      "[step: 2740] loss: 0.21534278988838196\n",
      "[step: 2741] loss: 0.21518462896347046\n",
      "[step: 2742] loss: 0.21514524519443512\n",
      "[step: 2743] loss: 0.21513649821281433\n",
      "[step: 2744] loss: 0.21507182717323303\n",
      "[step: 2745] loss: 0.21491439640522003\n",
      "[step: 2746] loss: 0.2146923840045929\n",
      "[step: 2747] loss: 0.21445858478546143\n",
      "[step: 2748] loss: 0.21426331996917725\n",
      "[step: 2749] loss: 0.2141249179840088\n",
      "[step: 2750] loss: 0.21403123438358307\n",
      "[step: 2751] loss: 0.21395114064216614\n",
      "[step: 2752] loss: 0.2138553112745285\n",
      "[step: 2753] loss: 0.21372951567173004\n",
      "[step: 2754] loss: 0.2135726660490036\n",
      "[step: 2755] loss: 0.21339955925941467\n",
      "[step: 2756] loss: 0.2132258117198944\n",
      "[step: 2757] loss: 0.21306326985359192\n",
      "[step: 2758] loss: 0.21291792392730713\n",
      "[step: 2759] loss: 0.2127881497144699\n",
      "[step: 2760] loss: 0.2126694768667221\n",
      "[step: 2761] loss: 0.21255283057689667\n",
      "[step: 2762] loss: 0.21243348717689514\n",
      "[step: 2763] loss: 0.21230775117874146\n",
      "[step: 2764] loss: 0.21217605471611023\n",
      "[step: 2765] loss: 0.21203631162643433\n",
      "[step: 2766] loss: 0.21189334988594055\n",
      "[step: 2767] loss: 0.21174690127372742\n",
      "[step: 2768] loss: 0.2116001844406128\n",
      "[step: 2769] loss: 0.21145448088645935\n",
      "[step: 2770] loss: 0.2113099843263626\n",
      "[step: 2771] loss: 0.2111661434173584\n",
      "[step: 2772] loss: 0.21102501451969147\n",
      "[step: 2773] loss: 0.2108839750289917\n",
      "[step: 2774] loss: 0.21074481308460236\n",
      "[step: 2775] loss: 0.2106054425239563\n",
      "[step: 2776] loss: 0.21046698093414307\n",
      "[step: 2777] loss: 0.2103293389081955\n",
      "[step: 2778] loss: 0.21019050478935242\n",
      "[step: 2779] loss: 0.21005262434482574\n",
      "[step: 2780] loss: 0.20991548895835876\n",
      "[step: 2781] loss: 0.20977619290351868\n",
      "[step: 2782] loss: 0.2096382975578308\n",
      "[step: 2783] loss: 0.2095005363225937\n",
      "[step: 2784] loss: 0.20936302840709686\n",
      "[step: 2785] loss: 0.20922653377056122\n",
      "[step: 2786] loss: 0.20909208059310913\n",
      "[step: 2787] loss: 0.20896142721176147\n",
      "[step: 2788] loss: 0.20883628726005554\n",
      "[step: 2789] loss: 0.20872202515602112\n",
      "[step: 2790] loss: 0.20862923562526703\n",
      "[step: 2791] loss: 0.20857301354408264\n",
      "[step: 2792] loss: 0.20858697593212128\n",
      "[step: 2793] loss: 0.20873767137527466\n",
      "[step: 2794] loss: 0.20913954079151154\n",
      "[step: 2795] loss: 0.210048109292984\n",
      "[step: 2796] loss: 0.21186429262161255\n",
      "[step: 2797] loss: 0.2155137062072754\n",
      "[step: 2798] loss: 0.2219436764717102\n",
      "[step: 2799] loss: 0.233298659324646\n",
      "[step: 2800] loss: 0.24724812805652618\n",
      "[step: 2801] loss: 0.2609941363334656\n",
      "[step: 2802] loss: 0.25621187686920166\n",
      "[step: 2803] loss: 0.23493491113185883\n",
      "[step: 2804] loss: 0.21139244735240936\n",
      "[step: 2805] loss: 0.20877015590667725\n",
      "[step: 2806] loss: 0.2237270027399063\n",
      "[step: 2807] loss: 0.23275844752788544\n",
      "[step: 2808] loss: 0.22391490638256073\n",
      "[step: 2809] loss: 0.20890061557292938\n",
      "[step: 2810] loss: 0.20776602625846863\n",
      "[step: 2811] loss: 0.21765880286693573\n",
      "[step: 2812] loss: 0.2209138572216034\n",
      "[step: 2813] loss: 0.21296164393424988\n",
      "[step: 2814] loss: 0.2057669758796692\n",
      "[step: 2815] loss: 0.20884938538074493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2816] loss: 0.21469059586524963\n",
      "[step: 2817] loss: 0.2127014547586441\n",
      "[step: 2818] loss: 0.2065628170967102\n",
      "[step: 2819] loss: 0.20545434951782227\n",
      "[step: 2820] loss: 0.2092979997396469\n",
      "[step: 2821] loss: 0.2106563150882721\n",
      "[step: 2822] loss: 0.20711810886859894\n",
      "[step: 2823] loss: 0.20446047186851501\n",
      "[step: 2824] loss: 0.20599156618118286\n",
      "[step: 2825] loss: 0.20801006257534027\n",
      "[step: 2826] loss: 0.20685537159442902\n",
      "[step: 2827] loss: 0.20434218645095825\n",
      "[step: 2828] loss: 0.20413726568222046\n",
      "[step: 2829] loss: 0.20566098392009735\n",
      "[step: 2830] loss: 0.20592495799064636\n",
      "[step: 2831] loss: 0.20442655682563782\n",
      "[step: 2832] loss: 0.20330557227134705\n",
      "[step: 2833] loss: 0.20380271971225739\n",
      "[step: 2834] loss: 0.2045784294605255\n",
      "[step: 2835] loss: 0.20413058996200562\n",
      "[step: 2836] loss: 0.2030409872531891\n",
      "[step: 2837] loss: 0.20271143317222595\n",
      "[step: 2838] loss: 0.20317983627319336\n",
      "[step: 2839] loss: 0.20338773727416992\n",
      "[step: 2840] loss: 0.20283320546150208\n",
      "[step: 2841] loss: 0.2021869271993637\n",
      "[step: 2842] loss: 0.20210915803909302\n",
      "[step: 2843] loss: 0.2023714780807495\n",
      "[step: 2844] loss: 0.20233556628227234\n",
      "[step: 2845] loss: 0.20188090205192566\n",
      "[step: 2846] loss: 0.2014857828617096\n",
      "[step: 2847] loss: 0.20144133269786835\n",
      "[step: 2848] loss: 0.2015388011932373\n",
      "[step: 2849] loss: 0.201432466506958\n",
      "[step: 2850] loss: 0.2011020928621292\n",
      "[step: 2851] loss: 0.20081400871276855\n",
      "[step: 2852] loss: 0.2007337510585785\n",
      "[step: 2853] loss: 0.20073875784873962\n",
      "[step: 2854] loss: 0.20062893629074097\n",
      "[step: 2855] loss: 0.2003837525844574\n",
      "[step: 2856] loss: 0.20014721155166626\n",
      "[step: 2857] loss: 0.20002517104148865\n",
      "[step: 2858] loss: 0.19997012615203857\n",
      "[step: 2859] loss: 0.1998712420463562\n",
      "[step: 2860] loss: 0.19968640804290771\n",
      "[step: 2861] loss: 0.19947977364063263\n",
      "[step: 2862] loss: 0.1993257999420166\n",
      "[step: 2863] loss: 0.19922757148742676\n",
      "[step: 2864] loss: 0.19912879168987274\n",
      "[step: 2865] loss: 0.19898615777492523\n",
      "[step: 2866] loss: 0.19880957901477814\n",
      "[step: 2867] loss: 0.19864097237586975\n",
      "[step: 2868] loss: 0.1985064148902893\n",
      "[step: 2869] loss: 0.19839340448379517\n",
      "[step: 2870] loss: 0.19827055931091309\n",
      "[step: 2871] loss: 0.19812336564064026\n",
      "[step: 2872] loss: 0.19796179234981537\n",
      "[step: 2873] loss: 0.19780734181404114\n",
      "[step: 2874] loss: 0.19767004251480103\n",
      "[step: 2875] loss: 0.19754338264465332\n",
      "[step: 2876] loss: 0.19741244614124298\n",
      "[step: 2877] loss: 0.19726994633674622\n",
      "[step: 2878] loss: 0.1971181035041809\n",
      "[step: 2879] loss: 0.19696712493896484\n",
      "[step: 2880] loss: 0.1968238353729248\n",
      "[step: 2881] loss: 0.19668729603290558\n",
      "[step: 2882] loss: 0.19655254483222961\n",
      "[step: 2883] loss: 0.19641229510307312\n",
      "[step: 2884] loss: 0.19626659154891968\n",
      "[step: 2885] loss: 0.1961180567741394\n",
      "[step: 2886] loss: 0.19597087800502777\n",
      "[step: 2887] loss: 0.19582685828208923\n",
      "[step: 2888] loss: 0.19568566977977753\n",
      "[step: 2889] loss: 0.19554558396339417\n",
      "[step: 2890] loss: 0.19540277123451233\n",
      "[step: 2891] loss: 0.1952572464942932\n",
      "[step: 2892] loss: 0.19511020183563232\n",
      "[step: 2893] loss: 0.19496282935142517\n",
      "[step: 2894] loss: 0.19481605291366577\n",
      "[step: 2895] loss: 0.19467033445835114\n",
      "[step: 2896] loss: 0.19452553987503052\n",
      "[step: 2897] loss: 0.19438064098358154\n",
      "[step: 2898] loss: 0.1942349076271057\n",
      "[step: 2899] loss: 0.1940879225730896\n",
      "[step: 2900] loss: 0.19394081830978394\n",
      "[step: 2901] loss: 0.19379225373268127\n",
      "[step: 2902] loss: 0.19364316761493683\n",
      "[step: 2903] loss: 0.1934947520494461\n",
      "[step: 2904] loss: 0.19334615767002106\n",
      "[step: 2905] loss: 0.19319748878479004\n",
      "[step: 2906] loss: 0.19304922223091125\n",
      "[step: 2907] loss: 0.1928999423980713\n",
      "[step: 2908] loss: 0.19275090098381042\n",
      "[step: 2909] loss: 0.19260118901729584\n",
      "[step: 2910] loss: 0.1924516260623932\n",
      "[step: 2911] loss: 0.1923007071018219\n",
      "[step: 2912] loss: 0.19214946031570435\n",
      "[step: 2913] loss: 0.19199806451797485\n",
      "[step: 2914] loss: 0.1918463110923767\n",
      "[step: 2915] loss: 0.1916939616203308\n",
      "[step: 2916] loss: 0.1915418803691864\n",
      "[step: 2917] loss: 0.1913893222808838\n",
      "[step: 2918] loss: 0.19123613834381104\n",
      "[step: 2919] loss: 0.19108298420906067\n",
      "[step: 2920] loss: 0.19092947244644165\n",
      "[step: 2921] loss: 0.19077551364898682\n",
      "[step: 2922] loss: 0.19062164425849915\n",
      "[step: 2923] loss: 0.19046702980995178\n",
      "[step: 2924] loss: 0.190312460064888\n",
      "[step: 2925] loss: 0.19015716016292572\n",
      "[step: 2926] loss: 0.19000175595283508\n",
      "[step: 2927] loss: 0.18984603881835938\n",
      "[step: 2928] loss: 0.1896894872188568\n",
      "[step: 2929] loss: 0.18953339755535126\n",
      "[step: 2930] loss: 0.1893765777349472\n",
      "[step: 2931] loss: 0.1892193853855133\n",
      "[step: 2932] loss: 0.18906164169311523\n",
      "[step: 2933] loss: 0.1889037936925888\n",
      "[step: 2934] loss: 0.18874506652355194\n",
      "[step: 2935] loss: 0.1885872781276703\n",
      "[step: 2936] loss: 0.18842825293540955\n",
      "[step: 2937] loss: 0.18826982378959656\n",
      "[step: 2938] loss: 0.18811087310314178\n",
      "[step: 2939] loss: 0.18795163929462433\n",
      "[step: 2940] loss: 0.18779250979423523\n",
      "[step: 2941] loss: 0.18763431906700134\n",
      "[step: 2942] loss: 0.18747925758361816\n",
      "[step: 2943] loss: 0.18733017146587372\n",
      "[step: 2944] loss: 0.18719211220741272\n",
      "[step: 2945] loss: 0.1870807707309723\n",
      "[step: 2946] loss: 0.18702377378940582\n",
      "[step: 2947] loss: 0.18708863854408264\n",
      "[step: 2948] loss: 0.18741591274738312\n",
      "[step: 2949] loss: 0.1883387565612793\n",
      "[step: 2950] loss: 0.19052660465240479\n",
      "[step: 2951] loss: 0.19560939073562622\n",
      "[step: 2952] loss: 0.20608079433441162\n",
      "[step: 2953] loss: 0.22728495299816132\n",
      "[step: 2954] loss: 0.2561977505683899\n",
      "[step: 2955] loss: 0.28384077548980713\n",
      "[step: 2956] loss: 0.263071745634079\n",
      "[step: 2957] loss: 0.21226896345615387\n",
      "[step: 2958] loss: 0.18621467053890228\n",
      "[step: 2959] loss: 0.2110593318939209\n",
      "[step: 2960] loss: 0.23639614880084991\n",
      "[step: 2961] loss: 0.21387618780136108\n",
      "[step: 2962] loss: 0.185733824968338\n",
      "[step: 2963] loss: 0.19751350581645966\n",
      "[step: 2964] loss: 0.2158454954624176\n",
      "[step: 2965] loss: 0.2027711570262909\n",
      "[step: 2966] loss: 0.18487338721752167\n",
      "[step: 2967] loss: 0.1950191855430603\n",
      "[step: 2968] loss: 0.20530259609222412\n",
      "[step: 2969] loss: 0.19233174622058868\n",
      "[step: 2970] loss: 0.184226855635643\n",
      "[step: 2971] loss: 0.19394917786121368\n",
      "[step: 2972] loss: 0.19635407626628876\n",
      "[step: 2973] loss: 0.18595069646835327\n",
      "[step: 2974] loss: 0.18467816710472107\n",
      "[step: 2975] loss: 0.1917620301246643\n",
      "[step: 2976] loss: 0.18976318836212158\n",
      "[step: 2977] loss: 0.18309786915779114\n",
      "[step: 2978] loss: 0.18529082834720612\n",
      "[step: 2979] loss: 0.18905720114707947\n",
      "[step: 2980] loss: 0.18528077006340027\n",
      "[step: 2981] loss: 0.1822301745414734\n",
      "[step: 2982] loss: 0.18502244353294373\n",
      "[step: 2983] loss: 0.186111181974411\n",
      "[step: 2984] loss: 0.18278145790100098\n",
      "[step: 2985] loss: 0.18187715113162994\n",
      "[step: 2986] loss: 0.1841142624616623\n",
      "[step: 2987] loss: 0.18373475968837738\n",
      "[step: 2988] loss: 0.18141421675682068\n",
      "[step: 2989] loss: 0.18161505460739136\n",
      "[step: 2990] loss: 0.18291983008384705\n",
      "[step: 2991] loss: 0.18198144435882568\n",
      "[step: 2992] loss: 0.18061186373233795\n",
      "[step: 2993] loss: 0.18108567595481873\n",
      "[step: 2994] loss: 0.18169881403446198\n",
      "[step: 2995] loss: 0.18078789114952087\n",
      "[step: 2996] loss: 0.1799767017364502\n",
      "[step: 2997] loss: 0.18043777346611023\n",
      "[step: 2998] loss: 0.1806303858757019\n",
      "[step: 2999] loss: 0.17985492944717407\n",
      "[step: 3000] loss: 0.17941629886627197\n",
      "[step: 3001] loss: 0.17972415685653687\n",
      "[step: 3002] loss: 0.17969265580177307\n",
      "[step: 3003] loss: 0.1791098564863205\n",
      "[step: 3004] loss: 0.17882844805717468\n",
      "[step: 3005] loss: 0.17899248003959656\n",
      "[step: 3006] loss: 0.17889440059661865\n",
      "[step: 3007] loss: 0.17843839526176453\n",
      "[step: 3008] loss: 0.17822638154029846\n",
      "[step: 3009] loss: 0.1782885491847992\n",
      "[step: 3010] loss: 0.17815789580345154\n",
      "[step: 3011] loss: 0.17780977487564087\n",
      "[step: 3012] loss: 0.17761291563510895\n",
      "[step: 3013] loss: 0.17759600281715393\n",
      "[step: 3014] loss: 0.17747154831886292\n",
      "[step: 3015] loss: 0.17719382047653198\n",
      "[step: 3016] loss: 0.1769922375679016\n",
      "[step: 3017] loss: 0.1769263595342636\n",
      "[step: 3018] loss: 0.1768057942390442\n",
      "[step: 3019] loss: 0.17657458782196045\n",
      "[step: 3020] loss: 0.1763751208782196\n",
      "[step: 3021] loss: 0.17626887559890747\n",
      "[step: 3022] loss: 0.17615072429180145\n",
      "[step: 3023] loss: 0.17595615983009338\n",
      "[step: 3024] loss: 0.17575809359550476\n",
      "[step: 3025] loss: 0.1756206750869751\n",
      "[step: 3026] loss: 0.17549985647201538\n",
      "[step: 3027] loss: 0.17533046007156372\n",
      "[step: 3028] loss: 0.17514070868492126\n",
      "[step: 3029] loss: 0.17498356103897095\n",
      "[step: 3030] loss: 0.17485064268112183\n",
      "[step: 3031] loss: 0.17469753324985504\n",
      "[step: 3032] loss: 0.17451998591423035\n",
      "[step: 3033] loss: 0.17435023188591003\n",
      "[step: 3034] loss: 0.17420314252376556\n",
      "[step: 3035] loss: 0.1740567535161972\n",
      "[step: 3036] loss: 0.1738920956850052\n",
      "[step: 3037] loss: 0.17371979355812073\n",
      "[step: 3038] loss: 0.17356041073799133\n",
      "[step: 3039] loss: 0.17341092228889465\n",
      "[step: 3040] loss: 0.17325422167778015\n",
      "[step: 3041] loss: 0.1730872541666031\n",
      "[step: 3042] loss: 0.17292097210884094\n",
      "[step: 3043] loss: 0.17276287078857422\n",
      "[step: 3044] loss: 0.17260780930519104\n",
      "[step: 3045] loss: 0.17244715988636017\n",
      "[step: 3046] loss: 0.172280415892601\n",
      "[step: 3047] loss: 0.1721159815788269\n",
      "[step: 3048] loss: 0.17195691168308258\n",
      "[step: 3049] loss: 0.17179760336875916\n",
      "[step: 3050] loss: 0.171635240316391\n",
      "[step: 3051] loss: 0.17147007584571838\n",
      "[step: 3052] loss: 0.17130523920059204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3053] loss: 0.17114385962486267\n",
      "[step: 3054] loss: 0.17098194360733032\n",
      "[step: 3055] loss: 0.17081871628761292\n",
      "[step: 3056] loss: 0.1706530749797821\n",
      "[step: 3057] loss: 0.17048753798007965\n",
      "[step: 3058] loss: 0.17032337188720703\n",
      "[step: 3059] loss: 0.1701599657535553\n",
      "[step: 3060] loss: 0.16999545693397522\n",
      "[step: 3061] loss: 0.16982921957969666\n",
      "[step: 3062] loss: 0.169662207365036\n",
      "[step: 3063] loss: 0.1694970279932022\n",
      "[step: 3064] loss: 0.16933171451091766\n",
      "[step: 3065] loss: 0.16916555166244507\n",
      "[step: 3066] loss: 0.16899915039539337\n",
      "[step: 3067] loss: 0.16883191466331482\n",
      "[step: 3068] loss: 0.16866426169872284\n",
      "[step: 3069] loss: 0.16849657893180847\n",
      "[step: 3070] loss: 0.16832950711250305\n",
      "[step: 3071] loss: 0.1681618094444275\n",
      "[step: 3072] loss: 0.16799357533454895\n",
      "[step: 3073] loss: 0.16782483458518982\n",
      "[step: 3074] loss: 0.16765588521957397\n",
      "[step: 3075] loss: 0.16748690605163574\n",
      "[step: 3076] loss: 0.1673177182674408\n",
      "[step: 3077] loss: 0.1671489030122757\n",
      "[step: 3078] loss: 0.16697880625724792\n",
      "[step: 3079] loss: 0.16680878400802612\n",
      "[step: 3080] loss: 0.16663798689842224\n",
      "[step: 3081] loss: 0.16646772623062134\n",
      "[step: 3082] loss: 0.16629692912101746\n",
      "[step: 3083] loss: 0.16612567007541656\n",
      "[step: 3084] loss: 0.16595475375652313\n",
      "[step: 3085] loss: 0.16578355431556702\n",
      "[step: 3086] loss: 0.16561153531074524\n",
      "[step: 3087] loss: 0.16543906927108765\n",
      "[step: 3088] loss: 0.1652670055627823\n",
      "[step: 3089] loss: 0.1650940179824829\n",
      "[step: 3090] loss: 0.16492176055908203\n",
      "[step: 3091] loss: 0.16474859416484833\n",
      "[step: 3092] loss: 0.16457495093345642\n",
      "[step: 3093] loss: 0.16440175473690033\n",
      "[step: 3094] loss: 0.16422787308692932\n",
      "[step: 3095] loss: 0.16405370831489563\n",
      "[step: 3096] loss: 0.16387972235679626\n",
      "[step: 3097] loss: 0.16370487213134766\n",
      "[step: 3098] loss: 0.163530170917511\n",
      "[step: 3099] loss: 0.16335558891296387\n",
      "[step: 3100] loss: 0.16317996382713318\n",
      "[step: 3101] loss: 0.16300436854362488\n",
      "[step: 3102] loss: 0.1628284454345703\n",
      "[step: 3103] loss: 0.16265282034873962\n",
      "[step: 3104] loss: 0.16247642040252686\n",
      "[step: 3105] loss: 0.16230042278766632\n",
      "[step: 3106] loss: 0.1621236503124237\n",
      "[step: 3107] loss: 0.16194605827331543\n",
      "[step: 3108] loss: 0.16176915168762207\n",
      "[step: 3109] loss: 0.16159150004386902\n",
      "[step: 3110] loss: 0.16141393780708313\n",
      "[step: 3111] loss: 0.16123642027378082\n",
      "[step: 3112] loss: 0.16105809807777405\n",
      "[step: 3113] loss: 0.16087990999221802\n",
      "[step: 3114] loss: 0.1607014238834381\n",
      "[step: 3115] loss: 0.1605226993560791\n",
      "[step: 3116] loss: 0.1603432446718216\n",
      "[step: 3117] loss: 0.16016434133052826\n",
      "[step: 3118] loss: 0.15998491644859314\n",
      "[step: 3119] loss: 0.15980468690395355\n",
      "[step: 3120] loss: 0.15962500870227814\n",
      "[step: 3121] loss: 0.159444659948349\n",
      "[step: 3122] loss: 0.15926489233970642\n",
      "[step: 3123] loss: 0.15908454358577728\n",
      "[step: 3124] loss: 0.1589033156633377\n",
      "[step: 3125] loss: 0.15872293710708618\n",
      "[step: 3126] loss: 0.15854096412658691\n",
      "[step: 3127] loss: 0.1583600640296936\n",
      "[step: 3128] loss: 0.15817850828170776\n",
      "[step: 3129] loss: 0.15799680352210999\n",
      "[step: 3130] loss: 0.15781468152999878\n",
      "[step: 3131] loss: 0.15763333439826965\n",
      "[step: 3132] loss: 0.15745139122009277\n",
      "[step: 3133] loss: 0.15726983547210693\n",
      "[step: 3134] loss: 0.15708842873573303\n",
      "[step: 3135] loss: 0.15690891444683075\n",
      "[step: 3136] loss: 0.15672987699508667\n",
      "[step: 3137] loss: 0.15655580163002014\n",
      "[step: 3138] loss: 0.15639010071754456\n",
      "[step: 3139] loss: 0.15624013543128967\n",
      "[step: 3140] loss: 0.1561192274093628\n",
      "[step: 3141] loss: 0.1560603380203247\n",
      "[step: 3142] loss: 0.15612280368804932\n",
      "[step: 3143] loss: 0.15643779933452606\n",
      "[step: 3144] loss: 0.15725377202033997\n",
      "[step: 3145] loss: 0.15914031863212585\n",
      "[step: 3146] loss: 0.16305968165397644\n",
      "[step: 3147] loss: 0.17124134302139282\n",
      "[step: 3148] loss: 0.18570366501808167\n",
      "[step: 3149] loss: 0.21018067002296448\n",
      "[step: 3150] loss: 0.23267598450183868\n",
      "[step: 3151] loss: 0.23981592059135437\n",
      "[step: 3152] loss: 0.20410653948783875\n",
      "[step: 3153] loss: 0.16307242214679718\n",
      "[step: 3154] loss: 0.1575835645198822\n",
      "[step: 3155] loss: 0.1837347447872162\n",
      "[step: 3156] loss: 0.1983817219734192\n",
      "[step: 3157] loss: 0.1761934608221054\n",
      "[step: 3158] loss: 0.15408644080162048\n",
      "[step: 3159] loss: 0.1625460535287857\n",
      "[step: 3160] loss: 0.17845460772514343\n",
      "[step: 3161] loss: 0.1717127114534378\n",
      "[step: 3162] loss: 0.1546647697687149\n",
      "[step: 3163] loss: 0.1567990630865097\n",
      "[step: 3164] loss: 0.1686704456806183\n",
      "[step: 3165] loss: 0.16559818387031555\n",
      "[step: 3166] loss: 0.15398865938186646\n",
      "[step: 3167] loss: 0.15379838645458221\n",
      "[step: 3168] loss: 0.16179192066192627\n",
      "[step: 3169] loss: 0.1609295904636383\n",
      "[step: 3170] loss: 0.15295669436454773\n",
      "[step: 3171] loss: 0.15232190489768982\n",
      "[step: 3172] loss: 0.15772178769111633\n",
      "[step: 3173] loss: 0.157276451587677\n",
      "[step: 3174] loss: 0.1520598828792572\n",
      "[step: 3175] loss: 0.15104295313358307\n",
      "[step: 3176] loss: 0.15443924069404602\n",
      "[step: 3177] loss: 0.15473447740077972\n",
      "[step: 3178] loss: 0.15117239952087402\n",
      "[step: 3179] loss: 0.15009106695652008\n",
      "[step: 3180] loss: 0.15223324298858643\n",
      "[step: 3181] loss: 0.15263929963111877\n",
      "[step: 3182] loss: 0.15035933256149292\n",
      "[step: 3183] loss: 0.14930564165115356\n",
      "[step: 3184] loss: 0.15050987899303436\n",
      "[step: 3185] loss: 0.1510399580001831\n",
      "[step: 3186] loss: 0.14961287379264832\n",
      "[step: 3187] loss: 0.14859886467456818\n",
      "[step: 3188] loss: 0.14919885993003845\n",
      "[step: 3189] loss: 0.14967933297157288\n",
      "[step: 3190] loss: 0.14886075258255005\n",
      "[step: 3191] loss: 0.14796136319637299\n",
      "[step: 3192] loss: 0.1481313705444336\n",
      "[step: 3193] loss: 0.14851850271224976\n",
      "[step: 3194] loss: 0.148105651140213\n",
      "[step: 3195] loss: 0.14737507700920105\n",
      "[step: 3196] loss: 0.14724186062812805\n",
      "[step: 3197] loss: 0.14748363196849823\n",
      "[step: 3198] loss: 0.14734016358852386\n",
      "[step: 3199] loss: 0.146799698472023\n",
      "[step: 3200] loss: 0.14649537205696106\n",
      "[step: 3201] loss: 0.14656609296798706\n",
      "[step: 3202] loss: 0.14654119312763214\n",
      "[step: 3203] loss: 0.14620156586170197\n",
      "[step: 3204] loss: 0.14584659039974213\n",
      "[step: 3205] loss: 0.14574724435806274\n",
      "[step: 3206] loss: 0.14573636651039124\n",
      "[step: 3207] loss: 0.14555306732654572\n",
      "[step: 3208] loss: 0.14523941278457642\n",
      "[step: 3209] loss: 0.14502698183059692\n",
      "[step: 3210] loss: 0.14495369791984558\n",
      "[step: 3211] loss: 0.1448514610528946\n",
      "[step: 3212] loss: 0.14462701976299286\n",
      "[step: 3213] loss: 0.1443808376789093\n",
      "[step: 3214] loss: 0.1442245990037918\n",
      "[step: 3215] loss: 0.1441228985786438\n",
      "[step: 3216] loss: 0.1439746618270874\n",
      "[step: 3217] loss: 0.14376044273376465\n",
      "[step: 3218] loss: 0.14355725049972534\n",
      "[step: 3219] loss: 0.14341014623641968\n",
      "[step: 3220] loss: 0.1432831883430481\n",
      "[step: 3221] loss: 0.14312037825584412\n",
      "[step: 3222] loss: 0.1429254710674286\n",
      "[step: 3223] loss: 0.14274048805236816\n",
      "[step: 3224] loss: 0.14258906245231628\n",
      "[step: 3225] loss: 0.14244695007801056\n",
      "[step: 3226] loss: 0.14228427410125732\n",
      "[step: 3227] loss: 0.1421021819114685\n",
      "[step: 3228] loss: 0.14192518591880798\n",
      "[step: 3229] loss: 0.14176759123802185\n",
      "[step: 3230] loss: 0.1416172981262207\n",
      "[step: 3231] loss: 0.14145736396312714\n",
      "[step: 3232] loss: 0.14128431677818298\n",
      "[step: 3233] loss: 0.14111164212226868\n",
      "[step: 3234] loss: 0.14094875752925873\n",
      "[step: 3235] loss: 0.14079289138317108\n",
      "[step: 3236] loss: 0.14063456654548645\n",
      "[step: 3237] loss: 0.14046819508075714\n",
      "[step: 3238] loss: 0.14029845595359802\n",
      "[step: 3239] loss: 0.14013296365737915\n",
      "[step: 3240] loss: 0.139972984790802\n",
      "[step: 3241] loss: 0.13981471955776215\n",
      "[step: 3242] loss: 0.13965272903442383\n",
      "[step: 3243] loss: 0.13948722183704376\n",
      "[step: 3244] loss: 0.13932117819786072\n",
      "[step: 3245] loss: 0.13915815949440002\n",
      "[step: 3246] loss: 0.13899777829647064\n",
      "[step: 3247] loss: 0.13883763551712036\n",
      "[step: 3248] loss: 0.13867539167404175\n",
      "[step: 3249] loss: 0.13851162791252136\n",
      "[step: 3250] loss: 0.1383472979068756\n",
      "[step: 3251] loss: 0.13818524777889252\n",
      "[step: 3252] loss: 0.13802412152290344\n",
      "[step: 3253] loss: 0.13786360621452332\n",
      "[step: 3254] loss: 0.13770249485969543\n",
      "[step: 3255] loss: 0.13754013180732727\n",
      "[step: 3256] loss: 0.13737763464450836\n",
      "[step: 3257] loss: 0.13721588253974915\n",
      "[step: 3258] loss: 0.13705387711524963\n",
      "[step: 3259] loss: 0.1368933618068695\n",
      "[step: 3260] loss: 0.13673263788223267\n",
      "[step: 3261] loss: 0.13657185435295105\n",
      "[step: 3262] loss: 0.13641029596328735\n",
      "[step: 3263] loss: 0.1362493485212326\n",
      "[step: 3264] loss: 0.1360877901315689\n",
      "[step: 3265] loss: 0.13592751324176788\n",
      "[step: 3266] loss: 0.1357675939798355\n",
      "[step: 3267] loss: 0.1356068253517151\n",
      "[step: 3268] loss: 0.1354472041130066\n",
      "[step: 3269] loss: 0.13528677821159363\n",
      "[step: 3270] loss: 0.13512668013572693\n",
      "[step: 3271] loss: 0.1349664032459259\n",
      "[step: 3272] loss: 0.1348067820072174\n",
      "[step: 3273] loss: 0.13464730978012085\n",
      "[step: 3274] loss: 0.1344877928495407\n",
      "[step: 3275] loss: 0.1343284249305725\n",
      "[step: 3276] loss: 0.13416963815689087\n",
      "[step: 3277] loss: 0.13401073217391968\n",
      "[step: 3278] loss: 0.13385161757469177\n",
      "[step: 3279] loss: 0.13369262218475342\n",
      "[step: 3280] loss: 0.13353374600410461\n",
      "[step: 3281] loss: 0.13337601721286774\n",
      "[step: 3282] loss: 0.1332174390554428\n",
      "[step: 3283] loss: 0.13305939733982086\n",
      "[step: 3284] loss: 0.13290122151374817\n",
      "[step: 3285] loss: 0.13274377584457397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3286] loss: 0.13258594274520874\n",
      "[step: 3287] loss: 0.13242843747138977\n",
      "[step: 3288] loss: 0.1322711706161499\n",
      "[step: 3289] loss: 0.13211411237716675\n",
      "[step: 3290] loss: 0.1319573074579239\n",
      "[step: 3291] loss: 0.13180038332939148\n",
      "[step: 3292] loss: 0.13164393603801727\n",
      "[step: 3293] loss: 0.13148756325244904\n",
      "[step: 3294] loss: 0.13133138418197632\n",
      "[step: 3295] loss: 0.13117516040802002\n",
      "[step: 3296] loss: 0.13101977109909058\n",
      "[step: 3297] loss: 0.13086356222629547\n",
      "[step: 3298] loss: 0.13070820271968842\n",
      "[step: 3299] loss: 0.1305527687072754\n",
      "[step: 3300] loss: 0.13039812445640564\n",
      "[step: 3301] loss: 0.13024333119392395\n",
      "[step: 3302] loss: 0.13008877635002136\n",
      "[step: 3303] loss: 0.12993431091308594\n",
      "[step: 3304] loss: 0.12978030741214752\n",
      "[step: 3305] loss: 0.12962612509727478\n",
      "[step: 3306] loss: 0.12947262823581696\n",
      "[step: 3307] loss: 0.1293189525604248\n",
      "[step: 3308] loss: 0.12916545569896698\n",
      "[step: 3309] loss: 0.12901252508163452\n",
      "[step: 3310] loss: 0.12885960936546326\n",
      "[step: 3311] loss: 0.12870723009109497\n",
      "[step: 3312] loss: 0.12855495512485504\n",
      "[step: 3313] loss: 0.12840312719345093\n",
      "[step: 3314] loss: 0.1282523274421692\n",
      "[step: 3315] loss: 0.12810146808624268\n",
      "[step: 3316] loss: 0.12795186042785645\n",
      "[step: 3317] loss: 0.12780314683914185\n",
      "[step: 3318] loss: 0.1276569962501526\n",
      "[step: 3319] loss: 0.12751558423042297\n",
      "[step: 3320] loss: 0.12738020718097687\n",
      "[step: 3321] loss: 0.12725794315338135\n",
      "[step: 3322] loss: 0.1271582841873169\n",
      "[step: 3323] loss: 0.1271006464958191\n",
      "[step: 3324] loss: 0.1271229386329651\n",
      "[step: 3325] loss: 0.12729603052139282\n",
      "[step: 3326] loss: 0.12776470184326172\n",
      "[step: 3327] loss: 0.1287868618965149\n",
      "[step: 3328] loss: 0.13092462718486786\n",
      "[step: 3329] loss: 0.13503429293632507\n",
      "[step: 3330] loss: 0.14301279187202454\n",
      "[step: 3331] loss: 0.15628394484519958\n",
      "[step: 3332] loss: 0.1774865686893463\n",
      "[step: 3333] loss: 0.1973903775215149\n",
      "[step: 3334] loss: 0.2059503197669983\n",
      "[step: 3335] loss: 0.17962780594825745\n",
      "[step: 3336] loss: 0.14157195389270782\n",
      "[step: 3337] loss: 0.12553924322128296\n",
      "[step: 3338] loss: 0.14216077327728271\n",
      "[step: 3339] loss: 0.16348135471343994\n",
      "[step: 3340] loss: 0.1572733223438263\n",
      "[step: 3341] loss: 0.13384413719177246\n",
      "[step: 3342] loss: 0.1253165900707245\n",
      "[step: 3343] loss: 0.1386781632900238\n",
      "[step: 3344] loss: 0.1479288786649704\n",
      "[step: 3345] loss: 0.13719876110553741\n",
      "[step: 3346] loss: 0.12493233382701874\n",
      "[step: 3347] loss: 0.12802691757678986\n",
      "[step: 3348] loss: 0.13716968894004822\n",
      "[step: 3349] loss: 0.13585889339447021\n",
      "[step: 3350] loss: 0.12633857131004333\n",
      "[step: 3351] loss: 0.12411043792963028\n",
      "[step: 3352] loss: 0.13020677864551544\n",
      "[step: 3353] loss: 0.13230231404304504\n",
      "[step: 3354] loss: 0.1269216537475586\n",
      "[step: 3355] loss: 0.12293757498264313\n",
      "[step: 3356] loss: 0.12553437054157257\n",
      "[step: 3357] loss: 0.12859779596328735\n",
      "[step: 3358] loss: 0.1264197826385498\n",
      "[step: 3359] loss: 0.12281488627195358\n",
      "[step: 3360] loss: 0.12310786545276642\n",
      "[step: 3361] loss: 0.12553662061691284\n",
      "[step: 3362] loss: 0.12541159987449646\n",
      "[step: 3363] loss: 0.12285637855529785\n",
      "[step: 3364] loss: 0.12190581113100052\n",
      "[step: 3365] loss: 0.12329690158367157\n",
      "[step: 3366] loss: 0.12398380041122437\n",
      "[step: 3367] loss: 0.1226525828242302\n",
      "[step: 3368] loss: 0.12138482183218002\n",
      "[step: 3369] loss: 0.12176380306482315\n",
      "[step: 3370] loss: 0.12257684022188187\n",
      "[step: 3371] loss: 0.12219671905040741\n",
      "[step: 3372] loss: 0.12112483382225037\n",
      "[step: 3373] loss: 0.12081008404493332\n",
      "[step: 3374] loss: 0.12131132185459137\n",
      "[step: 3375] loss: 0.12147674709558487\n",
      "[step: 3376] loss: 0.12086446583271027\n",
      "[step: 3377] loss: 0.12028367817401886\n",
      "[step: 3378] loss: 0.12033747881650925\n",
      "[step: 3379] loss: 0.12060049176216125\n",
      "[step: 3380] loss: 0.12043534964323044\n",
      "[step: 3381] loss: 0.11994784325361252\n",
      "[step: 3382] loss: 0.11968250572681427\n",
      "[step: 3383] loss: 0.11977270245552063\n",
      "[step: 3384] loss: 0.11982691287994385\n",
      "[step: 3385] loss: 0.11958566308021545\n",
      "[step: 3386] loss: 0.11924903094768524\n",
      "[step: 3387] loss: 0.11911630630493164\n",
      "[step: 3388] loss: 0.11914937198162079\n",
      "[step: 3389] loss: 0.11909472942352295\n",
      "[step: 3390] loss: 0.11887151002883911\n",
      "[step: 3391] loss: 0.11863964796066284\n",
      "[step: 3392] loss: 0.1185397058725357\n",
      "[step: 3393] loss: 0.11851494759321213\n",
      "[step: 3394] loss: 0.11842319369316101\n",
      "[step: 3395] loss: 0.11823658645153046\n",
      "[step: 3396] loss: 0.11805598437786102\n",
      "[step: 3397] loss: 0.1179555356502533\n",
      "[step: 3398] loss: 0.11789628118276596\n",
      "[step: 3399] loss: 0.11779585480690002\n",
      "[step: 3400] loss: 0.11763996630907059\n",
      "[step: 3401] loss: 0.11748659610748291\n",
      "[step: 3402] loss: 0.11737735569477081\n",
      "[step: 3403] loss: 0.11729579418897629\n",
      "[step: 3404] loss: 0.1171959638595581\n",
      "[step: 3405] loss: 0.11706265807151794\n",
      "[step: 3406] loss: 0.11692354083061218\n",
      "[step: 3407] loss: 0.11680677533149719\n",
      "[step: 3408] loss: 0.11671173572540283\n",
      "[step: 3409] loss: 0.11661312729120255\n",
      "[step: 3410] loss: 0.11649471521377563\n",
      "[step: 3411] loss: 0.11636687815189362\n",
      "[step: 3412] loss: 0.11624696850776672\n",
      "[step: 3413] loss: 0.11614196002483368\n",
      "[step: 3414] loss: 0.11604180186986923\n",
      "[step: 3415] loss: 0.1159331351518631\n",
      "[step: 3416] loss: 0.11581554263830185\n",
      "[step: 3417] loss: 0.1156967356801033\n",
      "[step: 3418] loss: 0.11558535695075989\n",
      "[step: 3419] loss: 0.11548077315092087\n",
      "[step: 3420] loss: 0.11537642776966095\n",
      "[step: 3421] loss: 0.11526627838611603\n",
      "[step: 3422] loss: 0.1151522845029831\n",
      "[step: 3423] loss: 0.11503969132900238\n",
      "[step: 3424] loss: 0.11493174731731415\n",
      "[step: 3425] loss: 0.11482559144496918\n",
      "[step: 3426] loss: 0.11472022533416748\n",
      "[step: 3427] loss: 0.11461191624403\n",
      "[step: 3428] loss: 0.11450199037790298\n",
      "[step: 3429] loss: 0.1143922433257103\n",
      "[step: 3430] loss: 0.1142842024564743\n",
      "[step: 3431] loss: 0.11417894810438156\n",
      "[step: 3432] loss: 0.11407318711280823\n",
      "[step: 3433] loss: 0.11396738886833191\n",
      "[step: 3434] loss: 0.11385997384786606\n",
      "[step: 3435] loss: 0.11375211924314499\n",
      "[step: 3436] loss: 0.11364580690860748\n",
      "[step: 3437] loss: 0.11354013532400131\n",
      "[step: 3438] loss: 0.11343566328287125\n",
      "[step: 3439] loss: 0.11333060264587402\n",
      "[step: 3440] loss: 0.11322522163391113\n",
      "[step: 3441] loss: 0.11311943084001541\n",
      "[step: 3442] loss: 0.11301442235708237\n",
      "[step: 3443] loss: 0.11291016638278961\n",
      "[step: 3444] loss: 0.11280572414398193\n",
      "[step: 3445] loss: 0.11270204186439514\n",
      "[step: 3446] loss: 0.11259874701499939\n",
      "[step: 3447] loss: 0.11249487847089767\n",
      "[step: 3448] loss: 0.1123913824558258\n",
      "[step: 3449] loss: 0.11228778213262558\n",
      "[step: 3450] loss: 0.1121838241815567\n",
      "[step: 3451] loss: 0.11208102107048035\n",
      "[step: 3452] loss: 0.11197848618030548\n",
      "[step: 3453] loss: 0.11187633872032166\n",
      "[step: 3454] loss: 0.11177410930395126\n",
      "[step: 3455] loss: 0.11167227476835251\n",
      "[step: 3456] loss: 0.11157052963972092\n",
      "[step: 3457] loss: 0.11146800220012665\n",
      "[step: 3458] loss: 0.1113663911819458\n",
      "[step: 3459] loss: 0.11126534640789032\n",
      "[step: 3460] loss: 0.11116406321525574\n",
      "[step: 3461] loss: 0.11106346547603607\n",
      "[step: 3462] loss: 0.11096301674842834\n",
      "[step: 3463] loss: 0.11086204648017883\n",
      "[step: 3464] loss: 0.11076132953166962\n",
      "[step: 3465] loss: 0.11066142469644547\n",
      "[step: 3466] loss: 0.11056117713451385\n",
      "[step: 3467] loss: 0.11046156287193298\n",
      "[step: 3468] loss: 0.11036212742328644\n",
      "[step: 3469] loss: 0.11026251316070557\n",
      "[step: 3470] loss: 0.11016330868005753\n",
      "[step: 3471] loss: 0.11006438732147217\n",
      "[step: 3472] loss: 0.10996544361114502\n",
      "[step: 3473] loss: 0.10986679792404175\n",
      "[step: 3474] loss: 0.10976817458868027\n",
      "[step: 3475] loss: 0.10966989398002625\n",
      "[step: 3476] loss: 0.10957161337137222\n",
      "[step: 3477] loss: 0.1094740629196167\n",
      "[step: 3478] loss: 0.10937635600566864\n",
      "[step: 3479] loss: 0.10927848517894745\n",
      "[step: 3480] loss: 0.10918072611093521\n",
      "[step: 3481] loss: 0.10908381640911102\n",
      "[step: 3482] loss: 0.10898668318986893\n",
      "[step: 3483] loss: 0.10888929665088654\n",
      "[step: 3484] loss: 0.10879287123680115\n",
      "[step: 3485] loss: 0.10869632661342621\n",
      "[step: 3486] loss: 0.10860031843185425\n",
      "[step: 3487] loss: 0.10850399732589722\n",
      "[step: 3488] loss: 0.10840752720832825\n",
      "[step: 3489] loss: 0.10831163823604584\n",
      "[step: 3490] loss: 0.10821619629859924\n",
      "[step: 3491] loss: 0.10812029242515564\n",
      "[step: 3492] loss: 0.10802515596151352\n",
      "[step: 3493] loss: 0.1079300045967102\n",
      "[step: 3494] loss: 0.10783475637435913\n",
      "[step: 3495] loss: 0.10773994028568268\n",
      "[step: 3496] loss: 0.10764528810977936\n",
      "[step: 3497] loss: 0.10755069553852081\n",
      "[step: 3498] loss: 0.10745618492364883\n",
      "[step: 3499] loss: 0.10736206918954849\n",
      "[step: 3500] loss: 0.10726813971996307\n",
      "[step: 3501] loss: 0.10717408359050751\n",
      "[step: 3502] loss: 0.10708025097846985\n",
      "[step: 3503] loss: 0.10698642581701279\n",
      "[step: 3504] loss: 0.10689353942871094\n",
      "[step: 3505] loss: 0.10680053383111954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3506] loss: 0.10670796036720276\n",
      "[step: 3507] loss: 0.10661517083644867\n",
      "[step: 3508] loss: 0.10652286559343338\n",
      "[step: 3509] loss: 0.10643212497234344\n",
      "[step: 3510] loss: 0.10634192824363708\n",
      "[step: 3511] loss: 0.10625328868627548\n",
      "[step: 3512] loss: 0.10616748034954071\n",
      "[step: 3513] loss: 0.10608649998903275\n",
      "[step: 3514] loss: 0.10601343214511871\n",
      "[step: 3515] loss: 0.10595428943634033\n",
      "[step: 3516] loss: 0.10591922700405121\n",
      "[step: 3517] loss: 0.10592889785766602\n",
      "[step: 3518] loss: 0.1060190498828888\n",
      "[step: 3519] loss: 0.1062556803226471\n",
      "[step: 3520] loss: 0.10676687955856323\n",
      "[step: 3521] loss: 0.10777007788419724\n",
      "[step: 3522] loss: 0.1097169741988182\n",
      "[step: 3523] loss: 0.11326899379491806\n",
      "[step: 3524] loss: 0.11981135606765747\n",
      "[step: 3525] loss: 0.13046133518218994\n",
      "[step: 3526] loss: 0.14723113179206848\n",
      "[step: 3527] loss: 0.1651747226715088\n",
      "[step: 3528] loss: 0.1779167652130127\n",
      "[step: 3529] loss: 0.16559451818466187\n",
      "[step: 3530] loss: 0.13527655601501465\n",
      "[step: 3531] loss: 0.1085866242647171\n",
      "[step: 3532] loss: 0.10847490280866623\n",
      "[step: 3533] loss: 0.12741580605506897\n",
      "[step: 3534] loss: 0.13853788375854492\n",
      "[step: 3535] loss: 0.1283128410577774\n",
      "[step: 3536] loss: 0.10919088125228882\n",
      "[step: 3537] loss: 0.10557612031698227\n",
      "[step: 3538] loss: 0.11735957860946655\n",
      "[step: 3539] loss: 0.12417414784431458\n",
      "[step: 3540] loss: 0.11648936569690704\n",
      "[step: 3541] loss: 0.10549724847078323\n",
      "[step: 3542] loss: 0.10569747537374496\n",
      "[step: 3543] loss: 0.11329704523086548\n",
      "[step: 3544] loss: 0.11508709192276001\n",
      "[step: 3545] loss: 0.1087903380393982\n",
      "[step: 3546] loss: 0.103666290640831\n",
      "[step: 3547] loss: 0.10595840215682983\n",
      "[step: 3548] loss: 0.11034130305051804\n",
      "[step: 3549] loss: 0.10942046344280243\n",
      "[step: 3550] loss: 0.10484276711940765\n",
      "[step: 3551] loss: 0.10320785641670227\n",
      "[step: 3552] loss: 0.10581221431493759\n",
      "[step: 3553] loss: 0.10761843621730804\n",
      "[step: 3554] loss: 0.10571260750293732\n",
      "[step: 3555] loss: 0.10312610119581223\n",
      "[step: 3556] loss: 0.10323384404182434\n",
      "[step: 3557] loss: 0.10498401522636414\n",
      "[step: 3558] loss: 0.1052580326795578\n",
      "[step: 3559] loss: 0.10362935066223145\n",
      "[step: 3560] loss: 0.10243880748748779\n",
      "[step: 3561] loss: 0.10299812257289886\n",
      "[step: 3562] loss: 0.10394883155822754\n",
      "[step: 3563] loss: 0.10364925861358643\n",
      "[step: 3564] loss: 0.10250843316316605\n",
      "[step: 3565] loss: 0.10205022245645523\n",
      "[step: 3566] loss: 0.10257820039987564\n",
      "[step: 3567] loss: 0.10298167169094086\n",
      "[step: 3568] loss: 0.10256713628768921\n",
      "[step: 3569] loss: 0.10187745094299316\n",
      "[step: 3570] loss: 0.10171003639698029\n",
      "[step: 3571] loss: 0.10202954709529877\n",
      "[step: 3572] loss: 0.10216812044382095\n",
      "[step: 3573] loss: 0.10183806717395782\n",
      "[step: 3574] loss: 0.1014099270105362\n",
      "[step: 3575] loss: 0.10132370889186859\n",
      "[step: 3576] loss: 0.1015036478638649\n",
      "[step: 3577] loss: 0.1015324741601944\n",
      "[step: 3578] loss: 0.10128063708543777\n",
      "[step: 3579] loss: 0.10100269317626953\n",
      "[step: 3580] loss: 0.10094182938337326\n",
      "[step: 3581] loss: 0.10101722925901413\n",
      "[step: 3582] loss: 0.1009979397058487\n",
      "[step: 3583] loss: 0.10082219541072845\n",
      "[step: 3584] loss: 0.1006266251206398\n",
      "[step: 3585] loss: 0.10054919868707657\n",
      "[step: 3586] loss: 0.10056130588054657\n",
      "[step: 3587] loss: 0.10053357481956482\n",
      "[step: 3588] loss: 0.10040904581546783\n",
      "[step: 3589] loss: 0.10025603324174881\n",
      "[step: 3590] loss: 0.10016706585884094\n",
      "[step: 3591] loss: 0.10013993084430695\n",
      "[step: 3592] loss: 0.10010269284248352\n",
      "[step: 3593] loss: 0.10001260042190552\n",
      "[step: 3594] loss: 0.099892757833004\n",
      "[step: 3595] loss: 0.09979639947414398\n",
      "[step: 3596] loss: 0.09973998367786407\n",
      "[step: 3597] loss: 0.09969432651996613\n",
      "[step: 3598] loss: 0.09962539374828339\n",
      "[step: 3599] loss: 0.09952964633703232\n",
      "[step: 3600] loss: 0.09943455457687378\n",
      "[step: 3601] loss: 0.09936082363128662\n",
      "[step: 3602] loss: 0.09930402040481567\n",
      "[step: 3603] loss: 0.09924229234457016\n",
      "[step: 3604] loss: 0.09916451573371887\n",
      "[step: 3605] loss: 0.09907692670822144\n",
      "[step: 3606] loss: 0.09899622946977615\n",
      "[step: 3607] loss: 0.09892730414867401\n",
      "[step: 3608] loss: 0.09886351972818375\n",
      "[step: 3609] loss: 0.09879538416862488\n",
      "[step: 3610] loss: 0.0987187922000885\n",
      "[step: 3611] loss: 0.09863853454589844\n",
      "[step: 3612] loss: 0.09856302291154861\n",
      "[step: 3613] loss: 0.09849380701780319\n",
      "[step: 3614] loss: 0.09842722862958908\n",
      "[step: 3615] loss: 0.09835696965456009\n",
      "[step: 3616] loss: 0.09828264266252518\n",
      "[step: 3617] loss: 0.09820745885372162\n",
      "[step: 3618] loss: 0.09813334047794342\n",
      "[step: 3619] loss: 0.09806279093027115\n",
      "[step: 3620] loss: 0.09799449890851974\n",
      "[step: 3621] loss: 0.09792475402355194\n",
      "[step: 3622] loss: 0.09785245358943939\n",
      "[step: 3623] loss: 0.09777889400720596\n",
      "[step: 3624] loss: 0.09770657122135162\n",
      "[step: 3625] loss: 0.09763573110103607\n",
      "[step: 3626] loss: 0.09756585955619812\n",
      "[step: 3627] loss: 0.09749637544155121\n",
      "[step: 3628] loss: 0.09742545336484909\n",
      "[step: 3629] loss: 0.09735411405563354\n",
      "[step: 3630] loss: 0.0972825437784195\n",
      "[step: 3631] loss: 0.09721136838197708\n",
      "[step: 3632] loss: 0.0971413254737854\n",
      "[step: 3633] loss: 0.09707166254520416\n",
      "[step: 3634] loss: 0.09700189530849457\n",
      "[step: 3635] loss: 0.09693115949630737\n",
      "[step: 3636] loss: 0.09686072170734406\n",
      "[step: 3637] loss: 0.09679024666547775\n",
      "[step: 3638] loss: 0.096719890832901\n",
      "[step: 3639] loss: 0.09665031731128693\n",
      "[step: 3640] loss: 0.09657998383045197\n",
      "[step: 3641] loss: 0.09651080518960953\n",
      "[step: 3642] loss: 0.09644109010696411\n",
      "[step: 3643] loss: 0.09637132287025452\n",
      "[step: 3644] loss: 0.09630120545625687\n",
      "[step: 3645] loss: 0.09623183310031891\n",
      "[step: 3646] loss: 0.09616240113973618\n",
      "[step: 3647] loss: 0.09609303623437881\n",
      "[step: 3648] loss: 0.09602339565753937\n",
      "[step: 3649] loss: 0.09595438092947006\n",
      "[step: 3650] loss: 0.09588520228862762\n",
      "[step: 3651] loss: 0.09581601619720459\n",
      "[step: 3652] loss: 0.09574693441390991\n",
      "[step: 3653] loss: 0.0956774428486824\n",
      "[step: 3654] loss: 0.09560878574848175\n",
      "[step: 3655] loss: 0.09553995728492737\n",
      "[step: 3656] loss: 0.0954708456993103\n",
      "[step: 3657] loss: 0.09540233761072159\n",
      "[step: 3658] loss: 0.09533326327800751\n",
      "[step: 3659] loss: 0.09526491165161133\n",
      "[step: 3660] loss: 0.09519623219966888\n",
      "[step: 3661] loss: 0.09512774646282196\n",
      "[step: 3662] loss: 0.09505945444107056\n",
      "[step: 3663] loss: 0.09499089419841766\n",
      "[step: 3664] loss: 0.09492228180170059\n",
      "[step: 3665] loss: 0.09485457837581635\n",
      "[step: 3666] loss: 0.09478632360696793\n",
      "[step: 3667] loss: 0.09471795707941055\n",
      "[step: 3668] loss: 0.09465011954307556\n",
      "[step: 3669] loss: 0.09458141028881073\n",
      "[step: 3670] loss: 0.09451378881931305\n",
      "[step: 3671] loss: 0.09444589167833328\n",
      "[step: 3672] loss: 0.09437847137451172\n",
      "[step: 3673] loss: 0.09431011974811554\n",
      "[step: 3674] loss: 0.09424275159835815\n",
      "[step: 3675] loss: 0.09417474269866943\n",
      "[step: 3676] loss: 0.09410705417394638\n",
      "[step: 3677] loss: 0.09403932094573975\n",
      "[step: 3678] loss: 0.0939718633890152\n",
      "[step: 3679] loss: 0.09390442818403244\n",
      "[step: 3680] loss: 0.09383724629878998\n",
      "[step: 3681] loss: 0.09376974403858185\n",
      "[step: 3682] loss: 0.09370286762714386\n",
      "[step: 3683] loss: 0.09363538771867752\n",
      "[step: 3684] loss: 0.09356846660375595\n",
      "[step: 3685] loss: 0.09350107610225677\n",
      "[step: 3686] loss: 0.0934341549873352\n",
      "[step: 3687] loss: 0.09336714446544647\n",
      "[step: 3688] loss: 0.09329986572265625\n",
      "[step: 3689] loss: 0.0932331308722496\n",
      "[step: 3690] loss: 0.09316658973693848\n",
      "[step: 3691] loss: 0.09309922903776169\n",
      "[step: 3692] loss: 0.09303294867277145\n",
      "[step: 3693] loss: 0.09296610951423645\n",
      "[step: 3694] loss: 0.09289948642253876\n",
      "[step: 3695] loss: 0.09283310920000076\n",
      "[step: 3696] loss: 0.09276649355888367\n",
      "[step: 3697] loss: 0.09269998967647552\n",
      "[step: 3698] loss: 0.09263376891613007\n",
      "[step: 3699] loss: 0.09256714582443237\n",
      "[step: 3700] loss: 0.0925009474158287\n",
      "[step: 3701] loss: 0.09243473410606384\n",
      "[step: 3702] loss: 0.092368945479393\n",
      "[step: 3703] loss: 0.09230247139930725\n",
      "[step: 3704] loss: 0.09223654121160507\n",
      "[step: 3705] loss: 0.09217020869255066\n",
      "[step: 3706] loss: 0.09210439026355743\n",
      "[step: 3707] loss: 0.09203851968050003\n",
      "[step: 3708] loss: 0.09197258949279785\n",
      "[step: 3709] loss: 0.0919066071510315\n",
      "[step: 3710] loss: 0.09184091538190842\n",
      "[step: 3711] loss: 0.0917758047580719\n",
      "[step: 3712] loss: 0.09170961380004883\n",
      "[step: 3713] loss: 0.09164417535066605\n",
      "[step: 3714] loss: 0.09157910943031311\n",
      "[step: 3715] loss: 0.09151335060596466\n",
      "[step: 3716] loss: 0.09144821763038635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3717] loss: 0.09138291329145432\n",
      "[step: 3718] loss: 0.09131784737110138\n",
      "[step: 3719] loss: 0.09125319123268127\n",
      "[step: 3720] loss: 0.09118897467851639\n",
      "[step: 3721] loss: 0.09112592041492462\n",
      "[step: 3722] loss: 0.0910642147064209\n",
      "[step: 3723] loss: 0.09100516885519028\n",
      "[step: 3724] loss: 0.09095142036676407\n",
      "[step: 3725] loss: 0.09090638160705566\n",
      "[step: 3726] loss: 0.09087821841239929\n",
      "[step: 3727] loss: 0.09088144451379776\n",
      "[step: 3728] loss: 0.0909443348646164\n",
      "[step: 3729] loss: 0.09111914038658142\n",
      "[step: 3730] loss: 0.09151104837656021\n",
      "[step: 3731] loss: 0.09231197834014893\n",
      "[step: 3732] loss: 0.09391693770885468\n",
      "[step: 3733] loss: 0.0969877541065216\n",
      "[step: 3734] loss: 0.10288330912590027\n",
      "[step: 3735] loss: 0.11322931945323944\n",
      "[step: 3736] loss: 0.13077284395694733\n",
      "[step: 3737] loss: 0.15308912098407745\n",
      "[step: 3738] loss: 0.17417839169502258\n",
      "[step: 3739] loss: 0.16958826780319214\n",
      "[step: 3740] loss: 0.13802306354045868\n",
      "[step: 3741] loss: 0.10056273639202118\n",
      "[step: 3742] loss: 0.09278798848390579\n",
      "[step: 3743] loss: 0.11338029056787491\n",
      "[step: 3744] loss: 0.13026009500026703\n",
      "[step: 3745] loss: 0.12118076533079147\n",
      "[step: 3746] loss: 0.09756601601839066\n",
      "[step: 3747] loss: 0.0915653184056282\n",
      "[step: 3748] loss: 0.10594877600669861\n",
      "[step: 3749] loss: 0.11431843042373657\n",
      "[step: 3750] loss: 0.10356315970420837\n",
      "[step: 3751] loss: 0.09041528403759003\n",
      "[step: 3752] loss: 0.09353283047676086\n",
      "[step: 3753] loss: 0.10337843000888824\n",
      "[step: 3754] loss: 0.10193925350904465\n",
      "[step: 3755] loss: 0.09252290427684784\n",
      "[step: 3756] loss: 0.08992888033390045\n",
      "[step: 3757] loss: 0.09555409848690033\n",
      "[step: 3758] loss: 0.0982092097401619\n",
      "[step: 3759] loss: 0.09361086040735245\n",
      "[step: 3760] loss: 0.08948725461959839\n",
      "[step: 3761] loss: 0.09122462570667267\n",
      "[step: 3762] loss: 0.09438911080360413\n",
      "[step: 3763] loss: 0.09333405643701553\n",
      "[step: 3764] loss: 0.08982906490564346\n",
      "[step: 3765] loss: 0.08920195698738098\n",
      "[step: 3766] loss: 0.09152756631374359\n",
      "[step: 3767] loss: 0.09216371923685074\n",
      "[step: 3768] loss: 0.08996792137622833\n",
      "[step: 3769] loss: 0.0885164886713028\n",
      "[step: 3770] loss: 0.08964559435844421\n",
      "[step: 3771] loss: 0.09081172198057175\n",
      "[step: 3772] loss: 0.0898430347442627\n",
      "[step: 3773] loss: 0.088412344455719\n",
      "[step: 3774] loss: 0.08857053518295288\n",
      "[step: 3775] loss: 0.08950843662023544\n",
      "[step: 3776] loss: 0.08943738788366318\n",
      "[step: 3777] loss: 0.08847092092037201\n",
      "[step: 3778] loss: 0.08805327117443085\n",
      "[step: 3779] loss: 0.08848722279071808\n",
      "[step: 3780] loss: 0.08879488706588745\n",
      "[step: 3781] loss: 0.08840397000312805\n",
      "[step: 3782] loss: 0.08786081522703171\n",
      "[step: 3783] loss: 0.08784455060958862\n",
      "[step: 3784] loss: 0.0881512314081192\n",
      "[step: 3785] loss: 0.08814983069896698\n",
      "[step: 3786] loss: 0.08776009827852249\n",
      "[step: 3787] loss: 0.08750078082084656\n",
      "[step: 3788] loss: 0.0876220166683197\n",
      "[step: 3789] loss: 0.08776435256004333\n",
      "[step: 3790] loss: 0.08760178834199905\n",
      "[step: 3791] loss: 0.08732137084007263\n",
      "[step: 3792] loss: 0.08725123107433319\n",
      "[step: 3793] loss: 0.08734655380249023\n",
      "[step: 3794] loss: 0.0873408392071724\n",
      "[step: 3795] loss: 0.08717511594295502\n",
      "[step: 3796] loss: 0.08702108263969421\n",
      "[step: 3797] loss: 0.08699573576450348\n",
      "[step: 3798] loss: 0.08702051639556885\n",
      "[step: 3799] loss: 0.08697338402271271\n",
      "[step: 3800] loss: 0.08684951066970825\n",
      "[step: 3801] loss: 0.08674394339323044\n",
      "[step: 3802] loss: 0.08671483397483826\n",
      "[step: 3803] loss: 0.08670985698699951\n",
      "[step: 3804] loss: 0.08665463328361511\n",
      "[step: 3805] loss: 0.08655205368995667\n",
      "[step: 3806] loss: 0.08646998554468155\n",
      "[step: 3807] loss: 0.08643786609172821\n",
      "[step: 3808] loss: 0.08641405403614044\n",
      "[step: 3809] loss: 0.08635450154542923\n",
      "[step: 3810] loss: 0.08627035468816757\n",
      "[step: 3811] loss: 0.08620281517505646\n",
      "[step: 3812] loss: 0.08616351336240768\n",
      "[step: 3813] loss: 0.08612525463104248\n",
      "[step: 3814] loss: 0.08606879413127899\n",
      "[step: 3815] loss: 0.08599923551082611\n",
      "[step: 3816] loss: 0.08593682199716568\n",
      "[step: 3817] loss: 0.08588907122612\n",
      "[step: 3818] loss: 0.08584578335285187\n",
      "[step: 3819] loss: 0.08579327911138535\n",
      "[step: 3820] loss: 0.0857313871383667\n",
      "[step: 3821] loss: 0.08567096292972565\n",
      "[step: 3822] loss: 0.08561915159225464\n",
      "[step: 3823] loss: 0.08557280898094177\n",
      "[step: 3824] loss: 0.08552210032939911\n",
      "[step: 3825] loss: 0.0854644700884819\n",
      "[step: 3826] loss: 0.08540618419647217\n",
      "[step: 3827] loss: 0.08535277843475342\n",
      "[step: 3828] loss: 0.08530349284410477\n",
      "[step: 3829] loss: 0.08525305986404419\n",
      "[step: 3830] loss: 0.08519910275936127\n",
      "[step: 3831] loss: 0.08514302968978882\n",
      "[step: 3832] loss: 0.08508844673633575\n",
      "[step: 3833] loss: 0.0850367546081543\n",
      "[step: 3834] loss: 0.08498586714267731\n",
      "[step: 3835] loss: 0.08493418991565704\n",
      "[step: 3836] loss: 0.084880031645298\n",
      "[step: 3837] loss: 0.08482568711042404\n",
      "[step: 3838] loss: 0.0847727358341217\n",
      "[step: 3839] loss: 0.08472107350826263\n",
      "[step: 3840] loss: 0.08466978371143341\n",
      "[step: 3841] loss: 0.08461698889732361\n",
      "[step: 3842] loss: 0.08456394076347351\n",
      "[step: 3843] loss: 0.08451037853956223\n",
      "[step: 3844] loss: 0.08445863425731659\n",
      "[step: 3845] loss: 0.08440688252449036\n",
      "[step: 3846] loss: 0.0843544453382492\n",
      "[step: 3847] loss: 0.08430241048336029\n",
      "[step: 3848] loss: 0.08424939215183258\n",
      "[step: 3849] loss: 0.08419709652662277\n",
      "[step: 3850] loss: 0.08414511382579803\n",
      "[step: 3851] loss: 0.08409306406974792\n",
      "[step: 3852] loss: 0.08404134958982468\n",
      "[step: 3853] loss: 0.08398909866809845\n",
      "[step: 3854] loss: 0.08393696695566177\n",
      "[step: 3855] loss: 0.08388462662696838\n",
      "[step: 3856] loss: 0.08383214473724365\n",
      "[step: 3857] loss: 0.0837809294462204\n",
      "[step: 3858] loss: 0.08372890949249268\n",
      "[step: 3859] loss: 0.08367712050676346\n",
      "[step: 3860] loss: 0.08362530171871185\n",
      "[step: 3861] loss: 0.08357314765453339\n",
      "[step: 3862] loss: 0.08352166414260864\n",
      "[step: 3863] loss: 0.08346976339817047\n",
      "[step: 3864] loss: 0.08341825008392334\n",
      "[step: 3865] loss: 0.08336605876684189\n",
      "[step: 3866] loss: 0.083314910531044\n",
      "[step: 3867] loss: 0.08326306939125061\n",
      "[step: 3868] loss: 0.08321117609739304\n",
      "[step: 3869] loss: 0.08315993845462799\n",
      "[step: 3870] loss: 0.08310814946889877\n",
      "[step: 3871] loss: 0.08305682241916656\n",
      "[step: 3872] loss: 0.08300509303808212\n",
      "[step: 3873] loss: 0.08295407891273499\n",
      "[step: 3874] loss: 0.08290249109268188\n",
      "[step: 3875] loss: 0.08285106718540192\n",
      "[step: 3876] loss: 0.08279934525489807\n",
      "[step: 3877] loss: 0.08274814486503601\n",
      "[step: 3878] loss: 0.08269717544317245\n",
      "[step: 3879] loss: 0.08264542371034622\n",
      "[step: 3880] loss: 0.0825946033000946\n",
      "[step: 3881] loss: 0.08254295587539673\n",
      "[step: 3882] loss: 0.082491934299469\n",
      "[step: 3883] loss: 0.08244071155786514\n",
      "[step: 3884] loss: 0.08238920569419861\n",
      "[step: 3885] loss: 0.08233821392059326\n",
      "[step: 3886] loss: 0.08228757232427597\n",
      "[step: 3887] loss: 0.08223630487918854\n",
      "[step: 3888] loss: 0.08218521624803543\n",
      "[step: 3889] loss: 0.08213464915752411\n",
      "[step: 3890] loss: 0.08208301663398743\n",
      "[step: 3891] loss: 0.08203266561031342\n",
      "[step: 3892] loss: 0.08198148012161255\n",
      "[step: 3893] loss: 0.08193078637123108\n",
      "[step: 3894] loss: 0.08187999576330185\n",
      "[step: 3895] loss: 0.08182857930660248\n",
      "[step: 3896] loss: 0.08177812397480011\n",
      "[step: 3897] loss: 0.08172741532325745\n",
      "[step: 3898] loss: 0.08167630434036255\n",
      "[step: 3899] loss: 0.08162572979927063\n",
      "[step: 3900] loss: 0.08157551288604736\n",
      "[step: 3901] loss: 0.08152413368225098\n",
      "[step: 3902] loss: 0.08147411048412323\n",
      "[step: 3903] loss: 0.08142310380935669\n",
      "[step: 3904] loss: 0.08137267082929611\n",
      "[step: 3905] loss: 0.08132227510213852\n",
      "[step: 3906] loss: 0.0812717005610466\n",
      "[step: 3907] loss: 0.08122089505195618\n",
      "[step: 3908] loss: 0.08117087185382843\n",
      "[step: 3909] loss: 0.0811203122138977\n",
      "[step: 3910] loss: 0.08106998354196548\n",
      "[step: 3911] loss: 0.08101945370435715\n",
      "[step: 3912] loss: 0.08096937835216522\n",
      "[step: 3913] loss: 0.08091908693313599\n",
      "[step: 3914] loss: 0.08086860179901123\n",
      "[step: 3915] loss: 0.08081809431314468\n",
      "[step: 3916] loss: 0.08076807856559753\n",
      "[step: 3917] loss: 0.08071790635585785\n",
      "[step: 3918] loss: 0.08066780120134354\n",
      "[step: 3919] loss: 0.08061754703521729\n",
      "[step: 3920] loss: 0.0805673822760582\n",
      "[step: 3921] loss: 0.08051706105470657\n",
      "[step: 3922] loss: 0.08046743273735046\n",
      "[step: 3923] loss: 0.08041717112064362\n",
      "[step: 3924] loss: 0.08036720007658005\n",
      "[step: 3925] loss: 0.0803174078464508\n",
      "[step: 3926] loss: 0.08026747405529022\n",
      "[step: 3927] loss: 0.08021703362464905\n",
      "[step: 3928] loss: 0.08016737550497055\n",
      "[step: 3929] loss: 0.0801176056265831\n",
      "[step: 3930] loss: 0.08006775379180908\n",
      "[step: 3931] loss: 0.08001783490180969\n",
      "[step: 3932] loss: 0.07996781170368195\n",
      "[step: 3933] loss: 0.07991848140954971\n",
      "[step: 3934] loss: 0.07986856251955032\n",
      "[step: 3935] loss: 0.07981891930103302\n",
      "[step: 3936] loss: 0.07976920902729034\n",
      "[step: 3937] loss: 0.07971970736980438\n",
      "[step: 3938] loss: 0.07966970652341843\n",
      "[step: 3939] loss: 0.07962019741535187\n",
      "[step: 3940] loss: 0.07957041263580322\n",
      "[step: 3941] loss: 0.07952134311199188\n",
      "[step: 3942] loss: 0.07947143167257309\n",
      "[step: 3943] loss: 0.07942206412553787\n",
      "[step: 3944] loss: 0.07937245070934296\n",
      "[step: 3945] loss: 0.07932307571172714\n",
      "[step: 3946] loss: 0.07927363365888596\n",
      "[step: 3947] loss: 0.07922457158565521\n",
      "[step: 3948] loss: 0.0791749656200409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3949] loss: 0.07912574708461761\n",
      "[step: 3950] loss: 0.07907606661319733\n",
      "[step: 3951] loss: 0.07902685552835464\n",
      "[step: 3952] loss: 0.07897815108299255\n",
      "[step: 3953] loss: 0.07892918586730957\n",
      "[step: 3954] loss: 0.0788796991109848\n",
      "[step: 3955] loss: 0.0788310319185257\n",
      "[step: 3956] loss: 0.07878194004297256\n",
      "[step: 3957] loss: 0.078733429312706\n",
      "[step: 3958] loss: 0.07868526875972748\n",
      "[step: 3959] loss: 0.07863786071538925\n",
      "[step: 3960] loss: 0.07859121263027191\n",
      "[step: 3961] loss: 0.07854566723108292\n",
      "[step: 3962] loss: 0.07850301265716553\n",
      "[step: 3963] loss: 0.0784657895565033\n",
      "[step: 3964] loss: 0.07843552529811859\n",
      "[step: 3965] loss: 0.07841942459344864\n",
      "[step: 3966] loss: 0.0784279853105545\n",
      "[step: 3967] loss: 0.07848067581653595\n",
      "[step: 3968] loss: 0.07861189544200897\n",
      "[step: 3969] loss: 0.078887440264225\n",
      "[step: 3970] loss: 0.07941940426826477\n",
      "[step: 3971] loss: 0.08042958378791809\n",
      "[step: 3972] loss: 0.08227252960205078\n",
      "[step: 3973] loss: 0.08564834296703339\n",
      "[step: 3974] loss: 0.09144026041030884\n",
      "[step: 3975] loss: 0.1012316644191742\n",
      "[step: 3976] loss: 0.11505748331546783\n",
      "[step: 3977] loss: 0.1321655809879303\n",
      "[step: 3978] loss: 0.1420167088508606\n",
      "[step: 3979] loss: 0.13719891011714935\n",
      "[step: 3980] loss: 0.11266086995601654\n",
      "[step: 3981] loss: 0.08788464963436127\n",
      "[step: 3982] loss: 0.08002401888370514\n",
      "[step: 3983] loss: 0.09003784507513046\n",
      "[step: 3984] loss: 0.10348126292228699\n",
      "[step: 3985] loss: 0.10462985932826996\n",
      "[step: 3986] loss: 0.09301605820655823\n",
      "[step: 3987] loss: 0.08074381947517395\n",
      "[step: 3988] loss: 0.0802285373210907\n",
      "[step: 3989] loss: 0.08902957290410995\n",
      "[step: 3990] loss: 0.09421983361244202\n",
      "[step: 3991] loss: 0.08903532475233078\n",
      "[step: 3992] loss: 0.07981950044631958\n",
      "[step: 3993] loss: 0.07779987156391144\n",
      "[step: 3994] loss: 0.08338981866836548\n",
      "[step: 3995] loss: 0.08720903843641281\n",
      "[step: 3996] loss: 0.0839768797159195\n",
      "[step: 3997] loss: 0.07848900556564331\n",
      "[step: 3998] loss: 0.07759140431880951\n",
      "[step: 3999] loss: 0.08064194023609161\n",
      "[step: 4000] loss: 0.08231455832719803\n",
      "[step: 4001] loss: 0.08060954511165619\n",
      "[step: 4002] loss: 0.0780133530497551\n",
      "[step: 4003] loss: 0.07737252861261368\n",
      "[step: 4004] loss: 0.07861822843551636\n",
      "[step: 4005] loss: 0.07967313379049301\n",
      "[step: 4006] loss: 0.07909400761127472\n",
      "[step: 4007] loss: 0.07749416679143906\n",
      "[step: 4008] loss: 0.07673666626214981\n",
      "[step: 4009] loss: 0.07746344804763794\n",
      "[step: 4010] loss: 0.07837170362472534\n",
      "[step: 4011] loss: 0.07806022465229034\n",
      "[step: 4012] loss: 0.076888307929039\n",
      "[step: 4013] loss: 0.07633721083402634\n",
      "[step: 4014] loss: 0.07686369121074677\n",
      "[step: 4015] loss: 0.07744158804416656\n",
      "[step: 4016] loss: 0.07719770073890686\n",
      "[step: 4017] loss: 0.07648962736129761\n",
      "[step: 4018] loss: 0.07616716623306274\n",
      "[step: 4019] loss: 0.07638692855834961\n",
      "[step: 4020] loss: 0.0766502320766449\n",
      "[step: 4021] loss: 0.07657799124717712\n",
      "[step: 4022] loss: 0.0762491300702095\n",
      "[step: 4023] loss: 0.07598316669464111\n",
      "[step: 4024] loss: 0.07596669346094131\n",
      "[step: 4025] loss: 0.07610924541950226\n",
      "[step: 4026] loss: 0.07616087794303894\n",
      "[step: 4027] loss: 0.07599802315235138\n",
      "[step: 4028] loss: 0.07575872540473938\n",
      "[step: 4029] loss: 0.07566193491220474\n",
      "[step: 4030] loss: 0.07573679089546204\n",
      "[step: 4031] loss: 0.07580351829528809\n",
      "[step: 4032] loss: 0.07571866363286972\n",
      "[step: 4033] loss: 0.075546994805336\n",
      "[step: 4034] loss: 0.07544291019439697\n",
      "[step: 4035] loss: 0.07544608414173126\n",
      "[step: 4036] loss: 0.07547079771757126\n",
      "[step: 4037] loss: 0.07543638348579407\n",
      "[step: 4038] loss: 0.07534801959991455\n",
      "[step: 4039] loss: 0.07525582611560822\n",
      "[step: 4040] loss: 0.07519975304603577\n",
      "[step: 4041] loss: 0.07517939805984497\n",
      "[step: 4042] loss: 0.07516667991876602\n",
      "[step: 4043] loss: 0.07513007521629333\n",
      "[step: 4044] loss: 0.0750618427991867\n",
      "[step: 4045] loss: 0.07498738169670105\n",
      "[step: 4046] loss: 0.07493790984153748\n",
      "[step: 4047] loss: 0.0749160498380661\n",
      "[step: 4048] loss: 0.07489433884620667\n",
      "[step: 4049] loss: 0.07484918087720871\n",
      "[step: 4050] loss: 0.0747871845960617\n",
      "[step: 4051] loss: 0.07473061233758926\n",
      "[step: 4052] loss: 0.07468929886817932\n",
      "[step: 4053] loss: 0.07465779781341553\n",
      "[step: 4054] loss: 0.07462367415428162\n",
      "[step: 4055] loss: 0.0745805874466896\n",
      "[step: 4056] loss: 0.07453162968158722\n",
      "[step: 4057] loss: 0.07448175549507141\n",
      "[step: 4058] loss: 0.07443767040967941\n",
      "[step: 4059] loss: 0.07439884543418884\n",
      "[step: 4060] loss: 0.07436338812112808\n",
      "[step: 4061] loss: 0.07432404160499573\n",
      "[step: 4062] loss: 0.07427948713302612\n",
      "[step: 4063] loss: 0.0742323249578476\n",
      "[step: 4064] loss: 0.07418803870677948\n",
      "[step: 4065] loss: 0.07414762675762177\n",
      "[step: 4066] loss: 0.07410977780818939\n",
      "[step: 4067] loss: 0.07407014071941376\n",
      "[step: 4068] loss: 0.07402808964252472\n",
      "[step: 4069] loss: 0.07398451119661331\n",
      "[step: 4070] loss: 0.07394110411405563\n",
      "[step: 4071] loss: 0.0738995224237442\n",
      "[step: 4072] loss: 0.07385893911123276\n",
      "[step: 4073] loss: 0.07381844520568848\n",
      "[step: 4074] loss: 0.07377825677394867\n",
      "[step: 4075] loss: 0.07373695075511932\n",
      "[step: 4076] loss: 0.07369475811719894\n",
      "[step: 4077] loss: 0.07365231961011887\n",
      "[step: 4078] loss: 0.07361064851284027\n",
      "[step: 4079] loss: 0.07357022166252136\n",
      "[step: 4080] loss: 0.07352989912033081\n",
      "[step: 4081] loss: 0.07348893582820892\n",
      "[step: 4082] loss: 0.07344795763492584\n",
      "[step: 4083] loss: 0.0734063908457756\n",
      "[step: 4084] loss: 0.07336518168449402\n",
      "[step: 4085] loss: 0.07332378625869751\n",
      "[step: 4086] loss: 0.07328295707702637\n",
      "[step: 4087] loss: 0.07324180006980896\n",
      "[step: 4088] loss: 0.07320171594619751\n",
      "[step: 4089] loss: 0.07316094636917114\n",
      "[step: 4090] loss: 0.07312004268169403\n",
      "[step: 4091] loss: 0.0730789452791214\n",
      "[step: 4092] loss: 0.0730377733707428\n",
      "[step: 4093] loss: 0.07299694418907166\n",
      "[step: 4094] loss: 0.0729561448097229\n",
      "[step: 4095] loss: 0.07291560620069504\n",
      "[step: 4096] loss: 0.07287473976612091\n",
      "[step: 4097] loss: 0.07283411920070648\n",
      "[step: 4098] loss: 0.07279371470212936\n",
      "[step: 4099] loss: 0.07275272905826569\n",
      "[step: 4100] loss: 0.07271213084459305\n",
      "[step: 4101] loss: 0.07267116010189056\n",
      "[step: 4102] loss: 0.07263068109750748\n",
      "[step: 4103] loss: 0.07258980721235275\n",
      "[step: 4104] loss: 0.07254961878061295\n",
      "[step: 4105] loss: 0.07250863313674927\n",
      "[step: 4106] loss: 0.07246837019920349\n",
      "[step: 4107] loss: 0.07242794334888458\n",
      "[step: 4108] loss: 0.07238748669624329\n",
      "[step: 4109] loss: 0.0723467618227005\n",
      "[step: 4110] loss: 0.07230612635612488\n",
      "[step: 4111] loss: 0.07226553559303284\n",
      "[step: 4112] loss: 0.0722251683473587\n",
      "[step: 4113] loss: 0.07218476384878159\n",
      "[step: 4114] loss: 0.07214418798685074\n",
      "[step: 4115] loss: 0.07210399210453033\n",
      "[step: 4116] loss: 0.07206357270479202\n",
      "[step: 4117] loss: 0.07202330976724625\n",
      "[step: 4118] loss: 0.07198275625705719\n",
      "[step: 4119] loss: 0.07194280624389648\n",
      "[step: 4120] loss: 0.0719025656580925\n",
      "[step: 4121] loss: 0.07186209410429001\n",
      "[step: 4122] loss: 0.07182206213474274\n",
      "[step: 4123] loss: 0.07178160548210144\n",
      "[step: 4124] loss: 0.07174137979745865\n",
      "[step: 4125] loss: 0.07170112431049347\n",
      "[step: 4126] loss: 0.071661077439785\n",
      "[step: 4127] loss: 0.07162076979875565\n",
      "[step: 4128] loss: 0.07158036530017853\n",
      "[step: 4129] loss: 0.07154031842947006\n",
      "[step: 4130] loss: 0.07150030136108398\n",
      "[step: 4131] loss: 0.07146020233631134\n",
      "[step: 4132] loss: 0.0714203491806984\n",
      "[step: 4133] loss: 0.07138027250766754\n",
      "[step: 4134] loss: 0.07133986800909042\n",
      "[step: 4135] loss: 0.07130011916160583\n",
      "[step: 4136] loss: 0.07126040011644363\n",
      "[step: 4137] loss: 0.07122009992599487\n",
      "[step: 4138] loss: 0.07118022441864014\n",
      "[step: 4139] loss: 0.07114028930664062\n",
      "[step: 4140] loss: 0.07109985500574112\n",
      "[step: 4141] loss: 0.07106028497219086\n",
      "[step: 4142] loss: 0.07102032750844955\n",
      "[step: 4143] loss: 0.0709807425737381\n",
      "[step: 4144] loss: 0.0709405392408371\n",
      "[step: 4145] loss: 0.07090045511722565\n",
      "[step: 4146] loss: 0.07086116820573807\n",
      "[step: 4147] loss: 0.07082080841064453\n",
      "[step: 4148] loss: 0.07078129053115845\n",
      "[step: 4149] loss: 0.07074172794818878\n",
      "[step: 4150] loss: 0.07070177048444748\n",
      "[step: 4151] loss: 0.07066220045089722\n",
      "[step: 4152] loss: 0.07062239199876785\n",
      "[step: 4153] loss: 0.07058259844779968\n",
      "[step: 4154] loss: 0.07054299116134644\n",
      "[step: 4155] loss: 0.07050333172082901\n",
      "[step: 4156] loss: 0.07046366482973099\n",
      "[step: 4157] loss: 0.070423923432827\n",
      "[step: 4158] loss: 0.07038441300392151\n",
      "[step: 4159] loss: 0.07034460455179214\n",
      "[step: 4160] loss: 0.07030536979436874\n",
      "[step: 4161] loss: 0.0702662467956543\n",
      "[step: 4162] loss: 0.07022695243358612\n",
      "[step: 4163] loss: 0.0701875388622284\n",
      "[step: 4164] loss: 0.07014866173267365\n",
      "[step: 4165] loss: 0.07011005282402039\n",
      "[step: 4166] loss: 0.07007215172052383\n",
      "[step: 4167] loss: 0.07003476470708847\n",
      "[step: 4168] loss: 0.06999954581260681\n",
      "[step: 4169] loss: 0.06996643543243408\n",
      "[step: 4170] loss: 0.06993784755468369\n",
      "[step: 4171] loss: 0.06991662085056305\n",
      "[step: 4172] loss: 0.06990808248519897\n",
      "[step: 4173] loss: 0.06992211192846298\n",
      "[step: 4174] loss: 0.06997554004192352\n",
      "[step: 4175] loss: 0.07009873539209366\n",
      "[step: 4176] loss: 0.07034700363874435\n",
      "[step: 4177] loss: 0.07081480324268341\n",
      "[step: 4178] loss: 0.07168549299240112\n",
      "[step: 4179] loss: 0.07324713468551636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4180] loss: 0.07607406377792358\n",
      "[step: 4181] loss: 0.08088047802448273\n",
      "[step: 4182] loss: 0.08904427289962769\n",
      "[step: 4183] loss: 0.10084254294633865\n",
      "[step: 4184] loss: 0.11653026938438416\n",
      "[step: 4185] loss: 0.12831929326057434\n",
      "[step: 4186] loss: 0.12981955707073212\n",
      "[step: 4187] loss: 0.11164771020412445\n",
      "[step: 4188] loss: 0.08699020743370056\n",
      "[step: 4189] loss: 0.07241109013557434\n",
      "[step: 4190] loss: 0.07570059597492218\n",
      "[step: 4191] loss: 0.08826952427625656\n",
      "[step: 4192] loss: 0.09530821442604065\n",
      "[step: 4193] loss: 0.09027127176523209\n",
      "[step: 4194] loss: 0.07803428918123245\n",
      "[step: 4195] loss: 0.07111236453056335\n",
      "[step: 4196] loss: 0.0744592696428299\n",
      "[step: 4197] loss: 0.08181441575288773\n",
      "[step: 4198] loss: 0.08387202024459839\n",
      "[step: 4199] loss: 0.07770679891109467\n",
      "[step: 4200] loss: 0.07054473459720612\n",
      "[step: 4201] loss: 0.07000398635864258\n",
      "[step: 4202] loss: 0.07506242394447327\n",
      "[step: 4203] loss: 0.07804113626480103\n",
      "[step: 4204] loss: 0.07481373846530914\n",
      "[step: 4205] loss: 0.06990927457809448\n",
      "[step: 4206] loss: 0.06909207999706268\n",
      "[step: 4207] loss: 0.07196099311113358\n",
      "[step: 4208] loss: 0.07375717908143997\n",
      "[step: 4209] loss: 0.07218918204307556\n",
      "[step: 4210] loss: 0.06964238733053207\n",
      "[step: 4211] loss: 0.0689733475446701\n",
      "[step: 4212] loss: 0.07012438774108887\n",
      "[step: 4213] loss: 0.07106917351484299\n",
      "[step: 4214] loss: 0.07063831388950348\n",
      "[step: 4215] loss: 0.06947627663612366\n",
      "[step: 4216] loss: 0.06873056292533875\n",
      "[step: 4217] loss: 0.06890207529067993\n",
      "[step: 4218] loss: 0.06955202668905258\n",
      "[step: 4219] loss: 0.06977254152297974\n",
      "[step: 4220] loss: 0.06918562948703766\n",
      "[step: 4221] loss: 0.06839113682508469\n",
      "[step: 4222] loss: 0.06825242936611176\n",
      "[step: 4223] loss: 0.06875322759151459\n",
      "[step: 4224] loss: 0.06908050179481506\n",
      "[step: 4225] loss: 0.06875370442867279\n",
      "[step: 4226] loss: 0.06816782057285309\n",
      "[step: 4227] loss: 0.06797753274440765\n",
      "[step: 4228] loss: 0.06823593378067017\n",
      "[step: 4229] loss: 0.06846214830875397\n",
      "[step: 4230] loss: 0.06833755970001221\n",
      "[step: 4231] loss: 0.06802351772785187\n",
      "[step: 4232] loss: 0.06783926486968994\n",
      "[step: 4233] loss: 0.06786399334669113\n",
      "[step: 4234] loss: 0.0679582953453064\n",
      "[step: 4235] loss: 0.06797312200069427\n",
      "[step: 4236] loss: 0.06786760687828064\n",
      "[step: 4237] loss: 0.06770966947078705\n",
      "[step: 4238] loss: 0.06760454177856445\n",
      "[step: 4239] loss: 0.06760448962450027\n",
      "[step: 4240] loss: 0.06765568256378174\n",
      "[step: 4241] loss: 0.06765192747116089\n",
      "[step: 4242] loss: 0.06755486875772476\n",
      "[step: 4243] loss: 0.06743121147155762\n",
      "[step: 4244] loss: 0.06737196445465088\n",
      "[step: 4245] loss: 0.06738635152578354\n",
      "[step: 4246] loss: 0.06740152835845947\n",
      "[step: 4247] loss: 0.06736016273498535\n",
      "[step: 4248] loss: 0.06727930903434753\n",
      "[step: 4249] loss: 0.06720902025699615\n",
      "[step: 4250] loss: 0.06717412918806076\n",
      "[step: 4251] loss: 0.06716074794530869\n",
      "[step: 4252] loss: 0.06714249402284622\n",
      "[step: 4253] loss: 0.06710672378540039\n",
      "[step: 4254] loss: 0.06705716252326965\n",
      "[step: 4255] loss: 0.06700511276721954\n",
      "[step: 4256] loss: 0.06696172058582306\n",
      "[step: 4257] loss: 0.06693297624588013\n",
      "[step: 4258] loss: 0.06691142916679382\n",
      "[step: 4259] loss: 0.06688426434993744\n",
      "[step: 4260] loss: 0.0668429434299469\n",
      "[step: 4261] loss: 0.06679465621709824\n",
      "[step: 4262] loss: 0.06675245612859726\n",
      "[step: 4263] loss: 0.06672107428312302\n",
      "[step: 4264] loss: 0.06669507920742035\n",
      "[step: 4265] loss: 0.06666543334722519\n",
      "[step: 4266] loss: 0.06662869453430176\n",
      "[step: 4267] loss: 0.06658867001533508\n",
      "[step: 4268] loss: 0.06654967367649078\n",
      "[step: 4269] loss: 0.06651516258716583\n",
      "[step: 4270] loss: 0.06648246198892593\n",
      "[step: 4271] loss: 0.06645095348358154\n",
      "[step: 4272] loss: 0.06641819328069687\n",
      "[step: 4273] loss: 0.06638385355472565\n",
      "[step: 4274] loss: 0.06634727120399475\n",
      "[step: 4275] loss: 0.06631013751029968\n",
      "[step: 4276] loss: 0.06627489626407623\n",
      "[step: 4277] loss: 0.06624194979667664\n",
      "[step: 4278] loss: 0.06621009111404419\n",
      "[step: 4279] loss: 0.06617698073387146\n",
      "[step: 4280] loss: 0.06614207476377487\n",
      "[step: 4281] loss: 0.06610716879367828\n",
      "[step: 4282] loss: 0.06607198715209961\n",
      "[step: 4283] loss: 0.06603699922561646\n",
      "[step: 4284] loss: 0.06600350886583328\n",
      "[step: 4285] loss: 0.06597030162811279\n",
      "[step: 4286] loss: 0.0659368559718132\n",
      "[step: 4287] loss: 0.06590282917022705\n",
      "[step: 4288] loss: 0.06586892902851105\n",
      "[step: 4289] loss: 0.06583470851182938\n",
      "[step: 4290] loss: 0.06579992175102234\n",
      "[step: 4291] loss: 0.06576573848724365\n",
      "[step: 4292] loss: 0.06573212146759033\n",
      "[step: 4293] loss: 0.06569845974445343\n",
      "[step: 4294] loss: 0.06566501408815384\n",
      "[step: 4295] loss: 0.06563137471675873\n",
      "[step: 4296] loss: 0.06559748202562332\n",
      "[step: 4297] loss: 0.06556347757577896\n",
      "[step: 4298] loss: 0.06552949547767639\n",
      "[step: 4299] loss: 0.06549520045518875\n",
      "[step: 4300] loss: 0.06546157598495483\n",
      "[step: 4301] loss: 0.06542786955833435\n",
      "[step: 4302] loss: 0.06539411842823029\n",
      "[step: 4303] loss: 0.06536030024290085\n",
      "[step: 4304] loss: 0.0653267651796341\n",
      "[step: 4305] loss: 0.06529315561056137\n",
      "[step: 4306] loss: 0.06525928527116776\n",
      "[step: 4307] loss: 0.06522548943758011\n",
      "[step: 4308] loss: 0.06519144773483276\n",
      "[step: 4309] loss: 0.06515807658433914\n",
      "[step: 4310] loss: 0.06512397527694702\n",
      "[step: 4311] loss: 0.06509070098400116\n",
      "[step: 4312] loss: 0.06505699455738068\n",
      "[step: 4313] loss: 0.06502308696508408\n",
      "[step: 4314] loss: 0.06498944014310837\n",
      "[step: 4315] loss: 0.06495586037635803\n",
      "[step: 4316] loss: 0.06492231786251068\n",
      "[step: 4317] loss: 0.06488876044750214\n",
      "[step: 4318] loss: 0.06485497951507568\n",
      "[step: 4319] loss: 0.06482157111167908\n",
      "[step: 4320] loss: 0.06478770077228546\n",
      "[step: 4321] loss: 0.06475408375263214\n",
      "[step: 4322] loss: 0.06472089886665344\n",
      "[step: 4323] loss: 0.06468679010868073\n",
      "[step: 4324] loss: 0.06465315818786621\n",
      "[step: 4325] loss: 0.06461994349956512\n",
      "[step: 4326] loss: 0.06458625942468643\n",
      "[step: 4327] loss: 0.06455236673355103\n",
      "[step: 4328] loss: 0.06451880931854248\n",
      "[step: 4329] loss: 0.06448537111282349\n",
      "[step: 4330] loss: 0.06445173919200897\n",
      "[step: 4331] loss: 0.06441870331764221\n",
      "[step: 4332] loss: 0.06438489258289337\n",
      "[step: 4333] loss: 0.06435145437717438\n",
      "[step: 4334] loss: 0.06431781500577927\n",
      "[step: 4335] loss: 0.06428435444831848\n",
      "[step: 4336] loss: 0.06425099074840546\n",
      "[step: 4337] loss: 0.0642174631357193\n",
      "[step: 4338] loss: 0.06418401747941971\n",
      "[step: 4339] loss: 0.06415091454982758\n",
      "[step: 4340] loss: 0.06411699205636978\n",
      "[step: 4341] loss: 0.06408360600471497\n",
      "[step: 4342] loss: 0.06404988467693329\n",
      "[step: 4343] loss: 0.06401629745960236\n",
      "[step: 4344] loss: 0.06398284435272217\n",
      "[step: 4345] loss: 0.06394971907138824\n",
      "[step: 4346] loss: 0.06391632556915283\n",
      "[step: 4347] loss: 0.06388264149427414\n",
      "[step: 4348] loss: 0.06384965777397156\n",
      "[step: 4349] loss: 0.06381601095199585\n",
      "[step: 4350] loss: 0.06378278136253357\n",
      "[step: 4351] loss: 0.06374908983707428\n",
      "[step: 4352] loss: 0.06371601670980453\n",
      "[step: 4353] loss: 0.06368251144886017\n",
      "[step: 4354] loss: 0.06364927440881729\n",
      "[step: 4355] loss: 0.06361627578735352\n",
      "[step: 4356] loss: 0.06358228623867035\n",
      "[step: 4357] loss: 0.06354924291372299\n",
      "[step: 4358] loss: 0.06351591646671295\n",
      "[step: 4359] loss: 0.06348282098770142\n",
      "[step: 4360] loss: 0.06344941258430481\n",
      "[step: 4361] loss: 0.06341633200645447\n",
      "[step: 4362] loss: 0.06338295340538025\n",
      "[step: 4363] loss: 0.06335054337978363\n",
      "[step: 4364] loss: 0.06331725418567657\n",
      "[step: 4365] loss: 0.06328461319208145\n",
      "[step: 4366] loss: 0.06325245648622513\n",
      "[step: 4367] loss: 0.06322145462036133\n",
      "[step: 4368] loss: 0.06319057941436768\n",
      "[step: 4369] loss: 0.06316221505403519\n",
      "[step: 4370] loss: 0.06313596665859222\n",
      "[step: 4371] loss: 0.06311450898647308\n",
      "[step: 4372] loss: 0.06310102343559265\n",
      "[step: 4373] loss: 0.06309942901134491\n",
      "[step: 4374] loss: 0.0631198137998581\n",
      "[step: 4375] loss: 0.06317625939846039\n",
      "[step: 4376] loss: 0.06329652667045593\n",
      "[step: 4377] loss: 0.06352385133504868\n",
      "[step: 4378] loss: 0.06394220888614655\n",
      "[step: 4379] loss: 0.06468072533607483\n",
      "[step: 4380] loss: 0.0659978985786438\n",
      "[step: 4381] loss: 0.06823929399251938\n",
      "[step: 4382] loss: 0.07212898880243301\n",
      "[step: 4383] loss: 0.07825620472431183\n",
      "[step: 4384] loss: 0.08791612833738327\n",
      "[step: 4385] loss: 0.09978123009204865\n",
      "[step: 4386] loss: 0.11271494626998901\n",
      "[step: 4387] loss: 0.11663223803043365\n",
      "[step: 4388] loss: 0.10882358998060226\n",
      "[step: 4389] loss: 0.08820541948080063\n",
      "[step: 4390] loss: 0.0701182633638382\n",
      "[step: 4391] loss: 0.06495172530412674\n",
      "[step: 4392] loss: 0.0721038281917572\n",
      "[step: 4393] loss: 0.0820227712392807\n",
      "[step: 4394] loss: 0.0843987762928009\n",
      "[step: 4395] loss: 0.07799586653709412\n",
      "[step: 4396] loss: 0.06856966018676758\n",
      "[step: 4397] loss: 0.06443529576063156\n",
      "[step: 4398] loss: 0.0674213394522667\n",
      "[step: 4399] loss: 0.07272227108478546\n",
      "[step: 4400] loss: 0.07447373867034912\n",
      "[step: 4401] loss: 0.07040736824274063\n",
      "[step: 4402] loss: 0.06468300521373749\n",
      "[step: 4403] loss: 0.06276129186153412\n",
      "[step: 4404] loss: 0.06569038331508636\n",
      "[step: 4405] loss: 0.06912409514188766\n",
      "[step: 4406] loss: 0.0686408057808876\n",
      "[step: 4407] loss: 0.06493733078241348\n",
      "[step: 4408] loss: 0.06217414513230324\n",
      "[step: 4409] loss: 0.06291666626930237\n",
      "[step: 4410] loss: 0.06532987952232361\n",
      "[step: 4411] loss: 0.06604476273059845\n",
      "[step: 4412] loss: 0.06436838209629059\n",
      "[step: 4413] loss: 0.062442392110824585\n",
      "[step: 4414] loss: 0.062176819890737534\n",
      "[step: 4415] loss: 0.06315167993307114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4416] loss: 0.0638420358300209\n",
      "[step: 4417] loss: 0.06353363394737244\n",
      "[step: 4418] loss: 0.06266530603170395\n",
      "[step: 4419] loss: 0.06205626577138901\n",
      "[step: 4420] loss: 0.06204380840063095\n",
      "[step: 4421] loss: 0.06239516660571098\n",
      "[step: 4422] loss: 0.06266091018915176\n",
      "[step: 4423] loss: 0.06252928078174591\n",
      "[step: 4424] loss: 0.06206064671278\n",
      "[step: 4425] loss: 0.06163286417722702\n",
      "[step: 4426] loss: 0.06160786747932434\n",
      "[step: 4427] loss: 0.06190941482782364\n",
      "[step: 4428] loss: 0.06210048124194145\n",
      "[step: 4429] loss: 0.06191253289580345\n",
      "[step: 4430] loss: 0.06152573227882385\n",
      "[step: 4431] loss: 0.06131624057888985\n",
      "[step: 4432] loss: 0.06141034513711929\n",
      "[step: 4433] loss: 0.061594050377607346\n",
      "[step: 4434] loss: 0.06160729378461838\n",
      "[step: 4435] loss: 0.061428606510162354\n",
      "[step: 4436] loss: 0.061236780136823654\n",
      "[step: 4437] loss: 0.06116616353392601\n",
      "[step: 4438] loss: 0.06119925528764725\n",
      "[step: 4439] loss: 0.061240360140800476\n",
      "[step: 4440] loss: 0.061225708574056625\n",
      "[step: 4441] loss: 0.06115264445543289\n",
      "[step: 4442] loss: 0.06105789542198181\n",
      "[step: 4443] loss: 0.06098278984427452\n",
      "[step: 4444] loss: 0.06095170974731445\n",
      "[step: 4445] loss: 0.0609574168920517\n",
      "[step: 4446] loss: 0.06096472963690758\n",
      "[step: 4447] loss: 0.060933537781238556\n",
      "[step: 4448] loss: 0.06086160987615585\n",
      "[step: 4449] loss: 0.06078613921999931\n",
      "[step: 4450] loss: 0.060745880007743835\n",
      "[step: 4451] loss: 0.060739967972040176\n",
      "[step: 4452] loss: 0.060738276690244675\n",
      "[step: 4453] loss: 0.060712359845638275\n",
      "[step: 4454] loss: 0.06066209077835083\n",
      "[step: 4455] loss: 0.06060902401804924\n",
      "[step: 4456] loss: 0.060569100081920624\n",
      "[step: 4457] loss: 0.06054374948143959\n",
      "[step: 4458] loss: 0.06052366644144058\n",
      "[step: 4459] loss: 0.0604996420443058\n",
      "[step: 4460] loss: 0.060469310730695724\n",
      "[step: 4461] loss: 0.06043388694524765\n",
      "[step: 4462] loss: 0.06039588153362274\n",
      "[step: 4463] loss: 0.06035874783992767\n",
      "[step: 4464] loss: 0.060327257961034775\n",
      "[step: 4465] loss: 0.060301173478364944\n",
      "[step: 4466] loss: 0.06027747690677643\n",
      "[step: 4467] loss: 0.06025006249547005\n",
      "[step: 4468] loss: 0.06021711975336075\n",
      "[step: 4469] loss: 0.06018131226301193\n",
      "[step: 4470] loss: 0.06014648824930191\n",
      "[step: 4471] loss: 0.06011595577001572\n",
      "[step: 4472] loss: 0.060088928788900375\n",
      "[step: 4473] loss: 0.0600617341697216\n",
      "[step: 4474] loss: 0.060032621026039124\n",
      "[step: 4475] loss: 0.06000189483165741\n",
      "[step: 4476] loss: 0.0599706694483757\n",
      "[step: 4477] loss: 0.059938736259937286\n",
      "[step: 4478] loss: 0.0599074512720108\n",
      "[step: 4479] loss: 0.059877097606658936\n",
      "[step: 4480] loss: 0.05984736979007721\n",
      "[step: 4481] loss: 0.05981862545013428\n",
      "[step: 4482] loss: 0.059789955615997314\n",
      "[step: 4483] loss: 0.05976037681102753\n",
      "[step: 4484] loss: 0.059729255735874176\n",
      "[step: 4485] loss: 0.059698108583688736\n",
      "[step: 4486] loss: 0.05966736376285553\n",
      "[step: 4487] loss: 0.05963733792304993\n",
      "[step: 4488] loss: 0.05960739403963089\n",
      "[step: 4489] loss: 0.05957827717065811\n",
      "[step: 4490] loss: 0.05954894796013832\n",
      "[step: 4491] loss: 0.05951883643865585\n",
      "[step: 4492] loss: 0.05948880314826965\n",
      "[step: 4493] loss: 0.05945869907736778\n",
      "[step: 4494] loss: 0.05942839756608009\n",
      "[step: 4495] loss: 0.05939856544137001\n",
      "[step: 4496] loss: 0.05936810374259949\n",
      "[step: 4497] loss: 0.05933768302202225\n",
      "[step: 4498] loss: 0.05930796265602112\n",
      "[step: 4499] loss: 0.05927840620279312\n",
      "[step: 4500] loss: 0.05924879014492035\n",
      "[step: 4501] loss: 0.05921909585595131\n",
      "[step: 4502] loss: 0.0591888427734375\n",
      "[step: 4503] loss: 0.05915897339582443\n",
      "[step: 4504] loss: 0.05912864953279495\n",
      "[step: 4505] loss: 0.059098728001117706\n",
      "[step: 4506] loss: 0.05906917154788971\n",
      "[step: 4507] loss: 0.059038594365119934\n",
      "[step: 4508] loss: 0.059008873999118805\n",
      "[step: 4509] loss: 0.05897906422615051\n",
      "[step: 4510] loss: 0.058949220925569534\n",
      "[step: 4511] loss: 0.05891910940408707\n",
      "[step: 4512] loss: 0.05888914316892624\n",
      "[step: 4513] loss: 0.05885941907763481\n",
      "[step: 4514] loss: 0.05882953107357025\n",
      "[step: 4515] loss: 0.058799322694540024\n",
      "[step: 4516] loss: 0.058769360184669495\n",
      "[step: 4517] loss: 0.05873916298151016\n",
      "[step: 4518] loss: 0.058709222823381424\n",
      "[step: 4519] loss: 0.05867942050099373\n",
      "[step: 4520] loss: 0.05864947289228439\n",
      "[step: 4521] loss: 0.058619238436222076\n",
      "[step: 4522] loss: 0.05858949199318886\n",
      "[step: 4523] loss: 0.058559294790029526\n",
      "[step: 4524] loss: 0.0585293285548687\n",
      "[step: 4525] loss: 0.05849931016564369\n",
      "[step: 4526] loss: 0.05846909061074257\n",
      "[step: 4527] loss: 0.05843931436538696\n",
      "[step: 4528] loss: 0.05840948224067688\n",
      "[step: 4529] loss: 0.05837973207235336\n",
      "[step: 4530] loss: 0.05834949016571045\n",
      "[step: 4531] loss: 0.058319419622421265\n",
      "[step: 4532] loss: 0.05828940123319626\n",
      "[step: 4533] loss: 0.058259546756744385\n",
      "[step: 4534] loss: 0.05822957307100296\n",
      "[step: 4535] loss: 0.058199215680360794\n",
      "[step: 4536] loss: 0.05816967040300369\n",
      "[step: 4537] loss: 0.05813964456319809\n",
      "[step: 4538] loss: 0.05810931324958801\n",
      "[step: 4539] loss: 0.05807957798242569\n",
      "[step: 4540] loss: 0.058049220591783524\n",
      "[step: 4541] loss: 0.05801929906010628\n",
      "[step: 4542] loss: 0.05798948556184769\n",
      "[step: 4543] loss: 0.057959236204624176\n",
      "[step: 4544] loss: 0.05792911350727081\n",
      "[step: 4545] loss: 0.05789899826049805\n",
      "[step: 4546] loss: 0.05786922946572304\n",
      "[step: 4547] loss: 0.05783948302268982\n",
      "[step: 4548] loss: 0.057809390127658844\n",
      "[step: 4549] loss: 0.057779621332883835\n",
      "[step: 4550] loss: 0.05774983763694763\n",
      "[step: 4551] loss: 0.05772043764591217\n",
      "[step: 4552] loss: 0.057691507041454315\n",
      "[step: 4553] loss: 0.05766221880912781\n",
      "[step: 4554] loss: 0.05763404071331024\n",
      "[step: 4555] loss: 0.05760688707232475\n",
      "[step: 4556] loss: 0.0575808621942997\n",
      "[step: 4557] loss: 0.057557668536901474\n",
      "[step: 4558] loss: 0.057538021355867386\n",
      "[step: 4559] loss: 0.05752498283982277\n",
      "[step: 4560] loss: 0.05752211809158325\n",
      "[step: 4561] loss: 0.05753561109304428\n",
      "[step: 4562] loss: 0.05757671222090721\n",
      "[step: 4563] loss: 0.05766448751091957\n",
      "[step: 4564] loss: 0.05782805010676384\n",
      "[step: 4565] loss: 0.05812520906329155\n",
      "[step: 4566] loss: 0.05863700807094574\n",
      "[step: 4567] loss: 0.05953194200992584\n",
      "[step: 4568] loss: 0.061024557799100876\n",
      "[step: 4569] loss: 0.06358874589204788\n",
      "[step: 4570] loss: 0.06763056665658951\n",
      "[step: 4571] loss: 0.07420745491981506\n",
      "[step: 4572] loss: 0.08302219212055206\n",
      "[step: 4573] loss: 0.09473943710327148\n",
      "[step: 4574] loss: 0.10347948968410492\n",
      "[step: 4575] loss: 0.10678163170814514\n",
      "[step: 4576] loss: 0.09593434631824493\n",
      "[step: 4577] loss: 0.07838940620422363\n",
      "[step: 4578] loss: 0.06319831311702728\n",
      "[step: 4579] loss: 0.05939166247844696\n",
      "[step: 4580] loss: 0.06560555100440979\n",
      "[step: 4581] loss: 0.07373262941837311\n",
      "[step: 4582] loss: 0.07657603919506073\n",
      "[step: 4583] loss: 0.0712871178984642\n",
      "[step: 4584] loss: 0.06354883313179016\n",
      "[step: 4585] loss: 0.05930304154753685\n",
      "[step: 4586] loss: 0.06053803861141205\n",
      "[step: 4587] loss: 0.06446127593517303\n",
      "[step: 4588] loss: 0.06672099977731705\n",
      "[step: 4589] loss: 0.06532195955514908\n",
      "[step: 4590] loss: 0.061181943863630295\n",
      "[step: 4591] loss: 0.05793776735663414\n",
      "[step: 4592] loss: 0.05801774933934212\n",
      "[step: 4593] loss: 0.060537178069353104\n",
      "[step: 4594] loss: 0.062417931854724884\n",
      "[step: 4595] loss: 0.06143289431929588\n",
      "[step: 4596] loss: 0.058551229536533356\n",
      "[step: 4597] loss: 0.05661733075976372\n",
      "[step: 4598] loss: 0.057253606617450714\n",
      "[step: 4599] loss: 0.05912750959396362\n",
      "[step: 4600] loss: 0.05981912463903427\n",
      "[step: 4601] loss: 0.05862991511821747\n",
      "[step: 4602] loss: 0.05693057179450989\n",
      "[step: 4603] loss: 0.05637636035680771\n",
      "[step: 4604] loss: 0.05710034444928169\n",
      "[step: 4605] loss: 0.05793033912777901\n",
      "[step: 4606] loss: 0.05789261683821678\n",
      "[step: 4607] loss: 0.05714646354317665\n",
      "[step: 4608] loss: 0.05650610476732254\n",
      "[step: 4609] loss: 0.05638931319117546\n",
      "[step: 4610] loss: 0.05662282928824425\n",
      "[step: 4611] loss: 0.05683835223317146\n",
      "[step: 4612] loss: 0.05683121830224991\n",
      "[step: 4613] loss: 0.05662614479660988\n",
      "[step: 4614] loss: 0.056341372430324554\n",
      "[step: 4615] loss: 0.05612964183092117\n",
      "[step: 4616] loss: 0.05609332397580147\n",
      "[step: 4617] loss: 0.056220024824142456\n",
      "[step: 4618] loss: 0.05634712427854538\n",
      "[step: 4619] loss: 0.05629466846585274\n",
      "[step: 4620] loss: 0.05607033520936966\n",
      "[step: 4621] loss: 0.055846042931079865\n",
      "[step: 4622] loss: 0.055788803845644\n",
      "[step: 4623] loss: 0.055888816714286804\n",
      "[step: 4624] loss: 0.05598888918757439\n",
      "[step: 4625] loss: 0.055955737829208374\n",
      "[step: 4626] loss: 0.05580636486411095\n",
      "[step: 4627] loss: 0.05566515028476715\n",
      "[step: 4628] loss: 0.0556136779487133\n",
      "[step: 4629] loss: 0.05563664436340332\n",
      "[step: 4630] loss: 0.055665235966444016\n",
      "[step: 4631] loss: 0.05564872920513153\n",
      "[step: 4632] loss: 0.05558980256319046\n",
      "[step: 4633] loss: 0.055519647896289825\n",
      "[step: 4634] loss: 0.05546320974826813\n",
      "[step: 4635] loss: 0.055425968021154404\n",
      "[step: 4636] loss: 0.05540569871664047\n",
      "[step: 4637] loss: 0.05539544299244881\n",
      "[step: 4638] loss: 0.055382050573825836\n",
      "[step: 4639] loss: 0.05535295978188515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4640] loss: 0.055305320769548416\n",
      "[step: 4641] loss: 0.05525151640176773\n",
      "[step: 4642] loss: 0.055209577083587646\n",
      "[step: 4643] loss: 0.05518781393766403\n",
      "[step: 4644] loss: 0.055176496505737305\n",
      "[step: 4645] loss: 0.05516039580106735\n",
      "[step: 4646] loss: 0.055130138993263245\n",
      "[step: 4647] loss: 0.05508890002965927\n",
      "[step: 4648] loss: 0.055047690868377686\n",
      "[step: 4649] loss: 0.05501437932252884\n",
      "[step: 4650] loss: 0.054989442229270935\n",
      "[step: 4651] loss: 0.054966654628515244\n",
      "[step: 4652] loss: 0.05494219437241554\n",
      "[step: 4653] loss: 0.05491446703672409\n",
      "[step: 4654] loss: 0.05488486588001251\n",
      "[step: 4655] loss: 0.05485362932085991\n",
      "[step: 4656] loss: 0.05482204258441925\n",
      "[step: 4657] loss: 0.054790861904621124\n",
      "[step: 4658] loss: 0.05476149916648865\n",
      "[step: 4659] loss: 0.05473420023918152\n",
      "[step: 4660] loss: 0.054708562791347504\n",
      "[step: 4661] loss: 0.05468318238854408\n",
      "[step: 4662] loss: 0.05465519428253174\n",
      "[step: 4663] loss: 0.05462545156478882\n",
      "[step: 4664] loss: 0.05459437519311905\n",
      "[step: 4665] loss: 0.05456444248557091\n",
      "[step: 4666] loss: 0.05453559011220932\n",
      "[step: 4667] loss: 0.05450823903083801\n",
      "[step: 4668] loss: 0.0544808954000473\n",
      "[step: 4669] loss: 0.054453589022159576\n",
      "[step: 4670] loss: 0.05442565679550171\n",
      "[step: 4671] loss: 0.054397132247686386\n",
      "[step: 4672] loss: 0.05436849966645241\n",
      "[step: 4673] loss: 0.05433971434831619\n",
      "[step: 4674] loss: 0.05431099236011505\n",
      "[step: 4675] loss: 0.05428222939372063\n",
      "[step: 4676] loss: 0.05425369367003441\n",
      "[step: 4677] loss: 0.05422510206699371\n",
      "[step: 4678] loss: 0.05419744923710823\n",
      "[step: 4679] loss: 0.05416963994503021\n",
      "[step: 4680] loss: 0.054141949862241745\n",
      "[step: 4681] loss: 0.05411393195390701\n",
      "[step: 4682] loss: 0.05408518761396408\n",
      "[step: 4683] loss: 0.05405667796730995\n",
      "[step: 4684] loss: 0.054028186947107315\n",
      "[step: 4685] loss: 0.05399961397051811\n",
      "[step: 4686] loss: 0.053971439599990845\n",
      "[step: 4687] loss: 0.053943000733852386\n",
      "[step: 4688] loss: 0.053914349526166916\n",
      "[step: 4689] loss: 0.05388592183589935\n",
      "[step: 4690] loss: 0.05385759472846985\n",
      "[step: 4691] loss: 0.05382975935935974\n",
      "[step: 4692] loss: 0.053801022469997406\n",
      "[step: 4693] loss: 0.05377289652824402\n",
      "[step: 4694] loss: 0.053744614124298096\n",
      "[step: 4695] loss: 0.053716011345386505\n",
      "[step: 4696] loss: 0.05368728190660477\n",
      "[step: 4697] loss: 0.05365907400846481\n",
      "[step: 4698] loss: 0.05363060534000397\n",
      "[step: 4699] loss: 0.053602010011672974\n",
      "[step: 4700] loss: 0.053573429584503174\n",
      "[step: 4701] loss: 0.05354534089565277\n",
      "[step: 4702] loss: 0.05351662635803223\n",
      "[step: 4703] loss: 0.053488172590732574\n",
      "[step: 4704] loss: 0.05345964431762695\n",
      "[step: 4705] loss: 0.05343098193407059\n",
      "[step: 4706] loss: 0.05340253561735153\n",
      "[step: 4707] loss: 0.05337393656373024\n",
      "[step: 4708] loss: 0.053345419466495514\n",
      "[step: 4709] loss: 0.053317222744226456\n",
      "[step: 4710] loss: 0.053288478404283524\n",
      "[step: 4711] loss: 0.05326005443930626\n",
      "[step: 4712] loss: 0.0532313734292984\n",
      "[step: 4713] loss: 0.05320281535387039\n",
      "[step: 4714] loss: 0.05317419767379761\n",
      "[step: 4715] loss: 0.05314565822482109\n",
      "[step: 4716] loss: 0.05311668664216995\n",
      "[step: 4717] loss: 0.053088560700416565\n",
      "[step: 4718] loss: 0.053059741854667664\n",
      "[step: 4719] loss: 0.053031064569950104\n",
      "[step: 4720] loss: 0.05300230532884598\n",
      "[step: 4721] loss: 0.052973754703998566\n",
      "[step: 4722] loss: 0.052945271134376526\n",
      "[step: 4723] loss: 0.052916571497917175\n",
      "[step: 4724] loss: 0.052888259291648865\n",
      "[step: 4725] loss: 0.05285979062318802\n",
      "[step: 4726] loss: 0.05283138528466225\n",
      "[step: 4727] loss: 0.052803266793489456\n",
      "[step: 4728] loss: 0.052775196731090546\n",
      "[step: 4729] loss: 0.05274772644042969\n",
      "[step: 4730] loss: 0.052720632404088974\n",
      "[step: 4731] loss: 0.05269443243741989\n",
      "[step: 4732] loss: 0.05267002433538437\n",
      "[step: 4733] loss: 0.052647240459918976\n",
      "[step: 4734] loss: 0.05262770503759384\n",
      "[step: 4735] loss: 0.052613843232393265\n",
      "[step: 4736] loss: 0.052607789635658264\n",
      "[step: 4737] loss: 0.052615195512771606\n",
      "[step: 4738] loss: 0.05264504998922348\n",
      "[step: 4739] loss: 0.05270963907241821\n",
      "[step: 4740] loss: 0.052834104746580124\n",
      "[step: 4741] loss: 0.053052980452775955\n",
      "[step: 4742] loss: 0.05343855544924736\n",
      "[step: 4743] loss: 0.05408208817243576\n",
      "[step: 4744] loss: 0.0551903173327446\n",
      "[step: 4745] loss: 0.05697870999574661\n",
      "[step: 4746] loss: 0.06000745669007301\n",
      "[step: 4747] loss: 0.0645659863948822\n",
      "[step: 4748] loss: 0.07181353867053986\n",
      "[step: 4749] loss: 0.08071696013212204\n",
      "[step: 4750] loss: 0.09191420674324036\n",
      "[step: 4751] loss: 0.09806790947914124\n",
      "[step: 4752] loss: 0.09808562695980072\n",
      "[step: 4753] loss: 0.08497291803359985\n",
      "[step: 4754] loss: 0.06820227205753326\n",
      "[step: 4755] loss: 0.056236401200294495\n",
      "[step: 4756] loss: 0.05515239015221596\n",
      "[step: 4757] loss: 0.06196804344654083\n",
      "[step: 4758] loss: 0.0687682032585144\n",
      "[step: 4759] loss: 0.07007312774658203\n",
      "[step: 4760] loss: 0.06435155868530273\n",
      "[step: 4761] loss: 0.057538051158189774\n",
      "[step: 4762] loss: 0.05448276549577713\n",
      "[step: 4763] loss: 0.056004442274570465\n",
      "[step: 4764] loss: 0.059228114783763885\n",
      "[step: 4765] loss: 0.06067395582795143\n",
      "[step: 4766] loss: 0.059376757591962814\n",
      "[step: 4767] loss: 0.056212808936834335\n",
      "[step: 4768] loss: 0.05368979275226593\n",
      "[step: 4769] loss: 0.053298220038414\n",
      "[step: 4770] loss: 0.05471010506153107\n",
      "[step: 4771] loss: 0.05626257508993149\n",
      "[step: 4772] loss: 0.056210510432720184\n",
      "[step: 4773] loss: 0.05446437746286392\n",
      "[step: 4774] loss: 0.052478838711977005\n",
      "[step: 4775] loss: 0.05194297805428505\n",
      "[step: 4776] loss: 0.052993692457675934\n",
      "[step: 4777] loss: 0.05420700088143349\n",
      "[step: 4778] loss: 0.05419427901506424\n",
      "[step: 4779] loss: 0.05290082469582558\n",
      "[step: 4780] loss: 0.05162564292550087\n",
      "[step: 4781] loss: 0.05149190500378609\n",
      "[step: 4782] loss: 0.05229320004582405\n",
      "[step: 4783] loss: 0.05293416976928711\n",
      "[step: 4784] loss: 0.05271202325820923\n",
      "[step: 4785] loss: 0.05193834379315376\n",
      "[step: 4786] loss: 0.05136752501130104\n",
      "[step: 4787] loss: 0.051386758685112\n",
      "[step: 4788] loss: 0.051734380424022675\n",
      "[step: 4789] loss: 0.051929719746112823\n",
      "[step: 4790] loss: 0.051796913146972656\n",
      "[step: 4791] loss: 0.05150669813156128\n",
      "[step: 4792] loss: 0.0512937530875206\n",
      "[step: 4793] loss: 0.05122606083750725\n",
      "[step: 4794] loss: 0.051242586225271225\n",
      "[step: 4795] loss: 0.05127172917127609\n",
      "[step: 4796] loss: 0.051280438899993896\n",
      "[step: 4797] loss: 0.05125034600496292\n",
      "[step: 4798] loss: 0.05115459859371185\n",
      "[step: 4799] loss: 0.051015838980674744\n",
      "[step: 4800] loss: 0.050909727811813354\n",
      "[step: 4801] loss: 0.05089554563164711\n",
      "[step: 4802] loss: 0.050949737429618835\n",
      "[step: 4803] loss: 0.05098847299814224\n",
      "[step: 4804] loss: 0.05094459280371666\n",
      "[step: 4805] loss: 0.05082714557647705\n",
      "[step: 4806] loss: 0.05071500688791275\n",
      "[step: 4807] loss: 0.05067218840122223\n",
      "[step: 4808] loss: 0.050691381096839905\n",
      "[step: 4809] loss: 0.0507168211042881\n",
      "[step: 4810] loss: 0.05070040374994278\n",
      "[step: 4811] loss: 0.05064120888710022\n",
      "[step: 4812] loss: 0.05057001858949661\n",
      "[step: 4813] loss: 0.050519175827503204\n",
      "[step: 4814] loss: 0.050493981689214706\n",
      "[step: 4815] loss: 0.050480108708143234\n",
      "[step: 4816] loss: 0.05046337470412254\n",
      "[step: 4817] loss: 0.050438426434993744\n",
      "[step: 4818] loss: 0.05040759593248367\n",
      "[step: 4819] loss: 0.05037260800600052\n",
      "[step: 4820] loss: 0.05033554509282112\n",
      "[step: 4821] loss: 0.050297923386096954\n",
      "[step: 4822] loss: 0.05026446282863617\n",
      "[step: 4823] loss: 0.050238803029060364\n",
      "[step: 4824] loss: 0.050219498574733734\n",
      "[step: 4825] loss: 0.05019935965538025\n",
      "[step: 4826] loss: 0.050173163414001465\n",
      "[step: 4827] loss: 0.05013919621706009\n",
      "[step: 4828] loss: 0.05010174214839935\n",
      "[step: 4829] loss: 0.05006783455610275\n",
      "[step: 4830] loss: 0.0500398613512516\n",
      "[step: 4831] loss: 0.05001603439450264\n",
      "[step: 4832] loss: 0.04999284818768501\n",
      "[step: 4833] loss: 0.04996742308139801\n",
      "[step: 4834] loss: 0.04993845894932747\n",
      "[step: 4835] loss: 0.049908347427845\n",
      "[step: 4836] loss: 0.0498783104121685\n",
      "[step: 4837] loss: 0.04984929412603378\n",
      "[step: 4838] loss: 0.04982095956802368\n",
      "[step: 4839] loss: 0.049792852252721786\n",
      "[step: 4840] loss: 0.049765102565288544\n",
      "[step: 4841] loss: 0.04973769187927246\n",
      "[step: 4842] loss: 0.0497112013399601\n",
      "[step: 4843] loss: 0.049684181809425354\n",
      "[step: 4844] loss: 0.04965685307979584\n",
      "[step: 4845] loss: 0.049628280103206635\n",
      "[step: 4846] loss: 0.04959946870803833\n",
      "[step: 4847] loss: 0.0495704747736454\n",
      "[step: 4848] loss: 0.04954157769680023\n",
      "[step: 4849] loss: 0.04951406270265579\n",
      "[step: 4850] loss: 0.049486223608255386\n",
      "[step: 4851] loss: 0.04945944994688034\n",
      "[step: 4852] loss: 0.04943190515041351\n",
      "[step: 4853] loss: 0.04940393939614296\n",
      "[step: 4854] loss: 0.04937639459967613\n",
      "[step: 4855] loss: 0.04934816062450409\n",
      "[step: 4856] loss: 0.049320243299007416\n",
      "[step: 4857] loss: 0.049292199313640594\n",
      "[step: 4858] loss: 0.04926413297653198\n",
      "[step: 4859] loss: 0.0492359921336174\n",
      "[step: 4860] loss: 0.049207653850317\n",
      "[step: 4861] loss: 0.049179717898368835\n",
      "[step: 4862] loss: 0.049151450395584106\n",
      "[step: 4863] loss: 0.04912406951189041\n",
      "[step: 4864] loss: 0.049095913767814636\n",
      "[step: 4865] loss: 0.04906829074025154\n",
      "[step: 4866] loss: 0.04904009401798248\n",
      "[step: 4867] loss: 0.04901235178112984\n",
      "[step: 4868] loss: 0.048984095454216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4869] loss: 0.04895594343543053\n",
      "[step: 4870] loss: 0.04892796277999878\n",
      "[step: 4871] loss: 0.048899613320827484\n",
      "[step: 4872] loss: 0.048871587961912155\n",
      "[step: 4873] loss: 0.048843346536159515\n",
      "[step: 4874] loss: 0.04881509765982628\n",
      "[step: 4875] loss: 0.048787035048007965\n",
      "[step: 4876] loss: 0.04875888302922249\n",
      "[step: 4877] loss: 0.048730283975601196\n",
      "[step: 4878] loss: 0.04870249330997467\n",
      "[step: 4879] loss: 0.04867401719093323\n",
      "[step: 4880] loss: 0.04864596575498581\n",
      "[step: 4881] loss: 0.0486174002289772\n",
      "[step: 4882] loss: 0.048589400947093964\n",
      "[step: 4883] loss: 0.04856094717979431\n",
      "[step: 4884] loss: 0.04853282496333122\n",
      "[step: 4885] loss: 0.04850448668003082\n",
      "[step: 4886] loss: 0.04847622290253639\n",
      "[step: 4887] loss: 0.048447851091623306\n",
      "[step: 4888] loss: 0.04841940850019455\n",
      "[step: 4889] loss: 0.04839138314127922\n",
      "[step: 4890] loss: 0.048362936824560165\n",
      "[step: 4891] loss: 0.048334263265132904\n",
      "[step: 4892] loss: 0.048306044191122055\n",
      "[step: 4893] loss: 0.04827747121453285\n",
      "[step: 4894] loss: 0.04824887216091156\n",
      "[step: 4895] loss: 0.04822045937180519\n",
      "[step: 4896] loss: 0.04819176346063614\n",
      "[step: 4897] loss: 0.04816371947526932\n",
      "[step: 4898] loss: 0.048135094344615936\n",
      "[step: 4899] loss: 0.0481065958738327\n",
      "[step: 4900] loss: 0.04807829484343529\n",
      "[step: 4901] loss: 0.04804972559213638\n",
      "[step: 4902] loss: 0.0480211041867733\n",
      "[step: 4903] loss: 0.04799259081482887\n",
      "[step: 4904] loss: 0.047964200377464294\n",
      "[step: 4905] loss: 0.047935646027326584\n",
      "[step: 4906] loss: 0.04790739715099335\n",
      "[step: 4907] loss: 0.0478791780769825\n",
      "[step: 4908] loss: 0.047851480543613434\n",
      "[step: 4909] loss: 0.04782413691282272\n",
      "[step: 4910] loss: 0.04779757931828499\n",
      "[step: 4911] loss: 0.04777245223522186\n",
      "[step: 4912] loss: 0.04774952679872513\n",
      "[step: 4913] loss: 0.04773106426000595\n",
      "[step: 4914] loss: 0.04771898314356804\n",
      "[step: 4915] loss: 0.04771876335144043\n",
      "[step: 4916] loss: 0.047738999128341675\n",
      "[step: 4917] loss: 0.047795023769140244\n",
      "[step: 4918] loss: 0.047914110124111176\n",
      "[step: 4919] loss: 0.04814278334379196\n",
      "[step: 4920] loss: 0.048573702573776245\n",
      "[step: 4921] loss: 0.049343761056661606\n",
      "[step: 4922] loss: 0.05076352879405022\n",
      "[step: 4923] loss: 0.05320295691490173\n",
      "[step: 4924] loss: 0.0576319620013237\n",
      "[step: 4925] loss: 0.06461143493652344\n",
      "[step: 4926] loss: 0.07637462019920349\n",
      "[step: 4927] loss: 0.09043154865503311\n",
      "[step: 4928] loss: 0.10756075382232666\n",
      "[step: 4929] loss: 0.11134622991085052\n",
      "[step: 4930] loss: 0.10205869376659393\n",
      "[step: 4931] loss: 0.07578381896018982\n",
      "[step: 4932] loss: 0.05524162948131561\n",
      "[step: 4933] loss: 0.05252118408679962\n",
      "[step: 4934] loss: 0.06353870034217834\n",
      "[step: 4935] loss: 0.07395878434181213\n",
      "[step: 4936] loss: 0.07088729739189148\n",
      "[step: 4937] loss: 0.060153309255838394\n",
      "[step: 4938] loss: 0.05238018184900284\n",
      "[step: 4939] loss: 0.05363840237259865\n",
      "[step: 4940] loss: 0.05877520889043808\n",
      "[step: 4941] loss: 0.06002392619848251\n",
      "[step: 4942] loss: 0.05693706497550011\n",
      "[step: 4943] loss: 0.052782684564590454\n",
      "[step: 4944] loss: 0.05105739086866379\n",
      "[step: 4945] loss: 0.051619160920381546\n",
      "[step: 4946] loss: 0.05263451114296913\n",
      "[step: 4947] loss: 0.05305065959692001\n",
      "[step: 4948] loss: 0.05197577923536301\n",
      "[step: 4949] loss: 0.04981008172035217\n",
      "[step: 4950] loss: 0.048357363790273666\n",
      "[step: 4951] loss: 0.04911965876817703\n",
      "[step: 4952] loss: 0.05081569403409958\n",
      "[step: 4953] loss: 0.05065568909049034\n",
      "[step: 4954] loss: 0.04844139516353607\n",
      "[step: 4955] loss: 0.046927712857723236\n",
      "[step: 4956] loss: 0.04782577604055405\n",
      "[step: 4957] loss: 0.049389734864234924\n",
      "[step: 4958] loss: 0.04914116859436035\n",
      "[step: 4959] loss: 0.047418005764484406\n",
      "[step: 4960] loss: 0.04654315859079361\n",
      "[step: 4961] loss: 0.04730634018778801\n",
      "[step: 4962] loss: 0.04816543683409691\n",
      "[step: 4963] loss: 0.047828834503889084\n",
      "[step: 4964] loss: 0.04691734164953232\n",
      "[step: 4965] loss: 0.04660937935113907\n",
      "[step: 4966] loss: 0.046938132494688034\n",
      "[step: 4967] loss: 0.04714296758174896\n",
      "[step: 4968] loss: 0.04694657400250435\n",
      "[step: 4969] loss: 0.04670802503824234\n",
      "[step: 4970] loss: 0.04663994535803795\n",
      "[step: 4971] loss: 0.04658738896250725\n",
      "[step: 4972] loss: 0.04646722227334976\n",
      "[step: 4973] loss: 0.04643777757883072\n",
      "[step: 4974] loss: 0.04652881994843483\n",
      "[step: 4975] loss: 0.046512678265571594\n",
      "[step: 4976] loss: 0.04630139470100403\n",
      "[step: 4977] loss: 0.046114224940538406\n",
      "[step: 4978] loss: 0.04615287482738495\n",
      "[step: 4979] loss: 0.04629403352737427\n",
      "[step: 4980] loss: 0.04628303274512291\n",
      "[step: 4981] loss: 0.0461016520857811\n",
      "[step: 4982] loss: 0.04595286399126053\n",
      "[step: 4983] loss: 0.04596345126628876\n",
      "[step: 4984] loss: 0.0460350401699543\n",
      "[step: 4985] loss: 0.046024613082408905\n",
      "[step: 4986] loss: 0.04593214392662048\n",
      "[step: 4987] loss: 0.04585203528404236\n",
      "[step: 4988] loss: 0.04582629352807999\n",
      "[step: 4989] loss: 0.045817866921424866\n",
      "[step: 4990] loss: 0.045791368931531906\n",
      "[step: 4991] loss: 0.04575715214014053\n",
      "[step: 4992] loss: 0.04573184624314308\n",
      "[step: 4993] loss: 0.045703258365392685\n",
      "[step: 4994] loss: 0.04565727710723877\n",
      "[step: 4995] loss: 0.04561016708612442\n",
      "[step: 4996] loss: 0.04558644816279411\n",
      "[step: 4997] loss: 0.045581310987472534\n",
      "[step: 4998] loss: 0.045564744621515274\n",
      "[step: 4999] loss: 0.04552125930786133\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ0AAAEKCAYAAADJvIhZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfa31f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer=LSTM(txs,1,'WeekNumber_Month_Season_Year' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict=answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real=answer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21370.06054688], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16894.39999997],\n",
       "       [ 18365.09999997],\n",
       "       [ 18378.15999997],\n",
       "       [ 23510.48999996],\n",
       "       [ 36988.48999994],\n",
       "       [ 54060.09999991],\n",
       "       [ 20124.21999997],\n",
       "       [ 20113.02999997],\n",
       "       [ 21140.06999996],\n",
       "       [ 22366.87999996],\n",
       "       [ 22107.69999996],\n",
       "       [ 28952.85999995],\n",
       "       [ 57592.1199999 ],\n",
       "       [ 34684.20999994],\n",
       "       [ 16976.18999997],\n",
       "       [ 16347.59999997],\n",
       "       [ 17147.43999997],\n",
       "       [ 18164.19999997],\n",
       "       [ 18517.78999997],\n",
       "       [ 16963.54999997],\n",
       "       [ 16065.48999997],\n",
       "       [ 17665.99999997],\n",
       "       [ 17558.81999997],\n",
       "       [ 16633.40999997],\n",
       "       [ 15722.81999997],\n",
       "       [ 17823.36999997],\n",
       "       [ 16566.17999997],\n",
       "       [ 16348.05999997],\n",
       "       [ 15731.17999997],\n",
       "       [ 16628.30999997],\n",
       "       [ 16119.91999997],\n",
       "       [ 17330.69999997],\n",
       "       [ 16286.39999997],\n",
       "       [ 16680.23999997],\n",
       "       [ 18322.36999997],\n",
       "       [ 19616.21999997],\n",
       "       [ 19251.49999997],\n",
       "       [ 18947.80999997],\n",
       "       [ 21904.46999996],\n",
       "       [ 22764.00999996],\n",
       "       [ 24185.26999996],\n",
       "       [ 27390.80999995]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestPredictY=[item for sublist in predict for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15798.604,\n",
       " 14722.048,\n",
       " 15121.582,\n",
       " 14954.279,\n",
       " 17118.977,\n",
       " 15644.573,\n",
       " 15849.232,\n",
       " 15525.247,\n",
       " 14816.899,\n",
       " 14112.375,\n",
       " 14630.68,\n",
       " 16013.2,\n",
       " 17937.387,\n",
       " 19444.676,\n",
       " 19259.762,\n",
       " 18787.432,\n",
       " 24630.928,\n",
       " 23599.018,\n",
       " 24115.422,\n",
       " 18142.271,\n",
       " 61038.18,\n",
       " 28532.447,\n",
       " 15645.645,\n",
       " 19705.068,\n",
       " 21329.863,\n",
       " 30300.719,\n",
       " 40043.18,\n",
       " 36211.652,\n",
       " 22588.805]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denormalizedTestPredictY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestY=originalSales[train_size+seq_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc41d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sales=noOutlierSales(sales)\n",
    "tempxy=[list(txs['season']),list(txs['year']),list(txs['month']),list(txs['week_number']),sales]\n",
    "# tempxy=[list(txs['season']),list(txs['day_of_week01']),list(txs['week_number']),sales]\n",
    "xy=np.array(tempxy).transpose().astype(np.float)\n",
    "originalxy=np.array(tempxy).transpose().astype(np.float)\n",
    "xy=minMaxNormalizer(xy)\n",
    "\n",
    "#data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "data_dim=len(tempxy)\n",
    "#data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "seq_length=5\n",
    "#output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "\n",
    "output_dim=forecastDay\n",
    "#hidden_dim은 정말 임의로 설정\n",
    "hidden_dim=100\n",
    "#learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "learning_rate=0.01\n",
    "#iterations는 반복 횟수\n",
    "iterations=1000\n",
    "x=xy\n",
    "y=xy[:,[-1]]\n",
    "\n",
    "#build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0, len(y)-seq_length - forecastDay):\n",
    "    _x=x[i:i+seq_length]\n",
    "    _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "    _y = np.reshape(_y, (forecastDay))\n",
    "#     print(_x,\"->\",_y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    train_size = int(len(dataY) * 0.7)\n",
    "    \n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "X=tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y=tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn= None) \n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "denormalizedTestY=originalSales[train_size+seq_length:]\n",
    "# denormalizedTestY_original=sales[train_size+seq_length:]\n",
    "denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "with tf.Session() as sess:\n",
    "    #초기화\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalxy)\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(denormalizedTestY_feed) #실제 sales 파란색\n",
    "    plt.plot(test_predict)           #예측 sales 주황색\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x000000000CA1B1A8>\n"
     ]
    }
   ],
   "source": [
    "print(i for i in list(test_predict[-1]    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17721.039]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_predict[  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestPredictY=[item for sublist in test_predict for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a5cffacf1166>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrootMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdenormalizedTestY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdenormalizedTestPredictY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-a9533e569715>\u001b[0m in \u001b[0;36mrootMeanSquaredError\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0msum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0msum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rootMeanSquaredError(denormalizedTestY,denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
