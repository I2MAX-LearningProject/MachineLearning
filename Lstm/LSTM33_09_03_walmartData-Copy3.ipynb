{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "walmart 주단위 데이터 139주로 향후 4주 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime  \n",
    "columns=['date','sales']\n",
    "txs=pd.read_table('./lstmData/lstmPrac11.csv', sep=',',header=None,names=columns )\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-03-12</td>\n",
       "      <td>21043.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-03-19</td>\n",
       "      <td>22136.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-03-26</td>\n",
       "      <td>26229.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-04-02</td>\n",
       "      <td>57258.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-04-09</td>\n",
       "      <td>42960.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010-04-16</td>\n",
       "      <td>17596.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2010-04-23</td>\n",
       "      <td>16145.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>16555.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010-05-07</td>\n",
       "      <td>17413.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2010-05-14</td>\n",
       "      <td>18926.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010-05-21</td>\n",
       "      <td>14773.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2010-05-28</td>\n",
       "      <td>15580.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2010-06-04</td>\n",
       "      <td>17558.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2010-06-11</td>\n",
       "      <td>16637.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2010-06-18</td>\n",
       "      <td>16216.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2010-06-25</td>\n",
       "      <td>16328.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2010-07-02</td>\n",
       "      <td>16333.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2010-07-09</td>\n",
       "      <td>17688.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2010-07-16</td>\n",
       "      <td>17150.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010-07-23</td>\n",
       "      <td>15360.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2010-07-30</td>\n",
       "      <td>15381.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2010-08-06</td>\n",
       "      <td>17508.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2010-08-13</td>\n",
       "      <td>15536.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-08-20</td>\n",
       "      <td>15740.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2010-08-27</td>\n",
       "      <td>15793.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>57592.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2012-04-13</td>\n",
       "      <td>34684.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2012-04-20</td>\n",
       "      <td>16976.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2012-04-27</td>\n",
       "      <td>16347.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2012-05-04</td>\n",
       "      <td>17147.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2012-05-11</td>\n",
       "      <td>18164.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2012-05-18</td>\n",
       "      <td>18517.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2012-05-25</td>\n",
       "      <td>16963.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>16065.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2012-06-08</td>\n",
       "      <td>17666.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2012-06-15</td>\n",
       "      <td>17558.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>16633.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2012-06-29</td>\n",
       "      <td>15722.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2012-07-06</td>\n",
       "      <td>17823.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2012-07-13</td>\n",
       "      <td>16566.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>16348.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2012-07-27</td>\n",
       "      <td>15731.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2012-08-03</td>\n",
       "      <td>16628.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2012-08-10</td>\n",
       "      <td>16119.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2012-08-17</td>\n",
       "      <td>17330.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>16286.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2012-08-31</td>\n",
       "      <td>16680.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2012-09-07</td>\n",
       "      <td>18322.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>19616.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2012-09-21</td>\n",
       "      <td>19251.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>18947.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>21904.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2012-10-12</td>\n",
       "      <td>22764.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>24185.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>27390.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     sales\n",
       "0    2010-02-05  24924.50\n",
       "1    2010-02-12  46039.49\n",
       "2    2010-02-19  41595.55\n",
       "3    2010-02-26  19403.54\n",
       "4    2010-03-05  21827.90\n",
       "5    2010-03-12  21043.39\n",
       "6    2010-03-19  22136.64\n",
       "7    2010-03-26  26229.21\n",
       "8    2010-04-02  57258.43\n",
       "9    2010-04-09  42960.91\n",
       "10   2010-04-16  17596.96\n",
       "11   2010-04-23  16145.35\n",
       "12   2010-04-30  16555.11\n",
       "13   2010-05-07  17413.94\n",
       "14   2010-05-14  18926.74\n",
       "15   2010-05-21  14773.04\n",
       "16   2010-05-28  15580.43\n",
       "17   2010-06-04  17558.09\n",
       "18   2010-06-11  16637.62\n",
       "19   2010-06-18  16216.27\n",
       "20   2010-06-25  16328.72\n",
       "21   2010-07-02  16333.14\n",
       "22   2010-07-09  17688.76\n",
       "23   2010-07-16  17150.84\n",
       "24   2010-07-23  15360.45\n",
       "25   2010-07-30  15381.82\n",
       "26   2010-08-06  17508.41\n",
       "27   2010-08-13  15536.40\n",
       "28   2010-08-20  15740.13\n",
       "29   2010-08-27  15793.87\n",
       "..          ...       ...\n",
       "113  2012-04-06  57592.12\n",
       "114  2012-04-13  34684.21\n",
       "115  2012-04-20  16976.19\n",
       "116  2012-04-27  16347.60\n",
       "117  2012-05-04  17147.44\n",
       "118  2012-05-11  18164.20\n",
       "119  2012-05-18  18517.79\n",
       "120  2012-05-25  16963.55\n",
       "121  2012-06-01  16065.49\n",
       "122  2012-06-08  17666.00\n",
       "123  2012-06-15  17558.82\n",
       "124  2012-06-22  16633.41\n",
       "125  2012-06-29  15722.82\n",
       "126  2012-07-06  17823.37\n",
       "127  2012-07-13  16566.18\n",
       "128  2012-07-20  16348.06\n",
       "129  2012-07-27  15731.18\n",
       "130  2012-08-03  16628.31\n",
       "131  2012-08-10  16119.92\n",
       "132  2012-08-17  17330.70\n",
       "133  2012-08-24  16286.40\n",
       "134  2012-08-31  16680.24\n",
       "135  2012-09-07  18322.37\n",
       "136  2012-09-14  19616.22\n",
       "137  2012-09-21  19251.50\n",
       "138  2012-09-28  18947.81\n",
       "139  2012-10-05  21904.47\n",
       "140  2012-10-12  22764.01\n",
       "141  2012-10-19  24185.27\n",
       "142  2012-10-26  27390.81\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noOutlierSales(sales):\n",
    "    mean=np.mean(sales)\n",
    "    std=np.std(sales)\n",
    "    for i in range(len(sales)):\n",
    "        if (sales[i]<mean-2*std or sales[i]>mean+2*std):\n",
    "             sales[i]=int(mean)\n",
    "    return sales\n",
    "def logSales(sales):\n",
    "    for i in range(len(sales)):\n",
    "        if sales[i] is 0:\n",
    "            sales[i]=1\n",
    "    return np.log(sales)\n",
    "def sqrtSales(sales):\n",
    "    return np.sqrt(sales)\n",
    "\n",
    "\n",
    "def rmse(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt(sum/len(a))\n",
    "\n",
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LSTM(txs, forecastDay, features):\n",
    "    tf.set_random_seed(7)\n",
    "    #Add basic date related features to the table\n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d\").year\n",
    "    dayOfWeek = lambda x: datetime.strptime(x, \"%Y-%m-%d\").weekday()\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d\").month\n",
    "    weekNumber = lambda x: datetime.strptime(x, \"%Y-%m-%d\").strftime('%V')\n",
    "    txs['year'] = txs['date'].map(year)\n",
    "    txs['month'] = txs['date'].map(month)\n",
    "    txs['weekNumber'] = txs['date'].map(weekNumber)\n",
    "    txs['dayOfWeek'] = txs['date'].map(dayOfWeek)\n",
    "\n",
    "    #Add non-basic date related features to the table\n",
    "    seasons = [0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 0]  # dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\").month - 1)]\n",
    "    day_of_week01s = [0, 0, 0, 0, 0, 1, 1]\n",
    "    day_of_week01 = lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\").weekday())]\n",
    "    txs['season'] = txs['date'].map(season)\n",
    "    txs['dayOfWeek01'] = txs['date'].map(day_of_week01)\n",
    "\n",
    "    #Backup originalSales\n",
    "    originalSales = list(txs['sales'])\n",
    "    sales = list(txs['sales'])\n",
    "\n",
    "    if features is 'DayOfWeek_WeekNumber_Month_Season' :\n",
    "        tempxy = [list(txs['dayOfWeek']), list(txs['weekNumber']),list(txs['month']),list(txs['season']) , sales]\n",
    "    elif features is'DayOfWeek01_WeekNumber_Month_Season' :\n",
    "        tempxy = [list(txs['dayOfWeek01']), list(txs['weekNumber']),list(txs['month']),list(txs['season']) , sales]\n",
    "\n",
    "    elif features is 'WeekNumber_Month_Season_Year' :\n",
    "        tempxy = [list(txs['weekNumber']), list(txs['month']), list(txs['season']),list(txs['year']), sales]\n",
    "\n",
    "    xy = np.array(tempxy).transpose().astype(np.float)\n",
    "\n",
    "    #Backup originalXY for denormalize\n",
    "    originalXY = np.array(tempxy).transpose().astype(np.float)\n",
    "    xy = minMaxNormalizer(xy)\n",
    "\n",
    "    #TRAIN PARAMETERS\n",
    "    # data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "    data_dim = 5\n",
    "    # data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "    seq_length = 10\n",
    "    # output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "    output_dim = forecastDay\n",
    "    # hidden_dim은 정말 임의로 설정\n",
    "    hidden_dim = 100\n",
    "    # learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    # Build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "    x = xy\n",
    "    y = xy[:, [-1]]\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(0, len(y) - seq_length - forecastDay+1):\n",
    "        _x = x[i:i + seq_length]\n",
    "        _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "        _y = np.reshape(_y, (forecastDay))\n",
    "        print(_x,\"->\",_y)\n",
    "        dataX.append(_x)\n",
    "        dataY.append(_y)\n",
    "    print('data set length:',len(y) - seq_length - forecastDay+1)\n",
    "    \n",
    "    train_size = int(len(dataY)-forecastDay)\n",
    "    train_size = int(len(dataY)*0.7)    \n",
    "    test_size = len(dataY) - train_size\n",
    "    print('train size:' , train_size)\n",
    "    print('test size:' , test_size)\n",
    "    trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "    print('trainX:', trainX)\n",
    "    print('testX:', testX)\n",
    "    trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "    Y = tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "    loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    denormalizedTestY=originalSales[train_size+seq_length:]\n",
    "#     denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "    \n",
    "    targets = tf.placeholder(tf.float32, [None, 1])\n",
    "    predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "    \n",
    "    count=0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # 초기화\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Training step\n",
    "        while(1):\n",
    "            count=count+1\n",
    "            _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "            print(\"[step: {}] loss: {}\".format(count, step_loss))\n",
    "            if(step_loss<0.5):\n",
    "                break\n",
    "\n",
    "        # Test step\n",
    "        # test_predict= sess.run(Y_pred, feed_dict={X: testX}\n",
    "        test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalXY)\n",
    "        realSale= minMaxDeNormalizer(testY[-1],originalXY)\n",
    "        # Plot predictions\n",
    "#         plt.plot(denormalizedTestY_feed)\n",
    "        plt.plot(realSale)      #실제 sales 파란색\n",
    "        plt.plot(test_predict[-1]) #예측 sales 주황색\n",
    "               \n",
    "        plt.xlabel(\"Time Period\")\n",
    "        plt.ylabel(\"Stock Price\")\n",
    "        plt.show()\n",
    "        \n",
    "    return test_predict[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    4.32776220e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.99406065e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.22243772e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    3.36913105e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]] -> [ 0.30554458  0.28033957  0.28745443  0.30236671  0.3286342   0.25651148\n",
      "  0.27053059]\n",
      "[[  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.99406065e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.22243772e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    3.36913105e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]] -> [ 0.28033957  0.28745443  0.30236671  0.3286342   0.25651148  0.27053059\n",
      "  0.30486966]\n",
      "[[  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    7.22243772e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    3.36913105e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]] -> [ 0.28745443  0.30236671  0.3286342   0.25651148  0.27053059  0.30486966\n",
      "  0.28888709]\n",
      "[[  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "    3.36913105e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]] -> [ 0.30236671  0.3286342   0.25651148  0.27053059  0.30486966  0.28888709\n",
      "  0.28157099]\n",
      "[[  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.79008448e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]] -> [ 0.3286342   0.25651148  0.27053059  0.30486966  0.28888709  0.28157099\n",
      "  0.28352351]\n",
      "[[  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.65386619e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]] -> [ 0.25651148  0.27053059  0.30486966  0.28888709  0.28157099  0.28352351\n",
      "  0.28360026]\n",
      "[[  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.84369251e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]] -> [ 0.27053059  0.30486966  0.28888709  0.28157099  0.28352351  0.28360026\n",
      "  0.30713855]\n",
      "[[  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "    4.55430535e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]] -> [ 0.30486966  0.28888709  0.28157099  0.28352351  0.28360026  0.30713855\n",
      "  0.29779838]\n",
      "[[  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    9.94205978e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]] -> [ 0.28888709  0.28157099  0.28352351  0.28360026  0.30713855  0.29779838\n",
      "  0.26671097]\n",
      "[[  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    7.45951182e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]] -> [ 0.28157099  0.28352351  0.28360026  0.30713855  0.29779838  0.26671097\n",
      "  0.26708202]\n",
      "[[  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.05544578e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]] -> [ 0.28352351  0.28360026  0.30713855  0.29779838  0.26671097  0.26708202\n",
      "  0.30400704]\n",
      "[[  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.80339567e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]] -> [ 0.28360026  0.30713855  0.29779838  0.26671097  0.26708202  0.30400704\n",
      "  0.26976607]\n",
      "[[  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.87454430e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]] -> [ 0.30713855  0.29779838  0.26671097  0.26708202  0.30400704  0.26976607\n",
      "  0.27330354]\n",
      "[[  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.02366713e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]] -> [ 0.29779838  0.26671097  0.26708202  0.30400704  0.26976607  0.27330354\n",
      "  0.27423665]\n",
      "[[  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    3.28634195e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]] -> [ 0.26671097  0.26708202  0.30400704  0.26976607  0.27330354  0.27423665\n",
      "  0.28201393]\n",
      "[[  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.56511481e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]] -> [ 0.26708202  0.30400704  0.26976607  0.27330354  0.27423665  0.28201393\n",
      "  0.31592412]\n",
      "[[  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49006079e-02\n",
      "    2.70530586e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]] -> [ 0.30400704  0.26976607  0.27330354  0.27423665  0.28201393  0.31592412\n",
      "  0.33605691]\n",
      "[[  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04869659e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]] -> [ 0.26976607  0.27330354  0.27423665  0.28201393  0.31592412  0.33605691\n",
      "  0.31467013]\n",
      "[[  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.88887091e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]] -> [ 0.27330354  0.27423665  0.28201393  0.31592412  0.33605691  0.31467013\n",
      "  0.3489052 ]\n",
      "[[  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.81570986e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]] -> [ 0.27423665  0.28201393  0.31592412  0.33605691  0.31467013  0.3489052\n",
      "  0.40609774]\n",
      "[[  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83523510e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]] -> [ 0.28201393  0.31592412  0.33605691  0.31467013  0.3489052   0.40609774\n",
      "  0.46843804]\n",
      "[[  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.83600256e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]] -> [ 0.31592412  0.33605691  0.31467013  0.3489052   0.40609774  0.46843804\n",
      "  0.44351623]\n",
      "[[  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.07138546e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]] -> [ 0.33605691  0.31467013  0.3489052   0.40609774  0.46843804  0.44351623\n",
      "  0.67094127]\n",
      "[[  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.97798379e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]] -> [ 0.31467013  0.3489052   0.40609774  0.46843804  0.44351623  0.67094127\n",
      "  0.59450633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.66710967e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]] -> [ 0.3489052   0.40609774  0.46843804  0.44351623  0.67094127  0.59450633\n",
      "  0.33944557]\n",
      "[[  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.67082024e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]] -> [ 0.40609774  0.46843804  0.44351623  0.67094127  0.59450633  0.33944557\n",
      "  0.33950547]\n",
      "[[  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    3.04007041e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]] -> [ 0.46843804  0.44351623  0.67094127  0.59450633  0.33944557  0.33950547\n",
      "  0.32678585]\n",
      "[[  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.69766072e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]] -> [ 0.44351623  0.67094127  0.59450633  0.33944557  0.33950547  0.32678585\n",
      "  0.39098335]\n",
      "[[  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.73303535e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]] -> [ 0.67094127  0.59450633  0.33944557  0.33950547  0.32678585  0.39098335\n",
      "  0.54690902]\n",
      "[[  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49006079e-02\n",
      "    2.74236649e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]] -> [ 0.59450633  0.33944557  0.33950547  0.32678585  0.39098335  0.54690902\n",
      "  0.77984384]\n",
      "[[  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    2.82013928e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]] -> [ 0.33944557  0.33950547  0.32678585  0.39098335  0.54690902  0.77984384\n",
      "  0.97116116]\n",
      "[[  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.15924123e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]] -> [ 0.33950547  0.32678585  0.39098335  0.54690902  0.77984384  0.97116116\n",
      "  0.33206939]\n",
      "[[  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.36056912e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]] -> [ 0.32678585  0.39098335  0.54690902  0.77984384  0.97116116  0.33206939\n",
      "  0.27754214]\n",
      "[[  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.14670132e-01]\n",
      " [  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]] -> [ 0.39098335  0.54690902  0.77984384  0.97116116  0.33206939  0.27754214\n",
      "  0.30142492]\n",
      "[[  6.77175975e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.48905197e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]] -> [ 0.54690902  0.77984384  0.97116116  0.33206939  0.27754214  0.30142492\n",
      "  0.30110838]\n",
      "[[  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.06097744e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]] -> [ 0.77984384  0.97116116  0.33206939  0.27754214  0.30142492  0.30110838\n",
      "  0.32055045]\n",
      "[[  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.68438043e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]] -> [ 0.97116116  0.33206939  0.27754214  0.30142492  0.30110838  0.32055045\n",
      "  0.37619313]\n",
      "[[  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    4.43516231e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]] -> [ 0.33206939  0.27754214  0.30142492  0.30110838  0.32055045  0.37619313\n",
      "  0.65785337]\n",
      "[[  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49006079e-02\n",
      "    6.70941268e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]] -> [ 0.27754214  0.30142492  0.30110838  0.32055045  0.37619313  0.65785337\n",
      "  0.81340763]\n",
      "[[  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    5.94506332e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]] -> [ 0.30142492  0.30110838  0.32055045  0.37619313  0.65785337  0.81340763\n",
      "  0.3362236 ]\n",
      "[[  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39445570e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]] -> [ 0.30110838  0.32055045  0.37619313  0.65785337  0.81340763  0.3362236\n",
      "  0.35295818]\n",
      "[[  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.39505474e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]] -> [ 0.32055045  0.37619313  0.65785337  0.81340763  0.3362236   0.35295818\n",
      "  0.36950194]\n",
      "[[  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49006079e-02\n",
      "    3.26785852e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]] -> [ 0.37619313  0.65785337  0.81340763  0.3362236   0.35295818  0.36950194\n",
      "  0.35307313]\n",
      "[[  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.90983350e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]] -> [ 0.65785337  0.81340763  0.3362236   0.35295818  0.36950194  0.35307313\n",
      "  0.3625687 ]\n",
      "[[  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    5.46909022e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]] -> [ 0.81340763  0.3362236   0.35295818  0.36950194  0.35307313  0.3625687\n",
      "  0.35418196]\n",
      "[[  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    7.79843840e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]] -> [ 0.3362236   0.35295818  0.36950194  0.35307313  0.3625687   0.35418196\n",
      "  0.41453223]\n",
      "[[  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    9.71161159e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]] -> [ 0.35295818  0.36950194  0.35307313  0.3625687   0.35418196  0.41453223\n",
      "  0.49941502]\n",
      "[[  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49006079e-02\n",
      "    3.32069387e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]] -> [ 0.36950194  0.35307313  0.3625687   0.35418196  0.41453223  0.49941502\n",
      "  0.87703509]\n",
      "[[  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    2.77542136e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]] -> [ 0.35307313  0.3625687   0.35418196  0.41453223  0.49941502  0.87703509\n",
      "  0.72079982]\n",
      "[[  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01424917e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]] -> [ 0.3625687   0.35418196  0.41453223  0.49941502  0.87703509  0.72079982\n",
      "  0.34966919]\n",
      "[[  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.01108381e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]] -> [ 0.35418196  0.41453223  0.49941502  0.87703509  0.72079982  0.34966919\n",
      "  0.29926229]\n",
      "[[  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.20550450e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]] -> [ 0.41453223  0.49941502  0.87703509  0.72079982  0.34966919  0.29926229\n",
      "  0.26282728]\n",
      "[[  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.76193132e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]] -> [ 0.49941502  0.87703509  0.72079982  0.34966919  0.29926229  0.26282728\n",
      "  0.27332906]\n",
      "[[  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    6.57853366e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]] -> [ 0.87703509  0.72079982  0.34966919  0.29926229  0.26282728  0.27332906\n",
      "  0.28535414]\n",
      "[[  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    8.13407633e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]] -> [ 0.72079982  0.34966919  0.29926229  0.26282728  0.27332906  0.28535414\n",
      "  0.27579329]\n",
      "[[  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49179714e-02\n",
      "    3.36223601e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]] -> [ 0.34966919  0.29926229  0.26282728  0.27332906  0.28535414  0.27579329\n",
      "  0.26007186]\n",
      "[[  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.52958182e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]] -> [ 0.29926229  0.26282728  0.27332906  0.28535414  0.27579329  0.26007186\n",
      "  0.27230826]\n",
      "[[  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.69501939e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]] -> [ 0.26282728  0.27332906  0.28535414  0.27579329  0.26007186  0.27230826\n",
      "  0.26676393]\n",
      "[[  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.53073129e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]] -> [ 0.27332906  0.28535414  0.27579329  0.26007186  0.27230826  0.26676393\n",
      "  0.28040069]\n",
      "[[  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.62568699e-01]\n",
      " [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]] -> [ 0.28535414  0.27579329  0.26007186  0.27230826  0.26676393  0.28040069\n",
      "  0.27182278]\n",
      "[[  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.54181961e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]] -> [ 0.27579329  0.26007186  0.27230826  0.26676393  0.28040069  0.27182278\n",
      "  0.27376315]\n",
      "[[  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.14532231e-01]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]] -> [ 0.26007186  0.27230826  0.26676393  0.28040069  0.27182278  0.27376315\n",
      "  0.27646855]\n",
      "[[  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    4.99415024e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]] -> [ 0.27230826  0.26676393  0.28040069  0.27182278  0.27376315  0.27646855\n",
      "  0.26558408]\n",
      "[[  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    8.77035087e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]] -> [ 0.26676393  0.28040069  0.27182278  0.27376315  0.27646855  0.26558408\n",
      "  0.25246145]\n",
      "[[  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49179714e-02\n",
      "    7.20799825e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]] -> [ 0.28040069  0.27182278  0.27376315  0.27646855  0.26558408  0.25246145\n",
      "  0.25505642]\n",
      "[[  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    3.49669191e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]] -> [ 0.27182278  0.27376315  0.27646855  0.26558408  0.25246145  0.25505642\n",
      "  0.25241943]\n",
      "[[  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.99262295e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]] -> [ 0.27376315  0.27646855  0.26558408  0.25246145  0.25505642  0.25241943\n",
      "  0.26526667]\n",
      "[[  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.62827276e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]] -> [ 0.27646855  0.26558408  0.25246145  0.25505642  0.25241943  0.26526667\n",
      "  0.30814424]\n",
      "[[  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49179714e-02\n",
      "    2.73329060e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]] -> [ 0.26558408  0.25246145  0.25505642  0.25241943  0.26526667  0.30814424\n",
      "  0.32184056]\n",
      "[[  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.85354142e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]] -> [ 0.25246145  0.25505642  0.25241943  0.26526667  0.30814424  0.32184056\n",
      "  0.31009972]\n",
      "[[  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.75793286e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]] -> [ 0.25505642  0.25241943  0.26526667  0.30814424  0.32184056  0.31009972\n",
      "  0.31840606]\n",
      "[[  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.60071864e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]] -> [ 0.25241943  0.26526667  0.30814424  0.32184056  0.31009972  0.31840606\n",
      "  0.3611185 ]\n",
      "[[  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.72308260e-01]\n",
      " [  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]] -> [ 0.26526667  0.30814424  0.32184056  0.31009972  0.31840606  0.3611185\n",
      "  0.40070673]\n",
      "[[  4.51450650e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.66763925e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]] -> [ 0.30814424  0.32184056  0.31009972  0.31840606  0.3611185   0.40070673\n",
      "  0.40546866]\n",
      "[[  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.80400687e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]] -> [ 0.32184056  0.31009972  0.31840606  0.3611185   0.40070673  0.40546866\n",
      "  0.54833717]\n",
      "[[  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.71822777e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]] -> [ 0.31009972  0.31840606  0.3611185   0.40070673  0.40546866  0.54833717\n",
      "  0.69256107]\n",
      "[[  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.73763147e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]] -> [ 0.31840606  0.3611185   0.40070673  0.40546866  0.54833717  0.69256107\n",
      "  0.32451558]\n",
      "[[  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.76468552e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]] -> [ 0.3611185   0.40070673  0.40546866  0.54833717  0.69256107  0.32451558\n",
      "  0.33078588]\n",
      "[[  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.65584076e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]] -> [ 0.40070673  0.40546866  0.54833717  0.69256107  0.32451558  0.33078588\n",
      "  0.36309221]\n",
      "[[  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52461448e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]] -> [ 0.40546866  0.54833717  0.69256107  0.32451558  0.33078588  0.36309221\n",
      "  0.43918317]\n",
      "[[  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.55056421e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]] -> [ 0.54833717  0.69256107  0.32451558  0.33078588  0.36309221  0.43918317\n",
      "  0.57830689]\n",
      "[[  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49179714e-02\n",
      "    2.52419428e-01]\n",
      " [  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]] -> [ 0.69256107  0.32451558  0.33078588  0.36309221  0.43918317  0.57830689\n",
      "  0.79477939]\n",
      "[[  6.07722029e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    2.65266672e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]] -> [ 0.32451558  0.33078588  0.36309221  0.43918317  0.57830689  0.79477939\n",
      "  0.81241583]\n",
      "[[  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.08144239e-01]\n",
      " [  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]] -> [ 0.33078588  0.36309221  0.43918317  0.57830689  0.79477939  0.81241583\n",
      "  0.40545269]\n",
      "[[  6.42449002e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.21840557e-01]\n",
      " [  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]] -> [ 0.36309221  0.43918317  0.57830689  0.79477939  0.81241583  0.40545269\n",
      "  0.28767286]\n",
      "[[  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.10099715e-01]\n",
      " [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]] -> [ 0.43918317  0.57830689  0.79477939  0.81241583  0.40545269  0.28767286\n",
      "  0.29334569]\n",
      "[[  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.18406060e-01]\n",
      " [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]] -> [ 0.57830689  0.79477939  0.81241583  0.40545269  0.28767286  0.29334569\n",
      "  0.31888217]\n",
      "[[  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.61118500e-01]\n",
      " [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]] -> [ 0.79477939  0.81241583  0.40545269  0.28767286  0.29334569  0.31888217\n",
      "  0.31910893]\n",
      "[[  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.00706729e-01]\n",
      " [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]] -> [ 0.81241583  0.40545269  0.28767286  0.29334569  0.31888217  0.31910893\n",
      "  0.40822408]\n",
      "[[  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    4.05468665e-01]\n",
      " [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]] -> [ 0.40545269  0.28767286  0.29334569  0.31888217  0.31910893  0.40822408\n",
      "  0.64224915]\n",
      "[[  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "    5.48337168e-01]\n",
      " [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]] -> [ 0.28767286  0.29334569  0.31888217  0.31910893  0.40822408  0.64224915\n",
      "  0.93867182]\n",
      "[[  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    6.92561066e-01]\n",
      " [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]] -> [ 0.29334569  0.31888217  0.31910893  0.40822408  0.64224915  0.93867182\n",
      "  0.34942662]\n",
      "[[  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.24515576e-01]\n",
      " [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]] -> [ 0.31888217  0.31910893  0.40822408  0.64224915  0.93867182  0.34942662\n",
      "  0.34923233]\n",
      "[[  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.30785878e-01]\n",
      " [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]] -> [ 0.31910893  0.40822408  0.64224915  0.93867182  0.34942662  0.34923233\n",
      "  0.36706532]\n",
      "[[  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "    3.63092208e-01]\n",
      " [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]] -> [ 0.40822408  0.64224915  0.93867182  0.34942662  0.34923233  0.36706532\n",
      "  0.38836702]\n",
      "[[  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.39183173e-01]\n",
      " [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]] -> [ 0.64224915  0.93867182  0.34942662  0.34923233  0.36706532  0.38836702\n",
      "  0.38386675]\n",
      "[[  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    5.78306893e-01]\n",
      " [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]] -> [ 0.93867182  0.34942662  0.34923233  0.36706532  0.38836702  0.38386675\n",
      "  0.50272259]\n",
      "[[  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    7.94779390e-01]\n",
      " [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]] -> [ 0.34942662  0.34923233  0.36706532  0.38836702  0.38386675  0.50272259\n",
      "  1.        ]\n",
      "[[  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    8.12415830e-01]\n",
      " [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]] -> [ 0.34923233  0.36706532  0.38836702  0.38386675  0.50272259  1.\n",
      "  0.60223881]\n",
      "[[  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "    4.05452690e-01]\n",
      " [  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]] -> [ 0.36706532  0.38836702  0.38386675  0.50272259  1.          0.60223881\n",
      "  0.29476585]\n",
      "[[  1.73634865e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.87672862e-01]\n",
      " [  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]] -> [ 0.38836702  0.38386675  0.50272259  1.          0.60223881  0.29476585\n",
      "  0.28385133]\n",
      "[[  3.47269731e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    2.93345687e-01]\n",
      " [  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]] -> [ 0.38386675  0.50272259  1.          0.60223881  0.29476585  0.28385133\n",
      "  0.29773934]\n",
      "[[  5.20904596e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.18882167e-01]\n",
      " [  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]] -> [ 0.50272259  1.          0.60223881  0.29476585  0.28385133  0.29773934\n",
      "  0.31539384]\n",
      "[[  6.94539461e-05   1.73634865e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.19108934e-01]\n",
      " [  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]] -> [ 1.          0.60223881  0.29476585  0.28385133  0.29773934  0.31539384\n",
      "  0.3215334 ]\n",
      "[[  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    4.08224076e-01]\n",
      " [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]] -> [ 0.60223881  0.29476585  0.28385133  0.29773934  0.31539384  0.3215334\n",
      "  0.29454637]\n",
      "[[  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    6.42249148e-01]\n",
      " [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]] -> [ 0.29476585  0.28385133  0.29773934  0.31539384  0.3215334   0.29454637\n",
      "  0.27895292]\n",
      "[[  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    9.38671818e-01]\n",
      " [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]] -> [ 0.28385133  0.29773934  0.31539384  0.3215334   0.29454637  0.27895292\n",
      "  0.30674335]\n",
      "[[  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49353349e-02\n",
      "    3.49426623e-01]\n",
      " [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]] -> [ 0.29773934  0.31539384  0.3215334   0.29454637  0.27895292  0.30674335\n",
      "  0.30488233]\n",
      "[[  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.49232326e-01]\n",
      " [  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]] -> [ 0.31539384  0.3215334   0.29454637  0.27895292  0.30674335  0.30488233\n",
      "  0.28881399]\n",
      "[[  1.73634865e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.67065321e-01]\n",
      " [  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]] -> [ 0.3215334   0.29454637  0.27895292  0.30674335  0.30488233  0.28881399\n",
      "  0.27300297]\n",
      "[[  1.90998352e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.88367020e-01]\n",
      " [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]] -> [ 0.29454637  0.27895292  0.30674335  0.30488233  0.28881399  0.27300297\n",
      "  0.30947584]\n",
      "[[  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.83866751e-01]\n",
      " [  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]] -> [ 0.27895292  0.30674335  0.30488233  0.28881399  0.27300297  0.30947584\n",
      "  0.28764664]\n",
      "[[  2.25725325e-04   5.20904596e-05   1.73634865e-05   3.49353349e-02\n",
      "    5.02722595e-01]\n",
      " [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]] -> [ 0.30674335  0.30488233  0.28881399  0.27300297  0.30947584  0.28764664\n",
      "  0.28385932]\n",
      "[[  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    1.00000000e+00]\n",
      " [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]] -> [ 0.30488233  0.28881399  0.27300297  0.30947584  0.28764664  0.28385932\n",
      "  0.27314813]\n",
      "[[  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    6.02238813e-01]\n",
      " [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]] -> [ 0.28881399  0.27300297  0.30947584  0.28764664  0.28385932  0.27314813\n",
      "  0.28872544]\n",
      "[[  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94765846e-01]\n",
      " [  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]] -> [ 0.27300297  0.30947584  0.28764664  0.28385932  0.27314813  0.28872544\n",
      "  0.27989801]\n",
      "[[  2.95179271e-04   6.94539461e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.83851332e-01]\n",
      " [  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]] -> [ 0.30947584  0.28764664  0.28385932  0.27314813  0.28872544  0.27989801\n",
      "  0.30092138]\n",
      "[[  3.12542758e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.97739344e-01]\n",
      " [  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]] -> [ 0.28764664  0.28385932  0.27314813  0.28872544  0.27989801  0.30092138\n",
      "  0.28278869]\n",
      "[[  3.29906244e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.15393842e-01]\n",
      " [  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]] -> [ 0.28385932  0.27314813  0.28872544  0.27989801  0.30092138  0.28278869\n",
      "  0.28962712]\n",
      "[[  3.47269731e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    3.21533397e-01]\n",
      " [  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]] -> [ 0.27314813  0.28872544  0.27989801  0.30092138  0.28278869  0.28962712\n",
      "  0.31814022]\n",
      "[[  3.64633217e-04   8.68174327e-05   1.73634865e-05   3.49353349e-02\n",
      "    2.94546372e-01]\n",
      " [  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]] -> [ 0.28872544  0.27989801  0.30092138  0.28278869  0.28962712  0.31814022\n",
      "  0.34060597]\n",
      "[[  3.81996704e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.78952919e-01]\n",
      " [  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]] -> [ 0.27989801  0.30092138  0.28278869  0.28962712  0.31814022  0.34060597\n",
      "  0.33427316]\n",
      "[[  3.99360190e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.06743353e-01]\n",
      " [  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]] -> [ 0.30092138  0.28278869  0.28962712  0.31814022  0.34060597  0.33427316\n",
      "  0.32900004]\n",
      "[[  4.16723677e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.04882335e-01]\n",
      " [  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]] -> [ 0.28278869  0.28962712  0.31814022  0.34060597  0.33427316  0.32900004\n",
      "  0.38033797]\n",
      "[[  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88813991e-01]\n",
      " [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.82788687e-01]] -> [ 0.28962712  0.31814022  0.34060597  0.33427316  0.32900004  0.38033797\n",
      "  0.39526258]\n",
      "[[  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73002973e-01]\n",
      " [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.82788687e-01]\n",
      " [  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.89627123e-01]] -> [ 0.31814022  0.34060597  0.33427316  0.32900004  0.38033797  0.39526258\n",
      "  0.41994061]\n",
      "[[  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.09475845e-01]\n",
      " [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.87646643e-01]\n",
      " [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.83859320e-01]\n",
      " [  5.20904596e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.73148132e-01]\n",
      " [  5.38268083e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.88725437e-01]\n",
      " [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.79898014e-01]\n",
      " [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    3.00921376e-01]\n",
      " [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.82788687e-01]\n",
      " [  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "    2.89627123e-01]\n",
      " [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "    3.18140225e-01]] -> [ 0.34060597  0.33427316  0.32900004  0.38033797  0.39526258  0.41994061\n",
      "  0.47559996]\n",
      "data set length: 127\n",
      "train size: 88\n",
      "test size: 39\n",
      "trainX: [[[  8.68174327e-05   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     4.32776220e-01]\n",
      "  [  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.99406065e-01]\n",
      "  [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.22243772e-01]\n",
      "  ..., \n",
      "  [  2.08361838e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "     4.55430535e-01]\n",
      "  [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "     9.94205978e-01]\n",
      "  [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "     7.45951182e-01]]\n",
      "\n",
      " [[  1.04180919e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.99406065e-01]\n",
      "  [  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.22243772e-01]\n",
      "  [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     3.36913105e-01]\n",
      "  ..., \n",
      "  [  2.25725325e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "     9.94205978e-01]\n",
      "  [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "     7.45951182e-01]\n",
      "  [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.05544578e-01]]\n",
      "\n",
      " [[  1.21544406e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     7.22243772e-01]\n",
      "  [  1.38907892e-04   3.47269731e-05   0.00000000e+00   3.49006079e-02\n",
      "     3.36913105e-01]\n",
      "  [  1.56271379e-04   5.20904596e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.79008448e-01]\n",
      "  ..., \n",
      "  [  2.43088811e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "     7.45951182e-01]\n",
      "  [  2.60452298e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "     3.05544578e-01]\n",
      "  [  2.77815785e-04   6.94539461e-05   1.73634865e-05   3.49006079e-02\n",
      "     2.80339567e-01]]\n",
      "\n",
      " ..., \n",
      " [[  6.59812488e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.10099715e-01]\n",
      "  [  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.18406060e-01]\n",
      "  [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.61118500e-01]\n",
      "  ..., \n",
      "  [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.24515576e-01]\n",
      "  [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.30785878e-01]\n",
      "  [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.63092208e-01]]\n",
      "\n",
      " [[  6.77175975e-04   1.56271379e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.18406060e-01]\n",
      "  [  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.61118500e-01]\n",
      "  [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     4.00706729e-01]\n",
      "  ..., \n",
      "  [  7.98720380e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.30785878e-01]\n",
      "  [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.63092208e-01]\n",
      "  [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.39183173e-01]]\n",
      "\n",
      " [[  6.94539461e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.61118500e-01]\n",
      "  [  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     4.00706729e-01]\n",
      "  [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     4.05468665e-01]\n",
      "  ..., \n",
      "  [  8.16083867e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.63092208e-01]\n",
      "  [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.39183173e-01]\n",
      "  [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     5.78306893e-01]]]\n",
      "testX: [[[  7.11902948e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     4.00706729e-01]\n",
      "  [  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     4.05468665e-01]\n",
      "  [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     5.48337168e-01]\n",
      "  ..., \n",
      "  [  8.33447354e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.39183173e-01]\n",
      "  [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     5.78306893e-01]\n",
      "  [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     7.94779390e-01]]\n",
      "\n",
      " [[  7.29266434e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     4.05468665e-01]\n",
      "  [  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     5.48337168e-01]\n",
      "  [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     6.92561066e-01]\n",
      "  ..., \n",
      "  [  8.50810840e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     5.78306893e-01]\n",
      "  [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     7.94779390e-01]\n",
      "  [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     8.12415830e-01]]\n",
      "\n",
      " [[  7.46629921e-04   1.73634865e-04   5.20904596e-05   3.49179714e-02\n",
      "     5.48337168e-01]\n",
      "  [  7.63993407e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     6.92561066e-01]\n",
      "  [  7.81356894e-04   1.90998352e-04   5.20904596e-05   3.49179714e-02\n",
      "     3.24515576e-01]\n",
      "  ..., \n",
      "  [  8.68174327e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     7.94779390e-01]\n",
      "  [  8.85537813e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     8.12415830e-01]\n",
      "  [  9.02901300e-04   2.08361838e-04   0.00000000e+00   3.49179714e-02\n",
      "     4.05452690e-01]]\n",
      "\n",
      " ..., \n",
      " [[  4.34087163e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.88813991e-01]\n",
      "  [  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.73002973e-01]\n",
      "  [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "     3.09475845e-01]\n",
      "  ..., \n",
      "  [  5.55631569e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.79898014e-01]\n",
      "  [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "     3.00921376e-01]\n",
      "  [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.82788687e-01]]\n",
      "\n",
      " [[  4.51450650e-04   1.04180919e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.73002973e-01]\n",
      "  [  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "     3.09475845e-01]\n",
      "  [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.87646643e-01]\n",
      "  ..., \n",
      "  [  5.72995056e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "     3.00921376e-01]\n",
      "  [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.82788687e-01]\n",
      "  [  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.89627123e-01]]\n",
      "\n",
      " [[  4.68814136e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "     3.09475845e-01]\n",
      "  [  4.86177623e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.87646643e-01]\n",
      "  [  5.03541109e-04   1.21544406e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.83859320e-01]\n",
      "  ..., \n",
      "  [  5.90358542e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.82788687e-01]\n",
      "  [  6.07722029e-04   1.38907892e-04   3.47269731e-05   3.49353349e-02\n",
      "     2.89627123e-01]\n",
      "  [  6.25085515e-04   1.56271379e-04   5.20904596e-05   3.49353349e-02\n",
      "     3.18140225e-01]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1] loss: 113.55048370361328\n",
      "[step: 2] loss: 109.21458435058594\n",
      "[step: 3] loss: 104.98590087890625\n",
      "[step: 4] loss: 100.85213470458984\n",
      "[step: 5] loss: 96.79940795898438\n",
      "[step: 6] loss: 92.81378173828125\n",
      "[step: 7] loss: 88.88075256347656\n",
      "[step: 8] loss: 84.98580169677734\n",
      "[step: 9] loss: 81.11466217041016\n",
      "[step: 10] loss: 77.253662109375\n",
      "[step: 11] loss: 73.38983154296875\n",
      "[step: 12] loss: 69.51152038574219\n",
      "[step: 13] loss: 65.60906982421875\n",
      "[step: 14] loss: 61.675750732421875\n",
      "[step: 15] loss: 57.70878601074219\n",
      "[step: 16] loss: 53.710418701171875\n",
      "[step: 17] loss: 49.689476013183594\n",
      "[step: 18] loss: 45.66318893432617\n",
      "[step: 19] loss: 41.659732818603516\n",
      "[step: 20] loss: 37.721153259277344\n",
      "[step: 21] loss: 33.90708923339844\n",
      "[step: 22] loss: 30.298763275146484\n",
      "[step: 23] loss: 27.002649307250977\n",
      "[step: 24] loss: 24.15163803100586\n",
      "[step: 25] loss: 21.898868560791016\n",
      "[step: 26] loss: 20.3945369720459\n",
      "[step: 27] loss: 19.733543395996094\n",
      "[step: 28] loss: 19.87580108642578\n",
      "[step: 29] loss: 20.58266830444336\n",
      "[step: 30] loss: 21.449941635131836\n",
      "[step: 31] loss: 22.062442779541016\n",
      "[step: 32] loss: 22.16541290283203\n",
      "[step: 33] loss: 21.731372833251953\n",
      "[step: 34] loss: 20.909208297729492\n",
      "[step: 35] loss: 19.922380447387695\n",
      "[step: 36] loss: 18.97955322265625\n",
      "[step: 37] loss: 18.224197387695312\n",
      "[step: 38] loss: 17.72174644470215\n",
      "[step: 39] loss: 17.471519470214844\n",
      "[step: 40] loss: 17.429407119750977\n",
      "[step: 41] loss: 17.531002044677734\n",
      "[step: 42] loss: 17.709489822387695\n",
      "[step: 43] loss: 17.90690803527832\n",
      "[step: 44] loss: 18.079429626464844\n",
      "[step: 45] loss: 18.19860076904297\n",
      "[step: 46] loss: 18.250152587890625\n",
      "[step: 47] loss: 18.23166275024414\n",
      "[step: 48] loss: 18.149860382080078\n",
      "[step: 49] loss: 18.017974853515625\n",
      "[step: 50] loss: 17.853199005126953\n",
      "[step: 51] loss: 17.674341201782227\n",
      "[step: 52] loss: 17.499664306640625\n",
      "[step: 53] loss: 17.34488296508789\n",
      "[step: 54] loss: 17.22147560119629\n",
      "[step: 55] loss: 17.135456085205078\n",
      "[step: 56] loss: 17.08687973022461\n",
      "[step: 57] loss: 17.070220947265625\n",
      "[step: 58] loss: 17.07573699951172\n",
      "[step: 59] loss: 17.09158706665039\n",
      "[step: 60] loss: 17.106307983398438\n",
      "[step: 61] loss: 17.111068725585938\n",
      "[step: 62] loss: 17.101043701171875\n",
      "[step: 63] loss: 17.075809478759766\n",
      "[step: 64] loss: 17.03856658935547\n",
      "[step: 65] loss: 16.994768142700195\n",
      "[step: 66] loss: 16.95043182373047\n",
      "[step: 67] loss: 16.910728454589844\n",
      "[step: 68] loss: 16.879064559936523\n",
      "[step: 69] loss: 16.85675621032715\n",
      "[step: 70] loss: 16.84318733215332\n",
      "[step: 71] loss: 16.836320877075195\n",
      "[step: 72] loss: 16.833362579345703\n",
      "[step: 73] loss: 16.83141326904297\n",
      "[step: 74] loss: 16.827951431274414\n",
      "[step: 75] loss: 16.82123565673828\n",
      "[step: 76] loss: 16.81041145324707\n",
      "[step: 77] loss: 16.795516967773438\n",
      "[step: 78] loss: 16.777324676513672\n",
      "[step: 79] loss: 16.757083892822266\n",
      "[step: 80] loss: 16.736270904541016\n",
      "[step: 81] loss: 16.716272354125977\n",
      "[step: 82] loss: 16.698190689086914\n",
      "[step: 83] loss: 16.68267059326172\n",
      "[step: 84] loss: 16.66985321044922\n",
      "[step: 85] loss: 16.65943145751953\n",
      "[step: 86] loss: 16.650741577148438\n",
      "[step: 87] loss: 16.642974853515625\n",
      "[step: 88] loss: 16.635303497314453\n",
      "[step: 89] loss: 16.62709617614746\n",
      "[step: 90] loss: 16.617965698242188\n",
      "[step: 91] loss: 16.60783576965332\n",
      "[step: 92] loss: 16.59684944152832\n",
      "[step: 93] loss: 16.585342407226562\n",
      "[step: 94] loss: 16.573705673217773\n",
      "[step: 95] loss: 16.562301635742188\n",
      "[step: 96] loss: 16.551380157470703\n",
      "[step: 97] loss: 16.541057586669922\n",
      "[step: 98] loss: 16.531309127807617\n",
      "[step: 99] loss: 16.522024154663086\n",
      "[step: 100] loss: 16.51301383972168\n",
      "[step: 101] loss: 16.50408363342285\n",
      "[step: 102] loss: 16.49509048461914\n",
      "[step: 103] loss: 16.485923767089844\n",
      "[step: 104] loss: 16.476566314697266\n",
      "[step: 105] loss: 16.467056274414062\n",
      "[step: 106] loss: 16.457475662231445\n",
      "[step: 107] loss: 16.44792366027832\n",
      "[step: 108] loss: 16.438495635986328\n",
      "[step: 109] loss: 16.42926025390625\n",
      "[step: 110] loss: 16.420246124267578\n",
      "[step: 111] loss: 16.411457061767578\n",
      "[step: 112] loss: 16.402843475341797\n",
      "[step: 113] loss: 16.39434814453125\n",
      "[step: 114] loss: 16.385908126831055\n",
      "[step: 115] loss: 16.377466201782227\n",
      "[step: 116] loss: 16.368993759155273\n",
      "[step: 117] loss: 16.360477447509766\n",
      "[step: 118] loss: 16.351926803588867\n",
      "[step: 119] loss: 16.343366622924805\n",
      "[step: 120] loss: 16.334823608398438\n",
      "[step: 121] loss: 16.326339721679688\n",
      "[step: 122] loss: 16.317930221557617\n",
      "[step: 123] loss: 16.309612274169922\n",
      "[step: 124] loss: 16.301376342773438\n",
      "[step: 125] loss: 16.293216705322266\n",
      "[step: 126] loss: 16.285118103027344\n",
      "[step: 127] loss: 16.27706527709961\n",
      "[step: 128] loss: 16.26903533935547\n",
      "[step: 129] loss: 16.261028289794922\n",
      "[step: 130] loss: 16.253040313720703\n",
      "[step: 131] loss: 16.245067596435547\n",
      "[step: 132] loss: 16.23712158203125\n",
      "[step: 133] loss: 16.229202270507812\n",
      "[step: 134] loss: 16.22132110595703\n",
      "[step: 135] loss: 16.213483810424805\n",
      "[step: 136] loss: 16.2056827545166\n",
      "[step: 137] loss: 16.19792366027832\n",
      "[step: 138] loss: 16.190196990966797\n",
      "[step: 139] loss: 16.1825008392334\n",
      "[step: 140] loss: 16.174827575683594\n",
      "[step: 141] loss: 16.16717529296875\n",
      "[step: 142] loss: 16.159542083740234\n",
      "[step: 143] loss: 16.151927947998047\n",
      "[step: 144] loss: 16.144329071044922\n",
      "[step: 145] loss: 16.136764526367188\n",
      "[step: 146] loss: 16.129213333129883\n",
      "[step: 147] loss: 16.121686935424805\n",
      "[step: 148] loss: 16.114185333251953\n",
      "[step: 149] loss: 16.106712341308594\n",
      "[step: 150] loss: 16.099254608154297\n",
      "[step: 151] loss: 16.091815948486328\n",
      "[step: 152] loss: 16.084392547607422\n",
      "[step: 153] loss: 16.076984405517578\n",
      "[step: 154] loss: 16.069583892822266\n",
      "[step: 155] loss: 16.062198638916016\n",
      "[step: 156] loss: 16.054821014404297\n",
      "[step: 157] loss: 16.047454833984375\n",
      "[step: 158] loss: 16.040096282958984\n",
      "[step: 159] loss: 16.03274917602539\n",
      "[step: 160] loss: 16.025409698486328\n",
      "[step: 161] loss: 16.018077850341797\n",
      "[step: 162] loss: 16.010757446289062\n",
      "[step: 163] loss: 16.003435134887695\n",
      "[step: 164] loss: 15.996112823486328\n",
      "[step: 165] loss: 15.988799095153809\n",
      "[step: 166] loss: 15.981483459472656\n",
      "[step: 167] loss: 15.974164962768555\n",
      "[step: 168] loss: 15.96684455871582\n",
      "[step: 169] loss: 15.95952033996582\n",
      "[step: 170] loss: 15.952192306518555\n",
      "[step: 171] loss: 15.944860458374023\n",
      "[step: 172] loss: 15.937520980834961\n",
      "[step: 173] loss: 15.93017578125\n",
      "[step: 174] loss: 15.922813415527344\n",
      "[step: 175] loss: 15.915444374084473\n",
      "[step: 176] loss: 15.908059120178223\n",
      "[step: 177] loss: 15.90065860748291\n",
      "[step: 178] loss: 15.893243789672852\n",
      "[step: 179] loss: 15.885807037353516\n",
      "[step: 180] loss: 15.878353118896484\n",
      "[step: 181] loss: 15.87087631225586\n",
      "[step: 182] loss: 15.863380432128906\n",
      "[step: 183] loss: 15.855856895446777\n",
      "[step: 184] loss: 15.848305702209473\n",
      "[step: 185] loss: 15.840726852416992\n",
      "[step: 186] loss: 15.833118438720703\n",
      "[step: 187] loss: 15.825475692749023\n",
      "[step: 188] loss: 15.81779670715332\n",
      "[step: 189] loss: 15.810079574584961\n",
      "[step: 190] loss: 15.802323341369629\n",
      "[step: 191] loss: 15.794529914855957\n",
      "[step: 192] loss: 15.786690711975098\n",
      "[step: 193] loss: 15.778804779052734\n",
      "[step: 194] loss: 15.770870208740234\n",
      "[step: 195] loss: 15.762889862060547\n",
      "[step: 196] loss: 15.754849433898926\n",
      "[step: 197] loss: 15.74675464630127\n",
      "[step: 198] loss: 15.73859691619873\n",
      "[step: 199] loss: 15.730380058288574\n",
      "[step: 200] loss: 15.722100257873535\n",
      "[step: 201] loss: 15.713748931884766\n",
      "[step: 202] loss: 15.705329895019531\n",
      "[step: 203] loss: 15.6968355178833\n",
      "[step: 204] loss: 15.688261032104492\n",
      "[step: 205] loss: 15.679609298706055\n",
      "[step: 206] loss: 15.670873641967773\n",
      "[step: 207] loss: 15.662044525146484\n",
      "[step: 208] loss: 15.653128623962402\n",
      "[step: 209] loss: 15.644113540649414\n",
      "[step: 210] loss: 15.635004043579102\n",
      "[step: 211] loss: 15.625788688659668\n",
      "[step: 212] loss: 15.616464614868164\n",
      "[step: 213] loss: 15.607030868530273\n",
      "[step: 214] loss: 15.597479820251465\n",
      "[step: 215] loss: 15.587808609008789\n",
      "[step: 216] loss: 15.578012466430664\n",
      "[step: 217] loss: 15.568086624145508\n",
      "[step: 218] loss: 15.558028221130371\n",
      "[step: 219] loss: 15.54782485961914\n",
      "[step: 220] loss: 15.537480354309082\n",
      "[step: 221] loss: 15.526985168457031\n",
      "[step: 222] loss: 15.51633358001709\n",
      "[step: 223] loss: 15.505526542663574\n",
      "[step: 224] loss: 15.494544982910156\n",
      "[step: 225] loss: 15.483394622802734\n",
      "[step: 226] loss: 15.472068786621094\n",
      "[step: 227] loss: 15.460556030273438\n",
      "[step: 228] loss: 15.44885540008545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 229] loss: 15.436954498291016\n",
      "[step: 230] loss: 15.424861907958984\n",
      "[step: 231] loss: 15.412550926208496\n",
      "[step: 232] loss: 15.400032997131348\n",
      "[step: 233] loss: 15.387287139892578\n",
      "[step: 234] loss: 15.374319076538086\n",
      "[step: 235] loss: 15.361112594604492\n",
      "[step: 236] loss: 15.347667694091797\n",
      "[step: 237] loss: 15.333978652954102\n",
      "[step: 238] loss: 15.320036888122559\n",
      "[step: 239] loss: 15.30583381652832\n",
      "[step: 240] loss: 15.291366577148438\n",
      "[step: 241] loss: 15.27662467956543\n",
      "[step: 242] loss: 15.261611938476562\n",
      "[step: 243] loss: 15.246313095092773\n",
      "[step: 244] loss: 15.230731010437012\n",
      "[step: 245] loss: 15.214852333068848\n",
      "[step: 246] loss: 15.198680877685547\n",
      "[step: 247] loss: 15.182205200195312\n",
      "[step: 248] loss: 15.165428161621094\n",
      "[step: 249] loss: 15.148344039916992\n",
      "[step: 250] loss: 15.130950927734375\n",
      "[step: 251] loss: 15.11324691772461\n",
      "[step: 252] loss: 15.095234870910645\n",
      "[step: 253] loss: 15.076910018920898\n",
      "[step: 254] loss: 15.058282852172852\n",
      "[step: 255] loss: 15.039352416992188\n",
      "[step: 256] loss: 15.020118713378906\n",
      "[step: 257] loss: 15.000593185424805\n",
      "[step: 258] loss: 14.980780601501465\n",
      "[step: 259] loss: 14.960694313049316\n",
      "[step: 260] loss: 14.940345764160156\n",
      "[step: 261] loss: 14.9197416305542\n",
      "[step: 262] loss: 14.898909568786621\n",
      "[step: 263] loss: 14.877856254577637\n",
      "[step: 264] loss: 14.856609344482422\n",
      "[step: 265] loss: 14.835193634033203\n",
      "[step: 266] loss: 14.813630104064941\n",
      "[step: 267] loss: 14.79195499420166\n",
      "[step: 268] loss: 14.770195007324219\n",
      "[step: 269] loss: 14.748390197753906\n",
      "[step: 270] loss: 14.72657299041748\n",
      "[step: 271] loss: 14.704788208007812\n",
      "[step: 272] loss: 14.68307876586914\n",
      "[step: 273] loss: 14.661494255065918\n",
      "[step: 274] loss: 14.640076637268066\n",
      "[step: 275] loss: 14.618881225585938\n",
      "[step: 276] loss: 14.597960472106934\n",
      "[step: 277] loss: 14.577360153198242\n",
      "[step: 278] loss: 14.557132720947266\n",
      "[step: 279] loss: 14.537334442138672\n",
      "[step: 280] loss: 14.518011093139648\n",
      "[step: 281] loss: 14.499208450317383\n",
      "[step: 282] loss: 14.480964660644531\n",
      "[step: 283] loss: 14.46331787109375\n",
      "[step: 284] loss: 14.44630241394043\n",
      "[step: 285] loss: 14.429935455322266\n",
      "[step: 286] loss: 14.414230346679688\n",
      "[step: 287] loss: 14.399190902709961\n",
      "[step: 288] loss: 14.384811401367188\n",
      "[step: 289] loss: 14.371075630187988\n",
      "[step: 290] loss: 14.357955932617188\n",
      "[step: 291] loss: 14.345420837402344\n",
      "[step: 292] loss: 14.333425521850586\n",
      "[step: 293] loss: 14.32191276550293\n",
      "[step: 294] loss: 14.31082534790039\n",
      "[step: 295] loss: 14.30010986328125\n",
      "[step: 296] loss: 14.289701461791992\n",
      "[step: 297] loss: 14.279537200927734\n",
      "[step: 298] loss: 14.269567489624023\n",
      "[step: 299] loss: 14.25973892211914\n",
      "[step: 300] loss: 14.250001907348633\n",
      "[step: 301] loss: 14.240330696105957\n",
      "[step: 302] loss: 14.230692863464355\n",
      "[step: 303] loss: 14.221073150634766\n",
      "[step: 304] loss: 14.211466789245605\n",
      "[step: 305] loss: 14.20186996459961\n",
      "[step: 306] loss: 14.192289352416992\n",
      "[step: 307] loss: 14.1827392578125\n",
      "[step: 308] loss: 14.173230171203613\n",
      "[step: 309] loss: 14.16379165649414\n",
      "[step: 310] loss: 14.154431343078613\n",
      "[step: 311] loss: 14.145170211791992\n",
      "[step: 312] loss: 14.13603401184082\n",
      "[step: 313] loss: 14.127029418945312\n",
      "[step: 314] loss: 14.118175506591797\n",
      "[step: 315] loss: 14.109475135803223\n",
      "[step: 316] loss: 14.100944519042969\n",
      "[step: 317] loss: 14.09257984161377\n",
      "[step: 318] loss: 14.08438777923584\n",
      "[step: 319] loss: 14.076364517211914\n",
      "[step: 320] loss: 14.068506240844727\n",
      "[step: 321] loss: 14.060811996459961\n",
      "[step: 322] loss: 14.053275108337402\n",
      "[step: 323] loss: 14.045886039733887\n",
      "[step: 324] loss: 14.038639068603516\n",
      "[step: 325] loss: 14.03152847290039\n",
      "[step: 326] loss: 14.024547576904297\n",
      "[step: 327] loss: 14.01768970489502\n",
      "[step: 328] loss: 14.010943412780762\n",
      "[step: 329] loss: 14.004308700561523\n",
      "[step: 330] loss: 13.997780799865723\n",
      "[step: 331] loss: 13.991352081298828\n",
      "[step: 332] loss: 13.985021591186523\n",
      "[step: 333] loss: 13.978780746459961\n",
      "[step: 334] loss: 13.972633361816406\n",
      "[step: 335] loss: 13.966572761535645\n",
      "[step: 336] loss: 13.960592269897461\n",
      "[step: 337] loss: 13.95469856262207\n",
      "[step: 338] loss: 13.948883056640625\n",
      "[step: 339] loss: 13.943143844604492\n",
      "[step: 340] loss: 13.937477111816406\n",
      "[step: 341] loss: 13.931882858276367\n",
      "[step: 342] loss: 13.926355361938477\n",
      "[step: 343] loss: 13.920892715454102\n",
      "[step: 344] loss: 13.915491104125977\n",
      "[step: 345] loss: 13.910144805908203\n",
      "[step: 346] loss: 13.904853820800781\n",
      "[step: 347] loss: 13.899611473083496\n",
      "[step: 348] loss: 13.894420623779297\n",
      "[step: 349] loss: 13.88926887512207\n",
      "[step: 350] loss: 13.884159088134766\n",
      "[step: 351] loss: 13.879084587097168\n",
      "[step: 352] loss: 13.874048233032227\n",
      "[step: 353] loss: 13.869039535522461\n",
      "[step: 354] loss: 13.864062309265137\n",
      "[step: 355] loss: 13.859111785888672\n",
      "[step: 356] loss: 13.854185104370117\n",
      "[step: 357] loss: 13.849283218383789\n",
      "[step: 358] loss: 13.844402313232422\n",
      "[step: 359] loss: 13.839546203613281\n",
      "[step: 360] loss: 13.834707260131836\n",
      "[step: 361] loss: 13.829886436462402\n",
      "[step: 362] loss: 13.825084686279297\n",
      "[step: 363] loss: 13.820302963256836\n",
      "[step: 364] loss: 13.815533638000488\n",
      "[step: 365] loss: 13.81077766418457\n",
      "[step: 366] loss: 13.806042671203613\n",
      "[step: 367] loss: 13.801316261291504\n",
      "[step: 368] loss: 13.796609878540039\n",
      "[step: 369] loss: 13.791914939880371\n",
      "[step: 370] loss: 13.787223815917969\n",
      "[step: 371] loss: 13.782553672790527\n",
      "[step: 372] loss: 13.777891159057617\n",
      "[step: 373] loss: 13.773240089416504\n",
      "[step: 374] loss: 13.768596649169922\n",
      "[step: 375] loss: 13.763962745666504\n",
      "[step: 376] loss: 13.759340286254883\n",
      "[step: 377] loss: 13.754722595214844\n",
      "[step: 378] loss: 13.750110626220703\n",
      "[step: 379] loss: 13.74550724029541\n",
      "[step: 380] loss: 13.740911483764648\n",
      "[step: 381] loss: 13.736320495605469\n",
      "[step: 382] loss: 13.731730461120605\n",
      "[step: 383] loss: 13.72714900970459\n",
      "[step: 384] loss: 13.722572326660156\n",
      "[step: 385] loss: 13.717994689941406\n",
      "[step: 386] loss: 13.713421821594238\n",
      "[step: 387] loss: 13.70884895324707\n",
      "[step: 388] loss: 13.7042818069458\n",
      "[step: 389] loss: 13.699711799621582\n",
      "[step: 390] loss: 13.695141792297363\n",
      "[step: 391] loss: 13.690570831298828\n",
      "[step: 392] loss: 13.686001777648926\n",
      "[step: 393] loss: 13.681429862976074\n",
      "[step: 394] loss: 13.676858901977539\n",
      "[step: 395] loss: 13.672284126281738\n",
      "[step: 396] loss: 13.667706489562988\n",
      "[step: 397] loss: 13.663125038146973\n",
      "[step: 398] loss: 13.658540725708008\n",
      "[step: 399] loss: 13.653951644897461\n",
      "[step: 400] loss: 13.649358749389648\n",
      "[step: 401] loss: 13.644759178161621\n",
      "[step: 402] loss: 13.640153884887695\n",
      "[step: 403] loss: 13.635547637939453\n",
      "[step: 404] loss: 13.630932807922363\n",
      "[step: 405] loss: 13.626311302185059\n",
      "[step: 406] loss: 13.621676445007324\n",
      "[step: 407] loss: 13.617042541503906\n",
      "[step: 408] loss: 13.612398147583008\n",
      "[step: 409] loss: 13.607746124267578\n",
      "[step: 410] loss: 13.603082656860352\n",
      "[step: 411] loss: 13.598411560058594\n",
      "[step: 412] loss: 13.593732833862305\n",
      "[step: 413] loss: 13.589042663574219\n",
      "[step: 414] loss: 13.584342956542969\n",
      "[step: 415] loss: 13.579631805419922\n",
      "[step: 416] loss: 13.574910163879395\n",
      "[step: 417] loss: 13.570178985595703\n",
      "[step: 418] loss: 13.565435409545898\n",
      "[step: 419] loss: 13.56067943572998\n",
      "[step: 420] loss: 13.555912017822266\n",
      "[step: 421] loss: 13.551132202148438\n",
      "[step: 422] loss: 13.546343803405762\n",
      "[step: 423] loss: 13.541537284851074\n",
      "[step: 424] loss: 13.536717414855957\n",
      "[step: 425] loss: 13.531885147094727\n",
      "[step: 426] loss: 13.527040481567383\n",
      "[step: 427] loss: 13.52218246459961\n",
      "[step: 428] loss: 13.517309188842773\n",
      "[step: 429] loss: 13.512418746948242\n",
      "[step: 430] loss: 13.507514953613281\n",
      "[step: 431] loss: 13.502599716186523\n",
      "[step: 432] loss: 13.497665405273438\n",
      "[step: 433] loss: 13.492717742919922\n",
      "[step: 434] loss: 13.487751960754395\n",
      "[step: 435] loss: 13.482772827148438\n",
      "[step: 436] loss: 13.477776527404785\n",
      "[step: 437] loss: 13.47276496887207\n",
      "[step: 438] loss: 13.46773624420166\n",
      "[step: 439] loss: 13.462690353393555\n",
      "[step: 440] loss: 13.45762825012207\n",
      "[step: 441] loss: 13.452547073364258\n",
      "[step: 442] loss: 13.4474515914917\n",
      "[step: 443] loss: 13.442337989807129\n",
      "[step: 444] loss: 13.437206268310547\n",
      "[step: 445] loss: 13.43205738067627\n",
      "[step: 446] loss: 13.426887512207031\n",
      "[step: 447] loss: 13.421701431274414\n",
      "[step: 448] loss: 13.416496276855469\n",
      "[step: 449] loss: 13.411272048950195\n",
      "[step: 450] loss: 13.40602970123291\n",
      "[step: 451] loss: 13.400768280029297\n",
      "[step: 452] loss: 13.395486831665039\n",
      "[step: 453] loss: 13.39018440246582\n",
      "[step: 454] loss: 13.384864807128906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 455] loss: 13.379521369934082\n",
      "[step: 456] loss: 13.37415885925293\n",
      "[step: 457] loss: 13.3687744140625\n",
      "[step: 458] loss: 13.363371849060059\n",
      "[step: 459] loss: 13.357945442199707\n",
      "[step: 460] loss: 13.352497100830078\n",
      "[step: 461] loss: 13.347025871276855\n",
      "[step: 462] loss: 13.341533660888672\n",
      "[step: 463] loss: 13.336018562316895\n",
      "[step: 464] loss: 13.330479621887207\n",
      "[step: 465] loss: 13.324913024902344\n",
      "[step: 466] loss: 13.319327354431152\n",
      "[step: 467] loss: 13.313713073730469\n",
      "[step: 468] loss: 13.308073043823242\n",
      "[step: 469] loss: 13.302410125732422\n",
      "[step: 470] loss: 13.29671859741211\n",
      "[step: 471] loss: 13.29100227355957\n",
      "[step: 472] loss: 13.285258293151855\n",
      "[step: 473] loss: 13.279485702514648\n",
      "[step: 474] loss: 13.273686408996582\n",
      "[step: 475] loss: 13.267852783203125\n",
      "[step: 476] loss: 13.261993408203125\n",
      "[step: 477] loss: 13.256105422973633\n",
      "[step: 478] loss: 13.250188827514648\n",
      "[step: 479] loss: 13.244233131408691\n",
      "[step: 480] loss: 13.238251686096191\n",
      "[step: 481] loss: 13.232236862182617\n",
      "[step: 482] loss: 13.226188659667969\n",
      "[step: 483] loss: 13.220108032226562\n",
      "[step: 484] loss: 13.213994026184082\n",
      "[step: 485] loss: 13.207843780517578\n",
      "[step: 486] loss: 13.20166015625\n",
      "[step: 487] loss: 13.195438385009766\n",
      "[step: 488] loss: 13.18918228149414\n",
      "[step: 489] loss: 13.182888984680176\n",
      "[step: 490] loss: 13.176557540893555\n",
      "[step: 491] loss: 13.17019271850586\n",
      "[step: 492] loss: 13.16378402709961\n",
      "[step: 493] loss: 13.157341003417969\n",
      "[step: 494] loss: 13.150854110717773\n",
      "[step: 495] loss: 13.144331932067871\n",
      "[step: 496] loss: 13.137765884399414\n",
      "[step: 497] loss: 13.131158828735352\n",
      "[step: 498] loss: 13.124509811401367\n",
      "[step: 499] loss: 13.11782455444336\n",
      "[step: 500] loss: 13.111091613769531\n",
      "[step: 501] loss: 13.104315757751465\n",
      "[step: 502] loss: 13.097498893737793\n",
      "[step: 503] loss: 13.09063720703125\n",
      "[step: 504] loss: 13.083730697631836\n",
      "[step: 505] loss: 13.076780319213867\n",
      "[step: 506] loss: 13.06978988647461\n",
      "[step: 507] loss: 13.062750816345215\n",
      "[step: 508] loss: 13.055665969848633\n",
      "[step: 509] loss: 13.048542022705078\n",
      "[step: 510] loss: 13.041367530822754\n",
      "[step: 511] loss: 13.034151077270508\n",
      "[step: 512] loss: 13.026888847351074\n",
      "[step: 513] loss: 13.019580841064453\n",
      "[step: 514] loss: 13.012229919433594\n",
      "[step: 515] loss: 13.00483226776123\n",
      "[step: 516] loss: 12.997390747070312\n",
      "[step: 517] loss: 12.989908218383789\n",
      "[step: 518] loss: 12.982382774353027\n",
      "[step: 519] loss: 12.974812507629395\n",
      "[step: 520] loss: 12.967199325561523\n",
      "[step: 521] loss: 12.959543228149414\n",
      "[step: 522] loss: 12.951852798461914\n",
      "[step: 523] loss: 12.944114685058594\n",
      "[step: 524] loss: 12.936346054077148\n",
      "[step: 525] loss: 12.928531646728516\n",
      "[step: 526] loss: 12.92068099975586\n",
      "[step: 527] loss: 12.91279411315918\n",
      "[step: 528] loss: 12.904874801635742\n",
      "[step: 529] loss: 12.896917343139648\n",
      "[step: 530] loss: 12.888928413391113\n",
      "[step: 531] loss: 12.880906105041504\n",
      "[step: 532] loss: 12.872854232788086\n",
      "[step: 533] loss: 12.864768981933594\n",
      "[step: 534] loss: 12.856656074523926\n",
      "[step: 535] loss: 12.848516464233398\n",
      "[step: 536] loss: 12.840346336364746\n",
      "[step: 537] loss: 12.832149505615234\n",
      "[step: 538] loss: 12.823930740356445\n",
      "[step: 539] loss: 12.815682411193848\n",
      "[step: 540] loss: 12.807414054870605\n",
      "[step: 541] loss: 12.79912281036377\n",
      "[step: 542] loss: 12.790809631347656\n",
      "[step: 543] loss: 12.782478332519531\n",
      "[step: 544] loss: 12.774131774902344\n",
      "[step: 545] loss: 12.765804290771484\n",
      "[step: 546] loss: 12.757667541503906\n",
      "[step: 547] loss: 12.750602722167969\n",
      "[step: 548] loss: 12.748912811279297\n",
      "[step: 549] loss: 12.756031036376953\n",
      "[step: 550] loss: 12.75976848602295\n",
      "[step: 551] loss: 12.724544525146484\n",
      "[step: 552] loss: 12.71599006652832\n",
      "[step: 553] loss: 12.727442741394043\n",
      "[step: 554] loss: 12.701935768127441\n",
      "[step: 555] loss: 12.69330883026123\n",
      "[step: 556] loss: 12.699445724487305\n",
      "[step: 557] loss: 12.677696228027344\n",
      "[step: 558] loss: 12.673095703125\n",
      "[step: 559] loss: 12.674036026000977\n",
      "[step: 560] loss: 12.654916763305664\n",
      "[step: 561] loss: 12.6528959274292\n",
      "[step: 562] loss: 12.650009155273438\n",
      "[step: 563] loss: 12.633492469787598\n",
      "[step: 564] loss: 12.632101058959961\n",
      "[step: 565] loss: 12.627126693725586\n",
      "[step: 566] loss: 12.612706184387207\n",
      "[step: 567] loss: 12.610736846923828\n",
      "[step: 568] loss: 12.605186462402344\n",
      "[step: 569] loss: 12.592205047607422\n",
      "[step: 570] loss: 12.589032173156738\n",
      "[step: 571] loss: 12.583893775939941\n",
      "[step: 572] loss: 12.571982383728027\n",
      "[step: 573] loss: 12.56716537475586\n",
      "[step: 574] loss: 12.562854766845703\n",
      "[step: 575] loss: 12.552220344543457\n",
      "[step: 576] loss: 12.54535961151123\n",
      "[step: 577] loss: 12.541470527648926\n",
      "[step: 578] loss: 12.53295612335205\n",
      "[step: 579] loss: 12.524308204650879\n",
      "[step: 580] loss: 12.51937484741211\n",
      "[step: 581] loss: 12.51346492767334\n",
      "[step: 582] loss: 12.504988670349121\n",
      "[step: 583] loss: 12.497618675231934\n",
      "[step: 584] loss: 12.492345809936523\n",
      "[step: 585] loss: 12.48626708984375\n",
      "[step: 586] loss: 12.478412628173828\n",
      "[step: 587] loss: 12.471025466918945\n",
      "[step: 588] loss: 12.465110778808594\n",
      "[step: 589] loss: 12.459382057189941\n",
      "[step: 590] loss: 12.452718734741211\n",
      "[step: 591] loss: 12.445364952087402\n",
      "[step: 592] loss: 12.438383102416992\n",
      "[step: 593] loss: 12.432157516479492\n",
      "[step: 594] loss: 12.426321029663086\n",
      "[step: 595] loss: 12.420379638671875\n",
      "[step: 596] loss: 12.414058685302734\n",
      "[step: 597] loss: 12.40749454498291\n",
      "[step: 598] loss: 12.400819778442383\n",
      "[step: 599] loss: 12.394233703613281\n",
      "[step: 600] loss: 12.387781143188477\n",
      "[step: 601] loss: 12.381458282470703\n",
      "[step: 602] loss: 12.375237464904785\n",
      "[step: 603] loss: 12.369091033935547\n",
      "[step: 604] loss: 12.363001823425293\n",
      "[step: 605] loss: 12.356972694396973\n",
      "[step: 606] loss: 12.351032257080078\n",
      "[step: 607] loss: 12.345266342163086\n",
      "[step: 608] loss: 12.339960098266602\n",
      "[step: 609] loss: 12.335990905761719\n",
      "[step: 610] loss: 12.336523056030273\n",
      "[step: 611] loss: 12.349332809448242\n",
      "[step: 612] loss: 12.396140098571777\n",
      "[step: 613] loss: 12.431659698486328\n",
      "[step: 614] loss: 12.40705394744873\n",
      "[step: 615] loss: 12.304452896118164\n",
      "[step: 616] loss: 12.35110855102539\n",
      "[step: 617] loss: 12.392611503601074\n",
      "[step: 618] loss: 12.29399585723877\n",
      "[step: 619] loss: 12.332505226135254\n",
      "[step: 620] loss: 12.357805252075195\n",
      "[step: 621] loss: 12.275754928588867\n",
      "[step: 622] loss: 12.331745147705078\n",
      "[step: 623] loss: 12.320087432861328\n",
      "[step: 624] loss: 12.26713752746582\n",
      "[step: 625] loss: 12.325542449951172\n",
      "[step: 626] loss: 12.278406143188477\n",
      "[step: 627] loss: 12.269024848937988\n",
      "[step: 628] loss: 12.297042846679688\n",
      "[step: 629] loss: 12.246184349060059\n",
      "[step: 630] loss: 12.268880844116211\n",
      "[step: 631] loss: 12.256181716918945\n",
      "[step: 632] loss: 12.23615837097168\n",
      "[step: 633] loss: 12.255409240722656\n",
      "[step: 634] loss: 12.227411270141602\n",
      "[step: 635] loss: 12.235640525817871\n",
      "[step: 636] loss: 12.233174324035645\n",
      "[step: 637] loss: 12.215133666992188\n",
      "[step: 638] loss: 12.228166580200195\n",
      "[step: 639] loss: 12.211856842041016\n",
      "[step: 640] loss: 12.208539962768555\n",
      "[step: 641] loss: 12.21231460571289\n",
      "[step: 642] loss: 12.19640827178955\n",
      "[step: 643] loss: 12.20032787322998\n",
      "[step: 644] loss: 12.194818496704102\n",
      "[step: 645] loss: 12.185845375061035\n",
      "[step: 646] loss: 12.189077377319336\n",
      "[step: 647] loss: 12.179930686950684\n",
      "[step: 648] loss: 12.176712036132812\n",
      "[step: 649] loss: 12.176491737365723\n",
      "[step: 650] loss: 12.167810440063477\n",
      "[step: 651] loss: 12.167031288146973\n",
      "[step: 652] loss: 12.164031028747559\n",
      "[step: 653] loss: 12.157022476196289\n",
      "[step: 654] loss: 12.156550407409668\n",
      "[step: 655] loss: 12.152189254760742\n",
      "[step: 656] loss: 12.146645545959473\n",
      "[step: 657] loss: 12.145689964294434\n",
      "[step: 658] loss: 12.14098834991455\n",
      "[step: 659] loss: 12.136289596557617\n",
      "[step: 660] loss: 12.134761810302734\n",
      "[step: 661] loss: 12.130231857299805\n",
      "[step: 662] loss: 12.125896453857422\n",
      "[step: 663] loss: 12.123926162719727\n",
      "[step: 664] loss: 12.119762420654297\n",
      "[step: 665] loss: 12.115522384643555\n",
      "[step: 666] loss: 12.113195419311523\n",
      "[step: 667] loss: 12.109453201293945\n",
      "[step: 668] loss: 12.105222702026367\n",
      "[step: 669] loss: 12.102556228637695\n",
      "[step: 670] loss: 12.099215507507324\n",
      "[step: 671] loss: 12.095049858093262\n",
      "[step: 672] loss: 12.091998100280762\n",
      "[step: 673] loss: 12.088957786560059\n",
      "[step: 674] loss: 12.085010528564453\n",
      "[step: 675] loss: 12.081567764282227\n",
      "[step: 676] loss: 12.078611373901367\n",
      "[step: 677] loss: 12.075035095214844\n",
      "[step: 678] loss: 12.071344375610352\n",
      "[step: 679] loss: 12.068204879760742\n",
      "[step: 680] loss: 12.064977645874023\n",
      "[step: 681] loss: 12.06135082244873\n",
      "[step: 682] loss: 12.057899475097656\n",
      "[step: 683] loss: 12.054744720458984\n",
      "[step: 684] loss: 12.051399230957031\n",
      "[step: 685] loss: 12.04784870147705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 686] loss: 12.044473648071289\n",
      "[step: 687] loss: 12.041263580322266\n",
      "[step: 688] loss: 12.037906646728516\n",
      "[step: 689] loss: 12.034423828125\n",
      "[step: 690] loss: 12.031042098999023\n",
      "[step: 691] loss: 12.027782440185547\n",
      "[step: 692] loss: 12.024458885192871\n",
      "[step: 693] loss: 12.02103328704834\n",
      "[step: 694] loss: 12.017622947692871\n",
      "[step: 695] loss: 12.014307975769043\n",
      "[step: 696] loss: 12.011006355285645\n",
      "[step: 697] loss: 12.007640838623047\n",
      "[step: 698] loss: 12.004234313964844\n",
      "[step: 699] loss: 12.000850677490234\n",
      "[step: 700] loss: 11.997514724731445\n",
      "[step: 701] loss: 11.994190216064453\n",
      "[step: 702] loss: 11.99083137512207\n",
      "[step: 703] loss: 11.987438201904297\n",
      "[step: 704] loss: 11.984046936035156\n",
      "[step: 705] loss: 11.980674743652344\n",
      "[step: 706] loss: 11.97732162475586\n",
      "[step: 707] loss: 11.973962783813477\n",
      "[step: 708] loss: 11.97059154510498\n",
      "[step: 709] loss: 11.967201232910156\n",
      "[step: 710] loss: 11.963802337646484\n",
      "[step: 711] loss: 11.96040153503418\n",
      "[step: 712] loss: 11.957008361816406\n",
      "[step: 713] loss: 11.95361614227295\n",
      "[step: 714] loss: 11.950224876403809\n",
      "[step: 715] loss: 11.946830749511719\n",
      "[step: 716] loss: 11.943428039550781\n",
      "[step: 717] loss: 11.940025329589844\n",
      "[step: 718] loss: 11.93661117553711\n",
      "[step: 719] loss: 11.933191299438477\n",
      "[step: 720] loss: 11.92977237701416\n",
      "[step: 721] loss: 11.926351547241211\n",
      "[step: 722] loss: 11.922933578491211\n",
      "[step: 723] loss: 11.919528007507324\n",
      "[step: 724] loss: 11.916158676147461\n",
      "[step: 725] loss: 11.912846565246582\n",
      "[step: 726] loss: 11.909683227539062\n",
      "[step: 727] loss: 11.906822204589844\n",
      "[step: 728] loss: 11.90471076965332\n",
      "[step: 729] loss: 11.904117584228516\n",
      "[step: 730] loss: 11.907642364501953\n",
      "[step: 731] loss: 11.918423652648926\n",
      "[step: 732] loss: 11.949685096740723\n",
      "[step: 733] loss: 11.991071701049805\n",
      "[step: 734] loss: 12.058185577392578\n",
      "[step: 735] loss: 11.997050285339355\n",
      "[step: 736] loss: 11.91006851196289\n",
      "[step: 737] loss: 11.876453399658203\n",
      "[step: 738] loss: 11.930608749389648\n",
      "[step: 739] loss: 11.964287757873535\n",
      "[step: 740] loss: 11.889959335327148\n",
      "[step: 741] loss: 11.864705085754395\n",
      "[step: 742] loss: 11.909605026245117\n",
      "[step: 743] loss: 11.902458190917969\n",
      "[step: 744] loss: 11.859540939331055\n",
      "[step: 745] loss: 11.856517791748047\n",
      "[step: 746] loss: 11.880794525146484\n",
      "[step: 747] loss: 11.868846893310547\n",
      "[step: 748] loss: 11.840285301208496\n",
      "[step: 749] loss: 11.852609634399414\n",
      "[step: 750] loss: 11.86346435546875\n",
      "[step: 751] loss: 11.837751388549805\n",
      "[step: 752] loss: 11.83041000366211\n",
      "[step: 753] loss: 11.844200134277344\n",
      "[step: 754] loss: 11.83505630493164\n",
      "[step: 755] loss: 11.818964004516602\n",
      "[step: 756] loss: 11.821718215942383\n",
      "[step: 757] loss: 11.825350761413574\n",
      "[step: 758] loss: 11.814651489257812\n",
      "[step: 759] loss: 11.806465148925781\n",
      "[step: 760] loss: 11.810243606567383\n",
      "[step: 761] loss: 11.808924674987793\n",
      "[step: 762] loss: 11.79843807220459\n",
      "[step: 763] loss: 11.795072555541992\n",
      "[step: 764] loss: 11.797243118286133\n",
      "[step: 765] loss: 11.792316436767578\n",
      "[step: 766] loss: 11.78469181060791\n",
      "[step: 767] loss: 11.78268814086914\n",
      "[step: 768] loss: 11.782503128051758\n",
      "[step: 769] loss: 11.777839660644531\n",
      "[step: 770] loss: 11.771749496459961\n",
      "[step: 771] loss: 11.76957893371582\n",
      "[step: 772] loss: 11.768447875976562\n",
      "[step: 773] loss: 11.764022827148438\n",
      "[step: 774] loss: 11.758834838867188\n",
      "[step: 775] loss: 11.756072044372559\n",
      "[step: 776] loss: 11.754234313964844\n",
      "[step: 777] loss: 11.75068473815918\n",
      "[step: 778] loss: 11.745957374572754\n",
      "[step: 779] loss: 11.74244499206543\n",
      "[step: 780] loss: 11.740105628967285\n",
      "[step: 781] loss: 11.737113952636719\n",
      "[step: 782] loss: 11.733026504516602\n",
      "[step: 783] loss: 11.729015350341797\n",
      "[step: 784] loss: 11.725919723510742\n",
      "[step: 785] loss: 11.723148345947266\n",
      "[step: 786] loss: 11.71976089477539\n",
      "[step: 787] loss: 11.715846061706543\n",
      "[step: 788] loss: 11.712063789367676\n",
      "[step: 789] loss: 11.708791732788086\n",
      "[step: 790] loss: 11.705721855163574\n",
      "[step: 791] loss: 11.702367782592773\n",
      "[step: 792] loss: 11.698661804199219\n",
      "[step: 793] loss: 11.694883346557617\n",
      "[step: 794] loss: 11.691305160522461\n",
      "[step: 795] loss: 11.687950134277344\n",
      "[step: 796] loss: 11.684619903564453\n",
      "[step: 797] loss: 11.681142807006836\n",
      "[step: 798] loss: 11.677482604980469\n",
      "[step: 799] loss: 11.673749923706055\n",
      "[step: 800] loss: 11.670048713684082\n",
      "[step: 801] loss: 11.66644287109375\n",
      "[step: 802] loss: 11.66290283203125\n",
      "[step: 803] loss: 11.659374237060547\n",
      "[step: 804] loss: 11.655803680419922\n",
      "[step: 805] loss: 11.652164459228516\n",
      "[step: 806] loss: 11.648465156555176\n",
      "[step: 807] loss: 11.644719123840332\n",
      "[step: 808] loss: 11.640955924987793\n",
      "[step: 809] loss: 11.637178421020508\n",
      "[step: 810] loss: 11.633395195007324\n",
      "[step: 811] loss: 11.62960433959961\n",
      "[step: 812] loss: 11.625808715820312\n",
      "[step: 813] loss: 11.622001647949219\n",
      "[step: 814] loss: 11.618181228637695\n",
      "[step: 815] loss: 11.61434555053711\n",
      "[step: 816] loss: 11.610493659973145\n",
      "[step: 817] loss: 11.606618881225586\n",
      "[step: 818] loss: 11.602733612060547\n",
      "[step: 819] loss: 11.59882926940918\n",
      "[step: 820] loss: 11.594913482666016\n",
      "[step: 821] loss: 11.590988159179688\n",
      "[step: 822] loss: 11.587085723876953\n",
      "[step: 823] loss: 11.583267211914062\n",
      "[step: 824] loss: 11.579657554626465\n",
      "[step: 825] loss: 11.576662063598633\n",
      "[step: 826] loss: 11.57522964477539\n",
      "[step: 827] loss: 11.578499794006348\n",
      "[step: 828] loss: 11.592779159545898\n",
      "[step: 829] loss: 11.642688751220703\n",
      "[step: 830] loss: 11.736479759216309\n",
      "[step: 831] loss: 11.950358390808105\n",
      "[step: 832] loss: 11.843179702758789\n",
      "[step: 833] loss: 11.653953552246094\n",
      "[step: 834] loss: 11.553231239318848\n",
      "[step: 835] loss: 11.709758758544922\n",
      "[step: 836] loss: 11.76905632019043\n",
      "[step: 837] loss: 11.545188903808594\n",
      "[step: 838] loss: 11.629669189453125\n",
      "[step: 839] loss: 11.758536338806152\n",
      "[step: 840] loss: 11.543490409851074\n",
      "[step: 841] loss: 11.602334976196289\n",
      "[step: 842] loss: 11.701589584350586\n",
      "[step: 843] loss: 11.518540382385254\n",
      "[step: 844] loss: 11.619515419006348\n",
      "[step: 845] loss: 11.6505126953125\n",
      "[step: 846] loss: 11.505132675170898\n",
      "[step: 847] loss: 11.645109176635742\n",
      "[step: 848] loss: 11.592340469360352\n",
      "[step: 849] loss: 11.510546684265137\n",
      "[step: 850] loss: 11.637236595153809\n",
      "[step: 851] loss: 11.529296875\n",
      "[step: 852] loss: 11.524964332580566\n",
      "[step: 853] loss: 11.582908630371094\n",
      "[step: 854] loss: 11.48101806640625\n",
      "[step: 855] loss: 11.530477523803711\n",
      "[step: 856] loss: 11.508913040161133\n",
      "[step: 857] loss: 11.473123550415039\n",
      "[step: 858] loss: 11.51439094543457\n",
      "[step: 859] loss: 11.463178634643555\n",
      "[step: 860] loss: 11.485420227050781\n",
      "[step: 861] loss: 11.481562614440918\n",
      "[step: 862] loss: 11.452375411987305\n",
      "[step: 863] loss: 11.481624603271484\n",
      "[step: 864] loss: 11.45045280456543\n",
      "[step: 865] loss: 11.451812744140625\n",
      "[step: 866] loss: 11.45809555053711\n",
      "[step: 867] loss: 11.432109832763672\n",
      "[step: 868] loss: 11.445598602294922\n",
      "[step: 869] loss: 11.432086944580078\n",
      "[step: 870] loss: 11.424747467041016\n",
      "[step: 871] loss: 11.431316375732422\n",
      "[step: 872] loss: 11.41427993774414\n",
      "[step: 873] loss: 11.418420791625977\n",
      "[step: 874] loss: 11.413985252380371\n",
      "[step: 875] loss: 11.403030395507812\n",
      "[step: 876] loss: 11.407886505126953\n",
      "[step: 877] loss: 11.397737503051758\n",
      "[step: 878] loss: 11.393343925476074\n",
      "[step: 879] loss: 11.39411735534668\n",
      "[step: 880] loss: 11.383831977844238\n",
      "[step: 881] loss: 11.382736206054688\n",
      "[step: 882] loss: 11.37956428527832\n",
      "[step: 883] loss: 11.371479034423828\n",
      "[step: 884] loss: 11.370851516723633\n",
      "[step: 885] loss: 11.365583419799805\n",
      "[step: 886] loss: 11.359633445739746\n",
      "[step: 887] loss: 11.35816764831543\n",
      "[step: 888] loss: 11.352263450622559\n",
      "[step: 889] loss: 11.347585678100586\n",
      "[step: 890] loss: 11.345163345336914\n",
      "[step: 891] loss: 11.339332580566406\n",
      "[step: 892] loss: 11.335153579711914\n",
      "[step: 893] loss: 11.332042694091797\n",
      "[step: 894] loss: 11.326448440551758\n",
      "[step: 895] loss: 11.32235050201416\n",
      "[step: 896] loss: 11.318819999694824\n",
      "[step: 897] loss: 11.313482284545898\n",
      "[step: 898] loss: 11.309221267700195\n",
      "[step: 899] loss: 11.30544662475586\n",
      "[step: 900] loss: 11.30032730102539\n",
      "[step: 901] loss: 11.295825958251953\n",
      "[step: 902] loss: 11.291878700256348\n",
      "[step: 903] loss: 11.286947250366211\n",
      "[step: 904] loss: 11.282196044921875\n",
      "[step: 905] loss: 11.27805233001709\n",
      "[step: 906] loss: 11.273303031921387\n",
      "[step: 907] loss: 11.268363952636719\n",
      "[step: 908] loss: 11.263946533203125\n",
      "[step: 909] loss: 11.259334564208984\n",
      "[step: 910] loss: 11.254331588745117\n",
      "[step: 911] loss: 11.24958610534668\n",
      "[step: 912] loss: 11.24498176574707\n",
      "[step: 913] loss: 11.2400484085083\n",
      "[step: 914] loss: 11.23503589630127\n",
      "[step: 915] loss: 11.230247497558594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 916] loss: 11.225390434265137\n",
      "[step: 917] loss: 11.220293045043945\n",
      "[step: 918] loss: 11.215226173400879\n",
      "[step: 919] loss: 11.210275650024414\n",
      "[step: 920] loss: 11.205223083496094\n",
      "[step: 921] loss: 11.200016021728516\n",
      "[step: 922] loss: 11.194809913635254\n",
      "[step: 923] loss: 11.189665794372559\n",
      "[step: 924] loss: 11.18446159362793\n",
      "[step: 925] loss: 11.17913818359375\n",
      "[step: 926] loss: 11.173760414123535\n",
      "[step: 927] loss: 11.16840934753418\n",
      "[step: 928] loss: 11.163039207458496\n",
      "[step: 929] loss: 11.157588958740234\n",
      "[step: 930] loss: 11.152059555053711\n",
      "[step: 931] loss: 11.146486282348633\n",
      "[step: 932] loss: 11.14090633392334\n",
      "[step: 933] loss: 11.135293960571289\n",
      "[step: 934] loss: 11.12962532043457\n",
      "[step: 935] loss: 11.123886108398438\n",
      "[step: 936] loss: 11.118085861206055\n",
      "[step: 937] loss: 11.11224365234375\n",
      "[step: 938] loss: 11.106365203857422\n",
      "[step: 939] loss: 11.100444793701172\n",
      "[step: 940] loss: 11.094476699829102\n",
      "[step: 941] loss: 11.08845043182373\n",
      "[step: 942] loss: 11.082367897033691\n",
      "[step: 943] loss: 11.076228141784668\n",
      "[step: 944] loss: 11.070030212402344\n",
      "[step: 945] loss: 11.063775062561035\n",
      "[step: 946] loss: 11.057467460632324\n",
      "[step: 947] loss: 11.051109313964844\n",
      "[step: 948] loss: 11.04470443725586\n",
      "[step: 949] loss: 11.03826904296875\n",
      "[step: 950] loss: 11.031835556030273\n",
      "[step: 951] loss: 11.025480270385742\n",
      "[step: 952] loss: 11.019427299499512\n",
      "[step: 953] loss: 11.014315605163574\n",
      "[step: 954] loss: 11.011775970458984\n",
      "[step: 955] loss: 11.017602920532227\n",
      "[step: 956] loss: 11.045040130615234\n",
      "[step: 957] loss: 11.149605751037598\n",
      "[step: 958] loss: 11.35738468170166\n",
      "[step: 959] loss: 11.86691951751709\n",
      "[step: 960] loss: 11.45678997039795\n",
      "[step: 961] loss: 11.044368743896484\n",
      "[step: 962] loss: 11.056640625\n",
      "[step: 963] loss: 11.302494049072266\n",
      "[step: 964] loss: 11.246856689453125\n",
      "[step: 965] loss: 10.945751190185547\n",
      "[step: 966] loss: 11.22630500793457\n",
      "[step: 967] loss: 11.35150146484375\n",
      "[step: 968] loss: 10.936117172241211\n",
      "[step: 969] loss: 11.253960609436035\n",
      "[step: 970] loss: 11.31036376953125\n",
      "[step: 971] loss: 10.937328338623047\n",
      "[step: 972] loss: 11.376808166503906\n",
      "[step: 973] loss: 11.151182174682617\n",
      "[step: 974] loss: 11.014505386352539\n",
      "[step: 975] loss: 11.322032928466797\n",
      "[step: 976] loss: 10.942558288574219\n",
      "[step: 977] loss: 11.088330268859863\n",
      "[step: 978] loss: 11.035036087036133\n",
      "[step: 979] loss: 10.901552200317383\n",
      "[step: 980] loss: 11.050053596496582\n",
      "[step: 981] loss: 10.866816520690918\n",
      "[step: 982] loss: 11.015983581542969\n",
      "[step: 983] loss: 10.911685943603516\n",
      "[step: 984] loss: 10.914178848266602\n",
      "[step: 985] loss: 10.944380760192871\n",
      "[step: 986] loss: 10.843160629272461\n",
      "[step: 987] loss: 10.924083709716797\n",
      "[step: 988] loss: 10.835277557373047\n",
      "[step: 989] loss: 10.880489349365234\n",
      "[step: 990] loss: 10.856624603271484\n",
      "[step: 991] loss: 10.82776927947998\n",
      "[step: 992] loss: 10.86349105834961\n",
      "[step: 993] loss: 10.799093246459961\n",
      "[step: 994] loss: 10.835287094116211\n",
      "[step: 995] loss: 10.798890113830566\n",
      "[step: 996] loss: 10.796099662780762\n",
      "[step: 997] loss: 10.803393363952637\n",
      "[step: 998] loss: 10.768001556396484\n",
      "[step: 999] loss: 10.790531158447266\n",
      "[step: 1000] loss: 10.761639595031738\n",
      "[step: 1001] loss: 10.758734703063965\n",
      "[step: 1002] loss: 10.76059627532959\n",
      "[step: 1003] loss: 10.733892440795898\n",
      "[step: 1004] loss: 10.743326187133789\n",
      "[step: 1005] loss: 10.72624397277832\n",
      "[step: 1006] loss: 10.716812133789062\n",
      "[step: 1007] loss: 10.719091415405273\n",
      "[step: 1008] loss: 10.699784278869629\n",
      "[step: 1009] loss: 10.698711395263672\n",
      "[step: 1010] loss: 10.692994117736816\n",
      "[step: 1011] loss: 10.677223205566406\n",
      "[step: 1012] loss: 10.677400588989258\n",
      "[step: 1013] loss: 10.667800903320312\n",
      "[step: 1014] loss: 10.655104637145996\n",
      "[step: 1015] loss: 10.654029846191406\n",
      "[step: 1016] loss: 10.643802642822266\n",
      "[step: 1017] loss: 10.632352828979492\n",
      "[step: 1018] loss: 10.629590034484863\n",
      "[step: 1019] loss: 10.62059211730957\n",
      "[step: 1020] loss: 10.60895824432373\n",
      "[step: 1021] loss: 10.604509353637695\n",
      "[step: 1022] loss: 10.597352981567383\n",
      "[step: 1023] loss: 10.585779190063477\n",
      "[step: 1024] loss: 10.578681945800781\n",
      "[step: 1025] loss: 10.573099136352539\n",
      "[step: 1026] loss: 10.56325626373291\n",
      "[step: 1027] loss: 10.55333423614502\n",
      "[step: 1028] loss: 10.546792984008789\n",
      "[step: 1029] loss: 10.539836883544922\n",
      "[step: 1030] loss: 10.530333518981934\n",
      "[step: 1031] loss: 10.52075481414795\n",
      "[step: 1032] loss: 10.513192176818848\n",
      "[step: 1033] loss: 10.50621509552002\n",
      "[step: 1034] loss: 10.49786376953125\n",
      "[step: 1035] loss: 10.488485336303711\n",
      "[step: 1036] loss: 10.479314804077148\n",
      "[step: 1037] loss: 10.471094131469727\n",
      "[step: 1038] loss: 10.463475227355957\n",
      "[step: 1039] loss: 10.45579719543457\n",
      "[step: 1040] loss: 10.447789192199707\n",
      "[step: 1041] loss: 10.439366340637207\n",
      "[step: 1042] loss: 10.43086051940918\n",
      "[step: 1043] loss: 10.422368049621582\n",
      "[step: 1044] loss: 10.414173126220703\n",
      "[step: 1045] loss: 10.406499862670898\n",
      "[step: 1046] loss: 10.400154113769531\n",
      "[step: 1047] loss: 10.39675521850586\n",
      "[step: 1048] loss: 10.402109146118164\n",
      "[step: 1049] loss: 10.428686141967773\n",
      "[step: 1050] loss: 10.525266647338867\n",
      "[step: 1051] loss: 10.73717975616455\n",
      "[step: 1052] loss: 11.257641792297363\n",
      "[step: 1053] loss: 11.182311058044434\n",
      "[step: 1054] loss: 10.880522727966309\n",
      "[step: 1055] loss: 10.341389656066895\n",
      "[step: 1056] loss: 10.766046524047852\n",
      "[step: 1057] loss: 11.214760780334473\n",
      "[step: 1058] loss: 10.408140182495117\n",
      "[step: 1059] loss: 10.701272964477539\n",
      "[step: 1060] loss: 11.185124397277832\n",
      "[step: 1061] loss: 10.332283020019531\n",
      "[step: 1062] loss: 10.926034927368164\n",
      "[step: 1063] loss: 10.992927551269531\n",
      "[step: 1064] loss: 10.366785049438477\n",
      "[step: 1065] loss: 11.210920333862305\n",
      "[step: 1066] loss: 10.670754432678223\n",
      "[step: 1067] loss: 10.645110130310059\n",
      "[step: 1068] loss: 10.88629150390625\n",
      "[step: 1069] loss: 10.282308578491211\n",
      "[step: 1070] loss: 10.705150604248047\n",
      "[step: 1071] loss: 10.290138244628906\n",
      "[step: 1072] loss: 10.601774215698242\n",
      "[step: 1073] loss: 10.421459197998047\n",
      "[step: 1074] loss: 10.418065071105957\n",
      "[step: 1075] loss: 10.465902328491211\n",
      "[step: 1076] loss: 10.277669906616211\n",
      "[step: 1077] loss: 10.455511093139648\n",
      "[step: 1078] loss: 10.244447708129883\n",
      "[step: 1079] loss: 10.432765007019043\n",
      "[step: 1080] loss: 10.247650146484375\n",
      "[step: 1081] loss: 10.353864669799805\n",
      "[step: 1082] loss: 10.25712776184082\n",
      "[step: 1083] loss: 10.284852981567383\n",
      "[step: 1084] loss: 10.280059814453125\n",
      "[step: 1085] loss: 10.234495162963867\n",
      "[step: 1086] loss: 10.285361289978027\n",
      "[step: 1087] loss: 10.195871353149414\n",
      "[step: 1088] loss: 10.26089859008789\n",
      "[step: 1089] loss: 10.18493366241455\n",
      "[step: 1090] loss: 10.23255729675293\n",
      "[step: 1091] loss: 10.18956184387207\n",
      "[step: 1092] loss: 10.192780494689941\n",
      "[step: 1093] loss: 10.191165924072266\n",
      "[step: 1094] loss: 10.159265518188477\n",
      "[step: 1095] loss: 10.183492660522461\n",
      "[step: 1096] loss: 10.14250659942627\n",
      "[step: 1097] loss: 10.164007186889648\n",
      "[step: 1098] loss: 10.138877868652344\n",
      "[step: 1099] loss: 10.134819030761719\n",
      "[step: 1100] loss: 10.136528968811035\n",
      "[step: 1101] loss: 10.111534118652344\n",
      "[step: 1102] loss: 10.123044967651367\n",
      "[step: 1103] loss: 10.101449966430664\n",
      "[step: 1104] loss: 10.100299835205078\n",
      "[step: 1105] loss: 10.097046852111816\n",
      "[step: 1106] loss: 10.079620361328125\n",
      "[step: 1107] loss: 10.08514404296875\n",
      "[step: 1108] loss: 10.069979667663574\n",
      "[step: 1109] loss: 10.063711166381836\n",
      "[step: 1110] loss: 10.06270980834961\n",
      "[step: 1111] loss: 10.047754287719727\n",
      "[step: 1112] loss: 10.045875549316406\n",
      "[step: 1113] loss: 10.039718627929688\n",
      "[step: 1114] loss: 10.027899742126465\n",
      "[step: 1115] loss: 10.026362419128418\n",
      "[step: 1116] loss: 10.017866134643555\n",
      "[step: 1117] loss: 10.008691787719727\n",
      "[step: 1118] loss: 10.005792617797852\n",
      "[step: 1119] loss: 9.997386932373047\n",
      "[step: 1120] loss: 9.988982200622559\n",
      "[step: 1121] loss: 9.985160827636719\n",
      "[step: 1122] loss: 9.977386474609375\n",
      "[step: 1123] loss: 9.96916389465332\n",
      "[step: 1124] loss: 9.964335441589355\n",
      "[step: 1125] loss: 9.957685470581055\n",
      "[step: 1126] loss: 9.949319839477539\n",
      "[step: 1127] loss: 9.943421363830566\n",
      "[step: 1128] loss: 9.93763256072998\n",
      "[step: 1129] loss: 9.929885864257812\n",
      "[step: 1130] loss: 9.92260456085205\n",
      "[step: 1131] loss: 9.916839599609375\n",
      "[step: 1132] loss: 9.910294532775879\n",
      "[step: 1133] loss: 9.902657508850098\n",
      "[step: 1134] loss: 9.895639419555664\n",
      "[step: 1135] loss: 9.889504432678223\n",
      "[step: 1136] loss: 9.882895469665527\n",
      "[step: 1137] loss: 9.87553596496582\n",
      "[step: 1138] loss: 9.868267059326172\n",
      "[step: 1139] loss: 9.861612319946289\n",
      "[step: 1140] loss: 9.855119705200195\n",
      "[step: 1141] loss: 9.848222732543945\n",
      "[step: 1142] loss: 9.840950012207031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1143] loss: 9.833660125732422\n",
      "[step: 1144] loss: 9.826623916625977\n",
      "[step: 1145] loss: 9.819784164428711\n",
      "[step: 1146] loss: 9.812980651855469\n",
      "[step: 1147] loss: 9.806046485900879\n",
      "[step: 1148] loss: 9.798968315124512\n",
      "[step: 1149] loss: 9.791769027709961\n",
      "[step: 1150] loss: 9.784525871276855\n",
      "[step: 1151] loss: 9.777268409729004\n",
      "[step: 1152] loss: 9.770030975341797\n",
      "[step: 1153] loss: 9.762832641601562\n",
      "[step: 1154] loss: 9.755718231201172\n",
      "[step: 1155] loss: 9.748826026916504\n",
      "[step: 1156] loss: 9.74243450164795\n",
      "[step: 1157] loss: 9.737462997436523\n",
      "[step: 1158] loss: 9.73613452911377\n",
      "[step: 1159] loss: 9.74635124206543\n",
      "[step: 1160] loss: 9.786678314208984\n",
      "[step: 1161] loss: 9.93890380859375\n",
      "[step: 1162] loss: 10.302835464477539\n",
      "[step: 1163] loss: 11.469165802001953\n",
      "[step: 1164] loss: 11.349489212036133\n",
      "[step: 1165] loss: 10.99930191040039\n",
      "[step: 1166] loss: 9.713382720947266\n",
      "[step: 1167] loss: 10.905214309692383\n",
      "[step: 1168] loss: 12.042289733886719\n",
      "[step: 1169] loss: 9.90267276763916\n",
      "[step: 1170] loss: 12.300542831420898\n",
      "[step: 1171] loss: 11.96987533569336\n",
      "[step: 1172] loss: 10.856060981750488\n",
      "[step: 1173] loss: 12.011472702026367\n",
      "[step: 1174] loss: 9.846229553222656\n",
      "[step: 1175] loss: 11.212221145629883\n",
      "[step: 1176] loss: 9.907804489135742\n",
      "[step: 1177] loss: 11.117576599121094\n",
      "[step: 1178] loss: 9.854129791259766\n",
      "[step: 1179] loss: 10.668131828308105\n",
      "[step: 1180] loss: 9.804615020751953\n",
      "[step: 1181] loss: 10.604982376098633\n",
      "[step: 1182] loss: 9.734869956970215\n",
      "[step: 1183] loss: 10.28028678894043\n",
      "[step: 1184] loss: 9.911803245544434\n",
      "[step: 1185] loss: 10.015250205993652\n",
      "[step: 1186] loss: 9.917336463928223\n",
      "[step: 1187] loss: 9.908731460571289\n",
      "[step: 1188] loss: 9.957016944885254\n",
      "[step: 1189] loss: 9.781471252441406\n",
      "[step: 1190] loss: 9.964360237121582\n",
      "[step: 1191] loss: 9.726491928100586\n",
      "[step: 1192] loss: 9.933189392089844\n",
      "[step: 1193] loss: 9.697553634643555\n",
      "[step: 1194] loss: 9.879884719848633\n",
      "[step: 1195] loss: 9.709972381591797\n",
      "[step: 1196] loss: 9.79471206665039\n",
      "[step: 1197] loss: 9.741660118103027\n",
      "[step: 1198] loss: 9.729682922363281\n",
      "[step: 1199] loss: 9.739826202392578\n",
      "[step: 1200] loss: 9.69328498840332\n",
      "[step: 1201] loss: 9.734493255615234\n",
      "[step: 1202] loss: 9.659393310546875\n",
      "[step: 1203] loss: 9.723053932189941\n",
      "[step: 1204] loss: 9.642221450805664\n",
      "[step: 1205] loss: 9.698122024536133\n",
      "[step: 1206] loss: 9.633356094360352\n",
      "[step: 1207] loss: 9.67149543762207\n",
      "[step: 1208] loss: 9.627275466918945\n",
      "[step: 1209] loss: 9.644691467285156\n",
      "[step: 1210] loss: 9.621240615844727\n",
      "[step: 1211] loss: 9.623021125793457\n",
      "[step: 1212] loss: 9.608917236328125\n",
      "[step: 1213] loss: 9.606989860534668\n",
      "[step: 1214] loss: 9.59744644165039\n",
      "[step: 1215] loss: 9.589933395385742\n",
      "[step: 1216] loss: 9.585561752319336\n",
      "[step: 1217] loss: 9.574518203735352\n",
      "[step: 1218] loss: 9.573379516601562\n",
      "[step: 1219] loss: 9.560613632202148\n",
      "[step: 1220] loss: 9.559568405151367\n",
      "[step: 1221] loss: 9.548578262329102\n",
      "[step: 1222] loss: 9.545575141906738\n",
      "[step: 1223] loss: 9.536617279052734\n",
      "[step: 1224] loss: 9.531900405883789\n",
      "[step: 1225] loss: 9.524746894836426\n",
      "[step: 1226] loss: 9.51896858215332\n",
      "[step: 1227] loss: 9.512638092041016\n",
      "[step: 1228] loss: 9.505938529968262\n",
      "[step: 1229] loss: 9.501044273376465\n",
      "[step: 1230] loss: 9.493058204650879\n",
      "[step: 1231] loss: 9.48954963684082\n",
      "[step: 1232] loss: 9.48031997680664\n",
      "[step: 1233] loss: 9.477884292602539\n",
      "[step: 1234] loss: 9.468202590942383\n",
      "[step: 1235] loss: 9.465836524963379\n",
      "[step: 1236] loss: 9.456716537475586\n",
      "[step: 1237] loss: 9.45334243774414\n",
      "[step: 1238] loss: 9.445684432983398\n",
      "[step: 1239] loss: 9.440711975097656\n",
      "[step: 1240] loss: 9.434746742248535\n",
      "[step: 1241] loss: 9.428298950195312\n",
      "[step: 1242] loss: 9.423572540283203\n",
      "[step: 1243] loss: 9.416336059570312\n",
      "[step: 1244] loss: 9.411885261535645\n",
      "[step: 1245] loss: 9.404932022094727\n",
      "[step: 1246] loss: 9.399835586547852\n",
      "[step: 1247] loss: 9.393852233886719\n",
      "[step: 1248] loss: 9.387740135192871\n",
      "[step: 1249] loss: 9.382612228393555\n",
      "[step: 1250] loss: 9.3760404586792\n",
      "[step: 1251] loss: 9.37090015411377\n",
      "[step: 1252] loss: 9.364785194396973\n",
      "[step: 1253] loss: 9.358955383300781\n",
      "[step: 1254] loss: 9.353540420532227\n",
      "[step: 1255] loss: 9.347268104553223\n",
      "[step: 1256] loss: 9.34190559387207\n",
      "[step: 1257] loss: 9.335966110229492\n",
      "[step: 1258] loss: 9.330062866210938\n",
      "[step: 1259] loss: 9.324593544006348\n",
      "[step: 1260] loss: 9.318520545959473\n",
      "[step: 1261] loss: 9.312864303588867\n",
      "[step: 1262] loss: 9.307168960571289\n",
      "[step: 1263] loss: 9.301186561584473\n",
      "[step: 1264] loss: 9.295553207397461\n",
      "[step: 1265] loss: 9.289762496948242\n",
      "[step: 1266] loss: 9.283839225769043\n",
      "[step: 1267] loss: 9.278179168701172\n",
      "[step: 1268] loss: 9.272348403930664\n",
      "[step: 1269] loss: 9.266454696655273\n",
      "[step: 1270] loss: 9.260759353637695\n",
      "[step: 1271] loss: 9.254910469055176\n",
      "[step: 1272] loss: 9.249027252197266\n",
      "[step: 1273] loss: 9.24327278137207\n",
      "[step: 1274] loss: 9.237444877624512\n",
      "[step: 1275] loss: 9.231544494628906\n",
      "[step: 1276] loss: 9.225740432739258\n",
      "[step: 1277] loss: 9.219915390014648\n",
      "[step: 1278] loss: 9.214008331298828\n",
      "[step: 1279] loss: 9.208146095275879\n",
      "[step: 1280] loss: 9.202312469482422\n",
      "[step: 1281] loss: 9.196418762207031\n",
      "[step: 1282] loss: 9.19050121307373\n",
      "[step: 1283] loss: 9.184627532958984\n",
      "[step: 1284] loss: 9.178743362426758\n",
      "[step: 1285] loss: 9.172811508178711\n",
      "[step: 1286] loss: 9.166874885559082\n",
      "[step: 1287] loss: 9.16096305847168\n",
      "[step: 1288] loss: 9.155038833618164\n",
      "[step: 1289] loss: 9.149076461791992\n",
      "[step: 1290] loss: 9.143110275268555\n",
      "[step: 1291] loss: 9.137147903442383\n",
      "[step: 1292] loss: 9.131182670593262\n",
      "[step: 1293] loss: 9.125198364257812\n",
      "[step: 1294] loss: 9.119195938110352\n",
      "[step: 1295] loss: 9.113182067871094\n",
      "[step: 1296] loss: 9.107166290283203\n",
      "[step: 1297] loss: 9.101151466369629\n",
      "[step: 1298] loss: 9.095115661621094\n",
      "[step: 1299] loss: 9.089059829711914\n",
      "[step: 1300] loss: 9.082999229431152\n",
      "[step: 1301] loss: 9.076923370361328\n",
      "[step: 1302] loss: 9.070837020874023\n",
      "[step: 1303] loss: 9.064743995666504\n",
      "[step: 1304] loss: 9.05864143371582\n",
      "[step: 1305] loss: 9.052528381347656\n",
      "[step: 1306] loss: 9.046399116516113\n",
      "[step: 1307] loss: 9.040260314941406\n",
      "[step: 1308] loss: 9.034111976623535\n",
      "[step: 1309] loss: 9.027952194213867\n",
      "[step: 1310] loss: 9.021785736083984\n",
      "[step: 1311] loss: 9.01562213897705\n",
      "[step: 1312] loss: 9.009476661682129\n",
      "[step: 1313] loss: 9.003395080566406\n",
      "[step: 1314] loss: 8.997491836547852\n",
      "[step: 1315] loss: 8.992059707641602\n",
      "[step: 1316] loss: 8.987991333007812\n",
      "[step: 1317] loss: 8.987658500671387\n",
      "[step: 1318] loss: 8.999592781066895\n",
      "[step: 1319] loss: 9.045119285583496\n",
      "[step: 1320] loss: 9.221376419067383\n",
      "[step: 1321] loss: 9.660778045654297\n",
      "[step: 1322] loss: 11.278543472290039\n",
      "[step: 1323] loss: 11.330042839050293\n",
      "[step: 1324] loss: 11.712621688842773\n",
      "[step: 1325] loss: 9.039989471435547\n",
      "[step: 1326] loss: 11.128257751464844\n",
      "[step: 1327] loss: 14.073843002319336\n",
      "[step: 1328] loss: 9.677788734436035\n",
      "[step: 1329] loss: 15.421483993530273\n",
      "[step: 1330] loss: 11.991562843322754\n",
      "[step: 1331] loss: 13.5103120803833\n",
      "[step: 1332] loss: 9.680985450744629\n",
      "[step: 1333] loss: 13.299264907836914\n",
      "[step: 1334] loss: 10.277397155761719\n",
      "[step: 1335] loss: 12.95952033996582\n",
      "[step: 1336] loss: 9.638304710388184\n",
      "[step: 1337] loss: 11.834293365478516\n",
      "[step: 1338] loss: 9.840702056884766\n",
      "[step: 1339] loss: 10.190217971801758\n",
      "[step: 1340] loss: 10.999993324279785\n",
      "[step: 1341] loss: 9.302108764648438\n",
      "[step: 1342] loss: 10.4522705078125\n",
      "[step: 1343] loss: 9.809687614440918\n",
      "[step: 1344] loss: 9.473559379577637\n",
      "[step: 1345] loss: 10.248106956481934\n",
      "[step: 1346] loss: 9.433304786682129\n",
      "[step: 1347] loss: 9.561300277709961\n",
      "[step: 1348] loss: 9.854166984558105\n",
      "[step: 1349] loss: 9.229063034057617\n",
      "[step: 1350] loss: 9.696725845336914\n",
      "[step: 1351] loss: 9.534960746765137\n",
      "[step: 1352] loss: 9.218521118164062\n",
      "[step: 1353] loss: 9.628911972045898\n",
      "[step: 1354] loss: 9.262168884277344\n",
      "[step: 1355] loss: 9.315133094787598\n",
      "[step: 1356] loss: 9.47542953491211\n",
      "[step: 1357] loss: 9.19437026977539\n",
      "[step: 1358] loss: 9.300130844116211\n",
      "[step: 1359] loss: 9.33978271484375\n",
      "[step: 1360] loss: 9.144810676574707\n",
      "[step: 1361] loss: 9.304929733276367\n",
      "[step: 1362] loss: 9.232316970825195\n",
      "[step: 1363] loss: 9.137414932250977\n",
      "[step: 1364] loss: 9.264688491821289\n",
      "[step: 1365] loss: 9.1385498046875\n",
      "[step: 1366] loss: 9.158123016357422\n",
      "[step: 1367] loss: 9.19789981842041\n",
      "[step: 1368] loss: 9.10051155090332\n",
      "[step: 1369] loss: 9.152582168579102\n",
      "[step: 1370] loss: 9.129108428955078\n",
      "[step: 1371] loss: 9.085515975952148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1372] loss: 9.131765365600586\n",
      "[step: 1373] loss: 9.076289176940918\n",
      "[step: 1374] loss: 9.08224105834961\n",
      "[step: 1375] loss: 9.090375900268555\n",
      "[step: 1376] loss: 9.045454978942871\n",
      "[step: 1377] loss: 9.073802947998047\n",
      "[step: 1378] loss: 9.044378280639648\n",
      "[step: 1379] loss: 9.034863471984863\n",
      "[step: 1380] loss: 9.045692443847656\n",
      "[step: 1381] loss: 9.011946678161621\n",
      "[step: 1382] loss: 9.025808334350586\n",
      "[step: 1383] loss: 9.008428573608398\n",
      "[step: 1384] loss: 8.997577667236328\n",
      "[step: 1385] loss: 9.002859115600586\n",
      "[step: 1386] loss: 8.979969024658203\n",
      "[step: 1387] loss: 8.986148834228516\n",
      "[step: 1388] loss: 8.972190856933594\n",
      "[step: 1389] loss: 8.965372085571289\n",
      "[step: 1390] loss: 8.964187622070312\n",
      "[step: 1391] loss: 8.949007034301758\n",
      "[step: 1392] loss: 8.951194763183594\n",
      "[step: 1393] loss: 8.938013076782227\n",
      "[step: 1394] loss: 8.934954643249512\n",
      "[step: 1395] loss: 8.928756713867188\n",
      "[step: 1396] loss: 8.919584274291992\n",
      "[step: 1397] loss: 8.91794204711914\n",
      "[step: 1398] loss: 8.906898498535156\n",
      "[step: 1399] loss: 8.905047416687012\n",
      "[step: 1400] loss: 8.896143913269043\n",
      "[step: 1401] loss: 8.891542434692383\n",
      "[step: 1402] loss: 8.885684967041016\n",
      "[step: 1403] loss: 8.878623962402344\n",
      "[step: 1404] loss: 8.87468147277832\n",
      "[step: 1405] loss: 8.86664867401123\n",
      "[step: 1406] loss: 8.863105773925781\n",
      "[step: 1407] loss: 8.855395317077637\n",
      "[step: 1408] loss: 8.851266860961914\n",
      "[step: 1409] loss: 8.844538688659668\n",
      "[step: 1410] loss: 8.839454650878906\n",
      "[step: 1411] loss: 8.833769798278809\n",
      "[step: 1412] loss: 8.827925682067871\n",
      "[step: 1413] loss: 8.822903633117676\n",
      "[step: 1414] loss: 8.816715240478516\n",
      "[step: 1415] loss: 8.811962127685547\n",
      "[step: 1416] loss: 8.805727005004883\n",
      "[step: 1417] loss: 8.801033973693848\n",
      "[step: 1418] loss: 8.79489517211914\n",
      "[step: 1419] loss: 8.79012393951416\n",
      "[step: 1420] loss: 8.784175872802734\n",
      "[step: 1421] loss: 8.779281616210938\n",
      "[step: 1422] loss: 8.773527145385742\n",
      "[step: 1423] loss: 8.768513679504395\n",
      "[step: 1424] loss: 8.762916564941406\n",
      "[step: 1425] loss: 8.757826805114746\n",
      "[step: 1426] loss: 8.752336502075195\n",
      "[step: 1427] loss: 8.74720573425293\n",
      "[step: 1428] loss: 8.741782188415527\n",
      "[step: 1429] loss: 8.736631393432617\n",
      "[step: 1430] loss: 8.731260299682617\n",
      "[step: 1431] loss: 8.726095199584961\n",
      "[step: 1432] loss: 8.720756530761719\n",
      "[step: 1433] loss: 8.715592384338379\n",
      "[step: 1434] loss: 8.710268020629883\n",
      "[step: 1435] loss: 8.705110549926758\n",
      "[step: 1436] loss: 8.69979476928711\n",
      "[step: 1437] loss: 8.694646835327148\n",
      "[step: 1438] loss: 8.689336776733398\n",
      "[step: 1439] loss: 8.684185981750488\n",
      "[step: 1440] loss: 8.678884506225586\n",
      "[step: 1441] loss: 8.673730850219727\n",
      "[step: 1442] loss: 8.668436050415039\n",
      "[step: 1443] loss: 8.663270950317383\n",
      "[step: 1444] loss: 8.65799331665039\n",
      "[step: 1445] loss: 8.65280532836914\n",
      "[step: 1446] loss: 8.647542953491211\n",
      "[step: 1447] loss: 8.642330169677734\n",
      "[step: 1448] loss: 8.637085914611816\n",
      "[step: 1449] loss: 8.631847381591797\n",
      "[step: 1450] loss: 8.626612663269043\n",
      "[step: 1451] loss: 8.621353149414062\n",
      "[step: 1452] loss: 8.61612319946289\n",
      "[step: 1453] loss: 8.610846519470215\n",
      "[step: 1454] loss: 8.605606079101562\n",
      "[step: 1455] loss: 8.600324630737305\n",
      "[step: 1456] loss: 8.59506607055664\n",
      "[step: 1457] loss: 8.589783668518066\n",
      "[step: 1458] loss: 8.584497451782227\n",
      "[step: 1459] loss: 8.579216003417969\n",
      "[step: 1460] loss: 8.573909759521484\n",
      "[step: 1461] loss: 8.568618774414062\n",
      "[step: 1462] loss: 8.5632963180542\n",
      "[step: 1463] loss: 8.557984352111816\n",
      "[step: 1464] loss: 8.552658081054688\n",
      "[step: 1465] loss: 8.547318458557129\n",
      "[step: 1466] loss: 8.541982650756836\n",
      "[step: 1467] loss: 8.536625862121582\n",
      "[step: 1468] loss: 8.531272888183594\n",
      "[step: 1469] loss: 8.525904655456543\n",
      "[step: 1470] loss: 8.520523071289062\n",
      "[step: 1471] loss: 8.515138626098633\n",
      "[step: 1472] loss: 8.509740829467773\n",
      "[step: 1473] loss: 8.504335403442383\n",
      "[step: 1474] loss: 8.498921394348145\n",
      "[step: 1475] loss: 8.493494033813477\n",
      "[step: 1476] loss: 8.488059043884277\n",
      "[step: 1477] loss: 8.482611656188965\n",
      "[step: 1478] loss: 8.477156639099121\n",
      "[step: 1479] loss: 8.471694946289062\n",
      "[step: 1480] loss: 8.466209411621094\n",
      "[step: 1481] loss: 8.460723876953125\n",
      "[step: 1482] loss: 8.455224990844727\n",
      "[step: 1483] loss: 8.449712753295898\n",
      "[step: 1484] loss: 8.444190979003906\n",
      "[step: 1485] loss: 8.438657760620117\n",
      "[step: 1486] loss: 8.433113098144531\n",
      "[step: 1487] loss: 8.427556991577148\n",
      "[step: 1488] loss: 8.421991348266602\n",
      "[step: 1489] loss: 8.416410446166992\n",
      "[step: 1490] loss: 8.410820960998535\n",
      "[step: 1491] loss: 8.405218124389648\n",
      "[step: 1492] loss: 8.399604797363281\n",
      "[step: 1493] loss: 8.393980026245117\n",
      "[step: 1494] loss: 8.388339042663574\n",
      "[step: 1495] loss: 8.38268756866455\n",
      "[step: 1496] loss: 8.37702465057373\n",
      "[step: 1497] loss: 8.371349334716797\n",
      "[step: 1498] loss: 8.36566162109375\n",
      "[step: 1499] loss: 8.359960556030273\n",
      "[step: 1500] loss: 8.354248046875\n",
      "[step: 1501] loss: 8.348522186279297\n",
      "[step: 1502] loss: 8.34278392791748\n",
      "[step: 1503] loss: 8.337035179138184\n",
      "[step: 1504] loss: 8.331270217895508\n",
      "[step: 1505] loss: 8.325493812561035\n",
      "[step: 1506] loss: 8.31970500946045\n",
      "[step: 1507] loss: 8.31390380859375\n",
      "[step: 1508] loss: 8.308091163635254\n",
      "[step: 1509] loss: 8.302263259887695\n",
      "[step: 1510] loss: 8.29642105102539\n",
      "[step: 1511] loss: 8.290567398071289\n",
      "[step: 1512] loss: 8.28470230102539\n",
      "[step: 1513] loss: 8.27882194519043\n",
      "[step: 1514] loss: 8.272928237915039\n",
      "[step: 1515] loss: 8.267023086547852\n",
      "[step: 1516] loss: 8.261106491088867\n",
      "[step: 1517] loss: 8.255172729492188\n",
      "[step: 1518] loss: 8.249227523803711\n",
      "[step: 1519] loss: 8.243270874023438\n",
      "[step: 1520] loss: 8.237298965454102\n",
      "[step: 1521] loss: 8.23131275177002\n",
      "[step: 1522] loss: 8.22531509399414\n",
      "[step: 1523] loss: 8.219304084777832\n",
      "[step: 1524] loss: 8.213282585144043\n",
      "[step: 1525] loss: 8.207244873046875\n",
      "[step: 1526] loss: 8.201190948486328\n",
      "[step: 1527] loss: 8.195127487182617\n",
      "[step: 1528] loss: 8.189050674438477\n",
      "[step: 1529] loss: 8.182957649230957\n",
      "[step: 1530] loss: 8.176856994628906\n",
      "[step: 1531] loss: 8.170741081237793\n",
      "[step: 1532] loss: 8.164609909057617\n",
      "[step: 1533] loss: 8.158464431762695\n",
      "[step: 1534] loss: 8.15230941772461\n",
      "[step: 1535] loss: 8.146139144897461\n",
      "[step: 1536] loss: 8.139957427978516\n",
      "[step: 1537] loss: 8.13376235961914\n",
      "[step: 1538] loss: 8.127552032470703\n",
      "[step: 1539] loss: 8.12132740020752\n",
      "[step: 1540] loss: 8.115093231201172\n",
      "[step: 1541] loss: 8.108845710754395\n",
      "[step: 1542] loss: 8.102581024169922\n",
      "[step: 1543] loss: 8.096307754516602\n",
      "[step: 1544] loss: 8.090020179748535\n",
      "[step: 1545] loss: 8.083724021911621\n",
      "[step: 1546] loss: 8.077414512634277\n",
      "[step: 1547] loss: 8.0711030960083\n",
      "[step: 1548] loss: 8.064813613891602\n",
      "[step: 1549] loss: 8.05860424041748\n",
      "[step: 1550] loss: 8.052679061889648\n",
      "[step: 1551] loss: 8.047670364379883\n",
      "[step: 1552] loss: 8.045793533325195\n",
      "[step: 1553] loss: 8.054429054260254\n",
      "[step: 1554] loss: 8.102578163146973\n",
      "[step: 1555] loss: 8.279925346374512\n",
      "[step: 1556] loss: 8.995704650878906\n",
      "[step: 1557] loss: 10.667231559753418\n",
      "[step: 1558] loss: 15.3004732131958\n",
      "[step: 1559] loss: 9.971250534057617\n",
      "[step: 1560] loss: 9.115246772766113\n",
      "[step: 1561] loss: 12.738460540771484\n",
      "[step: 1562] loss: 8.749238967895508\n",
      "[step: 1563] loss: 9.295948028564453\n",
      "[step: 1564] loss: 10.350317001342773\n",
      "[step: 1565] loss: 8.781847953796387\n",
      "[step: 1566] loss: 10.759459495544434\n",
      "[step: 1567] loss: 8.859292030334473\n",
      "[step: 1568] loss: 9.313243865966797\n",
      "[step: 1569] loss: 8.885783195495605\n",
      "[step: 1570] loss: 8.782437324523926\n",
      "[step: 1571] loss: 8.899886131286621\n",
      "[step: 1572] loss: 8.5895414352417\n",
      "[step: 1573] loss: 8.977298736572266\n",
      "[step: 1574] loss: 8.214376449584961\n",
      "[step: 1575] loss: 8.77812385559082\n",
      "[step: 1576] loss: 8.243701934814453\n",
      "[step: 1577] loss: 8.65955924987793\n",
      "[step: 1578] loss: 8.152522087097168\n",
      "[step: 1579] loss: 8.507938385009766\n",
      "[step: 1580] loss: 8.129154205322266\n",
      "[step: 1581] loss: 8.468111038208008\n",
      "[step: 1582] loss: 8.109743118286133\n",
      "[step: 1583] loss: 8.353589057922363\n",
      "[step: 1584] loss: 8.112159729003906\n",
      "[step: 1585] loss: 8.271038055419922\n",
      "[step: 1586] loss: 8.134315490722656\n",
      "[step: 1587] loss: 8.21475887298584\n",
      "[step: 1588] loss: 8.10643196105957\n",
      "[step: 1589] loss: 8.158422470092773\n",
      "[step: 1590] loss: 8.119314193725586\n",
      "[step: 1591] loss: 8.100025177001953\n",
      "[step: 1592] loss: 8.11707878112793\n",
      "[step: 1593] loss: 8.049524307250977\n",
      "[step: 1594] loss: 8.106216430664062\n",
      "[step: 1595] loss: 8.021571159362793\n",
      "[step: 1596] loss: 8.082062721252441\n",
      "[step: 1597] loss: 8.004472732543945\n",
      "[step: 1598] loss: 8.048853874206543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1599] loss: 7.990222454071045\n",
      "[step: 1600] loss: 8.021889686584473\n",
      "[step: 1601] loss: 7.986512184143066\n",
      "[step: 1602] loss: 7.988083362579346\n",
      "[step: 1603] loss: 7.981041431427002\n",
      "[step: 1604] loss: 7.956091403961182\n",
      "[step: 1605] loss: 7.974833965301514\n",
      "[step: 1606] loss: 7.936363220214844\n",
      "[step: 1607] loss: 7.9564619064331055\n",
      "[step: 1608] loss: 7.928061008453369\n",
      "[step: 1609] loss: 7.929750919342041\n",
      "[step: 1610] loss: 7.9231462478637695\n",
      "[step: 1611] loss: 7.905354976654053\n",
      "[step: 1612] loss: 7.912481784820557\n",
      "[step: 1613] loss: 7.890893459320068\n",
      "[step: 1614] loss: 7.894163131713867\n",
      "[step: 1615] loss: 7.883106231689453\n",
      "[step: 1616] loss: 7.873022079467773\n",
      "[step: 1617] loss: 7.874144077301025\n",
      "[step: 1618] loss: 7.8584184646606445\n",
      "[step: 1619] loss: 7.857738018035889\n",
      "[step: 1620] loss: 7.8501739501953125\n",
      "[step: 1621] loss: 7.839733600616455\n",
      "[step: 1622] loss: 7.839129447937012\n",
      "[step: 1623] loss: 7.828098773956299\n",
      "[step: 1624] loss: 7.822759628295898\n",
      "[step: 1625] loss: 7.819329261779785\n",
      "[step: 1626] loss: 7.808696746826172\n",
      "[step: 1627] loss: 7.805296897888184\n",
      "[step: 1628] loss: 7.799484729766846\n",
      "[step: 1629] loss: 7.790488243103027\n",
      "[step: 1630] loss: 7.787283897399902\n",
      "[step: 1631] loss: 7.780584812164307\n",
      "[step: 1632] loss: 7.772823333740234\n",
      "[step: 1633] loss: 7.768916606903076\n",
      "[step: 1634] loss: 7.762394905090332\n",
      "[step: 1635] loss: 7.755204677581787\n",
      "[step: 1636] loss: 7.750698089599609\n",
      "[step: 1637] loss: 7.744807720184326\n",
      "[step: 1638] loss: 7.737683296203613\n",
      "[step: 1639] loss: 7.732715606689453\n",
      "[step: 1640] loss: 7.727330684661865\n",
      "[step: 1641] loss: 7.720382213592529\n",
      "[step: 1642] loss: 7.7148871421813965\n",
      "[step: 1643] loss: 7.709756851196289\n",
      "[step: 1644] loss: 7.703371524810791\n",
      "[step: 1645] loss: 7.697284698486328\n",
      "[step: 1646] loss: 7.692063808441162\n",
      "[step: 1647] loss: 7.686391830444336\n",
      "[step: 1648] loss: 7.680098056793213\n",
      "[step: 1649] loss: 7.674449920654297\n",
      "[step: 1650] loss: 7.669123649597168\n",
      "[step: 1651] loss: 7.663265228271484\n",
      "[step: 1652] loss: 7.6572370529174805\n",
      "[step: 1653] loss: 7.651608943939209\n",
      "[step: 1654] loss: 7.646197319030762\n",
      "[step: 1655] loss: 7.640415191650391\n",
      "[step: 1656] loss: 7.634487628936768\n",
      "[step: 1657] loss: 7.62880802154541\n",
      "[step: 1658] loss: 7.623297214508057\n",
      "[step: 1659] loss: 7.617671012878418\n",
      "[step: 1660] loss: 7.611838340759277\n",
      "[step: 1661] loss: 7.606067657470703\n",
      "[step: 1662] loss: 7.600442886352539\n",
      "[step: 1663] loss: 7.594881057739258\n",
      "[step: 1664] loss: 7.589215278625488\n",
      "[step: 1665] loss: 7.583454132080078\n",
      "[step: 1666] loss: 7.577713966369629\n",
      "[step: 1667] loss: 7.572046279907227\n",
      "[step: 1668] loss: 7.566445350646973\n",
      "[step: 1669] loss: 7.560815811157227\n",
      "[step: 1670] loss: 7.555141448974609\n",
      "[step: 1671] loss: 7.5494256019592285\n",
      "[step: 1672] loss: 7.543704509735107\n",
      "[step: 1673] loss: 7.538021087646484\n",
      "[step: 1674] loss: 7.53236198425293\n",
      "[step: 1675] loss: 7.526722431182861\n",
      "[step: 1676] loss: 7.521076679229736\n",
      "[step: 1677] loss: 7.515422344207764\n",
      "[step: 1678] loss: 7.5097479820251465\n",
      "[step: 1679] loss: 7.50406551361084\n",
      "[step: 1680] loss: 7.498376846313477\n",
      "[step: 1681] loss: 7.492685794830322\n",
      "[step: 1682] loss: 7.486992835998535\n",
      "[step: 1683] loss: 7.481305122375488\n",
      "[step: 1684] loss: 7.475621700286865\n",
      "[step: 1685] loss: 7.469943046569824\n",
      "[step: 1686] loss: 7.464276313781738\n",
      "[step: 1687] loss: 7.458626747131348\n",
      "[step: 1688] loss: 7.453029155731201\n",
      "[step: 1689] loss: 7.447522163391113\n",
      "[step: 1690] loss: 7.442234992980957\n",
      "[step: 1691] loss: 7.437443733215332\n",
      "[step: 1692] loss: 7.433862209320068\n",
      "[step: 1693] loss: 7.433145523071289\n",
      "[step: 1694] loss: 7.440178871154785\n",
      "[step: 1695] loss: 7.465834617614746\n",
      "[step: 1696] loss: 7.548023223876953\n",
      "[step: 1697] loss: 7.75333309173584\n",
      "[step: 1698] loss: 8.375442504882812\n",
      "[step: 1699] loss: 9.367801666259766\n",
      "[step: 1700] loss: 11.620037078857422\n",
      "[step: 1701] loss: 9.705408096313477\n",
      "[step: 1702] loss: 7.745275497436523\n",
      "[step: 1703] loss: 7.9220290184021\n",
      "[step: 1704] loss: 9.33841323852539\n",
      "[step: 1705] loss: 9.59997272491455\n",
      "[step: 1706] loss: 7.939565658569336\n",
      "[step: 1707] loss: 9.41457748413086\n",
      "[step: 1708] loss: 9.489713668823242\n",
      "[step: 1709] loss: 7.623648643493652\n",
      "[step: 1710] loss: 9.540029525756836\n",
      "[step: 1711] loss: 8.749738693237305\n",
      "[step: 1712] loss: 8.530807495117188\n",
      "[step: 1713] loss: 8.960636138916016\n",
      "[step: 1714] loss: 7.5582380294799805\n",
      "[step: 1715] loss: 8.58978271484375\n",
      "[step: 1716] loss: 7.6004486083984375\n",
      "[step: 1717] loss: 8.2203369140625\n",
      "[step: 1718] loss: 7.875899791717529\n",
      "[step: 1719] loss: 7.795937538146973\n",
      "[step: 1720] loss: 8.118232727050781\n",
      "[step: 1721] loss: 7.4588189125061035\n",
      "[step: 1722] loss: 7.965334415435791\n",
      "[step: 1723] loss: 7.456786632537842\n",
      "[step: 1724] loss: 7.848361015319824\n",
      "[step: 1725] loss: 7.587613582611084\n",
      "[step: 1726] loss: 7.600398063659668\n",
      "[step: 1727] loss: 7.658452033996582\n",
      "[step: 1728] loss: 7.4597601890563965\n",
      "[step: 1729] loss: 7.651476860046387\n",
      "[step: 1730] loss: 7.414144039154053\n",
      "[step: 1731] loss: 7.607436656951904\n",
      "[step: 1732] loss: 7.43336296081543\n",
      "[step: 1733] loss: 7.510835647583008\n",
      "[step: 1734] loss: 7.452723503112793\n",
      "[step: 1735] loss: 7.413105010986328\n",
      "[step: 1736] loss: 7.4795732498168945\n",
      "[step: 1737] loss: 7.3608551025390625\n",
      "[step: 1738] loss: 7.460859775543213\n",
      "[step: 1739] loss: 7.357241630554199\n",
      "[step: 1740] loss: 7.395717144012451\n",
      "[step: 1741] loss: 7.383418560028076\n",
      "[step: 1742] loss: 7.331435203552246\n",
      "[step: 1743] loss: 7.382226467132568\n",
      "[step: 1744] loss: 7.309117317199707\n",
      "[step: 1745] loss: 7.347926616668701\n",
      "[step: 1746] loss: 7.324067115783691\n",
      "[step: 1747] loss: 7.296509742736816\n",
      "[step: 1748] loss: 7.326124668121338\n",
      "[step: 1749] loss: 7.277665615081787\n",
      "[step: 1750] loss: 7.293696403503418\n",
      "[step: 1751] loss: 7.2855658531188965\n",
      "[step: 1752] loss: 7.25778865814209\n",
      "[step: 1753] loss: 7.275934219360352\n",
      "[step: 1754] loss: 7.25095272064209\n",
      "[step: 1755] loss: 7.245693206787109\n",
      "[step: 1756] loss: 7.251357078552246\n",
      "[step: 1757] loss: 7.228363990783691\n",
      "[step: 1758] loss: 7.230416774749756\n",
      "[step: 1759] loss: 7.227248668670654\n",
      "[step: 1760] loss: 7.2097368240356445\n",
      "[step: 1761] loss: 7.2121381759643555\n",
      "[step: 1762] loss: 7.206397533416748\n",
      "[step: 1763] loss: 7.191961288452148\n",
      "[step: 1764] loss: 7.193540573120117\n",
      "[step: 1765] loss: 7.18739128112793\n",
      "[step: 1766] loss: 7.174506187438965\n",
      "[step: 1767] loss: 7.17485237121582\n",
      "[step: 1768] loss: 7.169050216674805\n",
      "[step: 1769] loss: 7.157925605773926\n",
      "[step: 1770] loss: 7.156156539916992\n",
      "[step: 1771] loss: 7.151703834533691\n",
      "[step: 1772] loss: 7.1419358253479\n",
      "[step: 1773] loss: 7.137421131134033\n",
      "[step: 1774] loss: 7.134543418884277\n",
      "[step: 1775] loss: 7.126186370849609\n",
      "[step: 1776] loss: 7.119565963745117\n",
      "[step: 1777] loss: 7.116719722747803\n",
      "[step: 1778] loss: 7.110533714294434\n",
      "[step: 1779] loss: 7.103145122528076\n",
      "[step: 1780] loss: 7.098555564880371\n",
      "[step: 1781] loss: 7.094351768493652\n",
      "[step: 1782] loss: 7.087759971618652\n",
      "[step: 1783] loss: 7.081358909606934\n",
      "[step: 1784] loss: 7.077071189880371\n",
      "[step: 1785] loss: 7.072124481201172\n",
      "[step: 1786] loss: 7.065787315368652\n",
      "[step: 1787] loss: 7.059889793395996\n",
      "[step: 1788] loss: 7.055231094360352\n",
      "[step: 1789] loss: 7.050251007080078\n",
      "[step: 1790] loss: 7.044234275817871\n",
      "[step: 1791] loss: 7.038463115692139\n",
      "[step: 1792] loss: 7.033433437347412\n",
      "[step: 1793] loss: 7.028531074523926\n",
      "[step: 1794] loss: 7.022946834564209\n",
      "[step: 1795] loss: 7.0171685218811035\n",
      "[step: 1796] loss: 7.011816501617432\n",
      "[step: 1797] loss: 7.006793022155762\n",
      "[step: 1798] loss: 7.0016255378723145\n",
      "[step: 1799] loss: 6.996063709259033\n",
      "[step: 1800] loss: 6.990499496459961\n",
      "[step: 1801] loss: 6.985162258148193\n",
      "[step: 1802] loss: 6.980031967163086\n",
      "[step: 1803] loss: 6.974830627441406\n",
      "[step: 1804] loss: 6.969438552856445\n",
      "[step: 1805] loss: 6.963957786560059\n",
      "[step: 1806] loss: 6.958532333374023\n",
      "[step: 1807] loss: 6.95326042175293\n",
      "[step: 1808] loss: 6.948036193847656\n",
      "[step: 1809] loss: 6.94279670715332\n",
      "[step: 1810] loss: 6.937456130981445\n",
      "[step: 1811] loss: 6.932058334350586\n",
      "[step: 1812] loss: 6.926649570465088\n",
      "[step: 1813] loss: 6.92127799987793\n",
      "[step: 1814] loss: 6.915961742401123\n",
      "[step: 1815] loss: 6.910669326782227\n",
      "[step: 1816] loss: 6.905385971069336\n",
      "[step: 1817] loss: 6.900090217590332\n",
      "[step: 1818] loss: 6.894771575927734\n",
      "[step: 1819] loss: 6.8894243240356445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 1820] loss: 6.88407039642334\n",
      "[step: 1821] loss: 6.878703594207764\n",
      "[step: 1822] loss: 6.873335361480713\n",
      "[step: 1823] loss: 6.867968559265137\n",
      "[step: 1824] loss: 6.862600803375244\n",
      "[step: 1825] loss: 6.857244491577148\n",
      "[step: 1826] loss: 6.851885795593262\n",
      "[step: 1827] loss: 6.846540451049805\n",
      "[step: 1828] loss: 6.841213703155518\n",
      "[step: 1829] loss: 6.8359174728393555\n",
      "[step: 1830] loss: 6.830690860748291\n",
      "[step: 1831] loss: 6.8256049156188965\n",
      "[step: 1832] loss: 6.82080078125\n",
      "[step: 1833] loss: 6.81663703918457\n",
      "[step: 1834] loss: 6.813837051391602\n",
      "[step: 1835] loss: 6.81428861618042\n",
      "[step: 1836] loss: 6.821911334991455\n",
      "[step: 1837] loss: 6.848149299621582\n",
      "[step: 1838] loss: 6.914730072021484\n",
      "[step: 1839] loss: 7.096224784851074\n",
      "[step: 1840] loss: 7.477756977081299\n",
      "[step: 1841] loss: 8.444906234741211\n",
      "[step: 1842] loss: 9.376909255981445\n",
      "[step: 1843] loss: 10.475869178771973\n",
      "[step: 1844] loss: 8.038141250610352\n",
      "[step: 1845] loss: 6.866901397705078\n",
      "[step: 1846] loss: 8.185665130615234\n",
      "[step: 1847] loss: 8.369573593139648\n",
      "[step: 1848] loss: 7.421543121337891\n",
      "[step: 1849] loss: 7.150297164916992\n",
      "[step: 1850] loss: 7.995833873748779\n",
      "[step: 1851] loss: 7.64394474029541\n",
      "[step: 1852] loss: 6.84252405166626\n",
      "[step: 1853] loss: 7.636329650878906\n",
      "[step: 1854] loss: 7.352397918701172\n",
      "[step: 1855] loss: 6.873530387878418\n",
      "[step: 1856] loss: 7.580552101135254\n",
      "[step: 1857] loss: 7.318431377410889\n",
      "[step: 1858] loss: 6.930844306945801\n",
      "[step: 1859] loss: 7.494104385375977\n",
      "[step: 1860] loss: 7.117287635803223\n",
      "[step: 1861] loss: 6.8656229972839355\n",
      "[step: 1862] loss: 7.3348188400268555\n",
      "[step: 1863] loss: 6.889180660247803\n",
      "[step: 1864] loss: 6.935812950134277\n",
      "[step: 1865] loss: 7.178421497344971\n",
      "[step: 1866] loss: 6.787138938903809\n",
      "[step: 1867] loss: 6.9355974197387695\n",
      "[step: 1868] loss: 6.949346542358398\n",
      "[step: 1869] loss: 6.740736961364746\n",
      "[step: 1870] loss: 6.898423194885254\n",
      "[step: 1871] loss: 6.837654113769531\n",
      "[step: 1872] loss: 6.7311320304870605\n",
      "[step: 1873] loss: 6.847265720367432\n",
      "[step: 1874] loss: 6.764536380767822\n",
      "[step: 1875] loss: 6.7095489501953125\n",
      "[step: 1876] loss: 6.796660423278809\n",
      "[step: 1877] loss: 6.708657741546631\n",
      "[step: 1878] loss: 6.7017717361450195\n",
      "[step: 1879] loss: 6.753937721252441\n",
      "[step: 1880] loss: 6.680281639099121\n",
      "[step: 1881] loss: 6.68097448348999\n",
      "[step: 1882] loss: 6.712261199951172\n",
      "[step: 1883] loss: 6.662025451660156\n",
      "[step: 1884] loss: 6.652048110961914\n",
      "[step: 1885] loss: 6.6808342933654785\n",
      "[step: 1886] loss: 6.6424665451049805\n",
      "[step: 1887] loss: 6.6297712326049805\n",
      "[step: 1888] loss: 6.6521501541137695\n",
      "[step: 1889] loss: 6.627568244934082\n",
      "[step: 1890] loss: 6.608319282531738\n",
      "[step: 1891] loss: 6.620460510253906\n",
      "[step: 1892] loss: 6.614901542663574\n",
      "[step: 1893] loss: 6.590175628662109\n",
      "[step: 1894] loss: 6.591317176818848\n",
      "[step: 1895] loss: 6.594937801361084\n",
      "[step: 1896] loss: 6.579219818115234\n",
      "[step: 1897] loss: 6.567497253417969\n",
      "[step: 1898] loss: 6.570285320281982\n",
      "[step: 1899] loss: 6.567393779754639\n",
      "[step: 1900] loss: 6.552392482757568\n",
      "[step: 1901] loss: 6.546311855316162\n",
      "[step: 1902] loss: 6.546942710876465\n",
      "[step: 1903] loss: 6.541821479797363\n",
      "[step: 1904] loss: 6.530066967010498\n",
      "[step: 1905] loss: 6.523927688598633\n",
      "[step: 1906] loss: 6.522883415222168\n",
      "[step: 1907] loss: 6.518434047698975\n",
      "[step: 1908] loss: 6.5092573165893555\n",
      "[step: 1909] loss: 6.501642227172852\n",
      "[step: 1910] loss: 6.498781204223633\n",
      "[step: 1911] loss: 6.495349884033203\n",
      "[step: 1912] loss: 6.4889726638793945\n",
      "[step: 1913] loss: 6.480871200561523\n",
      "[step: 1914] loss: 6.475381374359131\n",
      "[step: 1915] loss: 6.4716386795043945\n",
      "[step: 1916] loss: 6.46754264831543\n",
      "[step: 1917] loss: 6.461305618286133\n",
      "[step: 1918] loss: 6.4544677734375\n",
      "[step: 1919] loss: 6.448667049407959\n",
      "[step: 1920] loss: 6.444201946258545\n",
      "[step: 1921] loss: 6.4399003982543945\n",
      "[step: 1922] loss: 6.4346842765808105\n",
      "[step: 1923] loss: 6.428713798522949\n",
      "[step: 1924] loss: 6.4225969314575195\n",
      "[step: 1925] loss: 6.4171247482299805\n",
      "[step: 1926] loss: 6.412169456481934\n",
      "[step: 1927] loss: 6.407519817352295\n",
      "[step: 1928] loss: 6.4025492668151855\n",
      "[step: 1929] loss: 6.397276401519775\n",
      "[step: 1930] loss: 6.391627788543701\n",
      "[step: 1931] loss: 6.38600492477417\n",
      "[step: 1932] loss: 6.380455017089844\n",
      "[step: 1933] loss: 6.375176906585693\n",
      "[step: 1934] loss: 6.37003231048584\n",
      "[step: 1935] loss: 6.365025520324707\n",
      "[step: 1936] loss: 6.360001564025879\n",
      "[step: 1937] loss: 6.354972839355469\n",
      "[step: 1938] loss: 6.349869251251221\n",
      "[step: 1939] loss: 6.344738006591797\n",
      "[step: 1940] loss: 6.3395843505859375\n",
      "[step: 1941] loss: 6.334450721740723\n",
      "[step: 1942] loss: 6.3293681144714355\n",
      "[step: 1943] loss: 6.3243818283081055\n",
      "[step: 1944] loss: 6.319578647613525\n",
      "[step: 1945] loss: 6.3150739669799805\n",
      "[step: 1946] loss: 6.311114311218262\n",
      "[step: 1947] loss: 6.308083534240723\n",
      "[step: 1948] loss: 6.30694580078125\n",
      "[step: 1949] loss: 6.309185028076172\n",
      "[step: 1950] loss: 6.318664073944092\n",
      "[step: 1951] loss: 6.341231346130371\n",
      "[step: 1952] loss: 6.394458770751953\n",
      "[step: 1953] loss: 6.499927997589111\n",
      "[step: 1954] loss: 6.7332916259765625\n",
      "[step: 1955] loss: 7.115608215332031\n",
      "[step: 1956] loss: 7.843021869659424\n",
      "[step: 1957] loss: 8.286727905273438\n",
      "[step: 1958] loss: 8.415513038635254\n",
      "[step: 1959] loss: 7.106344223022461\n",
      "[step: 1960] loss: 6.297513484954834\n",
      "[step: 1961] loss: 6.688912868499756\n",
      "[step: 1962] loss: 7.289181232452393\n",
      "[step: 1963] loss: 7.1440629959106445\n",
      "[step: 1964] loss: 6.322858810424805\n",
      "[step: 1965] loss: 6.592339515686035\n",
      "[step: 1966] loss: 7.185288906097412\n",
      "[step: 1967] loss: 6.722520351409912\n",
      "[step: 1968] loss: 6.284023284912109\n",
      "[step: 1969] loss: 6.72890567779541\n",
      "[step: 1970] loss: 6.762022018432617\n",
      "[step: 1971] loss: 6.358148097991943\n",
      "[step: 1972] loss: 6.365792274475098\n",
      "[step: 1973] loss: 6.619840145111084\n",
      "[step: 1974] loss: 6.446262836456299\n",
      "[step: 1975] loss: 6.236863613128662\n",
      "[step: 1976] loss: 6.414215087890625\n",
      "[step: 1977] loss: 6.433131217956543\n",
      "[step: 1978] loss: 6.238736629486084\n",
      "[step: 1979] loss: 6.266940593719482\n",
      "[step: 1980] loss: 6.377590179443359\n",
      "[step: 1981] loss: 6.240439414978027\n",
      "[step: 1982] loss: 6.204598426818848\n",
      "[step: 1983] loss: 6.285783290863037\n",
      "[step: 1984] loss: 6.272226810455322\n",
      "[step: 1985] loss: 6.1684041023254395\n",
      "[step: 1986] loss: 6.207612037658691\n",
      "[step: 1987] loss: 6.244366645812988\n",
      "[step: 1988] loss: 6.197502136230469\n",
      "[step: 1989] loss: 6.143252372741699\n",
      "[step: 1990] loss: 6.191657066345215\n",
      "[step: 1991] loss: 6.201107025146484\n",
      "[step: 1992] loss: 6.1531219482421875\n",
      "[step: 1993] loss: 6.123373985290527\n",
      "[step: 1994] loss: 6.159017562866211\n",
      "[step: 1995] loss: 6.1587018966674805\n",
      "[step: 1996] loss: 6.124210357666016\n",
      "[step: 1997] loss: 6.100382328033447\n",
      "[step: 1998] loss: 6.121734142303467\n",
      "[step: 1999] loss: 6.126935005187988\n",
      "[step: 2000] loss: 6.104347229003906\n",
      "[step: 2001] loss: 6.0776190757751465\n",
      "[step: 2002] loss: 6.08317232131958\n",
      "[step: 2003] loss: 6.092610836029053\n",
      "[step: 2004] loss: 6.087656021118164\n",
      "[step: 2005] loss: 6.063564300537109\n",
      "[step: 2006] loss: 6.051739692687988\n",
      "[step: 2007] loss: 6.053450107574463\n",
      "[step: 2008] loss: 6.059926986694336\n",
      "[step: 2009] loss: 6.052596092224121\n",
      "[step: 2010] loss: 6.037857532501221\n",
      "[step: 2011] loss: 6.024144172668457\n",
      "[step: 2012] loss: 6.020740509033203\n",
      "[step: 2013] loss: 6.020942687988281\n",
      "[step: 2014] loss: 6.020700454711914\n",
      "[step: 2015] loss: 6.01357364654541\n",
      "[step: 2016] loss: 6.004125595092773\n",
      "[step: 2017] loss: 5.993467807769775\n",
      "[step: 2018] loss: 5.987359046936035\n",
      "[step: 2019] loss: 5.983309745788574\n",
      "[step: 2020] loss: 5.981861114501953\n",
      "[step: 2021] loss: 5.978547096252441\n",
      "[step: 2022] loss: 5.974285125732422\n",
      "[step: 2023] loss: 5.967478275299072\n",
      "[step: 2024] loss: 5.960521697998047\n",
      "[step: 2025] loss: 5.952720642089844\n",
      "[step: 2026] loss: 5.946281909942627\n",
      "[step: 2027] loss: 5.940089225769043\n",
      "[step: 2028] loss: 5.935170650482178\n",
      "[step: 2029] loss: 5.930360794067383\n",
      "[step: 2030] loss: 5.926318168640137\n",
      "[step: 2031] loss: 5.922270774841309\n",
      "[step: 2032] loss: 5.918716907501221\n",
      "[step: 2033] loss: 5.915389060974121\n",
      "[step: 2034] loss: 5.912996768951416\n",
      "[step: 2035] loss: 5.911978721618652\n",
      "[step: 2036] loss: 5.913449287414551\n",
      "[step: 2037] loss: 5.919166564941406\n",
      "[step: 2038] loss: 5.933596134185791\n",
      "[step: 2039] loss: 5.9628424644470215\n",
      "[step: 2040] loss: 6.021352767944336\n",
      "[step: 2041] loss: 6.127347946166992\n",
      "[step: 2042] loss: 6.332755088806152\n",
      "[step: 2043] loss: 6.650696754455566\n",
      "[step: 2044] loss: 7.142526149749756\n",
      "[step: 2045] loss: 7.489881992340088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2046] loss: 7.531370639801025\n",
      "[step: 2047] loss: 6.74040412902832\n",
      "[step: 2048] loss: 5.992555141448975\n",
      "[step: 2049] loss: 5.934745788574219\n",
      "[step: 2050] loss: 6.417402267456055\n",
      "[step: 2051] loss: 6.697197914123535\n",
      "[step: 2052] loss: 6.233452796936035\n",
      "[step: 2053] loss: 5.8565826416015625\n",
      "[step: 2054] loss: 6.049317359924316\n",
      "[step: 2055] loss: 6.300868511199951\n",
      "[step: 2056] loss: 6.103421211242676\n",
      "[step: 2057] loss: 5.834135055541992\n",
      "[step: 2058] loss: 5.984065055847168\n",
      "[step: 2059] loss: 6.1677775382995605\n",
      "[step: 2060] loss: 5.990408420562744\n",
      "[step: 2061] loss: 5.813753604888916\n",
      "[step: 2062] loss: 5.935678482055664\n",
      "[step: 2063] loss: 6.044799327850342\n",
      "[step: 2064] loss: 5.913268089294434\n",
      "[step: 2065] loss: 5.795724868774414\n",
      "[step: 2066] loss: 5.883060932159424\n",
      "[step: 2067] loss: 5.966337203979492\n",
      "[step: 2068] loss: 5.863677978515625\n",
      "[step: 2069] loss: 5.776018142700195\n",
      "[step: 2070] loss: 5.822834491729736\n",
      "[step: 2071] loss: 5.888762474060059\n",
      "[step: 2072] loss: 5.826279640197754\n",
      "[step: 2073] loss: 5.756460189819336\n",
      "[step: 2074] loss: 5.767044544219971\n",
      "[step: 2075] loss: 5.822007656097412\n",
      "[step: 2076] loss: 5.797067642211914\n",
      "[step: 2077] loss: 5.744938850402832\n",
      "[step: 2078] loss: 5.722747802734375\n",
      "[step: 2079] loss: 5.753340721130371\n",
      "[step: 2080] loss: 5.763164520263672\n",
      "[step: 2081] loss: 5.740121841430664\n",
      "[step: 2082] loss: 5.705179214477539\n",
      "[step: 2083] loss: 5.697443008422852\n",
      "[step: 2084] loss: 5.709784507751465\n",
      "[step: 2085] loss: 5.718829154968262\n",
      "[step: 2086] loss: 5.709566116333008\n",
      "[step: 2087] loss: 5.685727596282959\n",
      "[step: 2088] loss: 5.6695427894592285\n",
      "[step: 2089] loss: 5.665187835693359\n",
      "[step: 2090] loss: 5.672137260437012\n",
      "[step: 2091] loss: 5.674254894256592\n",
      "[step: 2092] loss: 5.670386791229248\n",
      "[step: 2093] loss: 5.658145904541016\n",
      "[step: 2094] loss: 5.644548416137695\n",
      "[step: 2095] loss: 5.632926940917969\n",
      "[step: 2096] loss: 5.626453876495361\n",
      "[step: 2097] loss: 5.624606609344482\n",
      "[step: 2098] loss: 5.623983383178711\n",
      "[step: 2099] loss: 5.623801231384277\n",
      "[step: 2100] loss: 5.621340751647949\n",
      "[step: 2101] loss: 5.617847442626953\n",
      "[step: 2102] loss: 5.611799240112305\n",
      "[step: 2103] loss: 5.605388641357422\n",
      "[step: 2104] loss: 5.598505973815918\n",
      "[step: 2105] loss: 5.591679573059082\n",
      "[step: 2106] loss: 5.585333824157715\n",
      "[step: 2107] loss: 5.579381942749023\n",
      "[step: 2108] loss: 5.574552536010742\n",
      "[step: 2109] loss: 5.5702128410339355\n",
      "[step: 2110] loss: 5.567488670349121\n",
      "[step: 2111] loss: 5.566685676574707\n",
      "[step: 2112] loss: 5.569450378417969\n",
      "[step: 2113] loss: 5.578510761260986\n",
      "[step: 2114] loss: 5.59975004196167\n",
      "[step: 2115] loss: 5.644270420074463\n",
      "[step: 2116] loss: 5.7318434715271\n",
      "[step: 2117] loss: 5.900221824645996\n",
      "[step: 2118] loss: 6.2090606689453125\n",
      "[step: 2119] loss: 6.694883346557617\n",
      "[step: 2120] loss: 7.2648186683654785\n",
      "[step: 2121] loss: 7.516608238220215\n",
      "[step: 2122] loss: 6.922189712524414\n",
      "[step: 2123] loss: 5.907195091247559\n",
      "[step: 2124] loss: 5.5343122482299805\n",
      "[step: 2125] loss: 6.035982131958008\n",
      "[step: 2126] loss: 6.5066728591918945\n",
      "[step: 2127] loss: 6.082552909851074\n",
      "[step: 2128] loss: 5.545112609863281\n",
      "[step: 2129] loss: 5.730686187744141\n",
      "[step: 2130] loss: 6.057314872741699\n",
      "[step: 2131] loss: 5.775110721588135\n",
      "[step: 2132] loss: 5.5050835609436035\n",
      "[step: 2133] loss: 5.739019393920898\n",
      "[step: 2134] loss: 5.877045631408691\n",
      "[step: 2135] loss: 5.582325458526611\n",
      "[step: 2136] loss: 5.520849227905273\n",
      "[step: 2137] loss: 5.719625949859619\n",
      "[step: 2138] loss: 5.714231967926025\n",
      "[step: 2139] loss: 5.483333587646484\n",
      "[step: 2140] loss: 5.536852836608887\n",
      "[step: 2141] loss: 5.667383193969727\n",
      "[step: 2142] loss: 5.578370094299316\n",
      "[step: 2143] loss: 5.447463035583496\n",
      "[step: 2144] loss: 5.522665023803711\n",
      "[step: 2145] loss: 5.595457553863525\n",
      "[step: 2146] loss: 5.489529609680176\n",
      "[step: 2147] loss: 5.429898262023926\n",
      "[step: 2148] loss: 5.484562873840332\n",
      "[step: 2149] loss: 5.528748989105225\n",
      "[step: 2150] loss: 5.443289756774902\n",
      "[step: 2151] loss: 5.4046406745910645\n",
      "[step: 2152] loss: 5.442903518676758\n",
      "[step: 2153] loss: 5.467117786407471\n",
      "[step: 2154] loss: 5.4214348793029785\n",
      "[step: 2155] loss: 5.3776655197143555\n",
      "[step: 2156] loss: 5.393235206604004\n",
      "[step: 2157] loss: 5.413993835449219\n",
      "[step: 2158] loss: 5.40510892868042\n",
      "[step: 2159] loss: 5.368971824645996\n",
      "[step: 2160] loss: 5.351666450500488\n",
      "[step: 2161] loss: 5.36147403717041\n",
      "[step: 2162] loss: 5.373040199279785\n",
      "[step: 2163] loss: 5.367942810058594\n",
      "[step: 2164] loss: 5.343443393707275\n",
      "[step: 2165] loss: 5.3269548416137695\n",
      "[step: 2166] loss: 5.3234357833862305\n",
      "[step: 2167] loss: 5.330018997192383\n",
      "[step: 2168] loss: 5.332488536834717\n",
      "[step: 2169] loss: 5.324173927307129\n",
      "[step: 2170] loss: 5.310862064361572\n",
      "[step: 2171] loss: 5.296839714050293\n",
      "[step: 2172] loss: 5.290534973144531\n",
      "[step: 2173] loss: 5.289478778839111\n",
      "[step: 2174] loss: 5.290193557739258\n",
      "[step: 2175] loss: 5.2893524169921875\n",
      "[step: 2176] loss: 5.283974647521973\n",
      "[step: 2177] loss: 5.276270866394043\n",
      "[step: 2178] loss: 5.2666802406311035\n",
      "[step: 2179] loss: 5.25800085067749\n",
      "[step: 2180] loss: 5.251281261444092\n",
      "[step: 2181] loss: 5.245870113372803\n",
      "[step: 2182] loss: 5.242444038391113\n",
      "[step: 2183] loss: 5.23944091796875\n",
      "[step: 2184] loss: 5.236990928649902\n",
      "[step: 2185] loss: 5.234818458557129\n",
      "[step: 2186] loss: 5.232606887817383\n",
      "[step: 2187] loss: 5.231315612792969\n",
      "[step: 2188] loss: 5.230404853820801\n",
      "[step: 2189] loss: 5.231521129608154\n",
      "[step: 2190] loss: 5.235410213470459\n",
      "[step: 2191] loss: 5.244248867034912\n",
      "[step: 2192] loss: 5.260807991027832\n",
      "[step: 2193] loss: 5.293262481689453\n",
      "[step: 2194] loss: 5.34596061706543\n",
      "[step: 2195] loss: 5.4413909912109375\n",
      "[step: 2196] loss: 5.583199977874756\n",
      "[step: 2197] loss: 5.816897392272949\n",
      "[step: 2198] loss: 6.057832717895508\n",
      "[step: 2199] loss: 6.322779655456543\n",
      "[step: 2200] loss: 6.271387100219727\n",
      "[step: 2201] loss: 5.952519416809082\n",
      "[step: 2202] loss: 5.438725471496582\n",
      "[step: 2203] loss: 5.164406776428223\n",
      "[step: 2204] loss: 5.269073486328125\n",
      "[step: 2205] loss: 5.5372772216796875\n",
      "[step: 2206] loss: 5.639150142669678\n",
      "[step: 2207] loss: 5.4134979248046875\n",
      "[step: 2208] loss: 5.172330856323242\n",
      "[step: 2209] loss: 5.186342239379883\n",
      "[step: 2210] loss: 5.3628034591674805\n",
      "[step: 2211] loss: 5.418296813964844\n",
      "[step: 2212] loss: 5.26640510559082\n",
      "[step: 2213] loss: 5.1311774253845215\n",
      "[step: 2214] loss: 5.157937049865723\n",
      "[step: 2215] loss: 5.260490417480469\n",
      "[step: 2216] loss: 5.279294013977051\n",
      "[step: 2217] loss: 5.18046760559082\n",
      "[step: 2218] loss: 5.101409435272217\n",
      "[step: 2219] loss: 5.117954730987549\n",
      "[step: 2220] loss: 5.176201820373535\n",
      "[step: 2221] loss: 5.187154769897461\n",
      "[step: 2222] loss: 5.132229804992676\n",
      "[step: 2223] loss: 5.079201698303223\n",
      "[step: 2224] loss: 5.07373046875\n",
      "[step: 2225] loss: 5.103747844696045\n",
      "[step: 2226] loss: 5.124935150146484\n",
      "[step: 2227] loss: 5.107082366943359\n",
      "[step: 2228] loss: 5.070507049560547\n",
      "[step: 2229] loss: 5.044480323791504\n",
      "[step: 2230] loss: 5.042913436889648\n",
      "[step: 2231] loss: 5.057310104370117\n",
      "[step: 2232] loss: 5.069120407104492\n",
      "[step: 2233] loss: 5.067356586456299\n",
      "[step: 2234] loss: 5.050682067871094\n",
      "[step: 2235] loss: 5.029962062835693\n",
      "[step: 2236] loss: 5.012845039367676\n",
      "[step: 2237] loss: 5.004229545593262\n",
      "[step: 2238] loss: 5.003364562988281\n",
      "[step: 2239] loss: 5.0070037841796875\n",
      "[step: 2240] loss: 5.011837959289551\n",
      "[step: 2241] loss: 5.0146684646606445\n",
      "[step: 2242] loss: 5.015902996063232\n",
      "[step: 2243] loss: 5.014155864715576\n",
      "[step: 2244] loss: 5.011817455291748\n",
      "[step: 2245] loss: 5.0076189041137695\n",
      "[step: 2246] loss: 5.004439353942871\n",
      "[step: 2247] loss: 5.000829696655273\n",
      "[step: 2248] loss: 5.00030517578125\n",
      "[step: 2249] loss: 5.001161575317383\n",
      "[step: 2250] loss: 5.007753372192383\n",
      "[step: 2251] loss: 5.017829895019531\n",
      "[step: 2252] loss: 5.039026260375977\n",
      "[step: 2253] loss: 5.067748069763184\n",
      "[step: 2254] loss: 5.122214317321777\n",
      "[step: 2255] loss: 5.190354347229004\n",
      "[step: 2256] loss: 5.310243129730225\n",
      "[step: 2257] loss: 5.4233174324035645\n",
      "[step: 2258] loss: 5.58868408203125\n",
      "[step: 2259] loss: 5.625101566314697\n",
      "[step: 2260] loss: 5.618126392364502\n",
      "[step: 2261] loss: 5.38701057434082\n",
      "[step: 2262] loss: 5.132020950317383\n",
      "[step: 2263] loss: 4.9339494705200195\n",
      "[step: 2264] loss: 4.902533531188965\n",
      "[step: 2265] loss: 5.005654811859131\n",
      "[step: 2266] loss: 5.120580196380615\n",
      "[step: 2267] loss: 5.153219223022461\n",
      "[step: 2268] loss: 5.044353485107422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2269] loss: 4.918698310852051\n",
      "[step: 2270] loss: 4.869498252868652\n",
      "[step: 2271] loss: 4.91412353515625\n",
      "[step: 2272] loss: 4.989826679229736\n",
      "[step: 2273] loss: 5.013955116271973\n",
      "[step: 2274] loss: 4.976161479949951\n",
      "[step: 2275] loss: 4.8965606689453125\n",
      "[step: 2276] loss: 4.846367359161377\n",
      "[step: 2277] loss: 4.847694396972656\n",
      "[step: 2278] loss: 4.881802082061768\n",
      "[step: 2279] loss: 4.917037010192871\n",
      "[step: 2280] loss: 4.919797897338867\n",
      "[step: 2281] loss: 4.894931793212891\n",
      "[step: 2282] loss: 4.852634429931641\n",
      "[step: 2283] loss: 4.820029258728027\n",
      "[step: 2284] loss: 4.80570650100708\n",
      "[step: 2285] loss: 4.8091607093811035\n",
      "[step: 2286] loss: 4.823053359985352\n",
      "[step: 2287] loss: 4.836416244506836\n",
      "[step: 2288] loss: 4.845432281494141\n",
      "[step: 2289] loss: 4.84456729888916\n",
      "[step: 2290] loss: 4.839774131774902\n",
      "[step: 2291] loss: 4.828032493591309\n",
      "[step: 2292] loss: 4.816802978515625\n",
      "[step: 2293] loss: 4.80328369140625\n",
      "[step: 2294] loss: 4.792193412780762\n",
      "[step: 2295] loss: 4.782161712646484\n",
      "[step: 2296] loss: 4.77483606338501\n",
      "[step: 2297] loss: 4.76884651184082\n",
      "[step: 2298] loss: 4.765772342681885\n",
      "[step: 2299] loss: 4.764506816864014\n",
      "[step: 2300] loss: 4.767621040344238\n",
      "[step: 2301] loss: 4.775424957275391\n",
      "[step: 2302] loss: 4.794837951660156\n",
      "[step: 2303] loss: 4.827795028686523\n",
      "[step: 2304] loss: 4.896888732910156\n",
      "[step: 2305] loss: 4.999406337738037\n",
      "[step: 2306] loss: 5.205249786376953\n",
      "[step: 2307] loss: 5.43582820892334\n",
      "[step: 2308] loss: 5.839900016784668\n",
      "[step: 2309] loss: 5.932062149047852\n",
      "[step: 2310] loss: 5.918577194213867\n",
      "[step: 2311] loss: 5.321566104888916\n",
      "[step: 2312] loss: 4.820272445678711\n",
      "[step: 2313] loss: 4.720165252685547\n",
      "[step: 2314] loss: 4.991775989532471\n",
      "[step: 2315] loss: 5.26000452041626\n",
      "[step: 2316] loss: 5.105955123901367\n",
      "[step: 2317] loss: 4.788687705993652\n",
      "[step: 2318] loss: 4.701495170593262\n",
      "[step: 2319] loss: 4.906275749206543\n",
      "[step: 2320] loss: 5.051418304443359\n",
      "[step: 2321] loss: 4.881784439086914\n",
      "[step: 2322] loss: 4.693859577178955\n",
      "[step: 2323] loss: 4.710446357727051\n",
      "[step: 2324] loss: 4.852828025817871\n",
      "[step: 2325] loss: 4.891757965087891\n",
      "[step: 2326] loss: 4.749646186828613\n",
      "[step: 2327] loss: 4.6513590812683105\n",
      "[step: 2328] loss: 4.691473960876465\n",
      "[step: 2329] loss: 4.771629333496094\n",
      "[step: 2330] loss: 4.770031929016113\n",
      "[step: 2331] loss: 4.67948579788208\n",
      "[step: 2332] loss: 4.624560832977295\n",
      "[step: 2333] loss: 4.645951271057129\n",
      "[step: 2334] loss: 4.693777084350586\n",
      "[step: 2335] loss: 4.699453830718994\n",
      "[step: 2336] loss: 4.648627758026123\n",
      "[step: 2337] loss: 4.605885028839111\n",
      "[step: 2338] loss: 4.599858283996582\n",
      "[step: 2339] loss: 4.624526023864746\n",
      "[step: 2340] loss: 4.646784782409668\n",
      "[step: 2341] loss: 4.6363935470581055\n",
      "[step: 2342] loss: 4.6094183921813965\n",
      "[step: 2343] loss: 4.580020904541016\n",
      "[step: 2344] loss: 4.567457675933838\n",
      "[step: 2345] loss: 4.573731422424316\n",
      "[step: 2346] loss: 4.585745811462402\n",
      "[step: 2347] loss: 4.595389366149902\n",
      "[step: 2348] loss: 4.592270374298096\n",
      "[step: 2349] loss: 4.580428123474121\n",
      "[step: 2350] loss: 4.563419342041016\n",
      "[step: 2351] loss: 4.547511100769043\n",
      "[step: 2352] loss: 4.535470008850098\n",
      "[step: 2353] loss: 4.528802394866943\n",
      "[step: 2354] loss: 4.525871753692627\n",
      "[step: 2355] loss: 4.5263285636901855\n",
      "[step: 2356] loss: 4.5282464027404785\n",
      "[step: 2357] loss: 4.530374050140381\n",
      "[step: 2358] loss: 4.533456802368164\n",
      "[step: 2359] loss: 4.535003662109375\n",
      "[step: 2360] loss: 4.538702487945557\n",
      "[step: 2361] loss: 4.542311668395996\n",
      "[step: 2362] loss: 4.551617622375488\n",
      "[step: 2363] loss: 4.562094211578369\n",
      "[step: 2364] loss: 4.583981990814209\n",
      "[step: 2365] loss: 4.607855796813965\n",
      "[step: 2366] loss: 4.658285140991211\n",
      "[step: 2367] loss: 4.70988655090332\n",
      "[step: 2368] loss: 4.813019752502441\n",
      "[step: 2369] loss: 4.882806777954102\n",
      "[step: 2370] loss: 5.015374183654785\n",
      "[step: 2371] loss: 5.014105319976807\n",
      "[step: 2372] loss: 5.02943229675293\n",
      "[step: 2373] loss: 4.850813865661621\n",
      "[step: 2374] loss: 4.675086498260498\n",
      "[step: 2375] loss: 4.506152629852295\n",
      "[step: 2376] loss: 4.443220138549805\n",
      "[step: 2377] loss: 4.483381748199463\n",
      "[step: 2378] loss: 4.568806171417236\n",
      "[step: 2379] loss: 4.637567520141602\n",
      "[step: 2380] loss: 4.610591888427734\n",
      "[step: 2381] loss: 4.542578220367432\n",
      "[step: 2382] loss: 4.457089424133301\n",
      "[step: 2383] loss: 4.416807174682617\n",
      "[step: 2384] loss: 4.428562164306641\n",
      "[step: 2385] loss: 4.468601703643799\n",
      "[step: 2386] loss: 4.508671760559082\n",
      "[step: 2387] loss: 4.511731147766113\n",
      "[step: 2388] loss: 4.494169235229492\n",
      "[step: 2389] loss: 4.450404167175293\n",
      "[step: 2390] loss: 4.411865711212158\n",
      "[step: 2391] loss: 4.38674259185791\n",
      "[step: 2392] loss: 4.380190372467041\n",
      "[step: 2393] loss: 4.387946128845215\n",
      "[step: 2394] loss: 4.4023284912109375\n",
      "[step: 2395] loss: 4.418851852416992\n",
      "[step: 2396] loss: 4.428302764892578\n",
      "[step: 2397] loss: 4.436570167541504\n",
      "[step: 2398] loss: 4.433122634887695\n",
      "[step: 2399] loss: 4.43179988861084\n",
      "[step: 2400] loss: 4.42219352722168\n",
      "[step: 2401] loss: 4.418107032775879\n",
      "[step: 2402] loss: 4.409502983093262\n",
      "[step: 2403] loss: 4.407809257507324\n",
      "[step: 2404] loss: 4.4031476974487305\n",
      "[step: 2405] loss: 4.40733528137207\n",
      "[step: 2406] loss: 4.409997940063477\n",
      "[step: 2407] loss: 4.426412582397461\n",
      "[step: 2408] loss: 4.441901683807373\n",
      "[step: 2409] loss: 4.481451511383057\n",
      "[step: 2410] loss: 4.516327857971191\n",
      "[step: 2411] loss: 4.594085693359375\n",
      "[step: 2412] loss: 4.643497467041016\n",
      "[step: 2413] loss: 4.751416206359863\n",
      "[step: 2414] loss: 4.758039951324463\n",
      "[step: 2415] loss: 4.799216270446777\n",
      "[step: 2416] loss: 4.67863655090332\n",
      "[step: 2417] loss: 4.564370155334473\n",
      "[step: 2418] loss: 4.403993606567383\n",
      "[step: 2419] loss: 4.304007053375244\n",
      "[step: 2420] loss: 4.279391288757324\n",
      "[step: 2421] loss: 4.320098876953125\n",
      "[step: 2422] loss: 4.388602256774902\n",
      "[step: 2423] loss: 4.425948143005371\n",
      "[step: 2424] loss: 4.432412147521973\n",
      "[step: 2425] loss: 4.377181053161621\n",
      "[step: 2426] loss: 4.318720817565918\n",
      "[step: 2427] loss: 4.267682075500488\n",
      "[step: 2428] loss: 4.247159004211426\n",
      "[step: 2429] loss: 4.254847526550293\n",
      "[step: 2430] loss: 4.278642654418945\n",
      "[step: 2431] loss: 4.308041572570801\n",
      "[step: 2432] loss: 4.32334566116333\n",
      "[step: 2433] loss: 4.334210395812988\n",
      "[step: 2434] loss: 4.3218464851379395\n",
      "[step: 2435] loss: 4.309177398681641\n",
      "[step: 2436] loss: 4.283883094787598\n",
      "[step: 2437] loss: 4.262927055358887\n",
      "[step: 2438] loss: 4.240653991699219\n",
      "[step: 2439] loss: 4.223913192749023\n",
      "[step: 2440] loss: 4.210238933563232\n",
      "[step: 2441] loss: 4.200519561767578\n",
      "[step: 2442] loss: 4.193521022796631\n",
      "[step: 2443] loss: 4.1885576248168945\n",
      "[step: 2444] loss: 4.184997081756592\n",
      "[step: 2445] loss: 4.182562828063965\n",
      "[step: 2446] loss: 4.1813812255859375\n",
      "[step: 2447] loss: 4.181890487670898\n",
      "[step: 2448] loss: 4.185802936553955\n",
      "[step: 2449] loss: 4.194962978363037\n",
      "[step: 2450] loss: 4.217242240905762\n",
      "[step: 2451] loss: 4.25814151763916\n",
      "[step: 2452] loss: 4.3540239334106445\n",
      "[step: 2453] loss: 4.5052056312561035\n",
      "[step: 2454] loss: 4.86744499206543\n",
      "[step: 2455] loss: 5.215551376342773\n",
      "[step: 2456] loss: 5.969135284423828\n",
      "[step: 2457] loss: 5.688783645629883\n",
      "[step: 2458] loss: 5.301053047180176\n",
      "[step: 2459] loss: 4.457151412963867\n",
      "[step: 2460] loss: 4.224122524261475\n",
      "[step: 2461] loss: 4.604459762573242\n",
      "[step: 2462] loss: 4.839083671569824\n",
      "[step: 2463] loss: 4.669698715209961\n",
      "[step: 2464] loss: 4.261449813842773\n",
      "[step: 2465] loss: 4.2862091064453125\n",
      "[step: 2466] loss: 4.59016227722168\n",
      "[step: 2467] loss: 4.523773670196533\n",
      "[step: 2468] loss: 4.240179061889648\n",
      "[step: 2469] loss: 4.192748069763184\n",
      "[step: 2470] loss: 4.38807487487793\n",
      "[step: 2471] loss: 4.429756164550781\n",
      "[step: 2472] loss: 4.216096878051758\n",
      "[step: 2473] loss: 4.130270957946777\n",
      "[step: 2474] loss: 4.286528587341309\n",
      "[step: 2475] loss: 4.332831859588623\n",
      "[step: 2476] loss: 4.198858737945557\n",
      "[step: 2477] loss: 4.104440689086914\n",
      "[step: 2478] loss: 4.170032978057861\n",
      "[step: 2479] loss: 4.240922451019287\n",
      "[step: 2480] loss: 4.179413795471191\n",
      "[step: 2481] loss: 4.11482572555542\n",
      "[step: 2482] loss: 4.103841781616211\n",
      "[step: 2483] loss: 4.134638786315918\n",
      "[step: 2484] loss: 4.152442932128906\n",
      "[step: 2485] loss: 4.131272315979004\n",
      "[step: 2486] loss: 4.09846830368042\n",
      "[step: 2487] loss: 4.0636138916015625\n",
      "[step: 2488] loss: 4.075985908508301\n",
      "[step: 2489] loss: 4.108524322509766\n",
      "[step: 2490] loss: 4.10679817199707\n",
      "[step: 2491] loss: 4.069967746734619\n",
      "[step: 2492] loss: 4.03742790222168\n",
      "[step: 2493] loss: 4.0417799949646\n",
      "[step: 2494] loss: 4.055310249328613\n",
      "[step: 2495] loss: 4.058935165405273\n",
      "[step: 2496] loss: 4.0533223152160645\n",
      "[step: 2497] loss: 4.044104099273682\n",
      "[step: 2498] loss: 4.028026580810547\n",
      "[step: 2499] loss: 4.010837554931641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2500] loss: 4.0051116943359375\n",
      "[step: 2501] loss: 4.010653495788574\n",
      "[step: 2502] loss: 4.016810417175293\n",
      "[step: 2503] loss: 4.014141082763672\n",
      "[step: 2504] loss: 4.008393287658691\n",
      "[step: 2505] loss: 4.002254962921143\n",
      "[step: 2506] loss: 3.994079113006592\n",
      "[step: 2507] loss: 3.9836313724517822\n",
      "[step: 2508] loss: 3.974827289581299\n",
      "[step: 2509] loss: 3.970982313156128\n",
      "[step: 2510] loss: 3.969491958618164\n",
      "[step: 2511] loss: 3.9677305221557617\n",
      "[step: 2512] loss: 3.965395927429199\n",
      "[step: 2513] loss: 3.964867115020752\n",
      "[step: 2514] loss: 3.965388298034668\n",
      "[step: 2515] loss: 3.9656684398651123\n",
      "[step: 2516] loss: 3.9658331871032715\n",
      "[step: 2517] loss: 3.967092990875244\n",
      "[step: 2518] loss: 3.9729113578796387\n",
      "[step: 2519] loss: 3.9804229736328125\n",
      "[step: 2520] loss: 3.9979043006896973\n",
      "[step: 2521] loss: 4.0187482833862305\n",
      "[step: 2522] loss: 4.066089153289795\n",
      "[step: 2523] loss: 4.118424892425537\n",
      "[step: 2524] loss: 4.234470367431641\n",
      "[step: 2525] loss: 4.3320207595825195\n",
      "[step: 2526] loss: 4.551314353942871\n",
      "[step: 2527] loss: 4.58188533782959\n",
      "[step: 2528] loss: 4.710341453552246\n",
      "[step: 2529] loss: 4.453032970428467\n",
      "[step: 2530] loss: 4.236296653747559\n",
      "[step: 2531] loss: 3.988161087036133\n",
      "[step: 2532] loss: 3.905893087387085\n",
      "[step: 2533] loss: 3.9813785552978516\n",
      "[step: 2534] loss: 4.103377819061279\n",
      "[step: 2535] loss: 4.188185691833496\n",
      "[step: 2536] loss: 4.1002607345581055\n",
      "[step: 2537] loss: 3.976994037628174\n",
      "[step: 2538] loss: 3.8883249759674072\n",
      "[step: 2539] loss: 3.896986961364746\n",
      "[step: 2540] loss: 3.968393564224243\n",
      "[step: 2541] loss: 4.0178093910217285\n",
      "[step: 2542] loss: 4.023001670837402\n",
      "[step: 2543] loss: 3.958933115005493\n",
      "[step: 2544] loss: 3.8987350463867188\n",
      "[step: 2545] loss: 3.8625648021698\n",
      "[step: 2546] loss: 3.8639683723449707\n",
      "[step: 2547] loss: 3.8902456760406494\n",
      "[step: 2548] loss: 3.9173505306243896\n",
      "[step: 2549] loss: 3.937682867050171\n",
      "[step: 2550] loss: 3.9232466220855713\n",
      "[step: 2551] loss: 3.899359941482544\n",
      "[step: 2552] loss: 3.8632242679595947\n",
      "[step: 2553] loss: 3.8382718563079834\n",
      "[step: 2554] loss: 3.8266539573669434\n",
      "[step: 2555] loss: 3.8266048431396484\n",
      "[step: 2556] loss: 3.8339099884033203\n",
      "[step: 2557] loss: 3.843648910522461\n",
      "[step: 2558] loss: 3.8558902740478516\n",
      "[step: 2559] loss: 3.863539457321167\n",
      "[step: 2560] loss: 3.8723223209381104\n",
      "[step: 2561] loss: 3.8729286193847656\n",
      "[step: 2562] loss: 3.878559112548828\n",
      "[step: 2563] loss: 3.8770580291748047\n",
      "[step: 2564] loss: 3.8856496810913086\n",
      "[step: 2565] loss: 3.88523006439209\n",
      "[step: 2566] loss: 3.899125576019287\n",
      "[step: 2567] loss: 3.9023725986480713\n",
      "[step: 2568] loss: 3.92558217048645\n",
      "[step: 2569] loss: 3.9339699745178223\n",
      "[step: 2570] loss: 3.966994047164917\n",
      "[step: 2571] loss: 3.973208427429199\n",
      "[step: 2572] loss: 4.007619857788086\n",
      "[step: 2573] loss: 3.9967665672302246\n",
      "[step: 2574] loss: 4.013714790344238\n",
      "[step: 2575] loss: 3.9728527069091797\n",
      "[step: 2576] loss: 3.9526686668395996\n",
      "[step: 2577] loss: 3.891047954559326\n",
      "[step: 2578] loss: 3.84559965133667\n",
      "[step: 2579] loss: 3.795437812805176\n",
      "[step: 2580] loss: 3.7617228031158447\n",
      "[step: 2581] loss: 3.741408348083496\n",
      "[step: 2582] loss: 3.7348313331604004\n",
      "[step: 2583] loss: 3.7388906478881836\n",
      "[step: 2584] loss: 3.7497622966766357\n",
      "[step: 2585] loss: 3.7661938667297363\n",
      "[step: 2586] loss: 3.7830519676208496\n",
      "[step: 2587] loss: 3.808774948120117\n",
      "[step: 2588] loss: 3.8305952548980713\n",
      "[step: 2589] loss: 3.8742332458496094\n",
      "[step: 2590] loss: 3.90480375289917\n",
      "[step: 2591] loss: 3.978745460510254\n",
      "[step: 2592] loss: 4.011529922485352\n",
      "[step: 2593] loss: 4.108691215515137\n",
      "[step: 2594] loss: 4.102053642272949\n",
      "[step: 2595] loss: 4.150495529174805\n",
      "[step: 2596] loss: 4.048767566680908\n",
      "[step: 2597] loss: 3.9718728065490723\n",
      "[step: 2598] loss: 3.8331809043884277\n",
      "[step: 2599] loss: 3.735996723175049\n",
      "[step: 2600] loss: 3.685833692550659\n",
      "[step: 2601] loss: 3.6906886100769043\n",
      "[step: 2602] loss: 3.7338662147521973\n",
      "[step: 2603] loss: 3.7812674045562744\n",
      "[step: 2604] loss: 3.8229947090148926\n",
      "[step: 2605] loss: 3.817152976989746\n",
      "[step: 2606] loss: 3.7999463081359863\n",
      "[step: 2607] loss: 3.7506065368652344\n",
      "[step: 2608] loss: 3.7100234031677246\n",
      "[step: 2609] loss: 3.6742846965789795\n",
      "[step: 2610] loss: 3.654407501220703\n",
      "[step: 2611] loss: 3.6474390029907227\n",
      "[step: 2612] loss: 3.6505892276763916\n",
      "[step: 2613] loss: 3.661078929901123\n",
      "[step: 2614] loss: 3.675426959991455\n",
      "[step: 2615] loss: 3.6965599060058594\n",
      "[step: 2616] loss: 3.714268922805786\n",
      "[step: 2617] loss: 3.7444539070129395\n",
      "[step: 2618] loss: 3.761686325073242\n",
      "[step: 2619] loss: 3.8028812408447266\n",
      "[step: 2620] loss: 3.8196959495544434\n",
      "[step: 2621] loss: 3.8740243911743164\n",
      "[step: 2622] loss: 3.8842179775238037\n",
      "[step: 2623] loss: 3.9372200965881348\n",
      "[step: 2624] loss: 3.9165916442871094\n",
      "[step: 2625] loss: 3.9285202026367188\n",
      "[step: 2626] loss: 3.857347011566162\n",
      "[step: 2627] loss: 3.805729389190674\n",
      "[step: 2628] loss: 3.7168283462524414\n",
      "[step: 2629] loss: 3.6517226696014404\n",
      "[step: 2630] loss: 3.6039950847625732\n",
      "[step: 2631] loss: 3.5849127769470215\n",
      "[step: 2632] loss: 3.5897974967956543\n",
      "[step: 2633] loss: 3.6100494861602783\n",
      "[step: 2634] loss: 3.6389284133911133\n",
      "[step: 2635] loss: 3.6626110076904297\n",
      "[step: 2636] loss: 3.6906380653381348\n",
      "[step: 2637] loss: 3.6985063552856445\n",
      "[step: 2638] loss: 3.7169241905212402\n",
      "[step: 2639] loss: 3.7081844806671143\n",
      "[step: 2640] loss: 3.716029405593872\n",
      "[step: 2641] loss: 3.6962809562683105\n",
      "[step: 2642] loss: 3.6942381858825684\n",
      "[step: 2643] loss: 3.669621229171753\n",
      "[step: 2644] loss: 3.6601316928863525\n",
      "[step: 2645] loss: 3.6362128257751465\n",
      "[step: 2646] loss: 3.6234066486358643\n",
      "[step: 2647] loss: 3.603513479232788\n",
      "[step: 2648] loss: 3.591022491455078\n",
      "[step: 2649] loss: 3.5760834217071533\n",
      "[step: 2650] loss: 3.566378116607666\n",
      "[step: 2651] loss: 3.556548833847046\n",
      "[step: 2652] loss: 3.5512406826019287\n",
      "[step: 2653] loss: 3.5468173027038574\n",
      "[step: 2654] loss: 3.547637462615967\n",
      "[step: 2655] loss: 3.5502877235412598\n",
      "[step: 2656] loss: 3.5623669624328613\n",
      "[step: 2657] loss: 3.57912540435791\n",
      "[step: 2658] loss: 3.6204042434692383\n",
      "[step: 2659] loss: 3.6728553771972656\n",
      "[step: 2660] loss: 3.7971487045288086\n",
      "[step: 2661] loss: 3.9236526489257812\n",
      "[step: 2662] loss: 4.2290754318237305\n",
      "[step: 2663] loss: 4.354091644287109\n",
      "[step: 2664] loss: 4.68613338470459\n",
      "[step: 2665] loss: 4.380580902099609\n",
      "[step: 2666] loss: 4.114016532897949\n",
      "[step: 2667] loss: 3.7125730514526367\n",
      "[step: 2668] loss: 3.5635619163513184\n",
      "[step: 2669] loss: 3.669656276702881\n",
      "[step: 2670] loss: 3.7980704307556152\n",
      "[step: 2671] loss: 3.8646080493927\n",
      "[step: 2672] loss: 3.688999652862549\n",
      "[step: 2673] loss: 3.558795928955078\n",
      "[step: 2674] loss: 3.55788516998291\n",
      "[step: 2675] loss: 3.62383770942688\n",
      "[step: 2676] loss: 3.6748976707458496\n",
      "[step: 2677] loss: 3.6084001064300537\n",
      "[step: 2678] loss: 3.5460076332092285\n",
      "[step: 2679] loss: 3.5164737701416016\n",
      "[step: 2680] loss: 3.518057107925415\n",
      "[step: 2681] loss: 3.5366666316986084\n",
      "[step: 2682] loss: 3.5412845611572266\n",
      "[step: 2683] loss: 3.5460214614868164\n",
      "[step: 2684] loss: 3.5259242057800293\n",
      "[step: 2685] loss: 3.4845688343048096\n",
      "[step: 2686] loss: 3.446286678314209\n",
      "[step: 2687] loss: 3.439141035079956\n",
      "[step: 2688] loss: 3.4659852981567383\n",
      "[step: 2689] loss: 3.495255470275879\n",
      "[step: 2690] loss: 3.4981918334960938\n",
      "[step: 2691] loss: 3.4701991081237793\n",
      "[step: 2692] loss: 3.434814929962158\n",
      "[step: 2693] loss: 3.4146108627319336\n",
      "[step: 2694] loss: 3.4132230281829834\n",
      "[step: 2695] loss: 3.4162163734436035\n",
      "[step: 2696] loss: 3.415639877319336\n",
      "[step: 2697] loss: 3.413975477218628\n",
      "[step: 2698] loss: 3.4168448448181152\n",
      "[step: 2699] loss: 3.426828622817993\n",
      "[step: 2700] loss: 3.4331178665161133\n",
      "[step: 2701] loss: 3.4379091262817383\n",
      "[step: 2702] loss: 3.4316043853759766\n",
      "[step: 2703] loss: 3.431464195251465\n",
      "[step: 2704] loss: 3.431313991546631\n",
      "[step: 2705] loss: 3.4399678707122803\n",
      "[step: 2706] loss: 3.44629168510437\n",
      "[step: 2707] loss: 3.4583144187927246\n",
      "[step: 2708] loss: 3.4682810306549072\n",
      "[step: 2709] loss: 3.4976491928100586\n",
      "[step: 2710] loss: 3.5239100456237793\n",
      "[step: 2711] loss: 3.590468406677246\n",
      "[step: 2712] loss: 3.629084587097168\n",
      "[step: 2713] loss: 3.733755350112915\n",
      "[step: 2714] loss: 3.7562367916107178\n",
      "[step: 2715] loss: 3.8533806800842285\n",
      "[step: 2716] loss: 3.7970423698425293\n",
      "[step: 2717] loss: 3.772519111633301\n",
      "[step: 2718] loss: 3.623396158218384\n",
      "[step: 2719] loss: 3.4954190254211426\n",
      "[step: 2720] loss: 3.3803229331970215\n",
      "[step: 2721] loss: 3.3278138637542725\n",
      "[step: 2722] loss: 3.338834762573242\n",
      "[step: 2723] loss: 3.3897905349731445\n",
      "[step: 2724] loss: 3.450777292251587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2725] loss: 3.4760189056396484\n",
      "[step: 2726] loss: 3.4789557456970215\n",
      "[step: 2727] loss: 3.4338810443878174\n",
      "[step: 2728] loss: 3.3914365768432617\n",
      "[step: 2729] loss: 3.345019578933716\n",
      "[step: 2730] loss: 3.3170125484466553\n",
      "[step: 2731] loss: 3.302438974380493\n",
      "[step: 2732] loss: 3.2999727725982666\n",
      "[step: 2733] loss: 3.3068933486938477\n",
      "[step: 2734] loss: 3.320767402648926\n",
      "[step: 2735] loss: 3.3427016735076904\n",
      "[step: 2736] loss: 3.361584186553955\n",
      "[step: 2737] loss: 3.390028476715088\n",
      "[step: 2738] loss: 3.4021248817443848\n",
      "[step: 2739] loss: 3.432384729385376\n",
      "[step: 2740] loss: 3.439560890197754\n",
      "[step: 2741] loss: 3.4767494201660156\n",
      "[step: 2742] loss: 3.483768939971924\n",
      "[step: 2743] loss: 3.524850368499756\n",
      "[step: 2744] loss: 3.5201497077941895\n",
      "[step: 2745] loss: 3.5455784797668457\n",
      "[step: 2746] loss: 3.5107157230377197\n",
      "[step: 2747] loss: 3.4994704723358154\n",
      "[step: 2748] loss: 3.435382604598999\n",
      "[step: 2749] loss: 3.3910603523254395\n",
      "[step: 2750] loss: 3.32846736907959\n",
      "[step: 2751] loss: 3.285032033920288\n",
      "[step: 2752] loss: 3.251643180847168\n",
      "[step: 2753] loss: 3.2346396446228027\n",
      "[step: 2754] loss: 3.23087215423584\n",
      "[step: 2755] loss: 3.2371201515197754\n",
      "[step: 2756] loss: 3.250699520111084\n",
      "[step: 2757] loss: 3.2673771381378174\n",
      "[step: 2758] loss: 3.2918245792388916\n",
      "[step: 2759] loss: 3.3140430450439453\n",
      "[step: 2760] loss: 3.3549582958221436\n",
      "[step: 2761] loss: 3.387678623199463\n",
      "[step: 2762] loss: 3.462151527404785\n",
      "[step: 2763] loss: 3.509608745574951\n",
      "[step: 2764] loss: 3.631129741668701\n",
      "[step: 2765] loss: 3.661902904510498\n",
      "[step: 2766] loss: 3.776681900024414\n",
      "[step: 2767] loss: 3.6968917846679688\n",
      "[step: 2768] loss: 3.659447193145752\n",
      "[step: 2769] loss: 3.4724650382995605\n",
      "[step: 2770] loss: 3.328183174133301\n",
      "[step: 2771] loss: 3.2213761806488037\n",
      "[step: 2772] loss: 3.1936705112457275\n",
      "[step: 2773] loss: 3.232452869415283\n",
      "[step: 2774] loss: 3.2947545051574707\n",
      "[step: 2775] loss: 3.3597373962402344\n",
      "[step: 2776] loss: 3.3599417209625244\n",
      "[step: 2777] loss: 3.3395066261291504\n",
      "[step: 2778] loss: 3.2734858989715576\n",
      "[step: 2779] loss: 3.2148046493530273\n",
      "[step: 2780] loss: 3.1729273796081543\n",
      "[step: 2781] loss: 3.1592049598693848\n",
      "[step: 2782] loss: 3.170060157775879\n",
      "[step: 2783] loss: 3.194997787475586\n",
      "[step: 2784] loss: 3.227200746536255\n",
      "[step: 2785] loss: 3.2505838871002197\n",
      "[step: 2786] loss: 3.276390314102173\n",
      "[step: 2787] loss: 3.2797536849975586\n",
      "[step: 2788] loss: 3.292029619216919\n",
      "[step: 2789] loss: 3.2779741287231445\n",
      "[step: 2790] loss: 3.279977798461914\n",
      "[step: 2791] loss: 3.2582411766052246\n",
      "[step: 2792] loss: 3.2534756660461426\n",
      "[step: 2793] loss: 3.229564905166626\n",
      "[step: 2794] loss: 3.2190487384796143\n",
      "[step: 2795] loss: 3.196613311767578\n",
      "[step: 2796] loss: 3.1849241256713867\n",
      "[step: 2797] loss: 3.1691017150878906\n",
      "[step: 2798] loss: 3.1621828079223633\n",
      "[step: 2799] loss: 3.1552672386169434\n",
      "[step: 2800] loss: 3.155283212661743\n",
      "[step: 2801] loss: 3.1564579010009766\n",
      "[step: 2802] loss: 3.165085792541504\n",
      "[step: 2803] loss: 3.1768717765808105\n",
      "[step: 2804] loss: 3.2049059867858887\n",
      "[step: 2805] loss: 3.2413980960845947\n",
      "[step: 2806] loss: 3.3231887817382812\n",
      "[step: 2807] loss: 3.4124484062194824\n",
      "[step: 2808] loss: 3.6188948154449463\n",
      "[step: 2809] loss: 3.7489728927612305\n",
      "[step: 2810] loss: 4.076409339904785\n",
      "[step: 2811] loss: 3.981332778930664\n",
      "[step: 2812] loss: 3.9828996658325195\n",
      "[step: 2813] loss: 3.5463922023773193\n",
      "[step: 2814] loss: 3.2418618202209473\n",
      "[step: 2815] loss: 3.1204183101654053\n",
      "[step: 2816] loss: 3.2116708755493164\n",
      "[step: 2817] loss: 3.399036407470703\n",
      "[step: 2818] loss: 3.400369167327881\n",
      "[step: 2819] loss: 3.3216185569763184\n",
      "[step: 2820] loss: 3.1504392623901367\n",
      "[step: 2821] loss: 3.092219114303589\n",
      "[step: 2822] loss: 3.1543402671813965\n",
      "[step: 2823] loss: 3.226646900177002\n",
      "[step: 2824] loss: 3.25689697265625\n",
      "[step: 2825] loss: 3.175900936126709\n",
      "[step: 2826] loss: 3.1048221588134766\n",
      "[step: 2827] loss: 3.0753538608551025\n",
      "[step: 2828] loss: 3.0925135612487793\n",
      "[step: 2829] loss: 3.1219642162323\n",
      "[step: 2830] loss: 3.122530937194824\n",
      "[step: 2831] loss: 3.112987518310547\n",
      "[step: 2832] loss: 3.0917768478393555\n",
      "[step: 2833] loss: 3.083343505859375\n",
      "[step: 2834] loss: 3.078918933868408\n",
      "[step: 2835] loss: 3.0651721954345703\n",
      "[step: 2836] loss: 3.043642997741699\n",
      "[step: 2837] loss: 3.024305820465088\n",
      "[step: 2838] loss: 3.0210845470428467\n",
      "[step: 2839] loss: 3.0340511798858643\n",
      "[step: 2840] loss: 3.052360773086548\n",
      "[step: 2841] loss: 3.0634756088256836\n",
      "[step: 2842] loss: 3.0610358715057373\n",
      "[step: 2843] loss: 3.0529704093933105\n",
      "[step: 2844] loss: 3.050243377685547\n",
      "[step: 2845] loss: 3.055025100708008\n",
      "[step: 2846] loss: 3.075352191925049\n",
      "[step: 2847] loss: 3.0910720825195312\n",
      "[step: 2848] loss: 3.1243441104888916\n",
      "[step: 2849] loss: 3.145293712615967\n",
      "[step: 2850] loss: 3.208064556121826\n",
      "[step: 2851] loss: 3.261638879776001\n",
      "[step: 2852] loss: 3.3874168395996094\n",
      "[step: 2853] loss: 3.47491455078125\n",
      "[step: 2854] loss: 3.638385057449341\n",
      "[step: 2855] loss: 3.651014804840088\n",
      "[step: 2856] loss: 3.6857974529266357\n",
      "[step: 2857] loss: 3.4781956672668457\n",
      "[step: 2858] loss: 3.284350633621216\n",
      "[step: 2859] loss: 3.0650205612182617\n",
      "[step: 2860] loss: 2.9653143882751465\n",
      "[step: 2861] loss: 2.988105058670044\n",
      "[step: 2862] loss: 3.0833518505096436\n",
      "[step: 2863] loss: 3.181178092956543\n",
      "[step: 2864] loss: 3.1860713958740234\n",
      "[step: 2865] loss: 3.143770217895508\n",
      "[step: 2866] loss: 3.0418031215667725\n",
      "[step: 2867] loss: 2.9736886024475098\n",
      "[step: 2868] loss: 2.950697422027588\n",
      "[step: 2869] loss: 2.969550609588623\n",
      "[step: 2870] loss: 3.005887031555176\n",
      "[step: 2871] loss: 3.033794403076172\n",
      "[step: 2872] loss: 3.0562198162078857\n",
      "[step: 2873] loss: 3.0460152626037598\n",
      "[step: 2874] loss: 3.03749942779541\n",
      "[step: 2875] loss: 3.001819610595703\n",
      "[step: 2876] loss: 2.969552755355835\n",
      "[step: 2877] loss: 2.935218334197998\n",
      "[step: 2878] loss: 2.9138171672821045\n",
      "[step: 2879] loss: 2.9075021743774414\n",
      "[step: 2880] loss: 2.913841962814331\n",
      "[step: 2881] loss: 2.9271018505096436\n",
      "[step: 2882] loss: 2.9398365020751953\n",
      "[step: 2883] loss: 2.9548215866088867\n",
      "[step: 2884] loss: 2.9655063152313232\n",
      "[step: 2885] loss: 2.9894227981567383\n",
      "[step: 2886] loss: 3.012647867202759\n",
      "[step: 2887] loss: 3.0651063919067383\n",
      "[step: 2888] loss: 3.114809989929199\n",
      "[step: 2889] loss: 3.2193098068237305\n",
      "[step: 2890] loss: 3.2973392009735107\n",
      "[step: 2891] loss: 3.474609613418579\n",
      "[step: 2892] loss: 3.515138626098633\n",
      "[step: 2893] loss: 3.6612181663513184\n",
      "[step: 2894] loss: 3.4874556064605713\n",
      "[step: 2895] loss: 3.360675811767578\n",
      "[step: 2896] loss: 3.0848827362060547\n",
      "[step: 2897] loss: 2.9161782264709473\n",
      "[step: 2898] loss: 2.872988224029541\n",
      "[step: 2899] loss: 2.9432246685028076\n",
      "[step: 2900] loss: 3.0648584365844727\n",
      "[step: 2901] loss: 3.1119890213012695\n",
      "[step: 2902] loss: 3.1138226985931396\n",
      "[step: 2903] loss: 3.0088589191436768\n",
      "[step: 2904] loss: 2.914008617401123\n",
      "[step: 2905] loss: 2.858945846557617\n",
      "[step: 2906] loss: 2.863290548324585\n",
      "[step: 2907] loss: 2.9097177982330322\n",
      "[step: 2908] loss: 2.957691192626953\n",
      "[step: 2909] loss: 2.999289035797119\n",
      "[step: 2910] loss: 2.989203929901123\n",
      "[step: 2911] loss: 2.9670095443725586\n",
      "[step: 2912] loss: 2.915560722351074\n",
      "[step: 2913] loss: 2.871622085571289\n",
      "[step: 2914] loss: 2.838042736053467\n",
      "[step: 2915] loss: 2.8232178688049316\n",
      "[step: 2916] loss: 2.824876070022583\n",
      "[step: 2917] loss: 2.837721824645996\n",
      "[step: 2918] loss: 2.8570992946624756\n",
      "[step: 2919] loss: 2.8762941360473633\n",
      "[step: 2920] loss: 2.900663375854492\n",
      "[step: 2921] loss: 2.918222665786743\n",
      "[step: 2922] loss: 2.950326442718506\n",
      "[step: 2923] loss: 2.968571662902832\n",
      "[step: 2924] loss: 3.0170035362243652\n",
      "[step: 2925] loss: 3.0377016067504883\n",
      "[step: 2926] loss: 3.1087841987609863\n",
      "[step: 2927] loss: 3.122837543487549\n",
      "[step: 2928] loss: 3.2022905349731445\n",
      "[step: 2929] loss: 3.178582191467285\n",
      "[step: 2930] loss: 3.211501121520996\n",
      "[step: 2931] loss: 3.127375602722168\n",
      "[step: 2932] loss: 3.074079751968384\n",
      "[step: 2933] loss: 2.9723992347717285\n",
      "[step: 2934] loss: 2.894369125366211\n",
      "[step: 2935] loss: 2.8360648155212402\n",
      "[step: 2936] loss: 2.8041882514953613\n",
      "[step: 2937] loss: 2.7975523471832275\n",
      "[step: 2938] loss: 2.807966709136963\n",
      "[step: 2939] loss: 2.833528995513916\n",
      "[step: 2940] loss: 2.859454393386841\n",
      "[step: 2941] loss: 2.894373893737793\n",
      "[step: 2942] loss: 2.914870500564575\n",
      "[step: 2943] loss: 2.93711519241333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 2944] loss: 2.9386634826660156\n",
      "[step: 2945] loss: 2.9425058364868164\n",
      "[step: 2946] loss: 2.9280333518981934\n",
      "[step: 2947] loss: 2.927455425262451\n",
      "[step: 2948] loss: 2.911308765411377\n",
      "[step: 2949] loss: 2.9202170372009277\n",
      "[step: 2950] loss: 2.9081177711486816\n",
      "[step: 2951] loss: 2.922654628753662\n",
      "[step: 2952] loss: 2.9057443141937256\n",
      "[step: 2953] loss: 2.911097526550293\n",
      "[step: 2954] loss: 2.883265733718872\n",
      "[step: 2955] loss: 2.8752188682556152\n",
      "[step: 2956] loss: 2.8471744060516357\n",
      "[step: 2957] loss: 2.838621139526367\n",
      "[step: 2958] loss: 2.824306011199951\n",
      "[step: 2959] loss: 2.8247756958007812\n",
      "[step: 2960] loss: 2.8236725330352783\n",
      "[step: 2961] loss: 2.8306002616882324\n",
      "[step: 2962] loss: 2.8359487056732178\n",
      "[step: 2963] loss: 2.8501596450805664\n",
      "[step: 2964] loss: 2.864950656890869\n",
      "[step: 2965] loss: 2.9034600257873535\n",
      "[step: 2966] loss: 2.9416189193725586\n",
      "[step: 2967] loss: 3.035578966140747\n",
      "[step: 2968] loss: 3.100085735321045\n",
      "[step: 2969] loss: 3.264600992202759\n",
      "[step: 2970] loss: 3.2880547046661377\n",
      "[step: 2971] loss: 3.4160079956054688\n",
      "[step: 2972] loss: 3.2644994258880615\n",
      "[step: 2973] loss: 3.1657793521881104\n",
      "[step: 2974] loss: 2.9377222061157227\n",
      "[step: 2975] loss: 2.7866220474243164\n",
      "[step: 2976] loss: 2.7215466499328613\n",
      "[step: 2977] loss: 2.743870735168457\n",
      "[step: 2978] loss: 2.818098306655884\n",
      "[step: 2979] loss: 2.866687059402466\n",
      "[step: 2980] loss: 2.901780605316162\n",
      "[step: 2981] loss: 2.8562545776367188\n",
      "[step: 2982] loss: 2.8071823120117188\n",
      "[step: 2983] loss: 2.7530131340026855\n",
      "[step: 2984] loss: 2.7180991172790527\n",
      "[step: 2985] loss: 2.704911708831787\n",
      "[step: 2986] loss: 2.705925464630127\n",
      "[step: 2987] loss: 2.7203965187072754\n",
      "[step: 2988] loss: 2.738229990005493\n",
      "[step: 2989] loss: 2.768007278442383\n",
      "[step: 2990] loss: 2.788680076599121\n",
      "[step: 2991] loss: 2.8103983402252197\n",
      "[step: 2992] loss: 2.813103199005127\n",
      "[step: 2993] loss: 2.8101391792297363\n",
      "[step: 2994] loss: 2.791633129119873\n",
      "[step: 2995] loss: 2.778369426727295\n",
      "[step: 2996] loss: 2.7591757774353027\n",
      "[step: 2997] loss: 2.7566380500793457\n",
      "[step: 2998] loss: 2.748044490814209\n",
      "[step: 2999] loss: 2.756568431854248\n",
      "[step: 3000] loss: 2.751920461654663\n",
      "[step: 3001] loss: 2.7634315490722656\n",
      "[step: 3002] loss: 2.759829521179199\n",
      "[step: 3003] loss: 2.779104709625244\n",
      "[step: 3004] loss: 2.788606643676758\n",
      "[step: 3005] loss: 2.833787441253662\n",
      "[step: 3006] loss: 2.871108055114746\n",
      "[step: 3007] loss: 2.959075927734375\n",
      "[step: 3008] loss: 3.0228896141052246\n",
      "[step: 3009] loss: 3.150609254837036\n",
      "[step: 3010] loss: 3.1892640590667725\n",
      "[step: 3011] loss: 3.2781314849853516\n",
      "[step: 3012] loss: 3.179774522781372\n",
      "[step: 3013] loss: 3.0971131324768066\n",
      "[step: 3014] loss: 2.887178897857666\n",
      "[step: 3015] loss: 2.729295253753662\n",
      "[step: 3016] loss: 2.6263742446899414\n",
      "[step: 3017] loss: 2.613551616668701\n",
      "[step: 3018] loss: 2.668570041656494\n",
      "[step: 3019] loss: 2.743785858154297\n",
      "[step: 3020] loss: 2.8088083267211914\n",
      "[step: 3021] loss: 2.8058037757873535\n",
      "[step: 3022] loss: 2.7800281047821045\n",
      "[step: 3023] loss: 2.708195924758911\n",
      "[step: 3024] loss: 2.6536779403686523\n",
      "[step: 3025] loss: 2.614684581756592\n",
      "[step: 3026] loss: 2.602276563644409\n",
      "[step: 3027] loss: 2.6068673133850098\n",
      "[step: 3028] loss: 2.620056629180908\n",
      "[step: 3029] loss: 2.6356682777404785\n",
      "[step: 3030] loss: 2.6505048274993896\n",
      "[step: 3031] loss: 2.6727142333984375\n",
      "[step: 3032] loss: 2.689195156097412\n",
      "[step: 3033] loss: 2.724475383758545\n",
      "[step: 3034] loss: 2.7413253784179688\n",
      "[step: 3035] loss: 2.7863478660583496\n",
      "[step: 3036] loss: 2.792245864868164\n",
      "[step: 3037] loss: 2.835259199142456\n",
      "[step: 3038] loss: 2.8232452869415283\n",
      "[step: 3039] loss: 2.8601579666137695\n",
      "[step: 3040] loss: 2.8443679809570312\n",
      "[step: 3041] loss: 2.877347946166992\n",
      "[step: 3042] loss: 2.8658502101898193\n",
      "[step: 3043] loss: 2.8753161430358887\n",
      "[step: 3044] loss: 2.844447612762451\n",
      "[step: 3045] loss: 2.806295156478882\n",
      "[step: 3046] loss: 2.741151809692383\n",
      "[step: 3047] loss: 2.6784744262695312\n",
      "[step: 3048] loss: 2.6189751625061035\n",
      "[step: 3049] loss: 2.5824689865112305\n",
      "[step: 3050] loss: 2.561847686767578\n",
      "[step: 3051] loss: 2.555074691772461\n",
      "[step: 3052] loss: 2.5522706508636475\n",
      "[step: 3053] loss: 2.5499448776245117\n",
      "[step: 3054] loss: 2.547639846801758\n",
      "[step: 3055] loss: 2.54976224899292\n",
      "[step: 3056] loss: 2.560941219329834\n",
      "[step: 3057] loss: 2.5818798542022705\n",
      "[step: 3058] loss: 2.6228113174438477\n",
      "[step: 3059] loss: 2.671933650970459\n",
      "[step: 3060] loss: 2.7739157676696777\n",
      "[step: 3061] loss: 2.877427101135254\n",
      "[step: 3062] loss: 3.1254796981811523\n",
      "[step: 3063] loss: 3.292262077331543\n",
      "[step: 3064] loss: 3.702498197555542\n",
      "[step: 3065] loss: 3.6685266494750977\n",
      "[step: 3066] loss: 3.717310905456543\n",
      "[step: 3067] loss: 3.24661922454834\n",
      "[step: 3068] loss: 2.831422805786133\n",
      "[step: 3069] loss: 2.5931365489959717\n",
      "[step: 3070] loss: 2.6275155544281006\n",
      "[step: 3071] loss: 2.8449888229370117\n",
      "[step: 3072] loss: 2.940121650695801\n",
      "[step: 3073] loss: 2.896246910095215\n",
      "[step: 3074] loss: 2.686978816986084\n",
      "[step: 3075] loss: 2.547563314437866\n",
      "[step: 3076] loss: 2.5746707916259766\n",
      "[step: 3077] loss: 2.688643455505371\n",
      "[step: 3078] loss: 2.7754225730895996\n",
      "[step: 3079] loss: 2.7068638801574707\n",
      "[step: 3080] loss: 2.5839643478393555\n",
      "[step: 3081] loss: 2.501633882522583\n",
      "[step: 3082] loss: 2.5177955627441406\n",
      "[step: 3083] loss: 2.594717025756836\n",
      "[step: 3084] loss: 2.6429708003997803\n",
      "[step: 3085] loss: 2.63427734375\n",
      "[step: 3086] loss: 2.565234899520874\n",
      "[step: 3087] loss: 2.5086312294006348\n",
      "[step: 3088] loss: 2.488539457321167\n",
      "[step: 3089] loss: 2.502540349960327\n",
      "[step: 3090] loss: 2.5260276794433594\n",
      "[step: 3091] loss: 2.5401554107666016\n",
      "[step: 3092] loss: 2.5466365814208984\n",
      "[step: 3093] loss: 2.5396106243133545\n",
      "[step: 3094] loss: 2.534529685974121\n",
      "[step: 3095] loss: 2.5161352157592773\n",
      "[step: 3096] loss: 2.4950203895568848\n",
      "[step: 3097] loss: 2.471634864807129\n",
      "[step: 3098] loss: 2.4572179317474365\n",
      "[step: 3099] loss: 2.4545817375183105\n",
      "[step: 3100] loss: 2.460063934326172\n",
      "[step: 3101] loss: 2.4675605297088623\n",
      "[step: 3102] loss: 2.472121238708496\n",
      "[step: 3103] loss: 2.476659059524536\n",
      "[step: 3104] loss: 2.482534885406494\n",
      "[step: 3105] loss: 2.49945330619812\n",
      "[step: 3106] loss: 2.523772716522217\n",
      "[step: 3107] loss: 2.569760322570801\n",
      "[step: 3108] loss: 2.627060890197754\n",
      "[step: 3109] loss: 2.7409911155700684\n",
      "[step: 3110] loss: 2.8664662837982178\n",
      "[step: 3111] loss: 3.1449062824249268\n",
      "[step: 3112] loss: 3.301753044128418\n",
      "[step: 3113] loss: 3.6835098266601562\n",
      "[step: 3114] loss: 3.475640296936035\n",
      "[step: 3115] loss: 3.3469481468200684\n",
      "[step: 3116] loss: 2.836117744445801\n",
      "[step: 3117] loss: 2.523547649383545\n",
      "[step: 3118] loss: 2.4710142612457275\n",
      "[step: 3119] loss: 2.6277365684509277\n",
      "[step: 3120] loss: 2.8450701236724854\n",
      "[step: 3121] loss: 2.820146322250366\n",
      "[step: 3122] loss: 2.6948111057281494\n",
      "[step: 3123] loss: 2.515986204147339\n",
      "[step: 3124] loss: 2.457791328430176\n",
      "[step: 3125] loss: 2.527512550354004\n",
      "[step: 3126] loss: 2.6117992401123047\n",
      "[step: 3127] loss: 2.65620493888855\n",
      "[step: 3128] loss: 2.5836610794067383\n",
      "[step: 3129] loss: 2.4913299083709717\n",
      "[step: 3130] loss: 2.4307594299316406\n",
      "[step: 3131] loss: 2.4335317611694336\n",
      "[step: 3132] loss: 2.484503746032715\n",
      "[step: 3133] loss: 2.5283584594726562\n",
      "[step: 3134] loss: 2.5439624786376953\n",
      "[step: 3135] loss: 2.504849910736084\n",
      "[step: 3136] loss: 2.4512178897857666\n",
      "[step: 3137] loss: 2.4074745178222656\n",
      "[step: 3138] loss: 2.394517183303833\n",
      "[step: 3139] loss: 2.409942388534546\n",
      "[step: 3140] loss: 2.437162160873413\n",
      "[step: 3141] loss: 2.4613394737243652\n",
      "[step: 3142] loss: 2.46864652633667\n",
      "[step: 3143] loss: 2.4677844047546387\n",
      "[step: 3144] loss: 2.4520928859710693\n",
      "[step: 3145] loss: 2.4384422302246094\n",
      "[step: 3146] loss: 2.4197473526000977\n",
      "[step: 3147] loss: 2.40553617477417\n",
      "[step: 3148] loss: 2.3910374641418457\n",
      "[step: 3149] loss: 2.379883289337158\n",
      "[step: 3150] loss: 2.3718996047973633\n",
      "[step: 3151] loss: 2.367609977722168\n",
      "[step: 3152] loss: 2.366103172302246\n",
      "[step: 3153] loss: 2.365978956222534\n",
      "[step: 3154] loss: 2.366137742996216\n",
      "[step: 3155] loss: 2.36626935005188\n",
      "[step: 3156] loss: 2.367567539215088\n",
      "[step: 3157] loss: 2.371323823928833\n",
      "[step: 3158] loss: 2.3814423084259033\n",
      "[step: 3159] loss: 2.400641441345215\n",
      "[step: 3160] loss: 2.442272663116455\n",
      "[step: 3161] loss: 2.5143003463745117\n",
      "[step: 3162] loss: 2.674777030944824\n",
      "[step: 3163] loss: 2.916480541229248\n",
      "[step: 3164] loss: 3.4763998985290527\n",
      "[step: 3165] loss: 3.925057888031006\n",
      "[step: 3166] loss: 4.862290859222412\n",
      "[step: 3167] loss: 4.210253715515137\n",
      "[step: 3168] loss: 3.529181957244873\n",
      "[step: 3169] loss: 2.6410412788391113\n",
      "[step: 3170] loss: 2.5291757583618164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3171] loss: 3.065659523010254\n",
      "[step: 3172] loss: 3.182720422744751\n",
      "[step: 3173] loss: 2.9102635383605957\n",
      "[step: 3174] loss: 2.466353416442871\n",
      "[step: 3175] loss: 2.5726592540740967\n",
      "[step: 3176] loss: 2.9449777603149414\n",
      "[step: 3177] loss: 2.7574353218078613\n",
      "[step: 3178] loss: 2.446298122406006\n",
      "[step: 3179] loss: 2.4288387298583984\n",
      "[step: 3180] loss: 2.645785331726074\n",
      "[step: 3181] loss: 2.7436704635620117\n",
      "[step: 3182] loss: 2.488255262374878\n",
      "[step: 3183] loss: 2.355625867843628\n",
      "[step: 3184] loss: 2.481323003768921\n",
      "[step: 3185] loss: 2.583223581314087\n",
      "[step: 3186] loss: 2.529229164123535\n",
      "[step: 3187] loss: 2.37092924118042\n",
      "[step: 3188] loss: 2.359157085418701\n",
      "[step: 3189] loss: 2.4670658111572266\n",
      "[step: 3190] loss: 2.4922642707824707\n",
      "[step: 3191] loss: 2.4233522415161133\n",
      "[step: 3192] loss: 2.3368053436279297\n",
      "[step: 3193] loss: 2.3478217124938965\n",
      "[step: 3194] loss: 2.4134883880615234\n",
      "[step: 3195] loss: 2.428219795227051\n",
      "[step: 3196] loss: 2.3864619731903076\n",
      "[step: 3197] loss: 2.3282041549682617\n",
      "[step: 3198] loss: 2.3206920623779297\n",
      "[step: 3199] loss: 2.3546369075775146\n",
      "[step: 3200] loss: 2.379441499710083\n",
      "[step: 3201] loss: 2.3734729290008545\n",
      "[step: 3202] loss: 2.3379180431365967\n",
      "[step: 3203] loss: 2.3102540969848633\n",
      "[step: 3204] loss: 2.306800127029419\n",
      "[step: 3205] loss: 2.319246292114258\n",
      "[step: 3206] loss: 2.3334124088287354\n",
      "[step: 3207] loss: 2.334965229034424\n",
      "[step: 3208] loss: 2.327561140060425\n",
      "[step: 3209] loss: 2.313814640045166\n",
      "[step: 3210] loss: 2.3001513481140137\n",
      "[step: 3211] loss: 2.291215419769287\n",
      "[step: 3212] loss: 2.28790283203125\n",
      "[step: 3213] loss: 2.2911906242370605\n",
      "[step: 3214] loss: 2.297471523284912\n",
      "[step: 3215] loss: 2.3033950328826904\n",
      "[step: 3216] loss: 2.305847406387329\n",
      "[step: 3217] loss: 2.3045847415924072\n",
      "[step: 3218] loss: 2.3005802631378174\n",
      "[step: 3219] loss: 2.29636287689209\n",
      "[step: 3220] loss: 2.291811943054199\n",
      "[step: 3221] loss: 2.2884984016418457\n",
      "[step: 3222] loss: 2.2845096588134766\n",
      "[step: 3223] loss: 2.281482219696045\n",
      "[step: 3224] loss: 2.278674602508545\n",
      "[step: 3225] loss: 2.277851104736328\n",
      "[step: 3226] loss: 2.2784371376037598\n",
      "[step: 3227] loss: 2.2818603515625\n",
      "[step: 3228] loss: 2.2880520820617676\n",
      "[step: 3229] loss: 2.300849676132202\n",
      "[step: 3230] loss: 2.322035312652588\n",
      "[step: 3231] loss: 2.365170478820801\n",
      "[step: 3232] loss: 2.4316916465759277\n",
      "[step: 3233] loss: 2.5723702907562256\n",
      "[step: 3234] loss: 2.7568113803863525\n",
      "[step: 3235] loss: 3.1541953086853027\n",
      "[step: 3236] loss: 3.453073740005493\n",
      "[step: 3237] loss: 4.03048038482666\n",
      "[step: 3238] loss: 3.7936272621154785\n",
      "[step: 3239] loss: 3.4743103981018066\n",
      "[step: 3240] loss: 2.7310690879821777\n",
      "[step: 3241] loss: 2.3210480213165283\n",
      "[step: 3242] loss: 2.427879571914673\n",
      "[step: 3243] loss: 2.758404016494751\n",
      "[step: 3244] loss: 2.9507038593292236\n",
      "[step: 3245] loss: 2.632145404815674\n",
      "[step: 3246] loss: 2.319817543029785\n",
      "[step: 3247] loss: 2.3307487964630127\n",
      "[step: 3248] loss: 2.5443615913391113\n",
      "[step: 3249] loss: 2.6494531631469727\n",
      "[step: 3250] loss: 2.443629503250122\n",
      "[step: 3251] loss: 2.265672445297241\n",
      "[step: 3252] loss: 2.3038320541381836\n",
      "[step: 3253] loss: 2.4467334747314453\n",
      "[step: 3254] loss: 2.4940099716186523\n",
      "[step: 3255] loss: 2.360461711883545\n",
      "[step: 3256] loss: 2.2549588680267334\n",
      "[step: 3257] loss: 2.2707114219665527\n",
      "[step: 3258] loss: 2.3492484092712402\n",
      "[step: 3259] loss: 2.380801200866699\n",
      "[step: 3260] loss: 2.3223228454589844\n",
      "[step: 3261] loss: 2.267289638519287\n",
      "[step: 3262] loss: 2.2559216022491455\n",
      "[step: 3263] loss: 2.277527332305908\n",
      "[step: 3264] loss: 2.291156053543091\n",
      "[step: 3265] loss: 2.2818965911865234\n",
      "[step: 3266] loss: 2.2709031105041504\n",
      "[step: 3267] loss: 2.260861873626709\n",
      "[step: 3268] loss: 2.2515311241149902\n",
      "[step: 3269] loss: 2.2373404502868652\n",
      "[step: 3270] loss: 2.2290472984313965\n",
      "[step: 3271] loss: 2.234842300415039\n",
      "[step: 3272] loss: 2.247795581817627\n",
      "[step: 3273] loss: 2.2541279792785645\n",
      "[step: 3274] loss: 2.2405152320861816\n",
      "[step: 3275] loss: 2.21962833404541\n",
      "[step: 3276] loss: 2.2051634788513184\n",
      "[step: 3277] loss: 2.2045958042144775\n",
      "[step: 3278] loss: 2.2112021446228027\n",
      "[step: 3279] loss: 2.213974952697754\n",
      "[step: 3280] loss: 2.210733413696289\n",
      "[step: 3281] loss: 2.2062435150146484\n",
      "[step: 3282] loss: 2.206740379333496\n",
      "[step: 3283] loss: 2.2114009857177734\n",
      "[step: 3284] loss: 2.2161684036254883\n",
      "[step: 3285] loss: 2.216952323913574\n",
      "[step: 3286] loss: 2.215367555618286\n",
      "[step: 3287] loss: 2.2147562503814697\n",
      "[step: 3288] loss: 2.2204389572143555\n",
      "[step: 3289] loss: 2.2294068336486816\n",
      "[step: 3290] loss: 2.246914863586426\n",
      "[step: 3291] loss: 2.264881134033203\n",
      "[step: 3292] loss: 2.3040103912353516\n",
      "[step: 3293] loss: 2.351404905319214\n",
      "[step: 3294] loss: 2.4574098587036133\n",
      "[step: 3295] loss: 2.573566436767578\n",
      "[step: 3296] loss: 2.8176097869873047\n",
      "[step: 3297] loss: 2.98344087600708\n",
      "[step: 3298] loss: 3.3298654556274414\n",
      "[step: 3299] loss: 3.240490198135376\n",
      "[step: 3300] loss: 3.205777168273926\n",
      "[step: 3301] loss: 2.712851047515869\n",
      "[step: 3302] loss: 2.349282741546631\n",
      "[step: 3303] loss: 2.1812381744384766\n",
      "[step: 3304] loss: 2.2731332778930664\n",
      "[step: 3305] loss: 2.498124837875366\n",
      "[step: 3306] loss: 2.582479476928711\n",
      "[step: 3307] loss: 2.530033826828003\n",
      "[step: 3308] loss: 2.311878204345703\n",
      "[step: 3309] loss: 2.1806507110595703\n",
      "[step: 3310] loss: 2.2057862281799316\n",
      "[step: 3311] loss: 2.319685935974121\n",
      "[step: 3312] loss: 2.412201404571533\n",
      "[step: 3313] loss: 2.3653388023376465\n",
      "[step: 3314] loss: 2.2695086002349854\n",
      "[step: 3315] loss: 2.1805076599121094\n",
      "[step: 3316] loss: 2.166968822479248\n",
      "[step: 3317] loss: 2.2149181365966797\n",
      "[step: 3318] loss: 2.2675015926361084\n",
      "[step: 3319] loss: 2.2901194095611572\n",
      "[step: 3320] loss: 2.255125045776367\n",
      "[step: 3321] loss: 2.2110238075256348\n",
      "[step: 3322] loss: 2.171627998352051\n",
      "[step: 3323] loss: 2.1580684185028076\n",
      "[step: 3324] loss: 2.1663031578063965\n",
      "[step: 3325] loss: 2.184652805328369\n",
      "[step: 3326] loss: 2.2035436630249023\n",
      "[step: 3327] loss: 2.210453987121582\n",
      "[step: 3328] loss: 2.2123396396636963\n",
      "[step: 3329] loss: 2.196830987930298\n",
      "[step: 3330] loss: 2.1784040927886963\n",
      "[step: 3331] loss: 2.15624737739563\n",
      "[step: 3332] loss: 2.1406373977661133\n",
      "[step: 3333] loss: 2.1330504417419434\n",
      "[step: 3334] loss: 2.1330254077911377\n",
      "[step: 3335] loss: 2.137531042098999\n",
      "[step: 3336] loss: 2.14285945892334\n",
      "[step: 3337] loss: 2.1482901573181152\n",
      "[step: 3338] loss: 2.1530332565307617\n",
      "[step: 3339] loss: 2.1618337631225586\n",
      "[step: 3340] loss: 2.1730830669403076\n",
      "[step: 3341] loss: 2.1942665576934814\n",
      "[step: 3342] loss: 2.2208075523376465\n",
      "[step: 3343] loss: 2.2681708335876465\n",
      "[step: 3344] loss: 2.3258376121520996\n",
      "[step: 3345] loss: 2.4379477500915527\n",
      "[step: 3346] loss: 2.554425001144409\n",
      "[step: 3347] loss: 2.7945523262023926\n",
      "[step: 3348] loss: 2.9266486167907715\n",
      "[step: 3349] loss: 3.2135066986083984\n",
      "[step: 3350] loss: 3.0791893005371094\n",
      "[step: 3351] loss: 2.9803571701049805\n",
      "[step: 3352] loss: 2.5671255588531494\n",
      "[step: 3353] loss: 2.2635610103607178\n",
      "[step: 3354] loss: 2.135268211364746\n",
      "[step: 3355] loss: 2.208343982696533\n",
      "[step: 3356] loss: 2.3893520832061768\n",
      "[step: 3357] loss: 2.4677281379699707\n",
      "[step: 3358] loss: 2.4453816413879395\n",
      "[step: 3359] loss: 2.278742551803589\n",
      "[step: 3360] loss: 2.1484427452087402\n",
      "[step: 3361] loss: 2.1263718605041504\n",
      "[step: 3362] loss: 2.197859764099121\n",
      "[step: 3363] loss: 2.293957233428955\n",
      "[step: 3364] loss: 2.3090476989746094\n",
      "[step: 3365] loss: 2.2680749893188477\n",
      "[step: 3366] loss: 2.1762022972106934\n",
      "[step: 3367] loss: 2.112631320953369\n",
      "[step: 3368] loss: 2.103818655014038\n",
      "[step: 3369] loss: 2.13992977142334\n",
      "[step: 3370] loss: 2.1888256072998047\n",
      "[step: 3371] loss: 2.210137367248535\n",
      "[step: 3372] loss: 2.2072324752807617\n",
      "[step: 3373] loss: 2.1697776317596436\n",
      "[step: 3374] loss: 2.1326751708984375\n",
      "[step: 3375] loss: 2.1036205291748047\n",
      "[step: 3376] loss: 2.092855215072632\n",
      "[step: 3377] loss: 2.0965869426727295\n",
      "[step: 3378] loss: 2.108626127243042\n",
      "[step: 3379] loss: 2.1236305236816406\n",
      "[step: 3380] loss: 2.135345697402954\n",
      "[step: 3381] loss: 2.1467080116271973\n",
      "[step: 3382] loss: 2.1491193771362305\n",
      "[step: 3383] loss: 2.1536548137664795\n",
      "[step: 3384] loss: 2.1479897499084473\n",
      "[step: 3385] loss: 2.1460912227630615\n",
      "[step: 3386] loss: 2.136749505996704\n",
      "[step: 3387] loss: 2.1329684257507324\n",
      "[step: 3388] loss: 2.126555919647217\n",
      "[step: 3389] loss: 2.1273341178894043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3390] loss: 2.1287012100219727\n",
      "[step: 3391] loss: 2.137744188308716\n",
      "[step: 3392] loss: 2.149014949798584\n",
      "[step: 3393] loss: 2.1722428798675537\n",
      "[step: 3394] loss: 2.2009294033050537\n",
      "[step: 3395] loss: 2.2573421001434326\n",
      "[step: 3396] loss: 2.3227639198303223\n",
      "[step: 3397] loss: 2.455472946166992\n",
      "[step: 3398] loss: 2.574795722961426\n",
      "[step: 3399] loss: 2.8221776485443115\n",
      "[step: 3400] loss: 2.910228729248047\n",
      "[step: 3401] loss: 3.1120858192443848\n",
      "[step: 3402] loss: 2.912558078765869\n",
      "[step: 3403] loss: 2.7237627506256104\n",
      "[step: 3404] loss: 2.3706772327423096\n",
      "[step: 3405] loss: 2.1397883892059326\n",
      "[step: 3406] loss: 2.0885348320007324\n",
      "[step: 3407] loss: 2.190603256225586\n",
      "[step: 3408] loss: 2.3528976440429688\n",
      "[step: 3409] loss: 2.3872361183166504\n",
      "[step: 3410] loss: 2.335844039916992\n",
      "[step: 3411] loss: 2.1845669746398926\n",
      "[step: 3412] loss: 2.0808520317077637\n",
      "[step: 3413] loss: 2.0735340118408203\n",
      "[step: 3414] loss: 2.1405177116394043\n",
      "[step: 3415] loss: 2.2256765365600586\n",
      "[step: 3416] loss: 2.2403388023376465\n",
      "[step: 3417] loss: 2.2111194133758545\n",
      "[step: 3418] loss: 2.129791259765625\n",
      "[step: 3419] loss: 2.0672709941864014\n",
      "[step: 3420] loss: 2.045402765274048\n",
      "[step: 3421] loss: 2.0647616386413574\n",
      "[step: 3422] loss: 2.104752540588379\n",
      "[step: 3423] loss: 2.1352739334106445\n",
      "[step: 3424] loss: 2.151604652404785\n",
      "[step: 3425] loss: 2.134070873260498\n",
      "[step: 3426] loss: 2.109649658203125\n",
      "[step: 3427] loss: 2.076420783996582\n",
      "[step: 3428] loss: 2.052868127822876\n",
      "[step: 3429] loss: 2.038942337036133\n",
      "[step: 3430] loss: 2.0355112552642822\n",
      "[step: 3431] loss: 2.039806842803955\n",
      "[step: 3432] loss: 2.048760175704956\n",
      "[step: 3433] loss: 2.060788154602051\n",
      "[step: 3434] loss: 2.0727415084838867\n",
      "[step: 3435] loss: 2.0885701179504395\n",
      "[step: 3436] loss: 2.100691795349121\n",
      "[step: 3437] loss: 2.120988368988037\n",
      "[step: 3438] loss: 2.1339709758758545\n",
      "[step: 3439] loss: 2.1623804569244385\n",
      "[step: 3440] loss: 2.1805906295776367\n",
      "[step: 3441] loss: 2.2270700931549072\n",
      "[step: 3442] loss: 2.2596588134765625\n",
      "[step: 3443] loss: 2.338782548904419\n",
      "[step: 3444] loss: 2.3898491859436035\n",
      "[step: 3445] loss: 2.501979351043701\n",
      "[step: 3446] loss: 2.5443906784057617\n",
      "[step: 3447] loss: 2.6325104236602783\n",
      "[step: 3448] loss: 2.593111276626587\n",
      "[step: 3449] loss: 2.5490002632141113\n",
      "[step: 3450] loss: 2.3955800533294678\n",
      "[step: 3451] loss: 2.239114999771118\n",
      "[step: 3452] loss: 2.097032070159912\n",
      "[step: 3453] loss: 2.0208091735839844\n",
      "[step: 3454] loss: 2.0209097862243652\n",
      "[step: 3455] loss: 2.0761208534240723\n",
      "[step: 3456] loss: 2.1503310203552246\n",
      "[step: 3457] loss: 2.195359468460083\n",
      "[step: 3458] loss: 2.206531524658203\n",
      "[step: 3459] loss: 2.1639785766601562\n",
      "[step: 3460] loss: 2.111978530883789\n",
      "[step: 3461] loss: 2.057124614715576\n",
      "[step: 3462] loss: 2.0276877880096436\n",
      "[step: 3463] loss: 2.019634246826172\n",
      "[step: 3464] loss: 2.026340961456299\n",
      "[step: 3465] loss: 2.0354647636413574\n",
      "[step: 3466] loss: 2.041830062866211\n",
      "[step: 3467] loss: 2.043405055999756\n",
      "[step: 3468] loss: 2.0446085929870605\n",
      "[step: 3469] loss: 2.0528340339660645\n",
      "[step: 3470] loss: 2.0640175342559814\n",
      "[step: 3471] loss: 2.087860584259033\n",
      "[step: 3472] loss: 2.1052701473236084\n",
      "[step: 3473] loss: 2.1338930130004883\n",
      "[step: 3474] loss: 2.1433050632476807\n",
      "[step: 3475] loss: 2.1712746620178223\n",
      "[step: 3476] loss: 2.1792378425598145\n",
      "[step: 3477] loss: 2.227344512939453\n",
      "[step: 3478] loss: 2.2590763568878174\n",
      "[step: 3479] loss: 2.3446617126464844\n",
      "[step: 3480] loss: 2.397027015686035\n",
      "[step: 3481] loss: 2.491501808166504\n",
      "[step: 3482] loss: 2.5063586235046387\n",
      "[step: 3483] loss: 2.5337533950805664\n",
      "[step: 3484] loss: 2.4393091201782227\n",
      "[step: 3485] loss: 2.3493258953094482\n",
      "[step: 3486] loss: 2.1931331157684326\n",
      "[step: 3487] loss: 2.079902410507202\n",
      "[step: 3488] loss: 2.004254102706909\n",
      "[step: 3489] loss: 1.9869328737258911\n",
      "[step: 3490] loss: 2.012876510620117\n",
      "[step: 3491] loss: 2.059892177581787\n",
      "[step: 3492] loss: 2.109907627105713\n",
      "[step: 3493] loss: 2.135129690170288\n",
      "[step: 3494] loss: 2.153923511505127\n",
      "[step: 3495] loss: 2.129549026489258\n",
      "[step: 3496] loss: 2.1079025268554688\n",
      "[step: 3497] loss: 2.0618488788604736\n",
      "[step: 3498] loss: 2.0258078575134277\n",
      "[step: 3499] loss: 1.9922205209732056\n",
      "[step: 3500] loss: 1.9716694355010986\n",
      "[step: 3501] loss: 1.962931513786316\n",
      "[step: 3502] loss: 1.9645497798919678\n",
      "[step: 3503] loss: 1.973438024520874\n",
      "[step: 3504] loss: 1.9858975410461426\n",
      "[step: 3505] loss: 2.002113103866577\n",
      "[step: 3506] loss: 2.017904758453369\n",
      "[step: 3507] loss: 2.043142557144165\n",
      "[step: 3508] loss: 2.069284200668335\n",
      "[step: 3509] loss: 2.1202259063720703\n",
      "[step: 3510] loss: 2.174654006958008\n",
      "[step: 3511] loss: 2.282674789428711\n",
      "[step: 3512] loss: 2.383751392364502\n",
      "[step: 3513] loss: 2.582402229309082\n",
      "[step: 3514] loss: 2.690207004547119\n",
      "[step: 3515] loss: 2.900722026824951\n",
      "[step: 3516] loss: 2.813537836074829\n",
      "[step: 3517] loss: 2.7383806705474854\n",
      "[step: 3518] loss: 2.4054818153381348\n",
      "[step: 3519] loss: 2.134932279586792\n",
      "[step: 3520] loss: 1.973463773727417\n",
      "[step: 3521] loss: 1.983877182006836\n",
      "[step: 3522] loss: 2.1140871047973633\n",
      "[step: 3523] loss: 2.2299275398254395\n",
      "[step: 3524] loss: 2.2794265747070312\n",
      "[step: 3525] loss: 2.177318572998047\n",
      "[step: 3526] loss: 2.050283193588257\n",
      "[step: 3527] loss: 1.9600653648376465\n",
      "[step: 3528] loss: 1.9574817419052124\n",
      "[step: 3529] loss: 2.020371913909912\n",
      "[step: 3530] loss: 2.0868172645568848\n",
      "[step: 3531] loss: 2.124009370803833\n",
      "[step: 3532] loss: 2.090895175933838\n",
      "[step: 3533] loss: 2.037173271179199\n",
      "[step: 3534] loss: 1.9770663976669312\n",
      "[step: 3535] loss: 1.946936011314392\n",
      "[step: 3536] loss: 1.9474118947982788\n",
      "[step: 3537] loss: 1.96768319606781\n",
      "[step: 3538] loss: 1.9926750659942627\n",
      "[step: 3539] loss: 2.0092692375183105\n",
      "[step: 3540] loss: 2.019318103790283\n",
      "[step: 3541] loss: 2.013803482055664\n",
      "[step: 3542] loss: 2.009660482406616\n",
      "[step: 3543] loss: 1.994955062866211\n",
      "[step: 3544] loss: 1.9829425811767578\n",
      "[step: 3545] loss: 1.965133547782898\n",
      "[step: 3546] loss: 1.9494068622589111\n",
      "[step: 3547] loss: 1.9349063634872437\n",
      "[step: 3548] loss: 1.9254281520843506\n",
      "[step: 3549] loss: 1.9210779666900635\n",
      "[step: 3550] loss: 1.9208145141601562\n",
      "[step: 3551] loss: 1.922638177871704\n",
      "[step: 3552] loss: 1.9245471954345703\n",
      "[step: 3553] loss: 1.9256088733673096\n",
      "[step: 3554] loss: 1.925798773765564\n",
      "[step: 3555] loss: 1.9272136688232422\n",
      "[step: 3556] loss: 1.9313161373138428\n",
      "[step: 3557] loss: 1.9424569606781006\n",
      "[step: 3558] loss: 1.9629143476486206\n",
      "[step: 3559] loss: 2.0046374797821045\n",
      "[step: 3560] loss: 2.074003219604492\n",
      "[step: 3561] loss: 2.2167816162109375\n",
      "[step: 3562] loss: 2.4306235313415527\n",
      "[step: 3563] loss: 2.8853607177734375\n",
      "[step: 3564] loss: 3.3142213821411133\n",
      "[step: 3565] loss: 4.134777069091797\n",
      "[step: 3566] loss: 3.8750267028808594\n",
      "[step: 3567] loss: 3.5108561515808105\n",
      "[step: 3568] loss: 2.472551107406616\n",
      "[step: 3569] loss: 1.9943124055862427\n",
      "[step: 3570] loss: 2.2683722972869873\n",
      "[step: 3571] loss: 2.691495895385742\n",
      "[step: 3572] loss: 2.796339511871338\n",
      "[step: 3573] loss: 2.2166249752044678\n",
      "[step: 3574] loss: 1.9606990814208984\n",
      "[step: 3575] loss: 2.2372045516967773\n",
      "[step: 3576] loss: 2.4369969367980957\n",
      "[step: 3577] loss: 2.297835111618042\n",
      "[step: 3578] loss: 1.9699031114578247\n",
      "[step: 3579] loss: 1.998867392539978\n",
      "[step: 3580] loss: 2.244213104248047\n",
      "[step: 3581] loss: 2.2302298545837402\n",
      "[step: 3582] loss: 2.040168046951294\n",
      "[step: 3583] loss: 1.9334615468978882\n",
      "[step: 3584] loss: 2.036604642868042\n",
      "[step: 3585] loss: 2.1399660110473633\n",
      "[step: 3586] loss: 2.0633087158203125\n",
      "[step: 3587] loss: 1.9564565420150757\n",
      "[step: 3588] loss: 1.9516091346740723\n",
      "[step: 3589] loss: 2.018521785736084\n",
      "[step: 3590] loss: 2.0376405715942383\n",
      "[step: 3591] loss: 1.9754951000213623\n",
      "[step: 3592] loss: 1.9287620782852173\n",
      "[step: 3593] loss: 1.9412968158721924\n",
      "[step: 3594] loss: 1.9763911962509155\n",
      "[step: 3595] loss: 1.9760117530822754\n",
      "[step: 3596] loss: 1.9389269351959229\n",
      "[step: 3597] loss: 1.914297103881836\n",
      "[step: 3598] loss: 1.9182384014129639\n",
      "[step: 3599] loss: 1.9349894523620605\n",
      "[step: 3600] loss: 1.935587763786316\n",
      "[step: 3601] loss: 1.9205491542816162\n",
      "[step: 3602] loss: 1.9073044061660767\n",
      "[step: 3603] loss: 1.9043859243392944\n",
      "[step: 3604] loss: 1.9084937572479248\n",
      "[step: 3605] loss: 1.9064149856567383\n",
      "[step: 3606] loss: 1.899260401725769\n",
      "[step: 3607] loss: 1.8934625387191772\n",
      "[step: 3608] loss: 1.894616723060608\n",
      "[step: 3609] loss: 1.8987754583358765\n",
      "[step: 3610] loss: 1.8969159126281738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3611] loss: 1.8885340690612793\n",
      "[step: 3612] loss: 1.8785641193389893\n",
      "[step: 3613] loss: 1.874624252319336\n",
      "[step: 3614] loss: 1.8777655363082886\n",
      "[step: 3615] loss: 1.8829854726791382\n",
      "[step: 3616] loss: 1.8846584558486938\n",
      "[step: 3617] loss: 1.881028413772583\n",
      "[step: 3618] loss: 1.8757798671722412\n",
      "[step: 3619] loss: 1.8724766969680786\n",
      "[step: 3620] loss: 1.8721874952316284\n",
      "[step: 3621] loss: 1.8728673458099365\n",
      "[step: 3622] loss: 1.8719844818115234\n",
      "[step: 3623] loss: 1.8689837455749512\n",
      "[step: 3624] loss: 1.865196943283081\n",
      "[step: 3625] loss: 1.862461805343628\n",
      "[step: 3626] loss: 1.8616725206375122\n",
      "[step: 3627] loss: 1.8619136810302734\n",
      "[step: 3628] loss: 1.8622143268585205\n",
      "[step: 3629] loss: 1.861675500869751\n",
      "[step: 3630] loss: 1.8609825372695923\n",
      "[step: 3631] loss: 1.8610345125198364\n",
      "[step: 3632] loss: 1.8635826110839844\n",
      "[step: 3633] loss: 1.869590401649475\n",
      "[step: 3634] loss: 1.8817903995513916\n",
      "[step: 3635] loss: 1.902742624282837\n",
      "[step: 3636] loss: 1.943650722503662\n",
      "[step: 3637] loss: 2.0120959281921387\n",
      "[step: 3638] loss: 2.156388998031616\n",
      "[step: 3639] loss: 2.3655271530151367\n",
      "[step: 3640] loss: 2.821028232574463\n",
      "[step: 3641] loss: 3.2036197185516357\n",
      "[step: 3642] loss: 3.9690163135528564\n",
      "[step: 3643] loss: 3.6596219539642334\n",
      "[step: 3644] loss: 3.2952678203582764\n",
      "[step: 3645] loss: 2.3661394119262695\n",
      "[step: 3646] loss: 1.939450740814209\n",
      "[step: 3647] loss: 2.1663551330566406\n",
      "[step: 3648] loss: 2.54164457321167\n",
      "[step: 3649] loss: 2.6632347106933594\n",
      "[step: 3650] loss: 2.1885764598846436\n",
      "[step: 3651] loss: 1.9197498559951782\n",
      "[step: 3652] loss: 2.09822940826416\n",
      "[step: 3653] loss: 2.3026041984558105\n",
      "[step: 3654] loss: 2.2502992153167725\n",
      "[step: 3655] loss: 1.9564579725265503\n",
      "[step: 3656] loss: 1.9040251970291138\n",
      "[step: 3657] loss: 2.0958962440490723\n",
      "[step: 3658] loss: 2.1604347229003906\n",
      "[step: 3659] loss: 2.036677360534668\n",
      "[step: 3660] loss: 1.8726752996444702\n",
      "[step: 3661] loss: 1.9004855155944824\n",
      "[step: 3662] loss: 2.033102035522461\n",
      "[step: 3663] loss: 2.045999526977539\n",
      "[step: 3664] loss: 1.9505245685577393\n",
      "[step: 3665] loss: 1.8568297624588013\n",
      "[step: 3666] loss: 1.877143144607544\n",
      "[step: 3667] loss: 1.9554554224014282\n",
      "[step: 3668] loss: 1.9714975357055664\n",
      "[step: 3669] loss: 1.923938274383545\n",
      "[step: 3670] loss: 1.859037160873413\n",
      "[step: 3671] loss: 1.8473480939865112\n",
      "[step: 3672] loss: 1.8820984363555908\n",
      "[step: 3673] loss: 1.9102199077606201\n",
      "[step: 3674] loss: 1.9078686237335205\n",
      "[step: 3675] loss: 1.8708688020706177\n",
      "[step: 3676] loss: 1.8400638103485107\n",
      "[step: 3677] loss: 1.8345286846160889\n",
      "[step: 3678] loss: 1.8506300449371338\n",
      "[step: 3679] loss: 1.8703789710998535\n",
      "[step: 3680] loss: 1.8717516660690308\n",
      "[step: 3681] loss: 1.8589814901351929\n",
      "[step: 3682] loss: 1.8378219604492188\n",
      "[step: 3683] loss: 1.8245352506637573\n",
      "[step: 3684] loss: 1.824145793914795\n",
      "[step: 3685] loss: 1.8324759006500244\n",
      "[step: 3686] loss: 1.841862440109253\n",
      "[step: 3687] loss: 1.8446502685546875\n",
      "[step: 3688] loss: 1.8423831462860107\n",
      "[step: 3689] loss: 1.8350861072540283\n",
      "[step: 3690] loss: 1.8275377750396729\n",
      "[step: 3691] loss: 1.8208298683166504\n",
      "[step: 3692] loss: 1.815922498703003\n",
      "[step: 3693] loss: 1.8130940198898315\n",
      "[step: 3694] loss: 1.8125498294830322\n",
      "[step: 3695] loss: 1.8140043020248413\n",
      "[step: 3696] loss: 1.8166842460632324\n",
      "[step: 3697] loss: 1.8196839094161987\n",
      "[step: 3698] loss: 1.822130799293518\n",
      "[step: 3699] loss: 1.8243517875671387\n",
      "[step: 3700] loss: 1.826162576675415\n",
      "[step: 3701] loss: 1.8293085098266602\n",
      "[step: 3702] loss: 1.8330001831054688\n",
      "[step: 3703] loss: 1.840020775794983\n",
      "[step: 3704] loss: 1.8481309413909912\n",
      "[step: 3705] loss: 1.8632779121398926\n",
      "[step: 3706] loss: 1.8818080425262451\n",
      "[step: 3707] loss: 1.9171550273895264\n",
      "[step: 3708] loss: 1.960947871208191\n",
      "[step: 3709] loss: 2.04349946975708\n",
      "[step: 3710] loss: 2.1365883350372314\n",
      "[step: 3711] loss: 2.310972213745117\n",
      "[step: 3712] loss: 2.4531564712524414\n",
      "[step: 3713] loss: 2.7070953845977783\n",
      "[step: 3714] loss: 2.737001657485962\n",
      "[step: 3715] loss: 2.7940337657928467\n",
      "[step: 3716] loss: 2.489588737487793\n",
      "[step: 3717] loss: 2.1868948936462402\n",
      "[step: 3718] loss: 1.8985929489135742\n",
      "[step: 3719] loss: 1.8029369115829468\n",
      "[step: 3720] loss: 1.8958133459091187\n",
      "[step: 3721] loss: 2.0532686710357666\n",
      "[step: 3722] loss: 2.1580843925476074\n",
      "[step: 3723] loss: 2.078868865966797\n",
      "[step: 3724] loss: 1.9359785318374634\n",
      "[step: 3725] loss: 1.8162024021148682\n",
      "[step: 3726] loss: 1.806983470916748\n",
      "[step: 3727] loss: 1.883039951324463\n",
      "[step: 3728] loss: 1.9573301076889038\n",
      "[step: 3729] loss: 1.9796991348266602\n",
      "[step: 3730] loss: 1.9200016260147095\n",
      "[step: 3731] loss: 1.849118947982788\n",
      "[step: 3732] loss: 1.802359700202942\n",
      "[step: 3733] loss: 1.8030295372009277\n",
      "[step: 3734] loss: 1.8323500156402588\n",
      "[step: 3735] loss: 1.8612065315246582\n",
      "[step: 3736] loss: 1.8752131462097168\n",
      "[step: 3737] loss: 1.8643269538879395\n",
      "[step: 3738] loss: 1.8480706214904785\n",
      "[step: 3739] loss: 1.8242570161819458\n",
      "[step: 3740] loss: 1.8060213327407837\n",
      "[step: 3741] loss: 1.7921442985534668\n",
      "[step: 3742] loss: 1.785845398902893\n",
      "[step: 3743] loss: 1.788733720779419\n",
      "[step: 3744] loss: 1.7993136644363403\n",
      "[step: 3745] loss: 1.813821792602539\n",
      "[step: 3746] loss: 1.8227813243865967\n",
      "[step: 3747] loss: 1.8280171155929565\n",
      "[step: 3748] loss: 1.8230760097503662\n",
      "[step: 3749] loss: 1.8189846277236938\n",
      "[step: 3750] loss: 1.8131667375564575\n",
      "[step: 3751] loss: 1.8124784231185913\n",
      "[step: 3752] loss: 1.8119773864746094\n",
      "[step: 3753] loss: 1.8127633333206177\n",
      "[step: 3754] loss: 1.8122973442077637\n",
      "[step: 3755] loss: 1.813417673110962\n",
      "[step: 3756] loss: 1.8162589073181152\n",
      "[step: 3757] loss: 1.8269648551940918\n",
      "[step: 3758] loss: 1.841607928276062\n",
      "[step: 3759] loss: 1.8719828128814697\n",
      "[step: 3760] loss: 1.9064518213272095\n",
      "[step: 3761] loss: 1.9766111373901367\n",
      "[step: 3762] loss: 2.051844358444214\n",
      "[step: 3763] loss: 2.2081291675567627\n",
      "[step: 3764] loss: 2.344879150390625\n",
      "[step: 3765] loss: 2.6145071983337402\n",
      "[step: 3766] loss: 2.714298725128174\n",
      "[step: 3767] loss: 2.8900251388549805\n",
      "[step: 3768] loss: 2.6638896465301514\n",
      "[step: 3769] loss: 2.404177188873291\n",
      "[step: 3770] loss: 2.016735553741455\n",
      "[step: 3771] loss: 1.7933253049850464\n",
      "[step: 3772] loss: 1.796372652053833\n",
      "[step: 3773] loss: 1.9569810628890991\n",
      "[step: 3774] loss: 2.1278200149536133\n",
      "[step: 3775] loss: 2.1166114807128906\n",
      "[step: 3776] loss: 1.991242527961731\n",
      "[step: 3777] loss: 1.8230788707733154\n",
      "[step: 3778] loss: 1.7632911205291748\n",
      "[step: 3779] loss: 1.8222641944885254\n",
      "[step: 3780] loss: 1.915681004524231\n",
      "[step: 3781] loss: 1.9626541137695312\n",
      "[step: 3782] loss: 1.91300630569458\n",
      "[step: 3783] loss: 1.8387099504470825\n",
      "[step: 3784] loss: 1.7809587717056274\n",
      "[step: 3785] loss: 1.7758150100708008\n",
      "[step: 3786] loss: 1.8027923107147217\n",
      "[step: 3787] loss: 1.829035758972168\n",
      "[step: 3788] loss: 1.8377021551132202\n",
      "[step: 3789] loss: 1.8255774974822998\n",
      "[step: 3790] loss: 1.8136794567108154\n",
      "[step: 3791] loss: 1.7972978353500366\n",
      "[step: 3792] loss: 1.783414363861084\n",
      "[step: 3793] loss: 1.7675373554229736\n",
      "[step: 3794] loss: 1.7557549476623535\n",
      "[step: 3795] loss: 1.7545889616012573\n",
      "[step: 3796] loss: 1.7653361558914185\n",
      "[step: 3797] loss: 1.7817301750183105\n",
      "[step: 3798] loss: 1.7897683382034302\n",
      "[step: 3799] loss: 1.7890292406082153\n",
      "[step: 3800] loss: 1.7769352197647095\n",
      "[step: 3801] loss: 1.766600489616394\n",
      "[step: 3802] loss: 1.7602360248565674\n",
      "[step: 3803] loss: 1.7599067687988281\n",
      "[step: 3804] loss: 1.7598261833190918\n",
      "[step: 3805] loss: 1.7564902305603027\n",
      "[step: 3806] loss: 1.7503668069839478\n",
      "[step: 3807] loss: 1.7443453073501587\n",
      "[step: 3808] loss: 1.7414345741271973\n",
      "[step: 3809] loss: 1.7428394556045532\n",
      "[step: 3810] loss: 1.746089220046997\n",
      "[step: 3811] loss: 1.7505881786346436\n",
      "[step: 3812] loss: 1.7547783851623535\n",
      "[step: 3813] loss: 1.764161229133606\n",
      "[step: 3814] loss: 1.7801958322525024\n",
      "[step: 3815] loss: 1.8157024383544922\n",
      "[step: 3816] loss: 1.8719770908355713\n",
      "[step: 3817] loss: 1.9844059944152832\n",
      "[step: 3818] loss: 2.1439356803894043\n",
      "[step: 3819] loss: 2.4703598022460938\n",
      "[step: 3820] loss: 2.794748067855835\n",
      "[step: 3821] loss: 3.4307425022125244\n",
      "[step: 3822] loss: 3.419816255569458\n",
      "[step: 3823] loss: 3.4327006340026855\n",
      "[step: 3824] loss: 2.545599937438965\n",
      "[step: 3825] loss: 1.9106907844543457\n",
      "[step: 3826] loss: 1.780305027961731\n",
      "[step: 3827] loss: 2.113699436187744\n",
      "[step: 3828] loss: 2.505272388458252\n",
      "[step: 3829] loss: 2.308565616607666\n",
      "[step: 3830] loss: 1.9315626621246338\n",
      "[step: 3831] loss: 1.7551778554916382\n",
      "[step: 3832] loss: 1.9405604600906372\n",
      "[step: 3833] loss: 2.179631233215332\n",
      "[step: 3834] loss: 2.0570006370544434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 3835] loss: 1.8211990594863892\n",
      "[step: 3836] loss: 1.7447521686553955\n",
      "[step: 3837] loss: 1.8790748119354248\n",
      "[step: 3838] loss: 2.0087389945983887\n",
      "[step: 3839] loss: 1.9201823472976685\n",
      "[step: 3840] loss: 1.7790508270263672\n",
      "[step: 3841] loss: 1.7359586954116821\n",
      "[step: 3842] loss: 1.8116949796676636\n",
      "[step: 3843] loss: 1.8877248764038086\n",
      "[step: 3844] loss: 1.8506908416748047\n",
      "[step: 3845] loss: 1.7734999656677246\n",
      "[step: 3846] loss: 1.7329208850860596\n",
      "[step: 3847] loss: 1.7576920986175537\n",
      "[step: 3848] loss: 1.799422025680542\n",
      "[step: 3849] loss: 1.8035669326782227\n",
      "[step: 3850] loss: 1.7773383855819702\n",
      "[step: 3851] loss: 1.7424153089523315\n",
      "[step: 3852] loss: 1.7295236587524414\n",
      "[step: 3853] loss: 1.7366812229156494\n",
      "[step: 3854] loss: 1.7499874830245972\n",
      "[step: 3855] loss: 1.7563269138336182\n",
      "[step: 3856] loss: 1.7506946325302124\n",
      "[step: 3857] loss: 1.7405335903167725\n",
      "[step: 3858] loss: 1.7252981662750244\n",
      "[step: 3859] loss: 1.714186191558838\n",
      "[step: 3860] loss: 1.7107698917388916\n",
      "[step: 3861] loss: 1.7162867784500122\n",
      "[step: 3862] loss: 1.7263119220733643\n",
      "[step: 3863] loss: 1.7311960458755493\n",
      "[step: 3864] loss: 1.7288554906845093\n",
      "[step: 3865] loss: 1.717638373374939\n",
      "[step: 3866] loss: 1.705822467803955\n",
      "[step: 3867] loss: 1.6987359523773193\n",
      "[step: 3868] loss: 1.6977343559265137\n",
      "[step: 3869] loss: 1.699722409248352\n",
      "[step: 3870] loss: 1.7015151977539062\n",
      "[step: 3871] loss: 1.701857089996338\n",
      "[step: 3872] loss: 1.7014926671981812\n",
      "[step: 3873] loss: 1.702637791633606\n",
      "[step: 3874] loss: 1.7050464153289795\n",
      "[step: 3875] loss: 1.7081100940704346\n",
      "[step: 3876] loss: 1.7102900743484497\n",
      "[step: 3877] loss: 1.7118743658065796\n",
      "[step: 3878] loss: 1.7133996486663818\n",
      "[step: 3879] loss: 1.7174526453018188\n",
      "[step: 3880] loss: 1.7235605716705322\n",
      "[step: 3881] loss: 1.7361109256744385\n",
      "[step: 3882] loss: 1.7516593933105469\n",
      "[step: 3883] loss: 1.7809550762176514\n",
      "[step: 3884] loss: 1.8160349130630493\n",
      "[step: 3885] loss: 1.884626030921936\n",
      "[step: 3886] loss: 1.963306188583374\n",
      "[step: 3887] loss: 2.11846923828125\n",
      "[step: 3888] loss: 2.2625436782836914\n",
      "[step: 3889] loss: 2.532841205596924\n",
      "[step: 3890] loss: 2.633770227432251\n",
      "[step: 3891] loss: 2.804858446121216\n",
      "[step: 3892] loss: 2.5592474937438965\n",
      "[step: 3893] loss: 2.2898597717285156\n",
      "[step: 3894] loss: 1.9069440364837646\n",
      "[step: 3895] loss: 1.7042105197906494\n",
      "[step: 3896] loss: 1.728994369506836\n",
      "[step: 3897] loss: 1.8961632251739502\n",
      "[step: 3898] loss: 2.058596611022949\n",
      "[step: 3899] loss: 2.026320457458496\n",
      "[step: 3900] loss: 1.8901106119155884\n",
      "[step: 3901] loss: 1.7299097776412964\n",
      "[step: 3902] loss: 1.686387538909912\n",
      "[step: 3903] loss: 1.7571043968200684\n",
      "[step: 3904] loss: 1.848647117614746\n",
      "[step: 3905] loss: 1.887943983078003\n",
      "[step: 3906] loss: 1.8272658586502075\n",
      "[step: 3907] loss: 1.746670126914978\n",
      "[step: 3908] loss: 1.69169020652771\n",
      "[step: 3909] loss: 1.6940264701843262\n",
      "[step: 3910] loss: 1.7302565574645996\n",
      "[step: 3911] loss: 1.7621831893920898\n",
      "[step: 3912] loss: 1.772613525390625\n",
      "[step: 3913] loss: 1.7532987594604492\n",
      "[step: 3914] loss: 1.7306156158447266\n",
      "[step: 3915] loss: 1.7066899538040161\n",
      "[step: 3916] loss: 1.6924083232879639\n",
      "[step: 3917] loss: 1.6843252182006836\n",
      "[step: 3918] loss: 1.683004379272461\n",
      "[step: 3919] loss: 1.6894264221191406\n",
      "[step: 3920] loss: 1.7008976936340332\n",
      "[step: 3921] loss: 1.713997721672058\n",
      "[step: 3922] loss: 1.7168543338775635\n",
      "[step: 3923] loss: 1.7130000591278076\n",
      "[step: 3924] loss: 1.6985524892807007\n",
      "[step: 3925] loss: 1.685124158859253\n",
      "[step: 3926] loss: 1.6749228239059448\n",
      "[step: 3927] loss: 1.6706503629684448\n",
      "[step: 3928] loss: 1.6688374280929565\n",
      "[step: 3929] loss: 1.666313648223877\n",
      "[step: 3930] loss: 1.6620395183563232\n",
      "[step: 3931] loss: 1.6568877696990967\n",
      "[step: 3932] loss: 1.6532375812530518\n",
      "[step: 3933] loss: 1.6522575616836548\n",
      "[step: 3934] loss: 1.653186559677124\n",
      "[step: 3935] loss: 1.654396414756775\n",
      "[step: 3936] loss: 1.6547077894210815\n",
      "[step: 3937] loss: 1.6542894840240479\n",
      "[step: 3938] loss: 1.6542694568634033\n",
      "[step: 3939] loss: 1.656434178352356\n",
      "[step: 3940] loss: 1.662796974182129\n",
      "[step: 3941] loss: 1.6748499870300293\n",
      "[step: 3942] loss: 1.6989264488220215\n",
      "[step: 3943] loss: 1.7393476963043213\n",
      "[step: 3944] loss: 1.8260657787322998\n",
      "[step: 3945] loss: 1.965904712677002\n",
      "[step: 3946] loss: 2.2799313068389893\n",
      "[step: 3947] loss: 2.678510904312134\n",
      "[step: 3948] loss: 3.5243325233459473\n",
      "[step: 3949] loss: 3.842761993408203\n",
      "[step: 3950] loss: 4.289031028747559\n",
      "[step: 3951] loss: 3.1327669620513916\n",
      "[step: 3952] loss: 2.0994277000427246\n",
      "[step: 3953] loss: 1.7441632747650146\n",
      "[step: 3954] loss: 2.2069525718688965\n",
      "[step: 3955] loss: 2.7990856170654297\n",
      "[step: 3956] loss: 2.377746343612671\n",
      "[step: 3957] loss: 1.8029696941375732\n",
      "[step: 3958] loss: 1.8080588579177856\n",
      "[step: 3959] loss: 2.1879262924194336\n",
      "[step: 3960] loss: 2.2724857330322266\n",
      "[step: 3961] loss: 1.8281984329223633\n",
      "[step: 3962] loss: 1.7114790678024292\n",
      "[step: 3963] loss: 2.0054080486297607\n",
      "[step: 3964] loss: 2.0584464073181152\n",
      "[step: 3965] loss: 1.8326170444488525\n",
      "[step: 3966] loss: 1.6753946542739868\n",
      "[step: 3967] loss: 1.8258357048034668\n",
      "[step: 3968] loss: 1.958577036857605\n",
      "[step: 3969] loss: 1.822394609451294\n",
      "[step: 3970] loss: 1.6944040060043335\n",
      "[step: 3971] loss: 1.735061526298523\n",
      "[step: 3972] loss: 1.8197669982910156\n",
      "[step: 3973] loss: 1.7912057638168335\n",
      "[step: 3974] loss: 1.7070012092590332\n",
      "[step: 3975] loss: 1.698399543762207\n",
      "[step: 3976] loss: 1.7431668043136597\n",
      "[step: 3977] loss: 1.7426173686981201\n",
      "[step: 3978] loss: 1.7010997533798218\n",
      "[step: 3979] loss: 1.67979097366333\n",
      "[step: 3980] loss: 1.6947615146636963\n",
      "[step: 3981] loss: 1.7046923637390137\n",
      "[step: 3982] loss: 1.685005784034729\n",
      "[step: 3983] loss: 1.669734239578247\n",
      "[step: 3984] loss: 1.6714242696762085\n",
      "[step: 3985] loss: 1.6764193773269653\n",
      "[step: 3986] loss: 1.6650804281234741\n",
      "[step: 3987] loss: 1.651386022567749\n",
      "[step: 3988] loss: 1.6551997661590576\n",
      "[step: 3989] loss: 1.6630797386169434\n",
      "[step: 3990] loss: 1.659763216972351\n",
      "[step: 3991] loss: 1.6420749425888062\n",
      "[step: 3992] loss: 1.6329364776611328\n",
      "[step: 3993] loss: 1.640831708908081\n",
      "[step: 3994] loss: 1.6505823135375977\n",
      "[step: 3995] loss: 1.6477622985839844\n",
      "[step: 3996] loss: 1.6335749626159668\n",
      "[step: 3997] loss: 1.6256213188171387\n",
      "[step: 3998] loss: 1.628558874130249\n",
      "[step: 3999] loss: 1.6341822147369385\n",
      "[step: 4000] loss: 1.63294517993927\n",
      "[step: 4001] loss: 1.6269381046295166\n",
      "[step: 4002] loss: 1.6238465309143066\n",
      "[step: 4003] loss: 1.6251872777938843\n",
      "[step: 4004] loss: 1.6265583038330078\n",
      "[step: 4005] loss: 1.623358130455017\n",
      "[step: 4006] loss: 1.6181334257125854\n",
      "[step: 4007] loss: 1.6152379512786865\n",
      "[step: 4008] loss: 1.6161787509918213\n",
      "[step: 4009] loss: 1.617788314819336\n",
      "[step: 4010] loss: 1.61715567111969\n",
      "[step: 4011] loss: 1.6147783994674683\n",
      "[step: 4012] loss: 1.612940788269043\n",
      "[step: 4013] loss: 1.613020658493042\n",
      "[step: 4014] loss: 1.6135530471801758\n",
      "[step: 4015] loss: 1.6131389141082764\n",
      "[step: 4016] loss: 1.6112630367279053\n",
      "[step: 4017] loss: 1.609450101852417\n",
      "[step: 4018] loss: 1.6086865663528442\n",
      "[step: 4019] loss: 1.60881769657135\n",
      "[step: 4020] loss: 1.6089367866516113\n",
      "[step: 4021] loss: 1.6084747314453125\n",
      "[step: 4022] loss: 1.6079962253570557\n",
      "[step: 4023] loss: 1.6084043979644775\n",
      "[step: 4024] loss: 1.6102023124694824\n",
      "[step: 4025] loss: 1.613884449005127\n",
      "[step: 4026] loss: 1.6192011833190918\n",
      "[step: 4027] loss: 1.628896951675415\n",
      "[step: 4028] loss: 1.6439138650894165\n",
      "[step: 4029] loss: 1.673409342765808\n",
      "[step: 4030] loss: 1.7186193466186523\n",
      "[step: 4031] loss: 1.8082120418548584\n",
      "[step: 4032] loss: 1.9343032836914062\n",
      "[step: 4033] loss: 2.1917636394500732\n",
      "[step: 4034] loss: 2.460092067718506\n",
      "[step: 4035] loss: 2.996798515319824\n",
      "[step: 4036] loss: 3.117647171020508\n",
      "[step: 4037] loss: 3.344804286956787\n",
      "[step: 4038] loss: 2.6540627479553223\n",
      "[step: 4039] loss: 2.0355396270751953\n",
      "[step: 4040] loss: 1.644636631011963\n",
      "[step: 4041] loss: 1.7478969097137451\n",
      "[step: 4042] loss: 2.1302175521850586\n",
      "[step: 4043] loss: 2.2366762161254883\n",
      "[step: 4044] loss: 2.049783706665039\n",
      "[step: 4045] loss: 1.7032498121261597\n",
      "[step: 4046] loss: 1.6406350135803223\n",
      "[step: 4047] loss: 1.8488911390304565\n",
      "[step: 4048] loss: 1.9763747453689575\n",
      "[step: 4049] loss: 1.8936649560928345\n",
      "[step: 4050] loss: 1.6764321327209473\n",
      "[step: 4051] loss: 1.6144095659255981\n",
      "[step: 4052] loss: 1.7260997295379639\n",
      "[step: 4053] loss: 1.8221173286437988\n",
      "[step: 4054] loss: 1.7987818717956543\n",
      "[step: 4055] loss: 1.6720216274261475\n",
      "[step: 4056] loss: 1.6060032844543457\n",
      "[step: 4057] loss: 1.6427083015441895\n",
      "[step: 4058] loss: 1.70853590965271\n",
      "[step: 4059] loss: 1.726326823234558\n",
      "[step: 4060] loss: 1.6758880615234375\n",
      "[step: 4061] loss: 1.626033067703247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4062] loss: 1.608923077583313\n",
      "[step: 4063] loss: 1.6248371601104736\n",
      "[step: 4064] loss: 1.6471121311187744\n",
      "[step: 4065] loss: 1.653531789779663\n",
      "[step: 4066] loss: 1.6463390588760376\n",
      "[step: 4067] loss: 1.623856544494629\n",
      "[step: 4068] loss: 1.6029713153839111\n",
      "[step: 4069] loss: 1.5919530391693115\n",
      "[step: 4070] loss: 1.5962200164794922\n",
      "[step: 4071] loss: 1.6116347312927246\n",
      "[step: 4072] loss: 1.6225731372833252\n",
      "[step: 4073] loss: 1.621386170387268\n",
      "[step: 4074] loss: 1.6059155464172363\n",
      "[step: 4075] loss: 1.5899169445037842\n",
      "[step: 4076] loss: 1.581296682357788\n",
      "[step: 4077] loss: 1.5817424058914185\n",
      "[step: 4078] loss: 1.5856356620788574\n",
      "[step: 4079] loss: 1.5881754159927368\n",
      "[step: 4080] loss: 1.5896693468093872\n",
      "[step: 4081] loss: 1.5910766124725342\n",
      "[step: 4082] loss: 1.5937010049819946\n",
      "[step: 4083] loss: 1.5944981575012207\n",
      "[step: 4084] loss: 1.5921739339828491\n",
      "[step: 4085] loss: 1.5872468948364258\n",
      "[step: 4086] loss: 1.5823886394500732\n",
      "[step: 4087] loss: 1.5789499282836914\n",
      "[step: 4088] loss: 1.5772472620010376\n",
      "[step: 4089] loss: 1.5754053592681885\n",
      "[step: 4090] loss: 1.5730986595153809\n",
      "[step: 4091] loss: 1.5704988241195679\n",
      "[step: 4092] loss: 1.5687000751495361\n",
      "[step: 4093] loss: 1.5679851770401\n",
      "[step: 4094] loss: 1.5679831504821777\n",
      "[step: 4095] loss: 1.5679913759231567\n",
      "[step: 4096] loss: 1.567953109741211\n",
      "[step: 4097] loss: 1.568429708480835\n",
      "[step: 4098] loss: 1.57051682472229\n",
      "[step: 4099] loss: 1.5750248432159424\n",
      "[step: 4100] loss: 1.5840027332305908\n",
      "[step: 4101] loss: 1.5986607074737549\n",
      "[step: 4102] loss: 1.6274564266204834\n",
      "[step: 4103] loss: 1.6739753484725952\n",
      "[step: 4104] loss: 1.7701047658920288\n",
      "[step: 4105] loss: 1.913774013519287\n",
      "[step: 4106] loss: 2.2162182331085205\n",
      "[step: 4107] loss: 2.556136131286621\n",
      "[step: 4108] loss: 3.2349629402160645\n",
      "[step: 4109] loss: 3.398935317993164\n",
      "[step: 4110] loss: 3.6327037811279297\n",
      "[step: 4111] loss: 2.707428455352783\n",
      "[step: 4112] loss: 1.9179069995880127\n",
      "[step: 4113] loss: 1.5996978282928467\n",
      "[step: 4114] loss: 1.9113171100616455\n",
      "[step: 4115] loss: 2.402333974838257\n",
      "[step: 4116] loss: 2.263953447341919\n",
      "[step: 4117] loss: 1.8342703580856323\n",
      "[step: 4118] loss: 1.5949156284332275\n",
      "[step: 4119] loss: 1.8067291975021362\n",
      "[step: 4120] loss: 2.0848617553710938\n",
      "[step: 4121] loss: 1.9274667501449585\n",
      "[step: 4122] loss: 1.647086262702942\n",
      "[step: 4123] loss: 1.6030685901641846\n",
      "[step: 4124] loss: 1.785710334777832\n",
      "[step: 4125] loss: 1.891831636428833\n",
      "[step: 4126] loss: 1.7343348264694214\n",
      "[step: 4127] loss: 1.588170051574707\n",
      "[step: 4128] loss: 1.6165194511413574\n",
      "[step: 4129] loss: 1.7242454290390015\n",
      "[step: 4130] loss: 1.75121009349823\n",
      "[step: 4131] loss: 1.652620792388916\n",
      "[step: 4132] loss: 1.5858862400054932\n",
      "[step: 4133] loss: 1.607522964477539\n",
      "[step: 4134] loss: 1.6559841632843018\n",
      "[step: 4135] loss: 1.662414312362671\n",
      "[step: 4136] loss: 1.6200170516967773\n",
      "[step: 4137] loss: 1.5919370651245117\n",
      "[step: 4138] loss: 1.5889651775360107\n",
      "[step: 4139] loss: 1.5975453853607178\n",
      "[step: 4140] loss: 1.599900245666504\n",
      "[step: 4141] loss: 1.5954546928405762\n",
      "[step: 4142] loss: 1.5932154655456543\n",
      "[step: 4143] loss: 1.5829660892486572\n",
      "[step: 4144] loss: 1.5687676668167114\n",
      "[step: 4145] loss: 1.55802583694458\n",
      "[step: 4146] loss: 1.5624432563781738\n",
      "[step: 4147] loss: 1.5767464637756348\n",
      "[step: 4148] loss: 1.580366849899292\n",
      "[step: 4149] loss: 1.5687651634216309\n",
      "[step: 4150] loss: 1.5505317449569702\n",
      "[step: 4151] loss: 1.543083906173706\n",
      "[step: 4152] loss: 1.5485577583312988\n",
      "[step: 4153] loss: 1.5563545227050781\n",
      "[step: 4154] loss: 1.5573723316192627\n",
      "[step: 4155] loss: 1.5520424842834473\n",
      "[step: 4156] loss: 1.5481981039047241\n",
      "[step: 4157] loss: 1.5479131937026978\n",
      "[step: 4158] loss: 1.5478023290634155\n",
      "[step: 4159] loss: 1.5439352989196777\n",
      "[step: 4160] loss: 1.5380383729934692\n",
      "[step: 4161] loss: 1.534195899963379\n",
      "[step: 4162] loss: 1.5344706773757935\n",
      "[step: 4163] loss: 1.5368375778198242\n",
      "[step: 4164] loss: 1.5379202365875244\n",
      "[step: 4165] loss: 1.5369536876678467\n",
      "[step: 4166] loss: 1.5355861186981201\n",
      "[step: 4167] loss: 1.5358567237854004\n",
      "[step: 4168] loss: 1.5373635292053223\n",
      "[step: 4169] loss: 1.5390288829803467\n",
      "[step: 4170] loss: 1.5394710302352905\n",
      "[step: 4171] loss: 1.5399909019470215\n",
      "[step: 4172] loss: 1.5412485599517822\n",
      "[step: 4173] loss: 1.5452336072921753\n",
      "[step: 4174] loss: 1.5512967109680176\n",
      "[step: 4175] loss: 1.5610077381134033\n",
      "[step: 4176] loss: 1.5739753246307373\n",
      "[step: 4177] loss: 1.596623420715332\n",
      "[step: 4178] loss: 1.6276497840881348\n",
      "[step: 4179] loss: 1.6861790418624878\n",
      "[step: 4180] loss: 1.758434534072876\n",
      "[step: 4181] loss: 1.90001380443573\n",
      "[step: 4182] loss: 2.039309501647949\n",
      "[step: 4183] loss: 2.3162121772766113\n",
      "[step: 4184] loss: 2.4537744522094727\n",
      "[step: 4185] loss: 2.716587781906128\n",
      "[step: 4186] loss: 2.5369873046875\n",
      "[step: 4187] loss: 2.3417716026306152\n",
      "[step: 4188] loss: 1.8982856273651123\n",
      "[step: 4189] loss: 1.602207899093628\n",
      "[step: 4190] loss: 1.5401597023010254\n",
      "[step: 4191] loss: 1.6872949600219727\n",
      "[step: 4192] loss: 1.8912184238433838\n",
      "[step: 4193] loss: 1.9186248779296875\n",
      "[step: 4194] loss: 1.8086788654327393\n",
      "[step: 4195] loss: 1.6143672466278076\n",
      "[step: 4196] loss: 1.5280089378356934\n",
      "[step: 4197] loss: 1.5813114643096924\n",
      "[step: 4198] loss: 1.6874200105667114\n",
      "[step: 4199] loss: 1.750566005706787\n",
      "[step: 4200] loss: 1.6985342502593994\n",
      "[step: 4201] loss: 1.6117327213287354\n",
      "[step: 4202] loss: 1.5408432483673096\n",
      "[step: 4203] loss: 1.533731460571289\n",
      "[step: 4204] loss: 1.5721099376678467\n",
      "[step: 4205] loss: 1.612917184829712\n",
      "[step: 4206] loss: 1.6337382793426514\n",
      "[step: 4207] loss: 1.6144684553146362\n",
      "[step: 4208] loss: 1.583479881286621\n",
      "[step: 4209] loss: 1.5452959537506104\n",
      "[step: 4210] loss: 1.522330403327942\n",
      "[step: 4211] loss: 1.5196948051452637\n",
      "[step: 4212] loss: 1.5348073244094849\n",
      "[step: 4213] loss: 1.5586574077606201\n",
      "[step: 4214] loss: 1.573052167892456\n",
      "[step: 4215] loss: 1.5753241777420044\n",
      "[step: 4216] loss: 1.5587278604507446\n",
      "[step: 4217] loss: 1.5408728122711182\n",
      "[step: 4218] loss: 1.525037169456482\n",
      "[step: 4219] loss: 1.515727162361145\n",
      "[step: 4220] loss: 1.5106154680252075\n",
      "[step: 4221] loss: 1.5075936317443848\n",
      "[step: 4222] loss: 1.506798267364502\n",
      "[step: 4223] loss: 1.5095527172088623\n",
      "[step: 4224] loss: 1.516031265258789\n",
      "[step: 4225] loss: 1.523673176765442\n",
      "[step: 4226] loss: 1.5311548709869385\n",
      "[step: 4227] loss: 1.5373915433883667\n",
      "[step: 4228] loss: 1.5454083681106567\n",
      "[step: 4229] loss: 1.5543277263641357\n",
      "[step: 4230] loss: 1.5707062482833862\n",
      "[step: 4231] loss: 1.5882353782653809\n",
      "[step: 4232] loss: 1.620225191116333\n",
      "[step: 4233] loss: 1.6530189514160156\n",
      "[step: 4234] loss: 1.7166481018066406\n",
      "[step: 4235] loss: 1.779371738433838\n",
      "[step: 4236] loss: 1.9002317190170288\n",
      "[step: 4237] loss: 1.9959560632705688\n",
      "[step: 4238] loss: 2.174478530883789\n",
      "[step: 4239] loss: 2.227536678314209\n",
      "[step: 4240] loss: 2.333587884902954\n",
      "[step: 4241] loss: 2.176426649093628\n",
      "[step: 4242] loss: 2.021003007888794\n",
      "[step: 4243] loss: 1.7459135055541992\n",
      "[step: 4244] loss: 1.561754584312439\n",
      "[step: 4245] loss: 1.498653531074524\n",
      "[step: 4246] loss: 1.556910753250122\n",
      "[step: 4247] loss: 1.6744998693466187\n",
      "[step: 4248] loss: 1.7475674152374268\n",
      "[step: 4249] loss: 1.760796070098877\n",
      "[step: 4250] loss: 1.6688519716262817\n",
      "[step: 4251] loss: 1.5715728998184204\n",
      "[step: 4252] loss: 1.504833698272705\n",
      "[step: 4253] loss: 1.4998652935028076\n",
      "[step: 4254] loss: 1.541444182395935\n",
      "[step: 4255] loss: 1.592321515083313\n",
      "[step: 4256] loss: 1.6294920444488525\n",
      "[step: 4257] loss: 1.6204994916915894\n",
      "[step: 4258] loss: 1.5930992364883423\n",
      "[step: 4259] loss: 1.5457797050476074\n",
      "[step: 4260] loss: 1.5091195106506348\n",
      "[step: 4261] loss: 1.4891679286956787\n",
      "[step: 4262] loss: 1.4891719818115234\n",
      "[step: 4263] loss: 1.503994107246399\n",
      "[step: 4264] loss: 1.5244306325912476\n",
      "[step: 4265] loss: 1.5454425811767578\n",
      "[step: 4266] loss: 1.5563071966171265\n",
      "[step: 4267] loss: 1.564058780670166\n",
      "[step: 4268] loss: 1.5590656995773315\n",
      "[step: 4269] loss: 1.5540995597839355\n",
      "[step: 4270] loss: 1.542032241821289\n",
      "[step: 4271] loss: 1.532250165939331\n",
      "[step: 4272] loss: 1.5205676555633545\n",
      "[step: 4273] loss: 1.5114134550094604\n",
      "[step: 4274] loss: 1.502673864364624\n",
      "[step: 4275] loss: 1.496472954750061\n",
      "[step: 4276] loss: 1.4915359020233154\n",
      "[step: 4277] loss: 1.4885737895965576\n",
      "[step: 4278] loss: 1.4863145351409912\n",
      "[step: 4279] loss: 1.4852581024169922\n",
      "[step: 4280] loss: 1.4848380088806152\n",
      "[step: 4281] loss: 1.486085295677185\n",
      "[step: 4282] loss: 1.4889886379241943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4283] loss: 1.4954440593719482\n",
      "[step: 4284] loss: 1.5061124563217163\n",
      "[step: 4285] loss: 1.5264443159103394\n",
      "[step: 4286] loss: 1.5589556694030762\n",
      "[step: 4287] loss: 1.622688889503479\n",
      "[step: 4288] loss: 1.7194631099700928\n",
      "[step: 4289] loss: 1.917612910270691\n",
      "[step: 4290] loss: 2.1677021980285645\n",
      "[step: 4291] loss: 2.679070472717285\n",
      "[step: 4292] loss: 3.002849578857422\n",
      "[step: 4293] loss: 3.5620622634887695\n",
      "[step: 4294] loss: 3.085409164428711\n",
      "[step: 4295] loss: 2.506145477294922\n",
      "[step: 4296] loss: 1.7379984855651855\n",
      "[step: 4297] loss: 1.5331217050552368\n",
      "[step: 4298] loss: 1.884918212890625\n",
      "[step: 4299] loss: 2.2008261680603027\n",
      "[step: 4300] loss: 2.167468786239624\n",
      "[step: 4301] loss: 1.7055168151855469\n",
      "[step: 4302] loss: 1.5121526718139648\n",
      "[step: 4303] loss: 1.7281792163848877\n",
      "[step: 4304] loss: 1.9248323440551758\n",
      "[step: 4305] loss: 1.8480812311172485\n",
      "[step: 4306] loss: 1.5651293992996216\n",
      "[step: 4307] loss: 1.5065207481384277\n",
      "[step: 4308] loss: 1.6769907474517822\n",
      "[step: 4309] loss: 1.7649495601654053\n",
      "[step: 4310] loss: 1.6744720935821533\n",
      "[step: 4311] loss: 1.5138994455337524\n",
      "[step: 4312] loss: 1.5033220052719116\n",
      "[step: 4313] loss: 1.6059000492095947\n",
      "[step: 4314] loss: 1.650697112083435\n",
      "[step: 4315] loss: 1.5944830179214478\n",
      "[step: 4316] loss: 1.507070779800415\n",
      "[step: 4317] loss: 1.4966387748718262\n",
      "[step: 4318] loss: 1.5416548252105713\n",
      "[step: 4319] loss: 1.5695046186447144\n",
      "[step: 4320] loss: 1.5524871349334717\n",
      "[step: 4321] loss: 1.5115914344787598\n",
      "[step: 4322] loss: 1.4917418956756592\n",
      "[step: 4323] loss: 1.4956402778625488\n",
      "[step: 4324] loss: 1.5075163841247559\n",
      "[step: 4325] loss: 1.511735200881958\n",
      "[step: 4326] loss: 1.5065562725067139\n",
      "[step: 4327] loss: 1.4988160133361816\n",
      "[step: 4328] loss: 1.4861427545547485\n",
      "[step: 4329] loss: 1.4742523431777954\n",
      "[step: 4330] loss: 1.4698665142059326\n",
      "[step: 4331] loss: 1.4770631790161133\n",
      "[step: 4332] loss: 1.4882519245147705\n",
      "[step: 4333] loss: 1.4900469779968262\n",
      "[step: 4334] loss: 1.4801610708236694\n",
      "[step: 4335] loss: 1.4642709493637085\n",
      "[step: 4336] loss: 1.4553897380828857\n",
      "[step: 4337] loss: 1.4572727680206299\n",
      "[step: 4338] loss: 1.4641456604003906\n",
      "[step: 4339] loss: 1.4682234525680542\n",
      "[step: 4340] loss: 1.4666900634765625\n",
      "[step: 4341] loss: 1.4635993242263794\n",
      "[step: 4342] loss: 1.4615994691848755\n",
      "[step: 4343] loss: 1.4611384868621826\n",
      "[step: 4344] loss: 1.4596660137176514\n",
      "[step: 4345] loss: 1.455604076385498\n",
      "[step: 4346] loss: 1.4504468441009521\n",
      "[step: 4347] loss: 1.4469914436340332\n",
      "[step: 4348] loss: 1.4461115598678589\n",
      "[step: 4349] loss: 1.446820616722107\n",
      "[step: 4350] loss: 1.447235107421875\n",
      "[step: 4351] loss: 1.4465491771697998\n",
      "[step: 4352] loss: 1.4454888105392456\n",
      "[step: 4353] loss: 1.4452239274978638\n",
      "[step: 4354] loss: 1.4463768005371094\n",
      "[step: 4355] loss: 1.4483190774917603\n",
      "[step: 4356] loss: 1.4507395029067993\n",
      "[step: 4357] loss: 1.453271746635437\n",
      "[step: 4358] loss: 1.457693338394165\n",
      "[step: 4359] loss: 1.4646480083465576\n",
      "[step: 4360] loss: 1.4779635667800903\n",
      "[step: 4361] loss: 1.4977691173553467\n",
      "[step: 4362] loss: 1.5331746339797974\n",
      "[step: 4363] loss: 1.583570957183838\n",
      "[step: 4364] loss: 1.6793177127838135\n",
      "[step: 4365] loss: 1.8033604621887207\n",
      "[step: 4366] loss: 2.0488736629486084\n",
      "[step: 4367] loss: 2.2720236778259277\n",
      "[step: 4368] loss: 2.702371835708618\n",
      "[step: 4369] loss: 2.751120090484619\n",
      "[step: 4370] loss: 2.859133720397949\n",
      "[step: 4371] loss: 2.317164659500122\n",
      "[step: 4372] loss: 1.8276218175888062\n",
      "[step: 4373] loss: 1.4939589500427246\n",
      "[step: 4374] loss: 1.5249414443969727\n",
      "[step: 4375] loss: 1.7983078956604004\n",
      "[step: 4376] loss: 1.9529402256011963\n",
      "[step: 4377] loss: 1.9001942873001099\n",
      "[step: 4378] loss: 1.6210644245147705\n",
      "[step: 4379] loss: 1.4591500759124756\n",
      "[step: 4380] loss: 1.5233139991760254\n",
      "[step: 4381] loss: 1.6804933547973633\n",
      "[step: 4382] loss: 1.7575100660324097\n",
      "[step: 4383] loss: 1.6387648582458496\n",
      "[step: 4384] loss: 1.4943904876708984\n",
      "[step: 4385] loss: 1.444232702255249\n",
      "[step: 4386] loss: 1.5106265544891357\n",
      "[step: 4387] loss: 1.6035106182098389\n",
      "[step: 4388] loss: 1.6099941730499268\n",
      "[step: 4389] loss: 1.5492850542068481\n",
      "[step: 4390] loss: 1.469329595565796\n",
      "[step: 4391] loss: 1.442606806755066\n",
      "[step: 4392] loss: 1.4697191715240479\n",
      "[step: 4393] loss: 1.5114033222198486\n",
      "[step: 4394] loss: 1.5322657823562622\n",
      "[step: 4395] loss: 1.5131982564926147\n",
      "[step: 4396] loss: 1.4830305576324463\n",
      "[step: 4397] loss: 1.453526496887207\n",
      "[step: 4398] loss: 1.4397717714309692\n",
      "[step: 4399] loss: 1.4424829483032227\n",
      "[step: 4400] loss: 1.4562627077102661\n",
      "[step: 4401] loss: 1.4722763299942017\n",
      "[step: 4402] loss: 1.4783234596252441\n",
      "[step: 4403] loss: 1.4750001430511475\n",
      "[step: 4404] loss: 1.4574041366577148\n",
      "[step: 4405] loss: 1.4381531476974487\n",
      "[step: 4406] loss: 1.425173282623291\n",
      "[step: 4407] loss: 1.4230366945266724\n",
      "[step: 4408] loss: 1.428593635559082\n",
      "[step: 4409] loss: 1.4360954761505127\n",
      "[step: 4410] loss: 1.4420173168182373\n",
      "[step: 4411] loss: 1.4440083503723145\n",
      "[step: 4412] loss: 1.4455318450927734\n",
      "[step: 4413] loss: 1.4461169242858887\n",
      "[step: 4414] loss: 1.44688880443573\n",
      "[step: 4415] loss: 1.4451963901519775\n",
      "[step: 4416] loss: 1.4422008991241455\n",
      "[step: 4417] loss: 1.4379595518112183\n",
      "[step: 4418] loss: 1.4349290132522583\n",
      "[step: 4419] loss: 1.4330459833145142\n",
      "[step: 4420] loss: 1.4333055019378662\n",
      "[step: 4421] loss: 1.433638095855713\n",
      "[step: 4422] loss: 1.4354162216186523\n",
      "[step: 4423] loss: 1.4376084804534912\n",
      "[step: 4424] loss: 1.4434306621551514\n",
      "[step: 4425] loss: 1.4521898031234741\n",
      "[step: 4426] loss: 1.4692416191101074\n",
      "[step: 4427] loss: 1.4933280944824219\n",
      "[step: 4428] loss: 1.5371465682983398\n",
      "[step: 4429] loss: 1.596437931060791\n",
      "[step: 4430] loss: 1.7084763050079346\n",
      "[step: 4431] loss: 1.839158296585083\n",
      "[step: 4432] loss: 2.0921530723571777\n",
      "[step: 4433] loss: 2.279788017272949\n",
      "[step: 4434] loss: 2.6293885707855225\n",
      "[step: 4435] loss: 2.5751895904541016\n",
      "[step: 4436] loss: 2.529139518737793\n",
      "[step: 4437] loss: 2.035952568054199\n",
      "[step: 4438] loss: 1.6338469982147217\n",
      "[step: 4439] loss: 1.4332352876663208\n",
      "[step: 4440] loss: 1.5179357528686523\n",
      "[step: 4441] loss: 1.7549190521240234\n",
      "[step: 4442] loss: 1.857362985610962\n",
      "[step: 4443] loss: 1.7930136919021606\n",
      "[step: 4444] loss: 1.5643481016159058\n",
      "[step: 4445] loss: 1.427363634109497\n",
      "[step: 4446] loss: 1.4643616676330566\n",
      "[step: 4447] loss: 1.5911792516708374\n",
      "[step: 4448] loss: 1.6793403625488281\n",
      "[step: 4449] loss: 1.618037223815918\n",
      "[step: 4450] loss: 1.5036944150924683\n",
      "[step: 4451] loss: 1.419107437133789\n",
      "[step: 4452] loss: 1.4278969764709473\n",
      "[step: 4453] loss: 1.4974371194839478\n",
      "[step: 4454] loss: 1.5482960939407349\n",
      "[step: 4455] loss: 1.549861192703247\n",
      "[step: 4456] loss: 1.4939327239990234\n",
      "[step: 4457] loss: 1.4393565654754639\n",
      "[step: 4458] loss: 1.4130492210388184\n",
      "[step: 4459] loss: 1.4233982563018799\n",
      "[step: 4460] loss: 1.4507670402526855\n",
      "[step: 4461] loss: 1.4715893268585205\n",
      "[step: 4462] loss: 1.480109453201294\n",
      "[step: 4463] loss: 1.4669501781463623\n",
      "[step: 4464] loss: 1.446453332901001\n",
      "[step: 4465] loss: 1.421770691871643\n",
      "[step: 4466] loss: 1.4055428504943848\n",
      "[step: 4467] loss: 1.4005281925201416\n",
      "[step: 4468] loss: 1.4067661762237549\n",
      "[step: 4469] loss: 1.4204210042953491\n",
      "[step: 4470] loss: 1.4324522018432617\n",
      "[step: 4471] loss: 1.4394199848175049\n",
      "[step: 4472] loss: 1.4362118244171143\n",
      "[step: 4473] loss: 1.4300239086151123\n",
      "[step: 4474] loss: 1.420541763305664\n",
      "[step: 4475] loss: 1.413388967514038\n",
      "[step: 4476] loss: 1.4074265956878662\n",
      "[step: 4477] loss: 1.4022631645202637\n",
      "[step: 4478] loss: 1.3971343040466309\n",
      "[step: 4479] loss: 1.3926206827163696\n",
      "[step: 4480] loss: 1.3894214630126953\n",
      "[step: 4481] loss: 1.3879013061523438\n",
      "[step: 4482] loss: 1.3878190517425537\n",
      "[step: 4483] loss: 1.3882386684417725\n",
      "[step: 4484] loss: 1.3884221315383911\n",
      "[step: 4485] loss: 1.3884844779968262\n",
      "[step: 4486] loss: 1.3889590501785278\n",
      "[step: 4487] loss: 1.390547513961792\n",
      "[step: 4488] loss: 1.3942639827728271\n",
      "[step: 4489] loss: 1.4007014036178589\n",
      "[step: 4490] loss: 1.4123115539550781\n",
      "[step: 4491] loss: 1.4306204319000244\n",
      "[step: 4492] loss: 1.4661505222320557\n",
      "[step: 4493] loss: 1.5227243900299072\n",
      "[step: 4494] loss: 1.6388068199157715\n",
      "[step: 4495] loss: 1.8079774379730225\n",
      "[step: 4496] loss: 2.1589481830596924\n",
      "[step: 4497] loss: 2.523735523223877\n",
      "[step: 4498] loss: 3.2197282314300537\n",
      "[step: 4499] loss: 3.2766458988189697\n",
      "[step: 4500] loss: 3.305060386657715\n",
      "[step: 4501] loss: 2.3306033611297607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4502] loss: 1.597697138786316\n",
      "[step: 4503] loss: 1.4677983522415161\n",
      "[step: 4504] loss: 1.875435709953308\n",
      "[step: 4505] loss: 2.290635347366333\n",
      "[step: 4506] loss: 1.9913580417633057\n",
      "[step: 4507] loss: 1.5429586172103882\n",
      "[step: 4508] loss: 1.4515371322631836\n",
      "[step: 4509] loss: 1.7325903177261353\n",
      "[step: 4510] loss: 1.9341466426849365\n",
      "[step: 4511] loss: 1.6792851686477661\n",
      "[step: 4512] loss: 1.424752950668335\n",
      "[step: 4513] loss: 1.4760570526123047\n",
      "[step: 4514] loss: 1.6667168140411377\n",
      "[step: 4515] loss: 1.7102599143981934\n",
      "[step: 4516] loss: 1.5185060501098633\n",
      "[step: 4517] loss: 1.4040323495864868\n",
      "[step: 4518] loss: 1.476738691329956\n",
      "[step: 4519] loss: 1.577850580215454\n",
      "[step: 4520] loss: 1.5711946487426758\n",
      "[step: 4521] loss: 1.458254337310791\n",
      "[step: 4522] loss: 1.4091365337371826\n",
      "[step: 4523] loss: 1.4528570175170898\n",
      "[step: 4524] loss: 1.5000109672546387\n",
      "[step: 4525] loss: 1.4918768405914307\n",
      "[step: 4526] loss: 1.4362741708755493\n",
      "[step: 4527] loss: 1.4093220233917236\n",
      "[step: 4528] loss: 1.4195853471755981\n",
      "[step: 4529] loss: 1.4387311935424805\n",
      "[step: 4530] loss: 1.440704107284546\n",
      "[step: 4531] loss: 1.4220256805419922\n",
      "[step: 4532] loss: 1.409680724143982\n",
      "[step: 4533] loss: 1.4032330513000488\n",
      "[step: 4534] loss: 1.40087890625\n",
      "[step: 4535] loss: 1.3991246223449707\n",
      "[step: 4536] loss: 1.3994674682617188\n",
      "[step: 4537] loss: 1.4045162200927734\n",
      "[step: 4538] loss: 1.4025359153747559\n",
      "[step: 4539] loss: 1.392809271812439\n",
      "[step: 4540] loss: 1.3796643018722534\n",
      "[step: 4541] loss: 1.3741685152053833\n",
      "[step: 4542] loss: 1.3806555271148682\n",
      "[step: 4543] loss: 1.3899204730987549\n",
      "[step: 4544] loss: 1.39273202419281\n",
      "[step: 4545] loss: 1.3850215673446655\n",
      "[step: 4546] loss: 1.3749747276306152\n",
      "[step: 4547] loss: 1.36930251121521\n",
      "[step: 4548] loss: 1.36954927444458\n",
      "[step: 4549] loss: 1.3717377185821533\n",
      "[step: 4550] loss: 1.3716264963150024\n",
      "[step: 4551] loss: 1.3697999715805054\n",
      "[step: 4552] loss: 1.3684751987457275\n",
      "[step: 4553] loss: 1.3696258068084717\n",
      "[step: 4554] loss: 1.3713983297348022\n",
      "[step: 4555] loss: 1.371595859527588\n",
      "[step: 4556] loss: 1.3695268630981445\n",
      "[step: 4557] loss: 1.3660798072814941\n",
      "[step: 4558] loss: 1.3634895086288452\n",
      "[step: 4559] loss: 1.3623859882354736\n",
      "[step: 4560] loss: 1.361954927444458\n",
      "[step: 4561] loss: 1.3612070083618164\n",
      "[step: 4562] loss: 1.359499216079712\n",
      "[step: 4563] loss: 1.3576349020004272\n",
      "[step: 4564] loss: 1.3562180995941162\n",
      "[step: 4565] loss: 1.355691909790039\n",
      "[step: 4566] loss: 1.3556196689605713\n",
      "[step: 4567] loss: 1.3553838729858398\n",
      "[step: 4568] loss: 1.3547741174697876\n",
      "[step: 4569] loss: 1.3540056943893433\n",
      "[step: 4570] loss: 1.3536057472229004\n",
      "[step: 4571] loss: 1.3539738655090332\n",
      "[step: 4572] loss: 1.355279564857483\n",
      "[step: 4573] loss: 1.3578282594680786\n",
      "[step: 4574] loss: 1.3620401620864868\n",
      "[step: 4575] loss: 1.3701746463775635\n",
      "[step: 4576] loss: 1.38443922996521\n",
      "[step: 4577] loss: 1.4133938550949097\n",
      "[step: 4578] loss: 1.46305251121521\n",
      "[step: 4579] loss: 1.5662086009979248\n",
      "[step: 4580] loss: 1.7295082807540894\n",
      "[step: 4581] loss: 2.079003095626831\n",
      "[step: 4582] loss: 2.4922609329223633\n",
      "[step: 4583] loss: 3.3251101970672607\n",
      "[step: 4584] loss: 3.500840187072754\n",
      "[step: 4585] loss: 3.714299201965332\n",
      "[step: 4586] loss: 2.523693799972534\n",
      "[step: 4587] loss: 1.6198545694351196\n",
      "[step: 4588] loss: 1.4711060523986816\n",
      "[step: 4589] loss: 1.984562635421753\n",
      "[step: 4590] loss: 2.462543487548828\n",
      "[step: 4591] loss: 1.9910629987716675\n",
      "[step: 4592] loss: 1.472815990447998\n",
      "[step: 4593] loss: 1.53287935256958\n",
      "[step: 4594] loss: 1.887477159500122\n",
      "[step: 4595] loss: 1.95548415184021\n",
      "[step: 4596] loss: 1.54558265209198\n",
      "[step: 4597] loss: 1.4035238027572632\n",
      "[step: 4598] loss: 1.661933422088623\n",
      "[step: 4599] loss: 1.76263427734375\n",
      "[step: 4600] loss: 1.5865955352783203\n",
      "[step: 4601] loss: 1.3811664581298828\n",
      "[step: 4602] loss: 1.467278242111206\n",
      "[step: 4603] loss: 1.647613286972046\n",
      "[step: 4604] loss: 1.5872827768325806\n",
      "[step: 4605] loss: 1.431621789932251\n",
      "[step: 4606] loss: 1.3800020217895508\n",
      "[step: 4607] loss: 1.4797205924987793\n",
      "[step: 4608] loss: 1.5473865270614624\n",
      "[step: 4609] loss: 1.4723811149597168\n",
      "[step: 4610] loss: 1.3885220289230347\n",
      "[step: 4611] loss: 1.3828659057617188\n",
      "[step: 4612] loss: 1.4412376880645752\n",
      "[step: 4613] loss: 1.462675929069519\n",
      "[step: 4614] loss: 1.418748378753662\n",
      "[step: 4615] loss: 1.3748879432678223\n",
      "[step: 4616] loss: 1.369398593902588\n",
      "[step: 4617] loss: 1.398520588874817\n",
      "[step: 4618] loss: 1.4140300750732422\n",
      "[step: 4619] loss: 1.3987007141113281\n",
      "[step: 4620] loss: 1.371309518814087\n",
      "[step: 4621] loss: 1.3558409214019775\n",
      "[step: 4622] loss: 1.3649277687072754\n",
      "[step: 4623] loss: 1.3799749612808228\n",
      "[step: 4624] loss: 1.3849290609359741\n",
      "[step: 4625] loss: 1.3726775646209717\n",
      "[step: 4626] loss: 1.3527770042419434\n",
      "[step: 4627] loss: 1.3439626693725586\n",
      "[step: 4628] loss: 1.3495328426361084\n",
      "[step: 4629] loss: 1.3623461723327637\n",
      "[step: 4630] loss: 1.3669785261154175\n",
      "[step: 4631] loss: 1.3588523864746094\n",
      "[step: 4632] loss: 1.3462797403335571\n",
      "[step: 4633] loss: 1.3377317190170288\n",
      "[step: 4634] loss: 1.33835768699646\n",
      "[step: 4635] loss: 1.3430302143096924\n",
      "[step: 4636] loss: 1.34703528881073\n",
      "[step: 4637] loss: 1.347428560256958\n",
      "[step: 4638] loss: 1.3448212146759033\n",
      "[step: 4639] loss: 1.3414450883865356\n",
      "[step: 4640] loss: 1.3373852968215942\n",
      "[step: 4641] loss: 1.3340263366699219\n",
      "[step: 4642] loss: 1.3313363790512085\n",
      "[step: 4643] loss: 1.3306818008422852\n",
      "[step: 4644] loss: 1.3318891525268555\n",
      "[step: 4645] loss: 1.3338143825531006\n",
      "[step: 4646] loss: 1.3351614475250244\n",
      "[step: 4647] loss: 1.3347516059875488\n",
      "[step: 4648] loss: 1.3334274291992188\n",
      "[step: 4649] loss: 1.331475019454956\n",
      "[step: 4650] loss: 1.3297911882400513\n",
      "[step: 4651] loss: 1.3281986713409424\n",
      "[step: 4652] loss: 1.3265172243118286\n",
      "[step: 4653] loss: 1.324825406074524\n",
      "[step: 4654] loss: 1.3232556581497192\n",
      "[step: 4655] loss: 1.3222171068191528\n",
      "[step: 4656] loss: 1.3215724229812622\n",
      "[step: 4657] loss: 1.3211500644683838\n",
      "[step: 4658] loss: 1.3207335472106934\n",
      "[step: 4659] loss: 1.320193886756897\n",
      "[step: 4660] loss: 1.3197575807571411\n",
      "[step: 4661] loss: 1.319502353668213\n",
      "[step: 4662] loss: 1.3195571899414062\n",
      "[step: 4663] loss: 1.3198957443237305\n",
      "[step: 4664] loss: 1.3205798864364624\n",
      "[step: 4665] loss: 1.3219245672225952\n",
      "[step: 4666] loss: 1.3243292570114136\n",
      "[step: 4667] loss: 1.3289930820465088\n",
      "[step: 4668] loss: 1.3369228839874268\n",
      "[step: 4669] loss: 1.3519184589385986\n",
      "[step: 4670] loss: 1.3766720294952393\n",
      "[step: 4671] loss: 1.425157904624939\n",
      "[step: 4672] loss: 1.502041220664978\n",
      "[step: 4673] loss: 1.659062385559082\n",
      "[step: 4674] loss: 1.878511905670166\n",
      "[step: 4675] loss: 2.3307058811187744\n",
      "[step: 4676] loss: 2.728426456451416\n",
      "[step: 4677] loss: 3.437490224838257\n",
      "[step: 4678] loss: 3.2174434661865234\n",
      "[step: 4679] loss: 2.8538174629211426\n",
      "[step: 4680] loss: 1.8601948022842407\n",
      "[step: 4681] loss: 1.3733148574829102\n",
      "[step: 4682] loss: 1.600769281387329\n",
      "[step: 4683] loss: 2.0493099689483643\n",
      "[step: 4684] loss: 2.1885945796966553\n",
      "[step: 4685] loss: 1.6779991388320923\n",
      "[step: 4686] loss: 1.3593864440917969\n",
      "[step: 4687] loss: 1.5478906631469727\n",
      "[step: 4688] loss: 1.8131022453308105\n",
      "[step: 4689] loss: 1.7737103700637817\n",
      "[step: 4690] loss: 1.4386268854141235\n",
      "[step: 4691] loss: 1.3553054332733154\n",
      "[step: 4692] loss: 1.5587077140808105\n",
      "[step: 4693] loss: 1.651968240737915\n",
      "[step: 4694] loss: 1.5312155485153198\n",
      "[step: 4695] loss: 1.3557603359222412\n",
      "[step: 4696] loss: 1.3793206214904785\n",
      "[step: 4697] loss: 1.5076079368591309\n",
      "[step: 4698] loss: 1.517707347869873\n",
      "[step: 4699] loss: 1.4242846965789795\n",
      "[step: 4700] loss: 1.3473427295684814\n",
      "[step: 4701] loss: 1.380584955215454\n",
      "[step: 4702] loss: 1.4373652935028076\n",
      "[step: 4703] loss: 1.428628921508789\n",
      "[step: 4704] loss: 1.3820337057113647\n",
      "[step: 4705] loss: 1.3488430976867676\n",
      "[step: 4706] loss: 1.3593980073928833\n",
      "[step: 4707] loss: 1.3744217157363892\n",
      "[step: 4708] loss: 1.3725554943084717\n",
      "[step: 4709] loss: 1.361648678779602\n",
      "[step: 4710] loss: 1.3486275672912598\n",
      "[step: 4711] loss: 1.3426845073699951\n",
      "[step: 4712] loss: 1.334315538406372\n",
      "[step: 4713] loss: 1.3325423002243042\n",
      "[step: 4714] loss: 1.3400375843048096\n",
      "[step: 4715] loss: 1.3445875644683838\n",
      "[step: 4716] loss: 1.3384413719177246\n",
      "[step: 4717] loss: 1.320115566253662\n",
      "[step: 4718] loss: 1.3095130920410156\n",
      "[step: 4719] loss: 1.3147919178009033\n",
      "[step: 4720] loss: 1.3265219926834106\n",
      "[step: 4721] loss: 1.331059455871582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4722] loss: 1.3216310739517212\n",
      "[step: 4723] loss: 1.310316801071167\n",
      "[step: 4724] loss: 1.3058513402938843\n",
      "[step: 4725] loss: 1.3080332279205322\n",
      "[step: 4726] loss: 1.310279369354248\n",
      "[step: 4727] loss: 1.3086016178131104\n",
      "[step: 4728] loss: 1.3064022064208984\n",
      "[step: 4729] loss: 1.306516408920288\n",
      "[step: 4730] loss: 1.3085598945617676\n",
      "[step: 4731] loss: 1.308607578277588\n",
      "[step: 4732] loss: 1.3049955368041992\n",
      "[step: 4733] loss: 1.3001434803009033\n",
      "[step: 4734] loss: 1.297153353691101\n",
      "[step: 4735] loss: 1.2969714403152466\n",
      "[step: 4736] loss: 1.2975044250488281\n",
      "[step: 4737] loss: 1.2970929145812988\n",
      "[step: 4738] loss: 1.2957556247711182\n",
      "[step: 4739] loss: 1.2948015928268433\n",
      "[step: 4740] loss: 1.2952125072479248\n",
      "[step: 4741] loss: 1.2962474822998047\n",
      "[step: 4742] loss: 1.2968008518218994\n",
      "[step: 4743] loss: 1.2965031862258911\n",
      "[step: 4744] loss: 1.2958165407180786\n",
      "[step: 4745] loss: 1.2959113121032715\n",
      "[step: 4746] loss: 1.2967796325683594\n",
      "[step: 4747] loss: 1.2984225749969482\n",
      "[step: 4748] loss: 1.3002266883850098\n",
      "[step: 4749] loss: 1.302870512008667\n",
      "[step: 4750] loss: 1.3069062232971191\n",
      "[step: 4751] loss: 1.3145493268966675\n",
      "[step: 4752] loss: 1.3258920907974243\n",
      "[step: 4753] loss: 1.3461213111877441\n",
      "[step: 4754] loss: 1.3742717504501343\n",
      "[step: 4755] loss: 1.4272058010101318\n",
      "[step: 4756] loss: 1.4987492561340332\n",
      "[step: 4757] loss: 1.6381654739379883\n",
      "[step: 4758] loss: 1.7989821434020996\n",
      "[step: 4759] loss: 2.111386775970459\n",
      "[step: 4760] loss: 2.3262524604797363\n",
      "[step: 4761] loss: 2.7077410221099854\n",
      "[step: 4762] loss: 2.553461790084839\n",
      "[step: 4763] loss: 2.3618083000183105\n",
      "[step: 4764] loss: 1.7681598663330078\n",
      "[step: 4765] loss: 1.3785345554351807\n",
      "[step: 4766] loss: 1.327362060546875\n",
      "[step: 4767] loss: 1.554599642753601\n",
      "[step: 4768] loss: 1.815284252166748\n",
      "[step: 4769] loss: 1.7598440647125244\n",
      "[step: 4770] loss: 1.538824200630188\n",
      "[step: 4771] loss: 1.3250128030776978\n",
      "[step: 4772] loss: 1.3324134349822998\n",
      "[step: 4773] loss: 1.4935054779052734\n",
      "[step: 4774] loss: 1.5850579738616943\n",
      "[step: 4775] loss: 1.5412989854812622\n",
      "[step: 4776] loss: 1.3852944374084473\n",
      "[step: 4777] loss: 1.2984282970428467\n",
      "[step: 4778] loss: 1.329783320426941\n",
      "[step: 4779] loss: 1.4152922630310059\n",
      "[step: 4780] loss: 1.464491605758667\n",
      "[step: 4781] loss: 1.4202158451080322\n",
      "[step: 4782] loss: 1.351523756980896\n",
      "[step: 4783] loss: 1.3040060997009277\n",
      "[step: 4784] loss: 1.3059030771255493\n",
      "[step: 4785] loss: 1.3381344079971313\n",
      "[step: 4786] loss: 1.3651950359344482\n",
      "[step: 4787] loss: 1.3721377849578857\n",
      "[step: 4788] loss: 1.3493356704711914\n",
      "[step: 4789] loss: 1.3197394609451294\n",
      "[step: 4790] loss: 1.292921543121338\n",
      "[step: 4791] loss: 1.284928798675537\n",
      "[step: 4792] loss: 1.297446846961975\n",
      "[step: 4793] loss: 1.3176159858703613\n",
      "[step: 4794] loss: 1.332123041152954\n",
      "[step: 4795] loss: 1.327521562576294\n",
      "[step: 4796] loss: 1.3119664192199707\n",
      "[step: 4797] loss: 1.2920048236846924\n",
      "[step: 4798] loss: 1.2804120779037476\n",
      "[step: 4799] loss: 1.2783159017562866\n",
      "[step: 4800] loss: 1.2812689542770386\n",
      "[step: 4801] loss: 1.28573739528656\n",
      "[step: 4802] loss: 1.2901191711425781\n",
      "[step: 4803] loss: 1.295072078704834\n",
      "[step: 4804] loss: 1.2987720966339111\n",
      "[step: 4805] loss: 1.3011012077331543\n",
      "[step: 4806] loss: 1.2994422912597656\n",
      "[step: 4807] loss: 1.2955182790756226\n",
      "[step: 4808] loss: 1.2904114723205566\n",
      "[step: 4809] loss: 1.2867529392242432\n",
      "[step: 4810] loss: 1.2836055755615234\n",
      "[step: 4811] loss: 1.2813923358917236\n",
      "[step: 4812] loss: 1.2786732912063599\n",
      "[step: 4813] loss: 1.276219367980957\n",
      "[step: 4814] loss: 1.2742531299591064\n",
      "[step: 4815] loss: 1.273642897605896\n",
      "[step: 4816] loss: 1.2738769054412842\n",
      "[step: 4817] loss: 1.274881362915039\n",
      "[step: 4818] loss: 1.2763968706130981\n",
      "[step: 4819] loss: 1.279302716255188\n",
      "[step: 4820] loss: 1.2843047380447388\n",
      "[step: 4821] loss: 1.2940921783447266\n",
      "[step: 4822] loss: 1.3092032670974731\n",
      "[step: 4823] loss: 1.3369454145431519\n",
      "[step: 4824] loss: 1.3772780895233154\n",
      "[step: 4825] loss: 1.4548369646072388\n",
      "[step: 4826] loss: 1.5607939958572388\n",
      "[step: 4827] loss: 1.7703964710235596\n",
      "[step: 4828] loss: 1.997583031654358\n",
      "[step: 4829] loss: 2.4305388927459717\n",
      "[step: 4830] loss: 2.619420051574707\n",
      "[step: 4831] loss: 2.914414644241333\n",
      "[step: 4832] loss: 2.459703207015991\n",
      "[step: 4833] loss: 1.957279920578003\n",
      "[step: 4834] loss: 1.419814944267273\n",
      "[step: 4835] loss: 1.3022249937057495\n",
      "[step: 4836] loss: 1.5644912719726562\n",
      "[step: 4837] loss: 1.8216896057128906\n",
      "[step: 4838] loss: 1.846999168395996\n",
      "[step: 4839] loss: 1.5218570232391357\n",
      "[step: 4840] loss: 1.2944496870040894\n",
      "[step: 4841] loss: 1.3472509384155273\n",
      "[step: 4842] loss: 1.5402650833129883\n",
      "[step: 4843] loss: 1.635947346687317\n",
      "[step: 4844] loss: 1.4851717948913574\n",
      "[step: 4845] loss: 1.3170186281204224\n",
      "[step: 4846] loss: 1.2844951152801514\n",
      "[step: 4847] loss: 1.3808776140213013\n",
      "[step: 4848] loss: 1.4739069938659668\n",
      "[step: 4849] loss: 1.4416321516036987\n",
      "[step: 4850] loss: 1.350590705871582\n",
      "[step: 4851] loss: 1.2853269577026367\n",
      "[step: 4852] loss: 1.2968335151672363\n",
      "[step: 4853] loss: 1.3439323902130127\n",
      "[step: 4854] loss: 1.3666837215423584\n",
      "[step: 4855] loss: 1.3534314632415771\n",
      "[step: 4856] loss: 1.3146989345550537\n",
      "[step: 4857] loss: 1.288718819618225\n",
      "[step: 4858] loss: 1.280814528465271\n",
      "[step: 4859] loss: 1.2870019674301147\n",
      "[step: 4860] loss: 1.2994837760925293\n",
      "[step: 4861] loss: 1.3078813552856445\n",
      "[step: 4862] loss: 1.3082654476165771\n",
      "[step: 4863] loss: 1.292494535446167\n",
      "[step: 4864] loss: 1.2716916799545288\n",
      "[step: 4865] loss: 1.2574857473373413\n",
      "[step: 4866] loss: 1.2588647603988647\n",
      "[step: 4867] loss: 1.2719786167144775\n",
      "[step: 4868] loss: 1.283300518989563\n",
      "[step: 4869] loss: 1.2851738929748535\n",
      "[step: 4870] loss: 1.2766435146331787\n",
      "[step: 4871] loss: 1.2671914100646973\n",
      "[step: 4872] loss: 1.2607650756835938\n",
      "[step: 4873] loss: 1.2578818798065186\n",
      "[step: 4874] loss: 1.2553977966308594\n",
      "[step: 4875] loss: 1.2522450685501099\n",
      "[step: 4876] loss: 1.2505155801773071\n",
      "[step: 4877] loss: 1.2520896196365356\n",
      "[step: 4878] loss: 1.2562235593795776\n",
      "[step: 4879] loss: 1.2601044178009033\n",
      "[step: 4880] loss: 1.2621073722839355\n",
      "[step: 4881] loss: 1.2622342109680176\n",
      "[step: 4882] loss: 1.2625616788864136\n",
      "[step: 4883] loss: 1.2637341022491455\n",
      "[step: 4884] loss: 1.2666410207748413\n",
      "[step: 4885] loss: 1.268982172012329\n",
      "[step: 4886] loss: 1.272337794303894\n",
      "[step: 4887] loss: 1.2753480672836304\n",
      "[step: 4888] loss: 1.2822203636169434\n",
      "[step: 4889] loss: 1.2915010452270508\n",
      "[step: 4890] loss: 1.308550238609314\n",
      "[step: 4891] loss: 1.3297024965286255\n",
      "[step: 4892] loss: 1.3658764362335205\n",
      "[step: 4893] loss: 1.4090688228607178\n",
      "[step: 4894] loss: 1.4875389337539673\n",
      "[step: 4895] loss: 1.5704501867294312\n",
      "[step: 4896] loss: 1.7245041131973267\n",
      "[step: 4897] loss: 1.8344824314117432\n",
      "[step: 4898] loss: 2.0370101928710938\n",
      "[step: 4899] loss: 2.0475008487701416\n",
      "[step: 4900] loss: 2.0934154987335205\n",
      "[step: 4901] loss: 1.8586045503616333\n",
      "[step: 4902] loss: 1.6292219161987305\n",
      "[step: 4903] loss: 1.3764727115631104\n",
      "[step: 4904] loss: 1.254806399345398\n",
      "[step: 4905] loss: 1.280853271484375\n",
      "[step: 4906] loss: 1.3968243598937988\n",
      "[step: 4907] loss: 1.5180550813674927\n",
      "[step: 4908] loss: 1.5279779434204102\n",
      "[step: 4909] loss: 1.4647873640060425\n",
      "[step: 4910] loss: 1.3416852951049805\n",
      "[step: 4911] loss: 1.2586711645126343\n",
      "[step: 4912] loss: 1.2457504272460938\n",
      "[step: 4913] loss: 1.2904900312423706\n",
      "[step: 4914] loss: 1.3528038263320923\n",
      "[step: 4915] loss: 1.384902000427246\n",
      "[step: 4916] loss: 1.3853695392608643\n",
      "[step: 4917] loss: 1.342170000076294\n",
      "[step: 4918] loss: 1.296459436416626\n",
      "[step: 4919] loss: 1.2577589750289917\n",
      "[step: 4920] loss: 1.240931510925293\n",
      "[step: 4921] loss: 1.2448347806930542\n",
      "[step: 4922] loss: 1.2628092765808105\n",
      "[step: 4923] loss: 1.286436915397644\n",
      "[step: 4924] loss: 1.302948236465454\n",
      "[step: 4925] loss: 1.312915563583374\n",
      "[step: 4926] loss: 1.305357575416565\n",
      "[step: 4927] loss: 1.2929625511169434\n",
      "[step: 4928] loss: 1.2732715606689453\n",
      "[step: 4929] loss: 1.257123351097107\n",
      "[step: 4930] loss: 1.2443552017211914\n",
      "[step: 4931] loss: 1.2362521886825562\n",
      "[step: 4932] loss: 1.2313809394836426\n",
      "[step: 4933] loss: 1.2291150093078613\n",
      "[step: 4934] loss: 1.2295918464660645\n",
      "[step: 4935] loss: 1.232783555984497\n",
      "[step: 4936] loss: 1.238222599029541\n",
      "[step: 4937] loss: 1.2448920011520386\n",
      "[step: 4938] loss: 1.2531712055206299\n",
      "[step: 4939] loss: 1.2628916501998901\n",
      "[step: 4940] loss: 1.2778645753860474\n",
      "[step: 4941] loss: 1.297410488128662\n",
      "[step: 4942] loss: 1.331465244293213\n",
      "[step: 4943] loss: 1.3741012811660767\n",
      "[step: 4944] loss: 1.4521026611328125\n",
      "[step: 4945] loss: 1.5404713153839111\n",
      "[step: 4946] loss: 1.7072958946228027\n",
      "[step: 4947] loss: 1.8527953624725342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 4948] loss: 2.1189160346984863\n",
      "[step: 4949] loss: 2.1975278854370117\n",
      "[step: 4950] loss: 2.325995922088623\n",
      "[step: 4951] loss: 2.060314178466797\n",
      "[step: 4952] loss: 1.7700873613357544\n",
      "[step: 4953] loss: 1.4095354080200195\n",
      "[step: 4954] loss: 1.2401816844940186\n",
      "[step: 4955] loss: 1.2953121662139893\n",
      "[step: 4956] loss: 1.4690651893615723\n",
      "[step: 4957] loss: 1.615341305732727\n",
      "[step: 4958] loss: 1.5626585483551025\n",
      "[step: 4959] loss: 1.4178571701049805\n",
      "[step: 4960] loss: 1.267865777015686\n",
      "[step: 4961] loss: 1.2356376647949219\n",
      "[step: 4962] loss: 1.3089717626571655\n",
      "[step: 4963] loss: 1.3967386484146118\n",
      "[step: 4964] loss: 1.4341351985931396\n",
      "[step: 4965] loss: 1.377877950668335\n",
      "[step: 4966] loss: 1.300889253616333\n",
      "[step: 4967] loss: 1.2441225051879883\n",
      "[step: 4968] loss: 1.2394860982894897\n",
      "[step: 4969] loss: 1.2699624300003052\n",
      "[step: 4970] loss: 1.303659439086914\n",
      "[step: 4971] loss: 1.3231918811798096\n",
      "[step: 4972] loss: 1.312208890914917\n",
      "[step: 4973] loss: 1.2896331548690796\n",
      "[step: 4974] loss: 1.2577080726623535\n",
      "[step: 4975] loss: 1.2339869737625122\n",
      "[step: 4976] loss: 1.2234511375427246\n",
      "[step: 4977] loss: 1.2288596630096436\n",
      "[step: 4978] loss: 1.245720386505127\n",
      "[step: 4979] loss: 1.2620813846588135\n",
      "[step: 4980] loss: 1.2718138694763184\n",
      "[step: 4981] loss: 1.2665793895721436\n",
      "[step: 4982] loss: 1.2557364702224731\n",
      "[step: 4983] loss: 1.241253137588501\n",
      "[step: 4984] loss: 1.230902075767517\n",
      "[step: 4985] loss: 1.2238773107528687\n",
      "[step: 4986] loss: 1.2189871072769165\n",
      "[step: 4987] loss: 1.2151727676391602\n",
      "[step: 4988] loss: 1.2127923965454102\n",
      "[step: 4989] loss: 1.2129523754119873\n",
      "[step: 4990] loss: 1.2159719467163086\n",
      "[step: 4991] loss: 1.221069574356079\n",
      "[step: 4992] loss: 1.2267245054244995\n",
      "[step: 4993] loss: 1.2328636646270752\n",
      "[step: 4994] loss: 1.2398511171340942\n",
      "[step: 4995] loss: 1.2510340213775635\n",
      "[step: 4996] loss: 1.2662758827209473\n",
      "[step: 4997] loss: 1.292836308479309\n",
      "[step: 4998] loss: 1.325937032699585\n",
      "[step: 4999] loss: 1.3851869106292725\n",
      "[step: 5000] loss: 1.4538203477859497\n",
      "[step: 5001] loss: 1.582992672920227\n",
      "[step: 5002] loss: 1.7088756561279297\n",
      "[step: 5003] loss: 1.9418317079544067\n",
      "[step: 5004] loss: 2.064854383468628\n",
      "[step: 5005] loss: 2.270179271697998\n",
      "[step: 5006] loss: 2.1345834732055664\n",
      "[step: 5007] loss: 1.9701260328292847\n",
      "[step: 5008] loss: 1.584691047668457\n",
      "[step: 5009] loss: 1.3073444366455078\n",
      "[step: 5010] loss: 1.2173175811767578\n",
      "[step: 5011] loss: 1.320639729499817\n",
      "[step: 5012] loss: 1.500022292137146\n",
      "[step: 5013] loss: 1.5660057067871094\n",
      "[step: 5014] loss: 1.512880563735962\n",
      "[step: 5015] loss: 1.3446550369262695\n",
      "[step: 5016] loss: 1.2299363613128662\n",
      "[step: 5017] loss: 1.225717544555664\n",
      "[step: 5018] loss: 1.3031705617904663\n",
      "[step: 5019] loss: 1.3860905170440674\n",
      "[step: 5020] loss: 1.395460605621338\n",
      "[step: 5021] loss: 1.3523988723754883\n",
      "[step: 5022] loss: 1.27390718460083\n",
      "[step: 5023] loss: 1.2252315282821655\n",
      "[step: 5024] loss: 1.2200876474380493\n",
      "[step: 5025] loss: 1.2461566925048828\n",
      "[step: 5026] loss: 1.2791593074798584\n",
      "[step: 5027] loss: 1.295682430267334\n",
      "[step: 5028] loss: 1.2966517210006714\n",
      "[step: 5029] loss: 1.27340829372406\n",
      "[step: 5030] loss: 1.246321678161621\n",
      "[step: 5031] loss: 1.219518780708313\n",
      "[step: 5032] loss: 1.2044825553894043\n",
      "[step: 5033] loss: 1.204618215560913\n",
      "[step: 5034] loss: 1.217139482498169\n",
      "[step: 5035] loss: 1.2343528270721436\n",
      "[step: 5036] loss: 1.2453069686889648\n",
      "[step: 5037] loss: 1.2494840621948242\n",
      "[step: 5038] loss: 1.2430598735809326\n",
      "[step: 5039] loss: 1.2353370189666748\n",
      "[step: 5040] loss: 1.226070523262024\n",
      "[step: 5041] loss: 1.21891450881958\n",
      "[step: 5042] loss: 1.2118513584136963\n",
      "[step: 5043] loss: 1.205061912536621\n",
      "[step: 5044] loss: 1.1989226341247559\n",
      "[step: 5045] loss: 1.1945830583572388\n",
      "[step: 5046] loss: 1.1926937103271484\n",
      "[step: 5047] loss: 1.1927769184112549\n",
      "[step: 5048] loss: 1.1936306953430176\n",
      "[step: 5049] loss: 1.194392442703247\n",
      "[step: 5050] loss: 1.1950141191482544\n",
      "[step: 5051] loss: 1.196233868598938\n",
      "[step: 5052] loss: 1.199133276939392\n",
      "[step: 5053] loss: 1.2043635845184326\n",
      "[step: 5054] loss: 1.2135556936264038\n",
      "[step: 5055] loss: 1.2272570133209229\n",
      "[step: 5056] loss: 1.2517905235290527\n",
      "[step: 5057] loss: 1.2888305187225342\n",
      "[step: 5058] loss: 1.3604873418807983\n",
      "[step: 5059] loss: 1.4642523527145386\n",
      "[step: 5060] loss: 1.6701023578643799\n",
      "[step: 5061] loss: 1.915535569190979\n",
      "[step: 5062] loss: 2.387934923171997\n",
      "[step: 5063] loss: 2.6557281017303467\n",
      "[step: 5064] loss: 3.0658481121063232\n",
      "[step: 5065] loss: 2.5965940952301025\n",
      "[step: 5066] loss: 2.031813144683838\n",
      "[step: 5067] loss: 1.380427360534668\n",
      "[step: 5068] loss: 1.2487123012542725\n",
      "[step: 5069] loss: 1.5842243432998657\n",
      "[step: 5070] loss: 1.860231637954712\n",
      "[step: 5071] loss: 1.8177127838134766\n",
      "[step: 5072] loss: 1.4060927629470825\n",
      "[step: 5073] loss: 1.218735694885254\n",
      "[step: 5074] loss: 1.3903207778930664\n",
      "[step: 5075] loss: 1.5937978029251099\n",
      "[step: 5076] loss: 1.5808818340301514\n",
      "[step: 5077] loss: 1.328004002571106\n",
      "[step: 5078] loss: 1.2065579891204834\n",
      "[step: 5079] loss: 1.3070344924926758\n",
      "[step: 5080] loss: 1.4379098415374756\n",
      "[step: 5081] loss: 1.4437178373336792\n",
      "[step: 5082] loss: 1.298326849937439\n",
      "[step: 5083] loss: 1.2079119682312012\n",
      "[step: 5084] loss: 1.245384931564331\n",
      "[step: 5085] loss: 1.3242111206054688\n",
      "[step: 5086] loss: 1.3465616703033447\n",
      "[step: 5087] loss: 1.2808477878570557\n",
      "[step: 5088] loss: 1.21949303150177\n",
      "[step: 5089] loss: 1.2126023769378662\n",
      "[step: 5090] loss: 1.2453408241271973\n",
      "[step: 5091] loss: 1.2708678245544434\n",
      "[step: 5092] loss: 1.2587151527404785\n",
      "[step: 5093] loss: 1.2307146787643433\n",
      "[step: 5094] loss: 1.2086565494537354\n",
      "[step: 5095] loss: 1.2054729461669922\n",
      "[step: 5096] loss: 1.2128486633300781\n",
      "[step: 5097] loss: 1.2201759815216064\n",
      "[step: 5098] loss: 1.223658561706543\n",
      "[step: 5099] loss: 1.2185192108154297\n",
      "[step: 5100] loss: 1.208938717842102\n",
      "[step: 5101] loss: 1.196180820465088\n",
      "[step: 5102] loss: 1.1874967813491821\n",
      "[step: 5103] loss: 1.1886295080184937\n",
      "[step: 5104] loss: 1.1970971822738647\n",
      "[step: 5105] loss: 1.2059212923049927\n",
      "[step: 5106] loss: 1.2063438892364502\n",
      "[step: 5107] loss: 1.1987664699554443\n",
      "[step: 5108] loss: 1.1880524158477783\n",
      "[step: 5109] loss: 1.1810245513916016\n",
      "[step: 5110] loss: 1.1795533895492554\n",
      "[step: 5111] loss: 1.1812902688980103\n",
      "[step: 5112] loss: 1.1826585531234741\n",
      "[step: 5113] loss: 1.182809591293335\n",
      "[step: 5114] loss: 1.1827938556671143\n",
      "[step: 5115] loss: 1.183650255203247\n",
      "[step: 5116] loss: 1.1855101585388184\n",
      "[step: 5117] loss: 1.1863906383514404\n",
      "[step: 5118] loss: 1.185645580291748\n",
      "[step: 5119] loss: 1.183219313621521\n",
      "[step: 5120] loss: 1.18061101436615\n",
      "[step: 5121] loss: 1.1787139177322388\n",
      "[step: 5122] loss: 1.1778885126113892\n",
      "[step: 5123] loss: 1.1773465871810913\n",
      "[step: 5124] loss: 1.176655650138855\n",
      "[step: 5125] loss: 1.1754602193832397\n",
      "[step: 5126] loss: 1.1744946241378784\n",
      "[step: 5127] loss: 1.1740965843200684\n",
      "[step: 5128] loss: 1.174757957458496\n",
      "[step: 5129] loss: 1.1762409210205078\n",
      "[step: 5130] loss: 1.1787370443344116\n",
      "[step: 5131] loss: 1.182315468788147\n",
      "[step: 5132] loss: 1.1885662078857422\n",
      "[step: 5133] loss: 1.198533535003662\n",
      "[step: 5134] loss: 1.21708345413208\n",
      "[step: 5135] loss: 1.2455525398254395\n",
      "[step: 5136] loss: 1.2993099689483643\n",
      "[step: 5137] loss: 1.376922369003296\n",
      "[step: 5138] loss: 1.530379295349121\n",
      "[step: 5139] loss: 1.721738338470459\n",
      "[step: 5140] loss: 2.099663257598877\n",
      "[step: 5141] loss: 2.382237195968628\n",
      "[step: 5142] loss: 2.8712522983551025\n",
      "[step: 5143] loss: 2.681973934173584\n",
      "[step: 5144] loss: 2.4005420207977295\n",
      "[step: 5145] loss: 1.6649055480957031\n",
      "[step: 5146] loss: 1.239422082901001\n",
      "[step: 5147] loss: 1.3006786108016968\n",
      "[step: 5148] loss: 1.6343631744384766\n",
      "[step: 5149] loss: 1.8713958263397217\n",
      "[step: 5150] loss: 1.6182887554168701\n",
      "[step: 5151] loss: 1.2887353897094727\n",
      "[step: 5152] loss: 1.207353115081787\n",
      "[step: 5153] loss: 1.397554636001587\n",
      "[step: 5154] loss: 1.584367275238037\n",
      "[step: 5155] loss: 1.476599931716919\n",
      "[step: 5156] loss: 1.2687180042266846\n",
      "[step: 5157] loss: 1.1826826333999634\n",
      "[step: 5158] loss: 1.2800745964050293\n",
      "[step: 5159] loss: 1.4093295335769653\n",
      "[step: 5160] loss: 1.3872857093811035\n",
      "[step: 5161] loss: 1.2710398435592651\n",
      "[step: 5162] loss: 1.1835745573043823\n",
      "[step: 5163] loss: 1.2069249153137207\n",
      "[step: 5164] loss: 1.2804142236709595\n",
      "[step: 5165] loss: 1.3049685955047607\n",
      "[step: 5166] loss: 1.2668949365615845\n",
      "[step: 5167] loss: 1.2037525177001953\n",
      "[step: 5168] loss: 1.1807867288589478\n",
      "[step: 5169] loss: 1.1993255615234375\n",
      "[step: 5170] loss: 1.2265874147415161\n",
      "[step: 5171] loss: 1.2345736026763916\n",
      "[step: 5172] loss: 1.2158634662628174\n",
      "[step: 5173] loss: 1.1934583187103271\n",
      "[step: 5174] loss: 1.178146481513977\n",
      "[step: 5175] loss: 1.1756633520126343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5176] loss: 1.1821320056915283\n",
      "[step: 5177] loss: 1.191140055656433\n",
      "[step: 5178] loss: 1.1971697807312012\n",
      "[step: 5179] loss: 1.19295334815979\n",
      "[step: 5180] loss: 1.1812189817428589\n",
      "[step: 5181] loss: 1.1666960716247559\n",
      "[step: 5182] loss: 1.1590886116027832\n",
      "[step: 5183] loss: 1.1612707376480103\n",
      "[step: 5184] loss: 1.1697067022323608\n",
      "[step: 5185] loss: 1.1772499084472656\n",
      "[step: 5186] loss: 1.1780385971069336\n",
      "[step: 5187] loss: 1.1743011474609375\n",
      "[step: 5188] loss: 1.1681779623031616\n",
      "[step: 5189] loss: 1.1637424230575562\n",
      "[step: 5190] loss: 1.1607327461242676\n",
      "[step: 5191] loss: 1.1580983400344849\n",
      "[step: 5192] loss: 1.1553685665130615\n",
      "[step: 5193] loss: 1.1529992818832397\n",
      "[step: 5194] loss: 1.1523635387420654\n",
      "[step: 5195] loss: 1.1536520719528198\n",
      "[step: 5196] loss: 1.1558828353881836\n",
      "[step: 5197] loss: 1.1578896045684814\n",
      "[step: 5198] loss: 1.1590278148651123\n",
      "[step: 5199] loss: 1.1595977544784546\n",
      "[step: 5200] loss: 1.1606764793395996\n",
      "[step: 5201] loss: 1.1623948812484741\n",
      "[step: 5202] loss: 1.1655027866363525\n",
      "[step: 5203] loss: 1.168893575668335\n",
      "[step: 5204] loss: 1.1740028858184814\n",
      "[step: 5205] loss: 1.1798491477966309\n",
      "[step: 5206] loss: 1.190209984779358\n",
      "[step: 5207] loss: 1.2038058042526245\n",
      "[step: 5208] loss: 1.2279078960418701\n",
      "[step: 5209] loss: 1.2588245868682861\n",
      "[step: 5210] loss: 1.3135420083999634\n",
      "[step: 5211] loss: 1.3798744678497314\n",
      "[step: 5212] loss: 1.501474142074585\n",
      "[step: 5213] loss: 1.6241309642791748\n",
      "[step: 5214] loss: 1.848422646522522\n",
      "[step: 5215] loss: 1.970139503479004\n",
      "[step: 5216] loss: 2.18137788772583\n",
      "[step: 5217] loss: 2.059751510620117\n",
      "[step: 5218] loss: 1.9223270416259766\n",
      "[step: 5219] loss: 1.548129916191101\n",
      "[step: 5220] loss: 1.2694734334945679\n",
      "[step: 5221] loss: 1.1592812538146973\n",
      "[step: 5222] loss: 1.2392780780792236\n",
      "[step: 5223] loss: 1.4113023281097412\n",
      "[step: 5224] loss: 1.4950062036514282\n",
      "[step: 5225] loss: 1.46942138671875\n",
      "[step: 5226] loss: 1.3122706413269043\n",
      "[step: 5227] loss: 1.184006929397583\n",
      "[step: 5228] loss: 1.153515338897705\n",
      "[step: 5229] loss: 1.216240644454956\n",
      "[step: 5230] loss: 1.3066306114196777\n",
      "[step: 5231] loss: 1.3388606309890747\n",
      "[step: 5232] loss: 1.312982201576233\n",
      "[step: 5233] loss: 1.232553482055664\n",
      "[step: 5234] loss: 1.1683754920959473\n",
      "[step: 5235] loss: 1.1478224992752075\n",
      "[step: 5236] loss: 1.1698400974273682\n",
      "[step: 5237] loss: 1.2095038890838623\n",
      "[step: 5238] loss: 1.2362834215164185\n",
      "[step: 5239] loss: 1.2426170110702515\n",
      "[step: 5240] loss: 1.219434380531311\n",
      "[step: 5241] loss: 1.1904268264770508\n",
      "[step: 5242] loss: 1.1619936227798462\n",
      "[step: 5243] loss: 1.1458415985107422\n",
      "[step: 5244] loss: 1.1433570384979248\n",
      "[step: 5245] loss: 1.15220308303833\n",
      "[step: 5246] loss: 1.167032241821289\n",
      "[step: 5247] loss: 1.180019497871399\n",
      "[step: 5248] loss: 1.189133882522583\n",
      "[step: 5249] loss: 1.1877788305282593\n",
      "[step: 5250] loss: 1.1822171211242676\n",
      "[step: 5251] loss: 1.1708064079284668\n",
      "[step: 5252] loss: 1.1604480743408203\n",
      "[step: 5253] loss: 1.1510059833526611\n",
      "[step: 5254] loss: 1.1443235874176025\n",
      "[step: 5255] loss: 1.1394078731536865\n",
      "[step: 5256] loss: 1.1357848644256592\n",
      "[step: 5257] loss: 1.1333122253417969\n",
      "[step: 5258] loss: 1.13214111328125\n",
      "[step: 5259] loss: 1.1323561668395996\n",
      "[step: 5260] loss: 1.1337534189224243\n",
      "[step: 5261] loss: 1.1360116004943848\n",
      "[step: 5262] loss: 1.1389352083206177\n",
      "[step: 5263] loss: 1.1429916620254517\n",
      "[step: 5264] loss: 1.1488233804702759\n",
      "[step: 5265] loss: 1.1586834192276\n",
      "[step: 5266] loss: 1.1736369132995605\n",
      "[step: 5267] loss: 1.2003083229064941\n",
      "[step: 5268] loss: 1.23918879032135\n",
      "[step: 5269] loss: 1.3116610050201416\n",
      "[step: 5270] loss: 1.4098323583602905\n",
      "[step: 5271] loss: 1.5993883609771729\n",
      "[step: 5272] loss: 1.808087706565857\n",
      "[step: 5273] loss: 2.200584888458252\n",
      "[step: 5274] loss: 2.4031968116760254\n",
      "[step: 5275] loss: 2.710439682006836\n",
      "[step: 5276] loss: 2.3546900749206543\n",
      "[step: 5277] loss: 1.916398525238037\n",
      "[step: 5278] loss: 1.3598636388778687\n",
      "[step: 5279] loss: 1.1655292510986328\n",
      "[step: 5280] loss: 1.361045479774475\n",
      "[step: 5281] loss: 1.6326137781143188\n",
      "[step: 5282] loss: 1.7212653160095215\n",
      "[step: 5283] loss: 1.4415251016616821\n",
      "[step: 5284] loss: 1.188859462738037\n",
      "[step: 5285] loss: 1.1820695400238037\n",
      "[step: 5286] loss: 1.3586506843566895\n",
      "[step: 5287] loss: 1.4943206310272217\n",
      "[step: 5288] loss: 1.392171025276184\n",
      "[step: 5289] loss: 1.220905065536499\n",
      "[step: 5290] loss: 1.1410335302352905\n",
      "[step: 5291] loss: 1.2076690196990967\n",
      "[step: 5292] loss: 1.3184285163879395\n",
      "[step: 5293] loss: 1.3291122913360596\n",
      "[step: 5294] loss: 1.254192590713501\n",
      "[step: 5295] loss: 1.1640841960906982\n",
      "[step: 5296] loss: 1.1452336311340332\n",
      "[step: 5297] loss: 1.1880598068237305\n",
      "[step: 5298] loss: 1.231870174407959\n",
      "[step: 5299] loss: 1.2356855869293213\n",
      "[step: 5300] loss: 1.1953508853912354\n",
      "[step: 5301] loss: 1.1578242778778076\n",
      "[step: 5302] loss: 1.1432164907455444\n",
      "[step: 5303] loss: 1.1521546840667725\n",
      "[step: 5304] loss: 1.1681660413742065\n",
      "[step: 5305] loss: 1.176019310951233\n",
      "[step: 5306] loss: 1.1744006872177124\n",
      "[step: 5307] loss: 1.1626253128051758\n",
      "[step: 5308] loss: 1.1492390632629395\n",
      "[step: 5309] loss: 1.1363195180892944\n",
      "[step: 5310] loss: 1.130171775817871\n",
      "[step: 5311] loss: 1.1330809593200684\n",
      "[step: 5312] loss: 1.1422383785247803\n",
      "[step: 5313] loss: 1.1518322229385376\n",
      "[step: 5314] loss: 1.153579831123352\n",
      "[step: 5315] loss: 1.147633671760559\n",
      "[step: 5316] loss: 1.135772705078125\n",
      "[step: 5317] loss: 1.1259369850158691\n",
      "[step: 5318] loss: 1.121114730834961\n",
      "[step: 5319] loss: 1.1210321187973022\n",
      "[step: 5320] loss: 1.122918725013733\n",
      "[step: 5321] loss: 1.1241397857666016\n",
      "[step: 5322] loss: 1.1245845556259155\n",
      "[step: 5323] loss: 1.1251943111419678\n",
      "[step: 5324] loss: 1.1272374391555786\n",
      "[step: 5325] loss: 1.1299562454223633\n",
      "[step: 5326] loss: 1.1326960325241089\n",
      "[step: 5327] loss: 1.1340742111206055\n",
      "[step: 5328] loss: 1.134596586227417\n",
      "[step: 5329] loss: 1.1346203088760376\n",
      "[step: 5330] loss: 1.1362041234970093\n",
      "[step: 5331] loss: 1.13876473903656\n",
      "[step: 5332] loss: 1.1440694332122803\n",
      "[step: 5333] loss: 1.1500070095062256\n",
      "[step: 5334] loss: 1.1599631309509277\n",
      "[step: 5335] loss: 1.171481728553772\n",
      "[step: 5336] loss: 1.1924514770507812\n",
      "[step: 5337] loss: 1.2183674573898315\n",
      "[step: 5338] loss: 1.2651679515838623\n",
      "[step: 5339] loss: 1.320247769355774\n",
      "[step: 5340] loss: 1.4189996719360352\n",
      "[step: 5341] loss: 1.518233060836792\n",
      "[step: 5342] loss: 1.6956346035003662\n",
      "[step: 5343] loss: 1.806657314300537\n",
      "[step: 5344] loss: 1.997172236442566\n",
      "[step: 5345] loss: 1.9465186595916748\n",
      "[step: 5346] loss: 1.8932788372039795\n",
      "[step: 5347] loss: 1.5893800258636475\n",
      "[step: 5348] loss: 1.326788067817688\n",
      "[step: 5349] loss: 1.1470974683761597\n",
      "[step: 5350] loss: 1.1337937116622925\n",
      "[step: 5351] loss: 1.2470662593841553\n",
      "[step: 5352] loss: 1.3700785636901855\n",
      "[step: 5353] loss: 1.4356515407562256\n",
      "[step: 5354] loss: 1.3571853637695312\n",
      "[step: 5355] loss: 1.2400779724121094\n",
      "[step: 5356] loss: 1.1385711431503296\n",
      "[step: 5357] loss: 1.1153934001922607\n",
      "[step: 5358] loss: 1.1618990898132324\n",
      "[step: 5359] loss: 1.228891372680664\n",
      "[step: 5360] loss: 1.2765971422195435\n",
      "[step: 5361] loss: 1.262709140777588\n",
      "[step: 5362] loss: 1.2192723751068115\n",
      "[step: 5363] loss: 1.159027099609375\n",
      "[step: 5364] loss: 1.1203441619873047\n",
      "[step: 5365] loss: 1.1117016077041626\n",
      "[step: 5366] loss: 1.1282436847686768\n",
      "[step: 5367] loss: 1.155853271484375\n",
      "[step: 5368] loss: 1.1774331331253052\n",
      "[step: 5369] loss: 1.1892582178115845\n",
      "[step: 5370] loss: 1.1816442012786865\n",
      "[step: 5371] loss: 1.1671054363250732\n",
      "[step: 5372] loss: 1.1439740657806396\n",
      "[step: 5373] loss: 1.123783826828003\n",
      "[step: 5374] loss: 1.1089946031570435\n",
      "[step: 5375] loss: 1.1027944087982178\n",
      "[step: 5376] loss: 1.1044037342071533\n",
      "[step: 5377] loss: 1.1112539768218994\n",
      "[step: 5378] loss: 1.1207267045974731\n",
      "[step: 5379] loss: 1.1298257112503052\n",
      "[step: 5380] loss: 1.1390780210494995\n",
      "[step: 5381] loss: 1.1456857919692993\n",
      "[step: 5382] loss: 1.1538259983062744\n",
      "[step: 5383] loss: 1.160057544708252\n",
      "[step: 5384] loss: 1.1704703569412231\n",
      "[step: 5385] loss: 1.179840087890625\n",
      "[step: 5386] loss: 1.1962902545928955\n",
      "[step: 5387] loss: 1.2122300863265991\n",
      "[step: 5388] loss: 1.2409870624542236\n",
      "[step: 5389] loss: 1.2687082290649414\n",
      "[step: 5390] loss: 1.3192065954208374\n",
      "[step: 5391] loss: 1.3620095252990723\n",
      "[step: 5392] loss: 1.4399837255477905\n",
      "[step: 5393] loss: 1.4863989353179932\n",
      "[step: 5394] loss: 1.5725769996643066\n",
      "[step: 5395] loss: 1.578719139099121\n",
      "[step: 5396] loss: 1.6061549186706543\n",
      "[step: 5397] loss: 1.5200273990631104\n",
      "[step: 5398] loss: 1.4356411695480347\n",
      "[step: 5399] loss: 1.2967555522918701\n",
      "[step: 5400] loss: 1.1866936683654785\n",
      "[step: 5401] loss: 1.1154682636260986\n",
      "[step: 5402] loss: 1.0998386144638062\n",
      "[step: 5403] loss: 1.1299755573272705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5404] loss: 1.1813266277313232\n",
      "[step: 5405] loss: 1.2338221073150635\n",
      "[step: 5406] loss: 1.2561105489730835\n",
      "[step: 5407] loss: 1.259488821029663\n",
      "[step: 5408] loss: 1.2273900508880615\n",
      "[step: 5409] loss: 1.1905275583267212\n",
      "[step: 5410] loss: 1.1480696201324463\n",
      "[step: 5411] loss: 1.1175532341003418\n",
      "[step: 5412] loss: 1.099591612815857\n",
      "[step: 5413] loss: 1.0948212146759033\n",
      "[step: 5414] loss: 1.0999906063079834\n",
      "[step: 5415] loss: 1.1113629341125488\n",
      "[step: 5416] loss: 1.1263068914413452\n",
      "[step: 5417] loss: 1.1412408351898193\n",
      "[step: 5418] loss: 1.1581820249557495\n",
      "[step: 5419] loss: 1.1711177825927734\n",
      "[step: 5420] loss: 1.1878480911254883\n",
      "[step: 5421] loss: 1.1978318691253662\n",
      "[step: 5422] loss: 1.214850902557373\n",
      "[step: 5423] loss: 1.2236863374710083\n",
      "[step: 5424] loss: 1.2436528205871582\n",
      "[step: 5425] loss: 1.254309892654419\n",
      "[step: 5426] loss: 1.2796217203140259\n",
      "[step: 5427] loss: 1.2924182415008545\n",
      "[step: 5428] loss: 1.321512222290039\n",
      "[step: 5429] loss: 1.3313252925872803\n",
      "[step: 5430] loss: 1.3572802543640137\n",
      "[step: 5431] loss: 1.3546440601348877\n",
      "[step: 5432] loss: 1.3660578727722168\n",
      "[step: 5433] loss: 1.3418258428573608\n",
      "[step: 5434] loss: 1.3280444145202637\n",
      "[step: 5435] loss: 1.2828776836395264\n",
      "[step: 5436] loss: 1.2466338872909546\n",
      "[step: 5437] loss: 1.196773886680603\n",
      "[step: 5438] loss: 1.1577781438827515\n",
      "[step: 5439] loss: 1.1230244636535645\n",
      "[step: 5440] loss: 1.1002001762390137\n",
      "[step: 5441] loss: 1.0874357223510742\n",
      "[step: 5442] loss: 1.0838596820831299\n",
      "[step: 5443] loss: 1.0872900485992432\n",
      "[step: 5444] loss: 1.095624566078186\n",
      "[step: 5445] loss: 1.1077814102172852\n",
      "[step: 5446] loss: 1.1221086978912354\n",
      "[step: 5447] loss: 1.1411397457122803\n",
      "[step: 5448] loss: 1.161855936050415\n",
      "[step: 5449] loss: 1.1930649280548096\n",
      "[step: 5450] loss: 1.2265541553497314\n",
      "[step: 5451] loss: 1.2826517820358276\n",
      "[step: 5452] loss: 1.3386821746826172\n",
      "[step: 5453] loss: 1.4368494749069214\n",
      "[step: 5454] loss: 1.5149576663970947\n",
      "[step: 5455] loss: 1.651062250137329\n",
      "[step: 5456] loss: 1.6994409561157227\n",
      "[step: 5457] loss: 1.783100962638855\n",
      "[step: 5458] loss: 1.6903237104415894\n",
      "[step: 5459] loss: 1.587877631187439\n",
      "[step: 5460] loss: 1.3760863542556763\n",
      "[step: 5461] loss: 1.2043299674987793\n",
      "[step: 5462] loss: 1.1018058061599731\n",
      "[step: 5463] loss: 1.0982611179351807\n",
      "[step: 5464] loss: 1.168656826019287\n",
      "[step: 5465] loss: 1.252944827079773\n",
      "[step: 5466] loss: 1.315006971359253\n",
      "[step: 5467] loss: 1.2992178201675415\n",
      "[step: 5468] loss: 1.2476835250854492\n",
      "[step: 5469] loss: 1.1665546894073486\n",
      "[step: 5470] loss: 1.1063807010650635\n",
      "[step: 5471] loss: 1.081246256828308\n",
      "[step: 5472] loss: 1.091937780380249\n",
      "[step: 5473] loss: 1.1244664192199707\n",
      "[step: 5474] loss: 1.1591455936431885\n",
      "[step: 5475] loss: 1.1864211559295654\n",
      "[step: 5476] loss: 1.1911547183990479\n",
      "[step: 5477] loss: 1.1856768131256104\n",
      "[step: 5478] loss: 1.1628257036209106\n",
      "[step: 5479] loss: 1.1406575441360474\n",
      "[step: 5480] loss: 1.1165390014648438\n",
      "[step: 5481] loss: 1.0988497734069824\n",
      "[step: 5482] loss: 1.086085319519043\n",
      "[step: 5483] loss: 1.079235315322876\n",
      "[step: 5484] loss: 1.0775177478790283\n",
      "[step: 5485] loss: 1.080378770828247\n",
      "[step: 5486] loss: 1.087018370628357\n",
      "[step: 5487] loss: 1.0959821939468384\n",
      "[step: 5488] loss: 1.107391357421875\n",
      "[step: 5489] loss: 1.1185929775238037\n",
      "[step: 5490] loss: 1.133195400238037\n",
      "[step: 5491] loss: 1.1468828916549683\n",
      "[step: 5492] loss: 1.1689200401306152\n",
      "[step: 5493] loss: 1.1915390491485596\n",
      "[step: 5494] loss: 1.2325724363327026\n",
      "[step: 5495] loss: 1.2758855819702148\n",
      "[step: 5496] loss: 1.3548556566238403\n",
      "[step: 5497] loss: 1.4304825067520142\n",
      "[step: 5498] loss: 1.5640571117401123\n",
      "[step: 5499] loss: 1.6542147397994995\n",
      "[step: 5500] loss: 1.8028484582901\n",
      "[step: 5501] loss: 1.79898202419281\n",
      "[step: 5502] loss: 1.796320915222168\n",
      "[step: 5503] loss: 1.5884181261062622\n",
      "[step: 5504] loss: 1.3804105520248413\n",
      "[step: 5505] loss: 1.1724462509155273\n",
      "[step: 5506] loss: 1.0791935920715332\n",
      "[step: 5507] loss: 1.1081634759902954\n",
      "[step: 5508] loss: 1.2078847885131836\n",
      "[step: 5509] loss: 1.3089793920516968\n",
      "[step: 5510] loss: 1.3244953155517578\n",
      "[step: 5511] loss: 1.2813574075698853\n",
      "[step: 5512] loss: 1.1820670366287231\n",
      "[step: 5513] loss: 1.105464220046997\n",
      "[step: 5514] loss: 1.076486349105835\n",
      "[step: 5515] loss: 1.0965420007705688\n",
      "[step: 5516] loss: 1.1407313346862793\n",
      "[step: 5517] loss: 1.177133321762085\n",
      "[step: 5518] loss: 1.1936118602752686\n",
      "[step: 5519] loss: 1.1769726276397705\n",
      "[step: 5520] loss: 1.1507542133331299\n",
      "[step: 5521] loss: 1.1182293891906738\n",
      "[step: 5522] loss: 1.0967271327972412\n",
      "[step: 5523] loss: 1.0856211185455322\n",
      "[step: 5524] loss: 1.0839139223098755\n",
      "[step: 5525] loss: 1.0867390632629395\n",
      "[step: 5526] loss: 1.092134714126587\n",
      "[step: 5527] loss: 1.0995113849639893\n",
      "[step: 5528] loss: 1.1071075201034546\n",
      "[step: 5529] loss: 1.1163334846496582\n",
      "[step: 5530] loss: 1.1208202838897705\n",
      "[step: 5531] loss: 1.1240040063858032\n",
      "[step: 5532] loss: 1.1193394660949707\n",
      "[step: 5533] loss: 1.1137440204620361\n",
      "[step: 5534] loss: 1.1042988300323486\n",
      "[step: 5535] loss: 1.0979499816894531\n",
      "[step: 5536] loss: 1.093116283416748\n",
      "[step: 5537] loss: 1.092695713043213\n",
      "[step: 5538] loss: 1.0939171314239502\n",
      "[step: 5539] loss: 1.0972975492477417\n",
      "[step: 5540] loss: 1.1010127067565918\n",
      "[step: 5541] loss: 1.1072607040405273\n",
      "[step: 5542] loss: 1.1160565614700317\n",
      "[step: 5543] loss: 1.1336166858673096\n",
      "[step: 5544] loss: 1.1591253280639648\n",
      "[step: 5545] loss: 1.2074429988861084\n",
      "[step: 5546] loss: 1.2697625160217285\n",
      "[step: 5547] loss: 1.3853319883346558\n",
      "[step: 5548] loss: 1.5090761184692383\n",
      "[step: 5549] loss: 1.736748218536377\n",
      "[step: 5550] loss: 1.8873379230499268\n",
      "[step: 5551] loss: 2.1430859565734863\n",
      "[step: 5552] loss: 2.0795867443084717\n",
      "[step: 5553] loss: 1.9924067258834839\n",
      "[step: 5554] loss: 1.607393741607666\n",
      "[step: 5555] loss: 1.279783010482788\n",
      "[step: 5556] loss: 1.1087522506713867\n",
      "[step: 5557] loss: 1.1564291715621948\n",
      "[step: 5558] loss: 1.3340415954589844\n",
      "[step: 5559] loss: 1.434008002281189\n",
      "[step: 5560] loss: 1.4123241901397705\n",
      "[step: 5561] loss: 1.2437580823898315\n",
      "[step: 5562] loss: 1.1068847179412842\n",
      "[step: 5563] loss: 1.0862572193145752\n",
      "[step: 5564] loss: 1.165892243385315\n",
      "[step: 5565] loss: 1.2636470794677734\n",
      "[step: 5566] loss: 1.2746468782424927\n",
      "[step: 5567] loss: 1.2177563905715942\n",
      "[step: 5568] loss: 1.1216392517089844\n",
      "[step: 5569] loss: 1.0662729740142822\n",
      "[step: 5570] loss: 1.0752263069152832\n",
      "[step: 5571] loss: 1.1243904829025269\n",
      "[step: 5572] loss: 1.1729563474655151\n",
      "[step: 5573] loss: 1.178755283355713\n",
      "[step: 5574] loss: 1.1517283916473389\n",
      "[step: 5575] loss: 1.1026513576507568\n",
      "[step: 5576] loss: 1.0672101974487305\n",
      "[step: 5577] loss: 1.05734121799469\n",
      "[step: 5578] loss: 1.0704206228256226\n",
      "[step: 5579] loss: 1.0935033559799194\n",
      "[step: 5580] loss: 1.1106605529785156\n",
      "[step: 5581] loss: 1.1169846057891846\n",
      "[step: 5582] loss: 1.1075602769851685\n",
      "[step: 5583] loss: 1.0930225849151611\n",
      "[step: 5584] loss: 1.0753872394561768\n",
      "[step: 5585] loss: 1.061957836151123\n",
      "[step: 5586] loss: 1.0540539026260376\n",
      "[step: 5587] loss: 1.0522682666778564\n",
      "[step: 5588] loss: 1.0554897785186768\n",
      "[step: 5589] loss: 1.0619442462921143\n",
      "[step: 5590] loss: 1.0700939893722534\n",
      "[step: 5591] loss: 1.0773816108703613\n",
      "[step: 5592] loss: 1.084374189376831\n",
      "[step: 5593] loss: 1.088378667831421\n",
      "[step: 5594] loss: 1.0928704738616943\n",
      "[step: 5595] loss: 1.0951017141342163\n",
      "[step: 5596] loss: 1.1000456809997559\n",
      "[step: 5597] loss: 1.1041970252990723\n",
      "[step: 5598] loss: 1.1128766536712646\n",
      "[step: 5599] loss: 1.1212100982666016\n",
      "[step: 5600] loss: 1.1362457275390625\n",
      "[step: 5601] loss: 1.1512374877929688\n",
      "[step: 5602] loss: 1.1779663562774658\n",
      "[step: 5603] loss: 1.2046067714691162\n",
      "[step: 5604] loss: 1.2524034976959229\n",
      "[step: 5605] loss: 1.294906735420227\n",
      "[step: 5606] loss: 1.3711507320404053\n",
      "[step: 5607] loss: 1.4213078022003174\n",
      "[step: 5608] loss: 1.5118565559387207\n",
      "[step: 5609] loss: 1.528554916381836\n",
      "[step: 5610] loss: 1.568807601928711\n",
      "[step: 5611] loss: 1.4924734830856323\n",
      "[step: 5612] loss: 1.4166467189788818\n",
      "[step: 5613] loss: 1.274397611618042\n",
      "[step: 5614] loss: 1.1570271253585815\n",
      "[step: 5615] loss: 1.073954463005066\n",
      "[step: 5616] loss: 1.0481215715408325\n",
      "[step: 5617] loss: 1.0725798606872559\n",
      "[step: 5618] loss: 1.1238138675689697\n",
      "[step: 5619] loss: 1.1802617311477661\n",
      "[step: 5620] loss: 1.20819091796875\n",
      "[step: 5621] loss: 1.2166916131973267\n",
      "[step: 5622] loss: 1.1866493225097656\n",
      "[step: 5623] loss: 1.1489547491073608\n",
      "[step: 5624] loss: 1.1030685901641846\n",
      "[step: 5625] loss: 1.0685832500457764\n",
      "[step: 5626] loss: 1.0476032495498657\n",
      "[step: 5627] loss: 1.0416650772094727\n",
      "[step: 5628] loss: 1.0473605394363403\n",
      "[step: 5629] loss: 1.0600652694702148\n",
      "[step: 5630] loss: 1.0760669708251953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5631] loss: 1.0912525653839111\n",
      "[step: 5632] loss: 1.1071932315826416\n",
      "[step: 5633] loss: 1.1186776161193848\n",
      "[step: 5634] loss: 1.1330199241638184\n",
      "[step: 5635] loss: 1.1409590244293213\n",
      "[step: 5636] loss: 1.1544783115386963\n",
      "[step: 5637] loss: 1.1597893238067627\n",
      "[step: 5638] loss: 1.1731683015823364\n",
      "[step: 5639] loss: 1.1772370338439941\n",
      "[step: 5640] loss: 1.192079782485962\n",
      "[step: 5641] loss: 1.1970595121383667\n",
      "[step: 5642] loss: 1.2149494886398315\n",
      "[step: 5643] loss: 1.2212755680084229\n",
      "[step: 5644] loss: 1.2412824630737305\n",
      "[step: 5645] loss: 1.2460658550262451\n",
      "[step: 5646] loss: 1.2641315460205078\n",
      "[step: 5647] loss: 1.261954665184021\n",
      "[step: 5648] loss: 1.2717617750167847\n",
      "[step: 5649] loss: 1.257265567779541\n",
      "[step: 5650] loss: 1.2527978420257568\n",
      "[step: 5651] loss: 1.2245903015136719\n",
      "[step: 5652] loss: 1.2045572996139526\n",
      "[step: 5653] loss: 1.1683529615402222\n",
      "[step: 5654] loss: 1.1398298740386963\n",
      "[step: 5655] loss: 1.1066224575042725\n",
      "[step: 5656] loss: 1.081357717514038\n",
      "[step: 5657] loss: 1.0596427917480469\n",
      "[step: 5658] loss: 1.0450079441070557\n",
      "[step: 5659] loss: 1.0357664823532104\n",
      "[step: 5660] loss: 1.0313979387283325\n",
      "[step: 5661] loss: 1.0307135581970215\n",
      "[step: 5662] loss: 1.0327669382095337\n",
      "[step: 5663] loss: 1.036994457244873\n",
      "[step: 5664] loss: 1.0430874824523926\n",
      "[step: 5665] loss: 1.0518673658370972\n",
      "[step: 5666] loss: 1.0633882284164429\n",
      "[step: 5667] loss: 1.0813934803009033\n",
      "[step: 5668] loss: 1.1055402755737305\n",
      "[step: 5669] loss: 1.146929383277893\n",
      "[step: 5670] loss: 1.201768159866333\n",
      "[step: 5671] loss: 1.3010581731796265\n",
      "[step: 5672] loss: 1.420245885848999\n",
      "[step: 5673] loss: 1.6385221481323242\n",
      "[step: 5674] loss: 1.8295261859893799\n",
      "[step: 5675] loss: 2.1528518199920654\n",
      "[step: 5676] loss: 2.1982738971710205\n",
      "[step: 5677] loss: 2.232287645339966\n",
      "[step: 5678] loss: 1.817865014076233\n",
      "[step: 5679] loss: 1.4047739505767822\n",
      "[step: 5680] loss: 1.1065688133239746\n",
      "[step: 5681] loss: 1.101915717124939\n",
      "[step: 5682] loss: 1.313010811805725\n",
      "[step: 5683] loss: 1.470184564590454\n",
      "[step: 5684] loss: 1.4674729108810425\n",
      "[step: 5685] loss: 1.2466320991516113\n",
      "[step: 5686] loss: 1.072301983833313\n",
      "[step: 5687] loss: 1.0642985105514526\n",
      "[step: 5688] loss: 1.1843223571777344\n",
      "[step: 5689] loss: 1.3024916648864746\n",
      "[step: 5690] loss: 1.2775170803070068\n",
      "[step: 5691] loss: 1.1732964515686035\n",
      "[step: 5692] loss: 1.0624120235443115\n",
      "[step: 5693] loss: 1.0386943817138672\n",
      "[step: 5694] loss: 1.0954142808914185\n",
      "[step: 5695] loss: 1.1607985496520996\n",
      "[step: 5696] loss: 1.1844401359558105\n",
      "[step: 5697] loss: 1.139100432395935\n",
      "[step: 5698] loss: 1.0782196521759033\n",
      "[step: 5699] loss: 1.0385318994522095\n",
      "[step: 5700] loss: 1.0419344902038574\n",
      "[step: 5701] loss: 1.072033166885376\n",
      "[step: 5702] loss: 1.0987553596496582\n",
      "[step: 5703] loss: 1.1067153215408325\n",
      "[step: 5704] loss: 1.087538719177246\n",
      "[step: 5705] loss: 1.0609207153320312\n",
      "[step: 5706] loss: 1.039166808128357\n",
      "[step: 5707] loss: 1.032212257385254\n",
      "[step: 5708] loss: 1.0373528003692627\n",
      "[step: 5709] loss: 1.0480395555496216\n",
      "[step: 5710] loss: 1.0579676628112793\n",
      "[step: 5711] loss: 1.061652660369873\n",
      "[step: 5712] loss: 1.0608035326004028\n",
      "[step: 5713] loss: 1.0541250705718994\n",
      "[step: 5714] loss: 1.0462315082550049\n",
      "[step: 5715] loss: 1.036982536315918\n",
      "[step: 5716] loss: 1.0291013717651367\n",
      "[step: 5717] loss: 1.0233509540557861\n",
      "[step: 5718] loss: 1.0207244157791138\n",
      "[step: 5719] loss: 1.021206021308899\n",
      "[step: 5720] loss: 1.0241668224334717\n",
      "[step: 5721] loss: 1.0286670923233032\n",
      "[step: 5722] loss: 1.0333101749420166\n",
      "[step: 5723] loss: 1.0380300283432007\n",
      "[step: 5724] loss: 1.0417555570602417\n",
      "[step: 5725] loss: 1.0461907386779785\n",
      "[step: 5726] loss: 1.0504324436187744\n",
      "[step: 5727] loss: 1.0578556060791016\n",
      "[step: 5728] loss: 1.0666600465774536\n",
      "[step: 5729] loss: 1.0817272663116455\n",
      "[step: 5730] loss: 1.099694013595581\n",
      "[step: 5731] loss: 1.1297814846038818\n",
      "[step: 5732] loss: 1.1648963689804077\n",
      "[step: 5733] loss: 1.225024700164795\n",
      "[step: 5734] loss: 1.2896943092346191\n",
      "[step: 5735] loss: 1.4016715288162231\n",
      "[step: 5736] loss: 1.494811773300171\n",
      "[step: 5737] loss: 1.6525307893753052\n",
      "[step: 5738] loss: 1.7017064094543457\n",
      "[step: 5739] loss: 1.7805888652801514\n",
      "[step: 5740] loss: 1.6428359746932983\n",
      "[step: 5741] loss: 1.4910846948623657\n",
      "[step: 5742] loss: 1.2484387159347534\n",
      "[step: 5743] loss: 1.081047534942627\n",
      "[step: 5744] loss: 1.0238168239593506\n",
      "[step: 5745] loss: 1.07587468624115\n",
      "[step: 5746] loss: 1.1831166744232178\n",
      "[step: 5747] loss: 1.2547953128814697\n",
      "[step: 5748] loss: 1.2745413780212402\n",
      "[step: 5749] loss: 1.201274037361145\n",
      "[step: 5750] loss: 1.112563133239746\n",
      "[step: 5751] loss: 1.0392494201660156\n",
      "[step: 5752] loss: 1.0163629055023193\n",
      "[step: 5753] loss: 1.0403673648834229\n",
      "[step: 5754] loss: 1.0865000486373901\n",
      "[step: 5755] loss: 1.130976676940918\n",
      "[step: 5756] loss: 1.1439990997314453\n",
      "[step: 5757] loss: 1.1354682445526123\n",
      "[step: 5758] loss: 1.0986655950546265\n",
      "[step: 5759] loss: 1.0606632232666016\n",
      "[step: 5760] loss: 1.0287137031555176\n",
      "[step: 5761] loss: 1.013173222541809\n",
      "[step: 5762] loss: 1.0136640071868896\n",
      "[step: 5763] loss: 1.025434970855713\n",
      "[step: 5764] loss: 1.0424535274505615\n",
      "[step: 5765] loss: 1.0578945875167847\n",
      "[step: 5766] loss: 1.070754051208496\n",
      "[step: 5767] loss: 1.0749852657318115\n",
      "[step: 5768] loss: 1.0764055252075195\n",
      "[step: 5769] loss: 1.0701704025268555\n",
      "[step: 5770] loss: 1.0641332864761353\n",
      "[step: 5771] loss: 1.054416298866272\n",
      "[step: 5772] loss: 1.0466649532318115\n",
      "[step: 5773] loss: 1.0381189584732056\n",
      "[step: 5774] loss: 1.0318541526794434\n",
      "[step: 5775] loss: 1.0261375904083252\n",
      "[step: 5776] loss: 1.022367238998413\n",
      "[step: 5777] loss: 1.0195324420928955\n",
      "[step: 5778] loss: 1.0183169841766357\n",
      "[step: 5779] loss: 1.0180509090423584\n",
      "[step: 5780] loss: 1.019361972808838\n",
      "[step: 5781] loss: 1.022037148475647\n",
      "[step: 5782] loss: 1.0274930000305176\n",
      "[step: 5783] loss: 1.0360386371612549\n",
      "[step: 5784] loss: 1.0514196157455444\n",
      "[step: 5785] loss: 1.074763536453247\n",
      "[step: 5786] loss: 1.1173384189605713\n",
      "[step: 5787] loss: 1.179781436920166\n",
      "[step: 5788] loss: 1.2970060110092163\n",
      "[step: 5789] loss: 1.4506964683532715\n",
      "[step: 5790] loss: 1.7392199039459229\n",
      "[step: 5791] loss: 2.00287127494812\n",
      "[step: 5792] loss: 2.4426136016845703\n",
      "[step: 5793] loss: 2.445645570755005\n",
      "[step: 5794] loss: 2.3774921894073486\n",
      "[step: 5795] loss: 1.746952772140503\n",
      "[step: 5796] loss: 1.2348872423171997\n",
      "[step: 5797] loss: 1.07413649559021\n",
      "[step: 5798] loss: 1.292527198791504\n",
      "[step: 5799] loss: 1.6043756008148193\n",
      "[step: 5800] loss: 1.5423117876052856\n",
      "[step: 5801] loss: 1.2741084098815918\n",
      "[step: 5802] loss: 1.047806978225708\n",
      "[step: 5803] loss: 1.1082980632781982\n",
      "[step: 5804] loss: 1.3217753171920776\n",
      "[step: 5805] loss: 1.3722518682479858\n",
      "[step: 5806] loss: 1.244982361793518\n",
      "[step: 5807] loss: 1.0577025413513184\n",
      "[step: 5808] loss: 1.0287938117980957\n",
      "[step: 5809] loss: 1.140385627746582\n",
      "[step: 5810] loss: 1.2312510013580322\n",
      "[step: 5811] loss: 1.2134084701538086\n",
      "[step: 5812] loss: 1.0920639038085938\n",
      "[step: 5813] loss: 1.0186411142349243\n",
      "[step: 5814] loss: 1.037799596786499\n",
      "[step: 5815] loss: 1.1040611267089844\n",
      "[step: 5816] loss: 1.1430130004882812\n",
      "[step: 5817] loss: 1.103724718093872\n",
      "[step: 5818] loss: 1.0436160564422607\n",
      "[step: 5819] loss: 1.0094053745269775\n",
      "[step: 5820] loss: 1.021965503692627\n",
      "[step: 5821] loss: 1.058365821838379\n",
      "[step: 5822] loss: 1.0763440132141113\n",
      "[step: 5823] loss: 1.0662816762924194\n",
      "[step: 5824] loss: 1.0343769788742065\n",
      "[step: 5825] loss: 1.0089017152786255\n",
      "[step: 5826] loss: 1.0033957958221436\n",
      "[step: 5827] loss: 1.0158439874649048\n",
      "[step: 5828] loss: 1.0334935188293457\n",
      "[step: 5829] loss: 1.0414199829101562\n",
      "[step: 5830] loss: 1.0378704071044922\n",
      "[step: 5831] loss: 1.0236417055130005\n",
      "[step: 5832] loss: 1.008545994758606\n",
      "[step: 5833] loss: 0.9990602731704712\n",
      "[step: 5834] loss: 0.9978216886520386\n",
      "[step: 5835] loss: 1.0028963088989258\n",
      "[step: 5836] loss: 1.010430097579956\n",
      "[step: 5837] loss: 1.0167920589447021\n",
      "[step: 5838] loss: 1.0188195705413818\n",
      "[step: 5839] loss: 1.017468810081482\n",
      "[step: 5840] loss: 1.0125724077224731\n",
      "[step: 5841] loss: 1.0067332983016968\n",
      "[step: 5842] loss: 1.0007975101470947\n",
      "[step: 5843] loss: 0.996353805065155\n",
      "[step: 5844] loss: 0.9934461712837219\n",
      "[step: 5845] loss: 0.9921277761459351\n",
      "[step: 5846] loss: 0.9920918941497803\n",
      "[step: 5847] loss: 0.9928159713745117\n",
      "[step: 5848] loss: 0.9940997362136841\n",
      "[step: 5849] loss: 0.9956938624382019\n",
      "[step: 5850] loss: 0.9976186752319336\n",
      "[step: 5851] loss: 0.9996908903121948\n",
      "[step: 5852] loss: 1.0023243427276611\n",
      "[step: 5853] loss: 1.0053397417068481\n",
      "[step: 5854] loss: 1.0096306800842285\n",
      "[step: 5855] loss: 1.0150434970855713\n",
      "[step: 5856] loss: 1.0234439373016357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 5857] loss: 1.0345345735549927\n",
      "[step: 5858] loss: 1.0530081987380981\n",
      "[step: 5859] loss: 1.0779597759246826\n",
      "[step: 5860] loss: 1.1211154460906982\n",
      "[step: 5861] loss: 1.1772973537445068\n",
      "[step: 5862] loss: 1.2769384384155273\n",
      "[step: 5863] loss: 1.3925977945327759\n",
      "[step: 5864] loss: 1.597517490386963\n",
      "[step: 5865] loss: 1.7657220363616943\n",
      "[step: 5866] loss: 2.033559799194336\n",
      "[step: 5867] loss: 2.0411572456359863\n",
      "[step: 5868] loss: 2.0159831047058105\n",
      "[step: 5869] loss: 1.6366136074066162\n",
      "[step: 5870] loss: 1.2734723091125488\n",
      "[step: 5871] loss: 1.0348737239837646\n",
      "[step: 5872] loss: 1.0493645668029785\n",
      "[step: 5873] loss: 1.2375661134719849\n",
      "[step: 5874] loss: 1.3759636878967285\n",
      "[step: 5875] loss: 1.3770270347595215\n",
      "[step: 5876] loss: 1.192176342010498\n",
      "[step: 5877] loss: 1.0332845449447632\n",
      "[step: 5878] loss: 1.003907322883606\n",
      "[step: 5879] loss: 1.0931789875030518\n",
      "[step: 5880] loss: 1.2022696733474731\n",
      "[step: 5881] loss: 1.2134976387023926\n",
      "[step: 5882] loss: 1.148210048675537\n",
      "[step: 5883] loss: 1.0455715656280518\n",
      "[step: 5884] loss: 0.9950711727142334\n",
      "[step: 5885] loss: 1.0148000717163086\n",
      "[step: 5886] loss: 1.0686357021331787\n",
      "[step: 5887] loss: 1.1088241338729858\n",
      "[step: 5888] loss: 1.0991230010986328\n",
      "[step: 5889] loss: 1.0604963302612305\n",
      "[step: 5890] loss: 1.0150902271270752\n",
      "[step: 5891] loss: 0.9945327639579773\n",
      "[step: 5892] loss: 1.0017179250717163\n",
      "[step: 5893] loss: 1.0229684114456177\n",
      "[step: 5894] loss: 1.0408411026000977\n",
      "[step: 5895] loss: 1.042763590812683\n",
      "[step: 5896] loss: 1.0325075387954712\n",
      "[step: 5897] loss: 1.0147950649261475\n",
      "[step: 5898] loss: 1.0013036727905273\n",
      "[step: 5899] loss: 0.9942330718040466\n",
      "[step: 5900] loss: 0.9935110211372375\n",
      "[step: 5901] loss: 0.9957683086395264\n",
      "[step: 5902] loss: 0.9986870288848877\n",
      "[step: 5903] loss: 1.00118088722229\n",
      "[step: 5904] loss: 1.0030980110168457\n",
      "[step: 5905] loss: 1.00558340549469\n",
      "[step: 5906] loss: 1.0067998170852661\n",
      "[step: 5907] loss: 1.0075335502624512\n",
      "[step: 5908] loss: 1.005380392074585\n",
      "[step: 5909] loss: 1.0018224716186523\n",
      "[step: 5910] loss: 0.9960838556289673\n",
      "[step: 5911] loss: 0.990551769733429\n",
      "[step: 5912] loss: 0.9857139587402344\n",
      "[step: 5913] loss: 0.9826931357383728\n",
      "[step: 5914] loss: 0.9812783002853394\n",
      "[step: 5915] loss: 0.9810793995857239\n",
      "[step: 5916] loss: 0.9812496304512024\n",
      "[step: 5917] loss: 0.9812445640563965\n",
      "[step: 5918] loss: 0.9808249473571777\n",
      "[step: 5919] loss: 0.9800322651863098\n",
      "[step: 5920] loss: 0.9792808890342712\n",
      "[step: 5921] loss: 0.979089081287384\n",
      "[step: 5922] loss: 0.9798762798309326\n",
      "[step: 5923] loss: 0.982319712638855\n",
      "[step: 5924] loss: 0.986900806427002\n",
      "[step: 5925] loss: 0.9952749013900757\n",
      "[step: 5926] loss: 1.0086112022399902\n",
      "[step: 5927] loss: 1.0330305099487305\n",
      "[step: 5928] loss: 1.0716004371643066\n",
      "[step: 5929] loss: 1.1460962295532227\n",
      "[step: 5930] loss: 1.2585737705230713\n",
      "[step: 5931] loss: 1.4827330112457275\n",
      "[step: 5932] loss: 1.7630376815795898\n",
      "[step: 5933] loss: 2.2950611114501953\n",
      "[step: 5934] loss: 2.611758232116699\n",
      "[step: 5935] loss: 3.0356879234313965\n",
      "[step: 5936] loss: 2.491692304611206\n",
      "[step: 5937] loss: 1.8089370727539062\n",
      "[step: 5938] loss: 1.175298810005188\n",
      "[step: 5939] loss: 1.1643987894058228\n",
      "[step: 5940] loss: 1.6034059524536133\n",
      "[step: 5941] loss: 1.7471129894256592\n",
      "[step: 5942] loss: 1.4887254238128662\n",
      "[step: 5943] loss: 1.0838736295700073\n",
      "[step: 5944] loss: 1.1191056966781616\n",
      "[step: 5945] loss: 1.434802532196045\n",
      "[step: 5946] loss: 1.462043285369873\n",
      "[step: 5947] loss: 1.2178916931152344\n",
      "[step: 5948] loss: 1.0051990747451782\n",
      "[step: 5949] loss: 1.1129608154296875\n",
      "[step: 5950] loss: 1.3194963932037354\n",
      "[step: 5951] loss: 1.2867830991744995\n",
      "[step: 5952] loss: 1.1029947996139526\n",
      "[step: 5953] loss: 0.9907975196838379\n",
      "[step: 5954] loss: 1.0867623090744019\n",
      "[step: 5955] loss: 1.2100597620010376\n",
      "[step: 5956] loss: 1.1710734367370605\n",
      "[step: 5957] loss: 1.051336646080017\n",
      "[step: 5958] loss: 0.9862887859344482\n",
      "[step: 5959] loss: 1.0507729053497314\n",
      "[step: 5960] loss: 1.1274646520614624\n",
      "[step: 5961] loss: 1.1055734157562256\n",
      "[step: 5962] loss: 1.0300774574279785\n",
      "[step: 5963] loss: 0.9807537198066711\n",
      "[step: 5964] loss: 1.015683889389038\n",
      "[step: 5965] loss: 1.069422721862793\n",
      "[step: 5966] loss: 1.070310354232788\n",
      "[step: 5967] loss: 1.0288110971450806\n",
      "[step: 5968] loss: 0.9819610118865967\n",
      "[step: 5969] loss: 0.9850084781646729\n",
      "[step: 5970] loss: 1.0180158615112305\n",
      "[step: 5971] loss: 1.0380220413208008\n",
      "[step: 5972] loss: 1.0302423238754272\n",
      "[step: 5973] loss: 0.9957878589630127\n",
      "[step: 5974] loss: 0.9751787185668945\n",
      "[step: 5975] loss: 0.978516697883606\n",
      "[step: 5976] loss: 0.9953384399414062\n",
      "[step: 5977] loss: 1.0104676485061646\n",
      "[step: 5978] loss: 1.0058019161224365\n",
      "[step: 5979] loss: 0.9917886853218079\n",
      "[step: 5980] loss: 0.977134644985199\n",
      "[step: 5981] loss: 0.9702802896499634\n",
      "[step: 5982] loss: 0.9740146994590759\n",
      "[step: 5983] loss: 0.9804423451423645\n",
      "[step: 5984] loss: 0.9857773780822754\n",
      "[step: 5985] loss: 0.9859111309051514\n",
      "[step: 5986] loss: 0.9815175533294678\n",
      "[step: 5987] loss: 0.9754601716995239\n",
      "[step: 5988] loss: 0.9695058465003967\n",
      "[step: 5989] loss: 0.9664526581764221\n",
      "[step: 5990] loss: 0.9663307666778564\n",
      "[step: 5991] loss: 0.9681656360626221\n",
      "[step: 5992] loss: 0.9711524248123169\n",
      "[step: 5993] loss: 0.9730132818222046\n",
      "[step: 5994] loss: 0.9733038544654846\n",
      "[step: 5995] loss: 0.9718431830406189\n",
      "[step: 5996] loss: 0.9688718914985657\n",
      "[step: 5997] loss: 0.9658973217010498\n",
      "[step: 5998] loss: 0.9631463289260864\n",
      "[step: 5999] loss: 0.96147620677948\n",
      "[step: 6000] loss: 0.960688054561615\n",
      "[step: 6001] loss: 0.9604262113571167\n",
      "[step: 6002] loss: 0.9606378078460693\n",
      "[step: 6003] loss: 0.9608852863311768\n",
      "[step: 6004] loss: 0.9613707661628723\n",
      "[step: 6005] loss: 0.9619622826576233\n",
      "[step: 6006] loss: 0.9627392888069153\n",
      "[step: 6007] loss: 0.9637205600738525\n",
      "[step: 6008] loss: 0.9648710489273071\n",
      "[step: 6009] loss: 0.966274082660675\n",
      "[step: 6010] loss: 0.9683912992477417\n",
      "[step: 6011] loss: 0.9712355136871338\n",
      "[step: 6012] loss: 0.9759297370910645\n",
      "[step: 6013] loss: 0.9824441075325012\n",
      "[step: 6014] loss: 0.9933502078056335\n",
      "[step: 6015] loss: 1.0089843273162842\n",
      "[step: 6016] loss: 1.0357767343521118\n",
      "[step: 6017] loss: 1.0737380981445312\n",
      "[step: 6018] loss: 1.1406199932098389\n",
      "[step: 6019] loss: 1.230063557624817\n",
      "[step: 6020] loss: 1.3915586471557617\n",
      "[step: 6021] loss: 1.5705173015594482\n",
      "[step: 6022] loss: 1.8775999546051025\n",
      "[step: 6023] loss: 2.0500221252441406\n",
      "[step: 6024] loss: 2.2765448093414307\n",
      "[step: 6025] loss: 2.0322647094726562\n",
      "[step: 6026] loss: 1.6954196691513062\n",
      "[step: 6027] loss: 1.2116793394088745\n",
      "[step: 6028] loss: 0.9820528030395508\n",
      "[step: 6029] loss: 1.0770318508148193\n",
      "[step: 6030] loss: 1.3157635927200317\n",
      "[step: 6031] loss: 1.4637055397033691\n",
      "[step: 6032] loss: 1.3094303607940674\n",
      "[step: 6033] loss: 1.0806176662445068\n",
      "[step: 6034] loss: 0.969009518623352\n",
      "[step: 6035] loss: 1.0496058464050293\n",
      "[step: 6036] loss: 1.1986639499664307\n",
      "[step: 6037] loss: 1.2308812141418457\n",
      "[step: 6038] loss: 1.14634108543396\n",
      "[step: 6039] loss: 1.0141198635101318\n",
      "[step: 6040] loss: 0.9658324122428894\n",
      "[step: 6041] loss: 1.0163357257843018\n",
      "[step: 6042] loss: 1.0898206233978271\n",
      "[step: 6043] loss: 1.1140730381011963\n",
      "[step: 6044] loss: 1.0623838901519775\n",
      "[step: 6045] loss: 0.9980309009552002\n",
      "[step: 6046] loss: 0.9662611484527588\n",
      "[step: 6047] loss: 0.9824024438858032\n",
      "[step: 6048] loss: 1.0176314115524292\n",
      "[step: 6049] loss: 1.0347633361816406\n",
      "[step: 6050] loss: 1.0257203578948975\n",
      "[step: 6051] loss: 0.9969391226768494\n",
      "[step: 6052] loss: 0.9734524488449097\n",
      "[step: 6053] loss: 0.9652903079986572\n",
      "[step: 6054] loss: 0.9715630412101746\n",
      "[step: 6055] loss: 0.9819743633270264\n",
      "[step: 6056] loss: 0.9884533882141113\n",
      "[step: 6057] loss: 0.989916980266571\n",
      "[step: 6058] loss: 0.984483540058136\n",
      "[step: 6059] loss: 0.9764449000358582\n",
      "[step: 6060] loss: 0.9664987921714783\n",
      "[step: 6061] loss: 0.958309531211853\n",
      "[step: 6062] loss: 0.9538698792457581\n",
      "[step: 6063] loss: 0.9547096490859985\n",
      "[step: 6064] loss: 0.9600527286529541\n",
      "[step: 6065] loss: 0.966661274433136\n",
      "[step: 6066] loss: 0.971734881401062\n",
      "[step: 6067] loss: 0.9722777009010315\n",
      "[step: 6068] loss: 0.9696722030639648\n",
      "[step: 6069] loss: 0.9644180536270142\n",
      "[step: 6070] loss: 0.9597344398498535\n",
      "[step: 6071] loss: 0.9561479687690735\n",
      "[step: 6072] loss: 0.9539331793785095\n",
      "[step: 6073] loss: 0.9521141052246094\n",
      "[step: 6074] loss: 0.9501721858978271\n",
      "[step: 6075] loss: 0.9479395747184753\n",
      "[step: 6076] loss: 0.9458019137382507\n",
      "[step: 6077] loss: 0.9443655014038086\n",
      "[step: 6078] loss: 0.9438120126724243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6079] loss: 0.9439470767974854\n",
      "[step: 6080] loss: 0.9443928599357605\n",
      "[step: 6081] loss: 0.9447311758995056\n",
      "[step: 6082] loss: 0.9448422193527222\n",
      "[step: 6083] loss: 0.9449570775032043\n",
      "[step: 6084] loss: 0.9454435110092163\n",
      "[step: 6085] loss: 0.9467741250991821\n",
      "[step: 6086] loss: 0.9493919014930725\n",
      "[step: 6087] loss: 0.9541947841644287\n",
      "[step: 6088] loss: 0.9618873596191406\n",
      "[step: 6089] loss: 0.9755855798721313\n",
      "[step: 6090] loss: 0.9976152181625366\n",
      "[step: 6091] loss: 1.0391439199447632\n",
      "[step: 6092] loss: 1.1051020622253418\n",
      "[step: 6093] loss: 1.2344942092895508\n",
      "[step: 6094] loss: 1.421114206314087\n",
      "[step: 6095] loss: 1.7867960929870605\n",
      "[step: 6096] loss: 2.1588916778564453\n",
      "[step: 6097] loss: 2.785153388977051\n",
      "[step: 6098] loss: 2.7823827266693115\n",
      "[step: 6099] loss: 2.616176128387451\n",
      "[step: 6100] loss: 1.6859298944473267\n",
      "[step: 6101] loss: 1.0712945461273193\n",
      "[step: 6102] loss: 1.1126277446746826\n",
      "[step: 6103] loss: 1.5494043827056885\n",
      "[step: 6104] loss: 1.8120182752609253\n",
      "[step: 6105] loss: 1.4042091369628906\n",
      "[step: 6106] loss: 1.0175390243530273\n",
      "[step: 6107] loss: 1.0897396802902222\n",
      "[step: 6108] loss: 1.3825287818908691\n",
      "[step: 6109] loss: 1.4604573249816895\n",
      "[step: 6110] loss: 1.1554765701293945\n",
      "[step: 6111] loss: 0.9664857387542725\n",
      "[step: 6112] loss: 1.0988273620605469\n",
      "[step: 6113] loss: 1.271397590637207\n",
      "[step: 6114] loss: 1.2526295185089111\n",
      "[step: 6115] loss: 1.050593614578247\n",
      "[step: 6116] loss: 0.9647107124328613\n",
      "[step: 6117] loss: 1.0747241973876953\n",
      "[step: 6118] loss: 1.165138840675354\n",
      "[step: 6119] loss: 1.1244232654571533\n",
      "[step: 6120] loss: 1.002943754196167\n",
      "[step: 6121] loss: 0.9639806747436523\n",
      "[step: 6122] loss: 1.0366864204406738\n",
      "[step: 6123] loss: 1.0869487524032593\n",
      "[step: 6124] loss: 1.056478500366211\n",
      "[step: 6125] loss: 0.9834374189376831\n",
      "[step: 6126] loss: 0.9564822912216187\n",
      "[step: 6127] loss: 0.998400866985321\n",
      "[step: 6128] loss: 1.0344181060791016\n",
      "[step: 6129] loss: 1.0239174365997314\n",
      "[step: 6130] loss: 0.9806993007659912\n",
      "[step: 6131] loss: 0.9517363905906677\n",
      "[step: 6132] loss: 0.964863657951355\n",
      "[step: 6133] loss: 0.9909738302230835\n",
      "[step: 6134] loss: 0.9994841814041138\n",
      "[step: 6135] loss: 0.9831020832061768\n",
      "[step: 6136] loss: 0.957350492477417\n",
      "[step: 6137] loss: 0.9470298886299133\n",
      "[step: 6138] loss: 0.9544546008110046\n",
      "[step: 6139] loss: 0.9670938849449158\n",
      "[step: 6140] loss: 0.9738575220108032\n",
      "[step: 6141] loss: 0.9673463106155396\n",
      "[step: 6142] loss: 0.9542293548583984\n",
      "[step: 6143] loss: 0.9442188739776611\n",
      "[step: 6144] loss: 0.9409580230712891\n",
      "[step: 6145] loss: 0.945679247379303\n",
      "[step: 6146] loss: 0.9512668251991272\n",
      "[step: 6147] loss: 0.9540959596633911\n",
      "[step: 6148] loss: 0.9525489807128906\n",
      "[step: 6149] loss: 0.9463708996772766\n",
      "[step: 6150] loss: 0.9403571486473083\n",
      "[step: 6151] loss: 0.9358963966369629\n",
      "[step: 6152] loss: 0.9354045391082764\n",
      "[step: 6153] loss: 0.937919020652771\n",
      "[step: 6154] loss: 0.9409172534942627\n",
      "[step: 6155] loss: 0.9431480169296265\n",
      "[step: 6156] loss: 0.9424001574516296\n",
      "[step: 6157] loss: 0.9399430155754089\n",
      "[step: 6158] loss: 0.9365614652633667\n",
      "[step: 6159] loss: 0.9334598779678345\n",
      "[step: 6160] loss: 0.9317260384559631\n",
      "[step: 6161] loss: 0.9309762716293335\n",
      "[step: 6162] loss: 0.9311789870262146\n",
      "[step: 6163] loss: 0.9317445755004883\n",
      "[step: 6164] loss: 0.9322465658187866\n",
      "[step: 6165] loss: 0.9327777624130249\n",
      "[step: 6166] loss: 0.9331045150756836\n",
      "[step: 6167] loss: 0.9333622455596924\n",
      "[step: 6168] loss: 0.933598518371582\n",
      "[step: 6169] loss: 0.9336274862289429\n",
      "[step: 6170] loss: 0.9336763620376587\n",
      "[step: 6171] loss: 0.9335007667541504\n",
      "[step: 6172] loss: 0.9335085153579712\n",
      "[step: 6173] loss: 0.9336628913879395\n",
      "[step: 6174] loss: 0.9342953562736511\n",
      "[step: 6175] loss: 0.9354265332221985\n",
      "[step: 6176] loss: 0.9374485015869141\n",
      "[step: 6177] loss: 0.9404097199440002\n",
      "[step: 6178] loss: 0.9454661011695862\n",
      "[step: 6179] loss: 0.9527701735496521\n",
      "[step: 6180] loss: 0.9652897119522095\n",
      "[step: 6181] loss: 0.9833509922027588\n",
      "[step: 6182] loss: 1.0151513814926147\n",
      "[step: 6183] loss: 1.0607500076293945\n",
      "[step: 6184] loss: 1.1433062553405762\n",
      "[step: 6185] loss: 1.2525384426116943\n",
      "[step: 6186] loss: 1.450865387916565\n",
      "[step: 6187] loss: 1.6570687294006348\n",
      "[step: 6188] loss: 2.0062601566314697\n",
      "[step: 6189] loss: 2.139486789703369\n",
      "[step: 6190] loss: 2.2777316570281982\n",
      "[step: 6191] loss: 1.876922845840454\n",
      "[step: 6192] loss: 1.4257268905639648\n",
      "[step: 6193] loss: 1.0265220403671265\n",
      "[step: 6194] loss: 0.9754257202148438\n",
      "[step: 6195] loss: 1.2079095840454102\n",
      "[step: 6196] loss: 1.406308650970459\n",
      "[step: 6197] loss: 1.4046382904052734\n",
      "[step: 6198] loss: 1.1446560621261597\n",
      "[step: 6199] loss: 0.9565582275390625\n",
      "[step: 6200] loss: 0.9808966517448425\n",
      "[step: 6201] loss: 1.1329962015151978\n",
      "[step: 6202] loss: 1.2382609844207764\n",
      "[step: 6203] loss: 1.1581918001174927\n",
      "[step: 6204] loss: 1.0171648263931274\n",
      "[step: 6205] loss: 0.9359899163246155\n",
      "[step: 6206] loss: 0.973391592502594\n",
      "[step: 6207] loss: 1.063909888267517\n",
      "[step: 6208] loss: 1.0963802337646484\n",
      "[step: 6209] loss: 1.0552130937576294\n",
      "[step: 6210] loss: 0.9747812747955322\n",
      "[step: 6211] loss: 0.9334408640861511\n",
      "[step: 6212] loss: 0.9512096643447876\n",
      "[step: 6213] loss: 0.9952381253242493\n",
      "[step: 6214] loss: 1.0202710628509521\n",
      "[step: 6215] loss: 1.0018932819366455\n",
      "[step: 6216] loss: 0.9651492834091187\n",
      "[step: 6217] loss: 0.9353408813476562\n",
      "[step: 6218] loss: 0.9320037364959717\n",
      "[step: 6219] loss: 0.9488338232040405\n",
      "[step: 6220] loss: 0.9666554927825928\n",
      "[step: 6221] loss: 0.9725791811943054\n",
      "[step: 6222] loss: 0.962602436542511\n",
      "[step: 6223] loss: 0.94724440574646\n",
      "[step: 6224] loss: 0.9336869716644287\n",
      "[step: 6225] loss: 0.9282678961753845\n",
      "[step: 6226] loss: 0.9299161434173584\n",
      "[step: 6227] loss: 0.9351454973220825\n",
      "[step: 6228] loss: 0.9405022859573364\n",
      "[step: 6229] loss: 0.9433212280273438\n",
      "[step: 6230] loss: 0.9438563585281372\n",
      "[step: 6231] loss: 0.9409009218215942\n",
      "[step: 6232] loss: 0.9363669753074646\n",
      "[step: 6233] loss: 0.9301982522010803\n",
      "[step: 6234] loss: 0.9242088794708252\n",
      "[step: 6235] loss: 0.9196233153343201\n",
      "[step: 6236] loss: 0.9175184965133667\n",
      "[step: 6237] loss: 0.9178587198257446\n",
      "[step: 6238] loss: 0.9200229644775391\n",
      "[step: 6239] loss: 0.9230459928512573\n",
      "[step: 6240] loss: 0.9256498217582703\n",
      "[step: 6241] loss: 0.9274815320968628\n",
      "[step: 6242] loss: 0.9281429648399353\n",
      "[step: 6243] loss: 0.9284415245056152\n",
      "[step: 6244] loss: 0.9283442497253418\n",
      "[step: 6245] loss: 0.9290084838867188\n",
      "[step: 6246] loss: 0.9300018548965454\n",
      "[step: 6247] loss: 0.9320307970046997\n",
      "[step: 6248] loss: 0.9344552755355835\n",
      "[step: 6249] loss: 0.9382174015045166\n",
      "[step: 6250] loss: 0.9427618384361267\n",
      "[step: 6251] loss: 0.9503137469291687\n",
      "[step: 6252] loss: 0.960385799407959\n",
      "[step: 6253] loss: 0.9777235984802246\n",
      "[step: 6254] loss: 1.0009499788284302\n",
      "[step: 6255] loss: 1.041522741317749\n",
      "[step: 6256] loss: 1.0933163166046143\n",
      "[step: 6257] loss: 1.1848769187927246\n",
      "[step: 6258] loss: 1.2875970602035522\n",
      "[step: 6259] loss: 1.4674957990646362\n",
      "[step: 6260] loss: 1.6093852519989014\n",
      "[step: 6261] loss: 1.8358545303344727\n",
      "[step: 6262] loss: 1.8421415090560913\n",
      "[step: 6263] loss: 1.8280675411224365\n",
      "[step: 6264] loss: 1.5153985023498535\n",
      "[step: 6265] loss: 1.2107223272323608\n",
      "[step: 6266] loss: 0.9820914268493652\n",
      "[step: 6267] loss: 0.9526726007461548\n",
      "[step: 6268] loss: 1.083828330039978\n",
      "[step: 6269] loss: 1.2215569019317627\n",
      "[step: 6270] loss: 1.2742254734039307\n",
      "[step: 6271] loss: 1.1572105884552002\n",
      "[step: 6272] loss: 1.0125751495361328\n",
      "[step: 6273] loss: 0.9283015727996826\n",
      "[step: 6274] loss: 0.9504666924476624\n",
      "[step: 6275] loss: 1.036677360534668\n",
      "[step: 6276] loss: 1.0989930629730225\n",
      "[step: 6277] loss: 1.1053913831710815\n",
      "[step: 6278] loss: 1.0366480350494385\n",
      "[step: 6279] loss: 0.9611608982086182\n",
      "[step: 6280] loss: 0.9181680679321289\n",
      "[step: 6281] loss: 0.9259527921676636\n",
      "[step: 6282] loss: 0.9663428664207458\n",
      "[step: 6283] loss: 1.0030381679534912\n",
      "[step: 6284] loss: 1.0169262886047363\n",
      "[step: 6285] loss: 0.99378901720047\n",
      "[step: 6286] loss: 0.9580055475234985\n",
      "[step: 6287] loss: 0.924841046333313\n",
      "[step: 6288] loss: 0.9104856252670288\n",
      "[step: 6289] loss: 0.9161292314529419\n",
      "[step: 6290] loss: 0.9335111975669861\n",
      "[step: 6291] loss: 0.9519122838973999\n",
      "[step: 6292] loss: 0.9605554342269897\n",
      "[step: 6293] loss: 0.9599555730819702\n",
      "[step: 6294] loss: 0.9484542608261108\n",
      "[step: 6295] loss: 0.934245228767395\n",
      "[step: 6296] loss: 0.9201184511184692\n",
      "[step: 6297] loss: 0.9104946255683899\n",
      "[step: 6298] loss: 0.9062467217445374\n",
      "[step: 6299] loss: 0.9069907665252686\n",
      "[step: 6300] loss: 0.9111950397491455\n",
      "[step: 6301] loss: 0.9170353412628174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6302] loss: 0.9236040711402893\n",
      "[step: 6303] loss: 0.9295254945755005\n",
      "[step: 6304] loss: 0.9354373216629028\n",
      "[step: 6305] loss: 0.9397192001342773\n",
      "[step: 6306] loss: 0.9447206854820251\n",
      "[step: 6307] loss: 0.9483604431152344\n",
      "[step: 6308] loss: 0.9539928436279297\n",
      "[step: 6309] loss: 0.9586517810821533\n",
      "[step: 6310] loss: 0.9668705463409424\n",
      "[step: 6311] loss: 0.97456294298172\n",
      "[step: 6312] loss: 0.9881520867347717\n",
      "[step: 6313] loss: 1.0015614032745361\n",
      "[step: 6314] loss: 1.0247654914855957\n",
      "[step: 6315] loss: 1.0475366115570068\n",
      "[step: 6316] loss: 1.086399793624878\n",
      "[step: 6317] loss: 1.121551752090454\n",
      "[step: 6318] loss: 1.1808916330337524\n",
      "[step: 6319] loss: 1.2236236333847046\n",
      "[step: 6320] loss: 1.29390287399292\n",
      "[step: 6321] loss: 1.316030740737915\n",
      "[step: 6322] loss: 1.3536640405654907\n",
      "[step: 6323] loss: 1.3085379600524902\n",
      "[step: 6324] loss: 1.2608695030212402\n",
      "[step: 6325] loss: 1.149698257446289\n",
      "[step: 6326] loss: 1.0490995645523071\n",
      "[step: 6327] loss: 0.9590030312538147\n",
      "[step: 6328] loss: 0.910444438457489\n",
      "[step: 6329] loss: 0.9051279425621033\n",
      "[step: 6330] loss: 0.933115541934967\n",
      "[step: 6331] loss: 0.977779746055603\n",
      "[step: 6332] loss: 1.0168509483337402\n",
      "[step: 6333] loss: 1.0469634532928467\n",
      "[step: 6334] loss: 1.0486079454421997\n",
      "[step: 6335] loss: 1.0395952463150024\n",
      "[step: 6336] loss: 1.0093024969100952\n",
      "[step: 6337] loss: 0.9787967205047607\n",
      "[step: 6338] loss: 0.9460995197296143\n",
      "[step: 6339] loss: 0.9217111468315125\n",
      "[step: 6340] loss: 0.9057047367095947\n",
      "[step: 6341] loss: 0.8989876508712769\n",
      "[step: 6342] loss: 0.8998246788978577\n",
      "[step: 6343] loss: 0.905968189239502\n",
      "[step: 6344] loss: 0.9153919219970703\n",
      "[step: 6345] loss: 0.9261307120323181\n",
      "[step: 6346] loss: 0.9384474158287048\n",
      "[step: 6347] loss: 0.950234591960907\n",
      "[step: 6348] loss: 0.9650851488113403\n",
      "[step: 6349] loss: 0.9789527654647827\n",
      "[step: 6350] loss: 0.9994474649429321\n",
      "[step: 6351] loss: 1.0184319019317627\n",
      "[step: 6352] loss: 1.0494219064712524\n",
      "[step: 6353] loss: 1.0767076015472412\n",
      "[step: 6354] loss: 1.1232380867004395\n",
      "[step: 6355] loss: 1.1587166786193848\n",
      "[step: 6356] loss: 1.2201119661331177\n",
      "[step: 6357] loss: 1.25177001953125\n",
      "[step: 6358] loss: 1.3077192306518555\n",
      "[step: 6359] loss: 1.3037161827087402\n",
      "[step: 6360] loss: 1.3085554838180542\n",
      "[step: 6361] loss: 1.240227460861206\n",
      "[step: 6362] loss: 1.1704883575439453\n",
      "[step: 6363] loss: 1.0684771537780762\n",
      "[step: 6364] loss: 0.9839828014373779\n",
      "[step: 6365] loss: 0.9247661828994751\n",
      "[step: 6366] loss: 0.9028401374816895\n",
      "[step: 6367] loss: 0.9147945642471313\n",
      "[step: 6368] loss: 0.9474454522132874\n",
      "[step: 6369] loss: 0.9880504012107849\n",
      "[step: 6370] loss: 1.0167207717895508\n",
      "[step: 6371] loss: 1.0364876985549927\n",
      "[step: 6372] loss: 1.0311942100524902\n",
      "[step: 6373] loss: 1.0182980298995972\n",
      "[step: 6374] loss: 0.989897608757019\n",
      "[step: 6375] loss: 0.9621971845626831\n",
      "[step: 6376] loss: 0.9339528679847717\n",
      "[step: 6377] loss: 0.912559986114502\n",
      "[step: 6378] loss: 0.898247480392456\n",
      "[step: 6379] loss: 0.89163738489151\n",
      "[step: 6380] loss: 0.8914792537689209\n",
      "[step: 6381] loss: 0.8960590958595276\n",
      "[step: 6382] loss: 0.9037618637084961\n",
      "[step: 6383] loss: 0.9130738973617554\n",
      "[step: 6384] loss: 0.9240608811378479\n",
      "[step: 6385] loss: 0.935461163520813\n",
      "[step: 6386] loss: 0.9501689672470093\n",
      "[step: 6387] loss: 0.9658627510070801\n",
      "[step: 6388] loss: 0.9895586371421814\n",
      "[step: 6389] loss: 1.015507698059082\n",
      "[step: 6390] loss: 1.0582833290100098\n",
      "[step: 6391] loss: 1.1029490232467651\n",
      "[step: 6392] loss: 1.17861008644104\n",
      "[step: 6393] loss: 1.2449700832366943\n",
      "[step: 6394] loss: 1.355431079864502\n",
      "[step: 6395] loss: 1.412254810333252\n",
      "[step: 6396] loss: 1.5010783672332764\n",
      "[step: 6397] loss: 1.4604897499084473\n",
      "[step: 6398] loss: 1.4134033918380737\n",
      "[step: 6399] loss: 1.2482327222824097\n",
      "[step: 6400] loss: 1.0966123342514038\n",
      "[step: 6401] loss: 0.9685925245285034\n",
      "[step: 6402] loss: 0.915421724319458\n",
      "[step: 6403] loss: 0.9361540079116821\n",
      "[step: 6404] loss: 0.9977350234985352\n",
      "[step: 6405] loss: 1.064948558807373\n",
      "[step: 6406] loss: 1.0852842330932617\n",
      "[step: 6407] loss: 1.0736194849014282\n",
      "[step: 6408] loss: 1.0169402360916138\n",
      "[step: 6409] loss: 0.9605985879898071\n",
      "[step: 6410] loss: 0.9179431796073914\n",
      "[step: 6411] loss: 0.9025143384933472\n",
      "[step: 6412] loss: 0.9115229845046997\n",
      "[step: 6413] loss: 0.9334465265274048\n",
      "[step: 6414] loss: 0.9582523107528687\n",
      "[step: 6415] loss: 0.9720357060432434\n",
      "[step: 6416] loss: 0.9776937365531921\n",
      "[step: 6417] loss: 0.9677627086639404\n",
      "[step: 6418] loss: 0.9546495676040649\n",
      "[step: 6419] loss: 0.9370110630989075\n",
      "[step: 6420] loss: 0.9225360155105591\n",
      "[step: 6421] loss: 0.9108284711837769\n",
      "[step: 6422] loss: 0.9023770689964294\n",
      "[step: 6423] loss: 0.8967249989509583\n",
      "[step: 6424] loss: 0.8928604125976562\n",
      "[step: 6425] loss: 0.8910652995109558\n",
      "[step: 6426] loss: 0.8913341760635376\n",
      "[step: 6427] loss: 0.8942441940307617\n",
      "[step: 6428] loss: 0.8995378613471985\n",
      "[step: 6429] loss: 0.9076486825942993\n",
      "[step: 6430] loss: 0.9176769256591797\n",
      "[step: 6431] loss: 0.9310474991798401\n",
      "[step: 6432] loss: 0.9469771981239319\n",
      "[step: 6433] loss: 0.9699671268463135\n",
      "[step: 6434] loss: 0.9989638924598694\n",
      "[step: 6435] loss: 1.0459372997283936\n",
      "[step: 6436] loss: 1.1046438217163086\n",
      "[step: 6437] loss: 1.2043910026550293\n",
      "[step: 6438] loss: 1.3114466667175293\n",
      "[step: 6439] loss: 1.4884047508239746\n",
      "[step: 6440] loss: 1.6042520999908447\n",
      "[step: 6441] loss: 1.7673180103302002\n",
      "[step: 6442] loss: 1.6963752508163452\n",
      "[step: 6443] loss: 1.5817838907241821\n",
      "[step: 6444] loss: 1.2696876525878906\n",
      "[step: 6445] loss: 1.0161924362182617\n",
      "[step: 6446] loss: 0.8948992490768433\n",
      "[step: 6447] loss: 0.9442089200019836\n",
      "[step: 6448] loss: 1.0879877805709839\n",
      "[step: 6449] loss: 1.1836838722229004\n",
      "[step: 6450] loss: 1.1890004873275757\n",
      "[step: 6451] loss: 1.068284034729004\n",
      "[step: 6452] loss: 0.9464393854141235\n",
      "[step: 6453] loss: 0.8871487379074097\n",
      "[step: 6454] loss: 0.9130502939224243\n",
      "[step: 6455] loss: 0.9859773516654968\n",
      "[step: 6456] loss: 1.0407061576843262\n",
      "[step: 6457] loss: 1.0530612468719482\n",
      "[step: 6458] loss: 1.0040781497955322\n",
      "[step: 6459] loss: 0.9421318769454956\n",
      "[step: 6460] loss: 0.8943780660629272\n",
      "[step: 6461] loss: 0.8833811283111572\n",
      "[step: 6462] loss: 0.9045502543449402\n",
      "[step: 6463] loss: 0.9373733997344971\n",
      "[step: 6464] loss: 0.9631408452987671\n",
      "[step: 6465] loss: 0.9644059538841248\n",
      "[step: 6466] loss: 0.9492435455322266\n",
      "[step: 6467] loss: 0.9209262132644653\n",
      "[step: 6468] loss: 0.8959205746650696\n",
      "[step: 6469] loss: 0.8810340166091919\n",
      "[step: 6470] loss: 0.8793442249298096\n",
      "[step: 6471] loss: 0.8877668380737305\n",
      "[step: 6472] loss: 0.9004620313644409\n",
      "[step: 6473] loss: 0.9128685593605042\n",
      "[step: 6474] loss: 0.9202176332473755\n",
      "[step: 6475] loss: 0.9236646890640259\n",
      "[step: 6476] loss: 0.9205866456031799\n",
      "[step: 6477] loss: 0.9154815077781677\n",
      "[step: 6478] loss: 0.9074879884719849\n",
      "[step: 6479] loss: 0.9003043174743652\n",
      "[step: 6480] loss: 0.8931187987327576\n",
      "[step: 6481] loss: 0.8874279856681824\n",
      "[step: 6482] loss: 0.8827258944511414\n",
      "[step: 6483] loss: 0.8793065547943115\n",
      "[step: 6484] loss: 0.8766912221908569\n",
      "[step: 6485] loss: 0.8748188018798828\n",
      "[step: 6486] loss: 0.8734963536262512\n",
      "[step: 6487] loss: 0.872583270072937\n",
      "[step: 6488] loss: 0.8718996644020081\n",
      "[step: 6489] loss: 0.8713682889938354\n",
      "[step: 6490] loss: 0.8709515333175659\n",
      "[step: 6491] loss: 0.8706077337265015\n",
      "[step: 6492] loss: 0.870278000831604\n",
      "[step: 6493] loss: 0.8699531555175781\n",
      "[step: 6494] loss: 0.8696537613868713\n",
      "[step: 6495] loss: 0.8693589568138123\n",
      "[step: 6496] loss: 0.8690541982650757\n",
      "[step: 6497] loss: 0.8687518239021301\n",
      "[step: 6498] loss: 0.8684749603271484\n",
      "[step: 6499] loss: 0.8682287931442261\n",
      "[step: 6500] loss: 0.8680335283279419\n",
      "[step: 6501] loss: 0.8679406642913818\n",
      "[step: 6502] loss: 0.8680648803710938\n",
      "[step: 6503] loss: 0.868606686592102\n",
      "[step: 6504] loss: 0.8700118064880371\n",
      "[step: 6505] loss: 0.8731421828269958\n",
      "[step: 6506] loss: 0.8800873160362244\n",
      "[step: 6507] loss: 0.8946621417999268\n",
      "[step: 6508] loss: 0.9270719289779663\n",
      "[step: 6509] loss: 0.9938457608222961\n",
      "[step: 6510] loss: 1.1473166942596436\n",
      "[step: 6511] loss: 1.4421569108963013\n",
      "[step: 6512] loss: 2.1190927028656006\n",
      "[step: 6513] loss: 3.0487060546875\n",
      "[step: 6514] loss: 4.7313103675842285\n",
      "[step: 6515] loss: 4.576473236083984\n",
      "[step: 6516] loss: 3.5474345684051514\n",
      "[step: 6517] loss: 1.454099416732788\n",
      "[step: 6518] loss: 1.5970797538757324\n",
      "[step: 6519] loss: 3.058196544647217\n",
      "[step: 6520] loss: 2.3296003341674805\n",
      "[step: 6521] loss: 1.1120080947875977\n",
      "[step: 6522] loss: 1.5442882776260376\n",
      "[step: 6523] loss: 2.1100659370422363\n",
      "[step: 6524] loss: 1.4115968942642212\n",
      "[step: 6525] loss: 1.0438547134399414\n",
      "[step: 6526] loss: 1.6673413515090942\n",
      "[step: 6527] loss: 1.531793475151062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6528] loss: 0.9748565554618835\n",
      "[step: 6529] loss: 1.3498414754867554\n",
      "[step: 6530] loss: 1.536238431930542\n",
      "[step: 6531] loss: 1.04036283493042\n",
      "[step: 6532] loss: 1.132112979888916\n",
      "[step: 6533] loss: 1.4222363233566284\n",
      "[step: 6534] loss: 1.114912986755371\n",
      "[step: 6535] loss: 0.9772124290466309\n",
      "[step: 6536] loss: 1.22864830493927\n",
      "[step: 6537] loss: 1.134045124053955\n",
      "[step: 6538] loss: 0.9198517799377441\n",
      "[step: 6539] loss: 1.064424753189087\n",
      "[step: 6540] loss: 1.1130468845367432\n",
      "[step: 6541] loss: 0.9431639313697815\n",
      "[step: 6542] loss: 0.9610725045204163\n",
      "[step: 6543] loss: 1.0506796836853027\n",
      "[step: 6544] loss: 0.9844784140586853\n",
      "[step: 6545] loss: 0.9165680408477783\n",
      "[step: 6546] loss: 0.9780210852622986\n",
      "[step: 6547] loss: 1.0041983127593994\n",
      "[step: 6548] loss: 0.9172298908233643\n",
      "[step: 6549] loss: 0.9197470545768738\n",
      "[step: 6550] loss: 0.9801805019378662\n",
      "[step: 6551] loss: 0.9387664198875427\n",
      "[step: 6552] loss: 0.8979027271270752\n",
      "[step: 6553] loss: 0.924716591835022\n",
      "[step: 6554] loss: 0.9444075226783752\n",
      "[step: 6555] loss: 0.9166710376739502\n",
      "[step: 6556] loss: 0.8894396424293518\n",
      "[step: 6557] loss: 0.9123111963272095\n",
      "[step: 6558] loss: 0.925864577293396\n",
      "[step: 6559] loss: 0.8989019393920898\n",
      "[step: 6560] loss: 0.8873103857040405\n",
      "[step: 6561] loss: 0.9006415605545044\n",
      "[step: 6562] loss: 0.9076496362686157\n",
      "[step: 6563] loss: 0.8938803672790527\n",
      "[step: 6564] loss: 0.8814513683319092\n",
      "[step: 6565] loss: 0.8910462856292725\n",
      "[step: 6566] loss: 0.8974148035049438\n",
      "[step: 6567] loss: 0.8875799179077148\n",
      "[step: 6568] loss: 0.8795918226242065\n",
      "[step: 6569] loss: 0.8822203874588013\n",
      "[step: 6570] loss: 0.8883966207504272\n",
      "[step: 6571] loss: 0.8850679397583008\n",
      "[step: 6572] loss: 0.8768431544303894\n",
      "[step: 6573] loss: 0.876788854598999\n",
      "[step: 6574] loss: 0.8807905316352844\n",
      "[step: 6575] loss: 0.8809207081794739\n",
      "[step: 6576] loss: 0.8764790296554565\n",
      "[step: 6577] loss: 0.8727961778640747\n",
      "[step: 6578] loss: 0.8744750022888184\n",
      "[step: 6579] loss: 0.8765565156936646\n",
      "[step: 6580] loss: 0.8749436736106873\n",
      "[step: 6581] loss: 0.8719109296798706\n",
      "[step: 6582] loss: 0.8702645301818848\n",
      "[step: 6583] loss: 0.8712823390960693\n",
      "[step: 6584] loss: 0.8723187446594238\n",
      "[step: 6585] loss: 0.8710154294967651\n",
      "[step: 6586] loss: 0.8690015077590942\n",
      "[step: 6587] loss: 0.8678452968597412\n",
      "[step: 6588] loss: 0.8680505752563477\n",
      "[step: 6589] loss: 0.8686386346817017\n",
      "[step: 6590] loss: 0.8680195808410645\n",
      "[step: 6591] loss: 0.8666790723800659\n",
      "[step: 6592] loss: 0.8656086921691895\n",
      "[step: 6593] loss: 0.8652271032333374\n",
      "[step: 6594] loss: 0.8654348254203796\n",
      "[step: 6595] loss: 0.865319550037384\n",
      "[step: 6596] loss: 0.8646049499511719\n",
      "[step: 6597] loss: 0.8637279272079468\n",
      "[step: 6598] loss: 0.8630051612854004\n",
      "[step: 6599] loss: 0.8627220392227173\n",
      "[step: 6600] loss: 0.8626608848571777\n",
      "[step: 6601] loss: 0.8624019026756287\n",
      "[step: 6602] loss: 0.8619289398193359\n",
      "[step: 6603] loss: 0.8612945675849915\n",
      "[step: 6604] loss: 0.8606895208358765\n",
      "[step: 6605] loss: 0.8603096604347229\n",
      "[step: 6606] loss: 0.8600448369979858\n",
      "[step: 6607] loss: 0.8598105907440186\n",
      "[step: 6608] loss: 0.859512209892273\n",
      "[step: 6609] loss: 0.8590742349624634\n",
      "[step: 6610] loss: 0.8586015701293945\n",
      "[step: 6611] loss: 0.8581452369689941\n",
      "[step: 6612] loss: 0.857742190361023\n",
      "[step: 6613] loss: 0.8574309349060059\n",
      "[step: 6614] loss: 0.8571484088897705\n",
      "[step: 6615] loss: 0.8568560481071472\n",
      "[step: 6616] loss: 0.8565415740013123\n",
      "[step: 6617] loss: 0.8561732769012451\n",
      "[step: 6618] loss: 0.855782687664032\n",
      "[step: 6619] loss: 0.8553987741470337\n",
      "[step: 6620] loss: 0.8550271391868591\n",
      "[step: 6621] loss: 0.8546918630599976\n",
      "[step: 6622] loss: 0.8543832898139954\n",
      "[step: 6623] loss: 0.8540799021720886\n",
      "[step: 6624] loss: 0.853787899017334\n",
      "[step: 6625] loss: 0.853482723236084\n",
      "[step: 6626] loss: 0.8531690835952759\n",
      "[step: 6627] loss: 0.8528523445129395\n",
      "[step: 6628] loss: 0.8525256514549255\n",
      "[step: 6629] loss: 0.8521997928619385\n",
      "[step: 6630] loss: 0.851874053478241\n",
      "[step: 6631] loss: 0.8515514731407166\n",
      "[step: 6632] loss: 0.8512341380119324\n",
      "[step: 6633] loss: 0.8509218096733093\n",
      "[step: 6634] loss: 0.8506135940551758\n",
      "[step: 6635] loss: 0.8503115773200989\n",
      "[step: 6636] loss: 0.8500111103057861\n",
      "[step: 6637] loss: 0.8497154712677002\n",
      "[step: 6638] loss: 0.8494212031364441\n",
      "[step: 6639] loss: 0.849130392074585\n",
      "[step: 6640] loss: 0.8488417267799377\n",
      "[step: 6641] loss: 0.8485552072525024\n",
      "[step: 6642] loss: 0.8482735753059387\n",
      "[step: 6643] loss: 0.8479944467544556\n",
      "[step: 6644] loss: 0.847722053527832\n",
      "[step: 6645] loss: 0.8474587202072144\n",
      "[step: 6646] loss: 0.847209095954895\n",
      "[step: 6647] loss: 0.8469793796539307\n",
      "[step: 6648] loss: 0.8467856645584106\n",
      "[step: 6649] loss: 0.846647322177887\n",
      "[step: 6650] loss: 0.8466046452522278\n",
      "[step: 6651] loss: 0.8467347621917725\n",
      "[step: 6652] loss: 0.8471585512161255\n",
      "[step: 6653] loss: 0.8481327295303345\n",
      "[step: 6654] loss: 0.850077748298645\n",
      "[step: 6655] loss: 0.8539192080497742\n",
      "[step: 6656] loss: 0.8611181974411011\n",
      "[step: 6657] loss: 0.8752889633178711\n",
      "[step: 6658] loss: 0.9015042185783386\n",
      "[step: 6659] loss: 0.9545704126358032\n",
      "[step: 6660] loss: 1.0499931573867798\n",
      "[step: 6661] loss: 1.248239278793335\n",
      "[step: 6662] loss: 1.564882755279541\n",
      "[step: 6663] loss: 2.19942569732666\n",
      "[step: 6664] loss: 2.823988199234009\n",
      "[step: 6665] loss: 3.7177248001098633\n",
      "[step: 6666] loss: 3.1752443313598633\n",
      "[step: 6667] loss: 2.172538995742798\n",
      "[step: 6668] loss: 1.0398597717285156\n",
      "[step: 6669] loss: 1.163830041885376\n",
      "[step: 6670] loss: 2.0095713138580322\n",
      "[step: 6671] loss: 1.9193850755691528\n",
      "[step: 6672] loss: 1.1906682252883911\n",
      "[step: 6673] loss: 0.9489313364028931\n",
      "[step: 6674] loss: 1.4675357341766357\n",
      "[step: 6675] loss: 1.6460046768188477\n",
      "[step: 6676] loss: 1.0980643033981323\n",
      "[step: 6677] loss: 0.9398031234741211\n",
      "[step: 6678] loss: 1.2935266494750977\n",
      "[step: 6679] loss: 1.3338091373443604\n",
      "[step: 6680] loss: 1.0116970539093018\n",
      "[step: 6681] loss: 0.9154061079025269\n",
      "[step: 6682] loss: 1.139479398727417\n",
      "[step: 6683] loss: 1.1826279163360596\n",
      "[step: 6684] loss: 0.9530465602874756\n",
      "[step: 6685] loss: 0.8981344699859619\n",
      "[step: 6686] loss: 1.0535857677459717\n",
      "[step: 6687] loss: 1.0624146461486816\n",
      "[step: 6688] loss: 0.9156436324119568\n",
      "[step: 6689] loss: 0.8810771703720093\n",
      "[step: 6690] loss: 0.9797025322914124\n",
      "[step: 6691] loss: 0.9999996423721313\n",
      "[step: 6692] loss: 0.9022372961044312\n",
      "[step: 6693] loss: 0.8699923753738403\n",
      "[step: 6694] loss: 0.9370681643486023\n",
      "[step: 6695] loss: 0.955014169216156\n",
      "[step: 6696] loss: 0.8953231573104858\n",
      "[step: 6697] loss: 0.862397313117981\n",
      "[step: 6698] loss: 0.8976773023605347\n",
      "[step: 6699] loss: 0.9254753589630127\n",
      "[step: 6700] loss: 0.8936896324157715\n",
      "[step: 6701] loss: 0.8591436743736267\n",
      "[step: 6702] loss: 0.8698742389678955\n",
      "[step: 6703] loss: 0.8949921131134033\n",
      "[step: 6704] loss: 0.8916923403739929\n",
      "[step: 6705] loss: 0.8657668828964233\n",
      "[step: 6706] loss: 0.8553136587142944\n",
      "[step: 6707] loss: 0.8685899376869202\n",
      "[step: 6708] loss: 0.8788330554962158\n",
      "[step: 6709] loss: 0.8704957962036133\n",
      "[step: 6710] loss: 0.8558003902435303\n",
      "[step: 6711] loss: 0.8533011078834534\n",
      "[step: 6712] loss: 0.8618239164352417\n",
      "[step: 6713] loss: 0.8659491539001465\n",
      "[step: 6714] loss: 0.8596041202545166\n",
      "[step: 6715] loss: 0.8511010408401489\n",
      "[step: 6716] loss: 0.8498259782791138\n",
      "[step: 6717] loss: 0.85463547706604\n",
      "[step: 6718] loss: 0.8572028279304504\n",
      "[step: 6719] loss: 0.8536249399185181\n",
      "[step: 6720] loss: 0.8482432961463928\n",
      "[step: 6721] loss: 0.8465065956115723\n",
      "[step: 6722] loss: 0.848751425743103\n",
      "[step: 6723] loss: 0.8509107232093811\n",
      "[step: 6724] loss: 0.8497215509414673\n",
      "[step: 6725] loss: 0.8463352918624878\n",
      "[step: 6726] loss: 0.8439857959747314\n",
      "[step: 6727] loss: 0.8442133665084839\n",
      "[step: 6728] loss: 0.8456969261169434\n",
      "[step: 6729] loss: 0.8461015224456787\n",
      "[step: 6730] loss: 0.8447121381759644\n",
      "[step: 6731] loss: 0.8426482677459717\n",
      "[step: 6732] loss: 0.8414443731307983\n",
      "[step: 6733] loss: 0.8415765166282654\n",
      "[step: 6734] loss: 0.8422141075134277\n",
      "[step: 6735] loss: 0.8422701358795166\n",
      "[step: 6736] loss: 0.8413993716239929\n",
      "[step: 6737] loss: 0.8401219844818115\n",
      "[step: 6738] loss: 0.8392124176025391\n",
      "[step: 6739] loss: 0.8389990329742432\n",
      "[step: 6740] loss: 0.8391703367233276\n",
      "[step: 6741] loss: 0.8391898274421692\n",
      "[step: 6742] loss: 0.8387528657913208\n",
      "[step: 6743] loss: 0.8379716277122498\n",
      "[step: 6744] loss: 0.8372159600257874\n",
      "[step: 6745] loss: 0.8367577195167542\n",
      "[step: 6746] loss: 0.8366066217422485\n",
      "[step: 6747] loss: 0.8365503549575806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6748] loss: 0.8363641500473022\n",
      "[step: 6749] loss: 0.8359543681144714\n",
      "[step: 6750] loss: 0.8354013562202454\n",
      "[step: 6751] loss: 0.8348791599273682\n",
      "[step: 6752] loss: 0.8344944715499878\n",
      "[step: 6753] loss: 0.8342593908309937\n",
      "[step: 6754] loss: 0.8340838551521301\n",
      "[step: 6755] loss: 0.8338639736175537\n",
      "[step: 6756] loss: 0.8335558176040649\n",
      "[step: 6757] loss: 0.833162784576416\n",
      "[step: 6758] loss: 0.8327541351318359\n",
      "[step: 6759] loss: 0.8323869109153748\n",
      "[step: 6760] loss: 0.8320847153663635\n",
      "[step: 6761] loss: 0.8318402767181396\n",
      "[step: 6762] loss: 0.8316041231155396\n",
      "[step: 6763] loss: 0.8313509225845337\n",
      "[step: 6764] loss: 0.8310587406158447\n",
      "[step: 6765] loss: 0.8307368159294128\n",
      "[step: 6766] loss: 0.8304088711738586\n",
      "[step: 6767] loss: 0.8300974369049072\n",
      "[step: 6768] loss: 0.829816460609436\n",
      "[step: 6769] loss: 0.8295633792877197\n",
      "[step: 6770] loss: 0.8293310403823853\n",
      "[step: 6771] loss: 0.8291124105453491\n",
      "[step: 6772] loss: 0.8289069533348083\n",
      "[step: 6773] loss: 0.828725278377533\n",
      "[step: 6774] loss: 0.8286072611808777\n",
      "[step: 6775] loss: 0.8286086320877075\n",
      "[step: 6776] loss: 0.8288435935974121\n",
      "[step: 6777] loss: 0.829491913318634\n",
      "[step: 6778] loss: 0.8309322595596313\n",
      "[step: 6779] loss: 0.833783745765686\n",
      "[step: 6780] loss: 0.8394913077354431\n",
      "[step: 6781] loss: 0.8503130078315735\n",
      "[step: 6782] loss: 0.8721529841423035\n",
      "[step: 6783] loss: 0.9129491448402405\n",
      "[step: 6784] loss: 0.9978366494178772\n",
      "[step: 6785] loss: 1.148191213607788\n",
      "[step: 6786] loss: 1.4649572372436523\n",
      "[step: 6787] loss: 1.9187508821487427\n",
      "[step: 6788] loss: 2.776731491088867\n",
      "[step: 6789] loss: 3.2189860343933105\n",
      "[step: 6790] loss: 3.554586887359619\n",
      "[step: 6791] loss: 2.2724266052246094\n",
      "[step: 6792] loss: 1.1833128929138184\n",
      "[step: 6793] loss: 1.0759742259979248\n",
      "[step: 6794] loss: 1.7432245016098022\n",
      "[step: 6795] loss: 2.0845282077789307\n",
      "[step: 6796] loss: 1.3488082885742188\n",
      "[step: 6797] loss: 0.961383581161499\n",
      "[step: 6798] loss: 1.4015291929244995\n",
      "[step: 6799] loss: 1.5831091403961182\n",
      "[step: 6800] loss: 1.2039669752120972\n",
      "[step: 6801] loss: 0.9161882400512695\n",
      "[step: 6802] loss: 1.1819956302642822\n",
      "[step: 6803] loss: 1.396346092224121\n",
      "[step: 6804] loss: 1.077444314956665\n",
      "[step: 6805] loss: 0.8873799443244934\n",
      "[step: 6806] loss: 1.077628493309021\n",
      "[step: 6807] loss: 1.1635444164276123\n",
      "[step: 6808] loss: 0.9851711392402649\n",
      "[step: 6809] loss: 0.8708536624908447\n",
      "[step: 6810] loss: 0.9907920956611633\n",
      "[step: 6811] loss: 1.0583595037460327\n",
      "[step: 6812] loss: 0.9223117828369141\n",
      "[step: 6813] loss: 0.8635140657424927\n",
      "[step: 6814] loss: 0.9502788782119751\n",
      "[step: 6815] loss: 0.970554769039154\n",
      "[step: 6816] loss: 0.8907952308654785\n",
      "[step: 6817] loss: 0.8611249923706055\n",
      "[step: 6818] loss: 0.9152238965034485\n",
      "[step: 6819] loss: 0.927848219871521\n",
      "[step: 6820] loss: 0.872319221496582\n",
      "[step: 6821] loss: 0.8598805069923401\n",
      "[step: 6822] loss: 0.890872597694397\n",
      "[step: 6823] loss: 0.8872798681259155\n",
      "[step: 6824] loss: 0.8572202920913696\n",
      "[step: 6825] loss: 0.8548843860626221\n",
      "[step: 6826] loss: 0.8727606534957886\n",
      "[step: 6827] loss: 0.865730345249176\n",
      "[step: 6828] loss: 0.8438690900802612\n",
      "[step: 6829] loss: 0.8471460342407227\n",
      "[step: 6830] loss: 0.8626988530158997\n",
      "[step: 6831] loss: 0.8558030128479004\n",
      "[step: 6832] loss: 0.837409496307373\n",
      "[step: 6833] loss: 0.8364960551261902\n",
      "[step: 6834] loss: 0.8493584990501404\n",
      "[step: 6835] loss: 0.8501030206680298\n",
      "[step: 6836] loss: 0.8371143341064453\n",
      "[step: 6837] loss: 0.831929624080658\n",
      "[step: 6838] loss: 0.8383578658103943\n",
      "[step: 6839] loss: 0.8410165309906006\n",
      "[step: 6840] loss: 0.8348958492279053\n",
      "[step: 6841] loss: 0.8306121230125427\n",
      "[step: 6842] loss: 0.8333684802055359\n",
      "[step: 6843] loss: 0.8352307081222534\n",
      "[step: 6844] loss: 0.8312076330184937\n",
      "[step: 6845] loss: 0.8275387287139893\n",
      "[step: 6846] loss: 0.8290255069732666\n",
      "[step: 6847] loss: 0.8315907716751099\n",
      "[step: 6848] loss: 0.8300991058349609\n",
      "[step: 6849] loss: 0.826331615447998\n",
      "[step: 6850] loss: 0.8253875970840454\n",
      "[step: 6851] loss: 0.8269106149673462\n",
      "[step: 6852] loss: 0.8271442651748657\n",
      "[step: 6853] loss: 0.8253836631774902\n",
      "[step: 6854] loss: 0.8241006135940552\n",
      "[step: 6855] loss: 0.8245643377304077\n",
      "[step: 6856] loss: 0.8249313235282898\n",
      "[step: 6857] loss: 0.8237280249595642\n",
      "[step: 6858] loss: 0.8222116231918335\n",
      "[step: 6859] loss: 0.821880578994751\n",
      "[step: 6860] loss: 0.8224014043807983\n",
      "[step: 6861] loss: 0.8223568797111511\n",
      "[step: 6862] loss: 0.8214122653007507\n",
      "[step: 6863] loss: 0.8206713199615479\n",
      "[step: 6864] loss: 0.8206082582473755\n",
      "[step: 6865] loss: 0.8206198215484619\n",
      "[step: 6866] loss: 0.8201043605804443\n",
      "[step: 6867] loss: 0.8192665576934814\n",
      "[step: 6868] loss: 0.818801999092102\n",
      "[step: 6869] loss: 0.8187483549118042\n",
      "[step: 6870] loss: 0.8186156749725342\n",
      "[step: 6871] loss: 0.8181710839271545\n",
      "[step: 6872] loss: 0.8176599144935608\n",
      "[step: 6873] loss: 0.817407488822937\n",
      "[step: 6874] loss: 0.8173273801803589\n",
      "[step: 6875] loss: 0.8171103000640869\n",
      "[step: 6876] loss: 0.816709578037262\n",
      "[step: 6877] loss: 0.8163161873817444\n",
      "[step: 6878] loss: 0.8160829544067383\n",
      "[step: 6879] loss: 0.8159347176551819\n",
      "[step: 6880] loss: 0.8157046437263489\n",
      "[step: 6881] loss: 0.8153852224349976\n",
      "[step: 6882] loss: 0.8150975704193115\n",
      "[step: 6883] loss: 0.814939022064209\n",
      "[step: 6884] loss: 0.8148593902587891\n",
      "[step: 6885] loss: 0.814792275428772\n",
      "[step: 6886] loss: 0.8147714138031006\n",
      "[step: 6887] loss: 0.8149250745773315\n",
      "[step: 6888] loss: 0.8153946399688721\n",
      "[step: 6889] loss: 0.8163444995880127\n",
      "[step: 6890] loss: 0.8179677724838257\n",
      "[step: 6891] loss: 0.8208634257316589\n",
      "[step: 6892] loss: 0.8258010149002075\n",
      "[step: 6893] loss: 0.8347635269165039\n",
      "[step: 6894] loss: 0.8499670028686523\n",
      "[step: 6895] loss: 0.8780537247657776\n",
      "[step: 6896] loss: 0.9251583814620972\n",
      "[step: 6897] loss: 1.0150864124298096\n",
      "[step: 6898] loss: 1.1574640274047852\n",
      "[step: 6899] loss: 1.4310575723648071\n",
      "[step: 6900] loss: 1.780640959739685\n",
      "[step: 6901] loss: 2.383859634399414\n",
      "[step: 6902] loss: 2.6807355880737305\n",
      "[step: 6903] loss: 2.8946547508239746\n",
      "[step: 6904] loss: 2.0645291805267334\n",
      "[step: 6905] loss: 1.2094331979751587\n",
      "[step: 6906] loss: 0.8569768667221069\n",
      "[step: 6907] loss: 1.2372264862060547\n",
      "[step: 6908] loss: 1.7110950946807861\n",
      "[step: 6909] loss: 1.4806151390075684\n",
      "[step: 6910] loss: 0.9940944314002991\n",
      "[step: 6911] loss: 0.8828310966491699\n",
      "[step: 6912] loss: 1.196071743965149\n",
      "[step: 6913] loss: 1.3698675632476807\n",
      "[step: 6914] loss: 1.0987595319747925\n",
      "[step: 6915] loss: 0.8567036986351013\n",
      "[step: 6916] loss: 0.9398729205131531\n",
      "[step: 6917] loss: 1.1339691877365112\n",
      "[step: 6918] loss: 1.125098466873169\n",
      "[step: 6919] loss: 0.9280098676681519\n",
      "[step: 6920] loss: 0.8516821265220642\n",
      "[step: 6921] loss: 0.9537456035614014\n",
      "[step: 6922] loss: 1.0299055576324463\n",
      "[step: 6923] loss: 0.966335654258728\n",
      "[step: 6924] loss: 0.8600841760635376\n",
      "[step: 6925] loss: 0.8561802506446838\n",
      "[step: 6926] loss: 0.9234178066253662\n",
      "[step: 6927] loss: 0.9434451460838318\n",
      "[step: 6928] loss: 0.891945481300354\n",
      "[step: 6929] loss: 0.8399367332458496\n",
      "[step: 6930] loss: 0.8484706878662109\n",
      "[step: 6931] loss: 0.8872146606445312\n",
      "[step: 6932] loss: 0.8948063850402832\n",
      "[step: 6933] loss: 0.8628440499305725\n",
      "[step: 6934] loss: 0.830249547958374\n",
      "[step: 6935] loss: 0.8326600790023804\n",
      "[step: 6936] loss: 0.8574271202087402\n",
      "[step: 6937] loss: 0.867181658744812\n",
      "[step: 6938] loss: 0.8493598699569702\n",
      "[step: 6939] loss: 0.8242939710617065\n",
      "[step: 6940] loss: 0.8195509910583496\n",
      "[step: 6941] loss: 0.8346160650253296\n",
      "[step: 6942] loss: 0.8473178148269653\n",
      "[step: 6943] loss: 0.8427978754043579\n",
      "[step: 6944] loss: 0.82669997215271\n",
      "[step: 6945] loss: 0.816328227519989\n",
      "[step: 6946] loss: 0.8181520700454712\n",
      "[step: 6947] loss: 0.8259502053260803\n",
      "[step: 6948] loss: 0.8302557468414307\n",
      "[step: 6949] loss: 0.827479898929596\n",
      "[step: 6950] loss: 0.8211658000946045\n",
      "[step: 6951] loss: 0.8154363632202148\n",
      "[step: 6952] loss: 0.813329815864563\n",
      "[step: 6953] loss: 0.8150039911270142\n",
      "[step: 6954] loss: 0.8184049129486084\n",
      "[step: 6955] loss: 0.8203170895576477\n",
      "[step: 6956] loss: 0.8184677362442017\n",
      "[step: 6957] loss: 0.814153790473938\n",
      "[step: 6958] loss: 0.8103867769241333\n",
      "[step: 6959] loss: 0.8093554377555847\n",
      "[step: 6960] loss: 0.8106887340545654\n",
      "[step: 6961] loss: 0.812440037727356\n",
      "[step: 6962] loss: 0.8130894303321838\n",
      "[step: 6963] loss: 0.8124586343765259\n",
      "[step: 6964] loss: 0.8111171126365662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 6965] loss: 0.8094104528427124\n",
      "[step: 6966] loss: 0.8077996969223022\n",
      "[step: 6967] loss: 0.8066403865814209\n",
      "[step: 6968] loss: 0.80632483959198\n",
      "[step: 6969] loss: 0.806719958782196\n",
      "[step: 6970] loss: 0.8073006272315979\n",
      "[step: 6971] loss: 0.8076284527778625\n",
      "[step: 6972] loss: 0.8074772357940674\n",
      "[step: 6973] loss: 0.80706787109375\n",
      "[step: 6974] loss: 0.8065037727355957\n",
      "[step: 6975] loss: 0.8058266639709473\n",
      "[step: 6976] loss: 0.8050438761711121\n",
      "[step: 6977] loss: 0.8042197227478027\n",
      "[step: 6978] loss: 0.8035197854042053\n",
      "[step: 6979] loss: 0.8030111789703369\n",
      "[step: 6980] loss: 0.802638053894043\n",
      "[step: 6981] loss: 0.8023344278335571\n",
      "[step: 6982] loss: 0.8020375370979309\n",
      "[step: 6983] loss: 0.8017991185188293\n",
      "[step: 6984] loss: 0.801641583442688\n",
      "[step: 6985] loss: 0.8015360832214355\n",
      "[step: 6986] loss: 0.8014686107635498\n",
      "[step: 6987] loss: 0.8014122247695923\n",
      "[step: 6988] loss: 0.8014318943023682\n",
      "[step: 6989] loss: 0.8015849590301514\n",
      "[step: 6990] loss: 0.8019527196884155\n",
      "[step: 6991] loss: 0.8026131391525269\n",
      "[step: 6992] loss: 0.8037534952163696\n",
      "[step: 6993] loss: 0.8056254982948303\n",
      "[step: 6994] loss: 0.8088651895523071\n",
      "[step: 6995] loss: 0.8141350746154785\n",
      "[step: 6996] loss: 0.8233771324157715\n",
      "[step: 6997] loss: 0.8383602499961853\n",
      "[step: 6998] loss: 0.8654431104660034\n",
      "[step: 6999] loss: 0.9089385867118835\n",
      "[step: 7000] loss: 0.9902238249778748\n",
      "[step: 7001] loss: 1.1133661270141602\n",
      "[step: 7002] loss: 1.345254898071289\n",
      "[step: 7003] loss: 1.630763292312622\n",
      "[step: 7004] loss: 2.121363639831543\n",
      "[step: 7005] loss: 2.375593662261963\n",
      "[step: 7006] loss: 2.6065800189971924\n",
      "[step: 7007] loss: 1.9997379779815674\n",
      "[step: 7008] loss: 1.3067145347595215\n",
      "[step: 7009] loss: 0.8493598103523254\n",
      "[step: 7010] loss: 1.0037641525268555\n",
      "[step: 7011] loss: 1.4410655498504639\n",
      "[step: 7012] loss: 1.4901896715164185\n",
      "[step: 7013] loss: 1.1719653606414795\n",
      "[step: 7014] loss: 0.8467931151390076\n",
      "[step: 7015] loss: 0.93100905418396\n",
      "[step: 7016] loss: 1.204496145248413\n",
      "[step: 7017] loss: 1.2189470529556274\n",
      "[step: 7018] loss: 1.0057588815689087\n",
      "[step: 7019] loss: 0.8253964781761169\n",
      "[step: 7020] loss: 0.8981586694717407\n",
      "[step: 7021] loss: 1.0638689994812012\n",
      "[step: 7022] loss: 1.0643088817596436\n",
      "[step: 7023] loss: 0.9287528991699219\n",
      "[step: 7024] loss: 0.820158064365387\n",
      "[step: 7025] loss: 0.8678052425384521\n",
      "[step: 7026] loss: 0.9671337008476257\n",
      "[step: 7027] loss: 0.9659085869789124\n",
      "[step: 7028] loss: 0.8845937848091125\n",
      "[step: 7029] loss: 0.8154628276824951\n",
      "[step: 7030] loss: 0.838760495185852\n",
      "[step: 7031] loss: 0.8994513750076294\n",
      "[step: 7032] loss: 0.9061776399612427\n",
      "[step: 7033] loss: 0.8619756698608398\n",
      "[step: 7034] loss: 0.8146206736564636\n",
      "[step: 7035] loss: 0.8183629512786865\n",
      "[step: 7036] loss: 0.8543407917022705\n",
      "[step: 7037] loss: 0.8705547451972961\n",
      "[step: 7038] loss: 0.8547917604446411\n",
      "[step: 7039] loss: 0.8202507495880127\n",
      "[step: 7040] loss: 0.8048914670944214\n",
      "[step: 7041] loss: 0.8179184198379517\n",
      "[step: 7042] loss: 0.8375494480133057\n",
      "[step: 7043] loss: 0.8450900912284851\n",
      "[step: 7044] loss: 0.8308092951774597\n",
      "[step: 7045] loss: 0.810519278049469\n",
      "[step: 7046] loss: 0.8010908961296082\n",
      "[step: 7047] loss: 0.8056312203407288\n",
      "[step: 7048] loss: 0.817359447479248\n",
      "[step: 7049] loss: 0.8236430287361145\n",
      "[step: 7050] loss: 0.8198128938674927\n",
      "[step: 7051] loss: 0.8098655939102173\n",
      "[step: 7052] loss: 0.8009987473487854\n",
      "[step: 7053] loss: 0.7981145977973938\n",
      "[step: 7054] loss: 0.8010834455490112\n",
      "[step: 7055] loss: 0.8056400418281555\n",
      "[step: 7056] loss: 0.8084413409233093\n",
      "[step: 7057] loss: 0.8078980445861816\n",
      "[step: 7058] loss: 0.8042399883270264\n",
      "[step: 7059] loss: 0.7998615503311157\n",
      "[step: 7060] loss: 0.7963062524795532\n",
      "[step: 7061] loss: 0.795024037361145\n",
      "[step: 7062] loss: 0.7958260774612427\n",
      "[step: 7063] loss: 0.7976999878883362\n",
      "[step: 7064] loss: 0.7995364665985107\n",
      "[step: 7065] loss: 0.8000672459602356\n",
      "[step: 7066] loss: 0.7993923425674438\n",
      "[step: 7067] loss: 0.797753095626831\n",
      "[step: 7068] loss: 0.7957726716995239\n",
      "[step: 7069] loss: 0.7940327525138855\n",
      "[step: 7070] loss: 0.7926833033561707\n",
      "[step: 7071] loss: 0.7918224930763245\n",
      "[step: 7072] loss: 0.7914360165596008\n",
      "[step: 7073] loss: 0.7913740277290344\n",
      "[step: 7074] loss: 0.791637659072876\n",
      "[step: 7075] loss: 0.7920223474502563\n",
      "[step: 7076] loss: 0.7924234867095947\n",
      "[step: 7077] loss: 0.792813777923584\n",
      "[step: 7078] loss: 0.7930971384048462\n",
      "[step: 7079] loss: 0.793448805809021\n",
      "[step: 7080] loss: 0.7938092350959778\n",
      "[step: 7081] loss: 0.794353723526001\n",
      "[step: 7082] loss: 0.7950872778892517\n",
      "[step: 7083] loss: 0.7962146401405334\n",
      "[step: 7084] loss: 0.7977986335754395\n",
      "[step: 7085] loss: 0.8003472089767456\n",
      "[step: 7086] loss: 0.8040076494216919\n",
      "[step: 7087] loss: 0.8100201487541199\n",
      "[step: 7088] loss: 0.818728506565094\n",
      "[step: 7089] loss: 0.8332785964012146\n",
      "[step: 7090] loss: 0.8543751239776611\n",
      "[step: 7091] loss: 0.8906804919242859\n",
      "[step: 7092] loss: 0.9423264861106873\n",
      "[step: 7093] loss: 1.033449411392212\n",
      "[step: 7094] loss: 1.1516900062561035\n",
      "[step: 7095] loss: 1.3571884632110596\n",
      "[step: 7096] loss: 1.5571892261505127\n",
      "[step: 7097] loss: 1.863953948020935\n",
      "[step: 7098] loss: 1.929703950881958\n",
      "[step: 7099] loss: 1.9383584260940552\n",
      "[step: 7100] loss: 1.5140881538391113\n",
      "[step: 7101] loss: 1.0891485214233398\n",
      "[step: 7102] loss: 0.8223080635070801\n",
      "[step: 7103] loss: 0.8768270611763\n",
      "[step: 7104] loss: 1.1185812950134277\n",
      "[step: 7105] loss: 1.2473456859588623\n",
      "[step: 7106] loss: 1.1783943176269531\n",
      "[step: 7107] loss: 0.9406488537788391\n",
      "[step: 7108] loss: 0.8047677874565125\n",
      "[step: 7109] loss: 0.8540822863578796\n",
      "[step: 7110] loss: 0.9895931482315063\n",
      "[step: 7111] loss: 1.0695226192474365\n",
      "[step: 7112] loss: 0.9966866970062256\n",
      "[step: 7113] loss: 0.8737718462944031\n",
      "[step: 7114] loss: 0.7996673583984375\n",
      "[step: 7115] loss: 0.8246984481811523\n",
      "[step: 7116] loss: 0.8996090888977051\n",
      "[step: 7117] loss: 0.9360494613647461\n",
      "[step: 7118] loss: 0.9108664393424988\n",
      "[step: 7119] loss: 0.8431769609451294\n",
      "[step: 7120] loss: 0.7977694272994995\n",
      "[step: 7121] loss: 0.8012444972991943\n",
      "[step: 7122] loss: 0.83643639087677\n",
      "[step: 7123] loss: 0.8659865856170654\n",
      "[step: 7124] loss: 0.8613219261169434\n",
      "[step: 7125] loss: 0.8334007263183594\n",
      "[step: 7126] loss: 0.8022676706314087\n",
      "[step: 7127] loss: 0.7905628085136414\n",
      "[step: 7128] loss: 0.800193727016449\n",
      "[step: 7129] loss: 0.818212628364563\n",
      "[step: 7130] loss: 0.830763041973114\n",
      "[step: 7131] loss: 0.828220546245575\n",
      "[step: 7132] loss: 0.8152264356613159\n",
      "[step: 7133] loss: 0.7986786365509033\n",
      "[step: 7134] loss: 0.7881998419761658\n",
      "[step: 7135] loss: 0.7868415713310242\n",
      "[step: 7136] loss: 0.7927389740943909\n",
      "[step: 7137] loss: 0.8013822436332703\n",
      "[step: 7138] loss: 0.8075098991394043\n",
      "[step: 7139] loss: 0.809188187122345\n",
      "[step: 7140] loss: 0.8055059909820557\n",
      "[step: 7141] loss: 0.7993147373199463\n",
      "[step: 7142] loss: 0.7922612428665161\n",
      "[step: 7143] loss: 0.7868262529373169\n",
      "[step: 7144] loss: 0.7837702035903931\n",
      "[step: 7145] loss: 0.7830047011375427\n",
      "[step: 7146] loss: 0.7840142250061035\n",
      "[step: 7147] loss: 0.786076545715332\n",
      "[step: 7148] loss: 0.7884647846221924\n",
      "[step: 7149] loss: 0.7905615568161011\n",
      "[step: 7150] loss: 0.7922601699829102\n",
      "[step: 7151] loss: 0.7930809855461121\n",
      "[step: 7152] loss: 0.7934364080429077\n",
      "[step: 7153] loss: 0.7931491136550903\n",
      "[step: 7154] loss: 0.7928221821784973\n",
      "[step: 7155] loss: 0.7923179268836975\n",
      "[step: 7156] loss: 0.7922155857086182\n",
      "[step: 7157] loss: 0.7922179698944092\n",
      "[step: 7158] loss: 0.7928977012634277\n",
      "[step: 7159] loss: 0.7940235733985901\n",
      "[step: 7160] loss: 0.7963758111000061\n",
      "[step: 7161] loss: 0.7998287677764893\n",
      "[step: 7162] loss: 0.8058837652206421\n",
      "[step: 7163] loss: 0.814511775970459\n",
      "[step: 7164] loss: 0.8290503025054932\n",
      "[step: 7165] loss: 0.8496275544166565\n",
      "[step: 7166] loss: 0.8847337961196899\n",
      "[step: 7167] loss: 0.9331666231155396\n",
      "[step: 7168] loss: 1.0171395540237427\n",
      "[step: 7169] loss: 1.1226907968521118\n",
      "[step: 7170] loss: 1.3028589487075806\n",
      "[step: 7171] loss: 1.4742236137390137\n",
      "[step: 7172] loss: 1.7339482307434082\n",
      "[step: 7173] loss: 1.795642614364624\n",
      "[step: 7174] loss: 1.8169803619384766\n",
      "[step: 7175] loss: 1.4754685163497925\n",
      "[step: 7176] loss: 1.1134798526763916\n",
      "[step: 7177] loss: 0.836307942867279\n",
      "[step: 7178] loss: 0.818930983543396\n",
      "[step: 7179] loss: 0.9976181387901306\n",
      "[step: 7180] loss: 1.157387137413025\n",
      "[step: 7181] loss: 1.1791231632232666\n",
      "[step: 7182] loss: 1.0044784545898438\n",
      "[step: 7183] loss: 0.8363686800003052\n",
      "[step: 7184] loss: 0.790298342704773\n",
      "[step: 7185] loss: 0.871476411819458\n",
      "[step: 7186] loss: 0.9836753606796265\n",
      "[step: 7187] loss: 1.008410930633545\n",
      "[step: 7188] loss: 0.9493539929389954\n",
      "[step: 7189] loss: 0.8450234532356262\n",
      "[step: 7190] loss: 0.7874705195426941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7191] loss: 0.8027989864349365\n",
      "[step: 7192] loss: 0.8579286336898804\n",
      "[step: 7193] loss: 0.9009122252464294\n",
      "[step: 7194] loss: 0.8908243179321289\n",
      "[step: 7195] loss: 0.8474302887916565\n",
      "[step: 7196] loss: 0.7995473146438599\n",
      "[step: 7197] loss: 0.7813122272491455\n",
      "[step: 7198] loss: 0.7957260012626648\n",
      "[step: 7199] loss: 0.8230786323547363\n",
      "[step: 7200] loss: 0.8414516448974609\n",
      "[step: 7201] loss: 0.8361217975616455\n",
      "[step: 7202] loss: 0.8162736296653748\n",
      "[step: 7203] loss: 0.7931092977523804\n",
      "[step: 7204] loss: 0.7801966667175293\n",
      "[step: 7205] loss: 0.7807302474975586\n",
      "[step: 7206] loss: 0.7904564142227173\n",
      "[step: 7207] loss: 0.8019832968711853\n",
      "[step: 7208] loss: 0.8083931803703308\n",
      "[step: 7209] loss: 0.8087154626846313\n",
      "[step: 7210] loss: 0.8023055195808411\n",
      "[step: 7211] loss: 0.7934379577636719\n",
      "[step: 7212] loss: 0.7843682169914246\n",
      "[step: 7213] loss: 0.7778698205947876\n",
      "[step: 7214] loss: 0.774722695350647\n",
      "[step: 7215] loss: 0.7749046683311462\n",
      "[step: 7216] loss: 0.7775479555130005\n",
      "[step: 7217] loss: 0.7812533974647522\n",
      "[step: 7218] loss: 0.7849819660186768\n",
      "[step: 7219] loss: 0.7876825332641602\n",
      "[step: 7220] loss: 0.7893814444541931\n",
      "[step: 7221] loss: 0.7895311713218689\n",
      "[step: 7222] loss: 0.7890615463256836\n",
      "[step: 7223] loss: 0.7877053618431091\n",
      "[step: 7224] loss: 0.7864533066749573\n",
      "[step: 7225] loss: 0.7850200533866882\n",
      "[step: 7226] loss: 0.784034013748169\n",
      "[step: 7227] loss: 0.7831534743309021\n",
      "[step: 7228] loss: 0.7828456163406372\n",
      "[step: 7229] loss: 0.7828190326690674\n",
      "[step: 7230] loss: 0.7836140394210815\n",
      "[step: 7231] loss: 0.7850924134254456\n",
      "[step: 7232] loss: 0.7881287336349487\n",
      "[step: 7233] loss: 0.7927563190460205\n",
      "[step: 7234] loss: 0.8008627891540527\n",
      "[step: 7235] loss: 0.8127965927124023\n",
      "[step: 7236] loss: 0.833310604095459\n",
      "[step: 7237] loss: 0.8630973100662231\n",
      "[step: 7238] loss: 0.915157675743103\n",
      "[step: 7239] loss: 0.9876874089241028\n",
      "[step: 7240] loss: 1.116039752960205\n",
      "[step: 7241] loss: 1.2718617916107178\n",
      "[step: 7242] loss: 1.5348881483078003\n",
      "[step: 7243] loss: 1.7354806661605835\n",
      "[step: 7244] loss: 1.997905969619751\n",
      "[step: 7245] loss: 1.8777427673339844\n",
      "[step: 7246] loss: 1.6376302242279053\n",
      "[step: 7247] loss: 1.1424199342727661\n",
      "[step: 7248] loss: 0.8290181756019592\n",
      "[step: 7249] loss: 0.8316969871520996\n",
      "[step: 7250] loss: 1.054553508758545\n",
      "[step: 7251] loss: 1.2552515268325806\n",
      "[step: 7252] loss: 1.1775991916656494\n",
      "[step: 7253] loss: 0.9648884534835815\n",
      "[step: 7254] loss: 0.7962619066238403\n",
      "[step: 7255] loss: 0.8189078569412231\n",
      "[step: 7256] loss: 0.9610360860824585\n",
      "[step: 7257] loss: 1.0450385808944702\n",
      "[step: 7258] loss: 1.0092195272445679\n",
      "[step: 7259] loss: 0.8749839067459106\n",
      "[step: 7260] loss: 0.7856616973876953\n",
      "[step: 7261] loss: 0.7980931997299194\n",
      "[step: 7262] loss: 0.8717959523200989\n",
      "[step: 7263] loss: 0.9250921010971069\n",
      "[step: 7264] loss: 0.8994027972221375\n",
      "[step: 7265] loss: 0.8343687057495117\n",
      "[step: 7266] loss: 0.7816329002380371\n",
      "[step: 7267] loss: 0.7806713581085205\n",
      "[step: 7268] loss: 0.8173766136169434\n",
      "[step: 7269] loss: 0.8481301069259644\n",
      "[step: 7270] loss: 0.8483010530471802\n",
      "[step: 7271] loss: 0.8165659308433533\n",
      "[step: 7272] loss: 0.7840691804885864\n",
      "[step: 7273] loss: 0.7721134424209595\n",
      "[step: 7274] loss: 0.7832657098770142\n",
      "[step: 7275] loss: 0.8031876683235168\n",
      "[step: 7276] loss: 0.8142839670181274\n",
      "[step: 7277] loss: 0.8110454082489014\n",
      "[step: 7278] loss: 0.7951955199241638\n",
      "[step: 7279] loss: 0.7789280414581299\n",
      "[step: 7280] loss: 0.769939661026001\n",
      "[step: 7281] loss: 0.7705140113830566\n",
      "[step: 7282] loss: 0.7775240540504456\n",
      "[step: 7283] loss: 0.7856401205062866\n",
      "[step: 7284] loss: 0.7907600402832031\n",
      "[step: 7285] loss: 0.7903289794921875\n",
      "[step: 7286] loss: 0.786069393157959\n",
      "[step: 7287] loss: 0.7792350053787231\n",
      "[step: 7288] loss: 0.7728450298309326\n",
      "[step: 7289] loss: 0.7683397531509399\n",
      "[step: 7290] loss: 0.7663540840148926\n",
      "[step: 7291] loss: 0.7664682865142822\n",
      "[step: 7292] loss: 0.7679937481880188\n",
      "[step: 7293] loss: 0.7700130939483643\n",
      "[step: 7294] loss: 0.7717769145965576\n",
      "[step: 7295] loss: 0.7731422781944275\n",
      "[step: 7296] loss: 0.7738088369369507\n",
      "[step: 7297] loss: 0.7741484045982361\n",
      "[step: 7298] loss: 0.7740459442138672\n",
      "[step: 7299] loss: 0.7739611268043518\n",
      "[step: 7300] loss: 0.7736599445343018\n",
      "[step: 7301] loss: 0.7735787630081177\n",
      "[step: 7302] loss: 0.7734239101409912\n",
      "[step: 7303] loss: 0.7736378908157349\n",
      "[step: 7304] loss: 0.7740561366081238\n",
      "[step: 7305] loss: 0.7752653360366821\n",
      "[step: 7306] loss: 0.7771722078323364\n",
      "[step: 7307] loss: 0.7808183431625366\n",
      "[step: 7308] loss: 0.7862424254417419\n",
      "[step: 7309] loss: 0.7955933213233948\n",
      "[step: 7310] loss: 0.8092643022537231\n",
      "[step: 7311] loss: 0.8326932191848755\n",
      "[step: 7312] loss: 0.8663324117660522\n",
      "[step: 7313] loss: 0.9247793555259705\n",
      "[step: 7314] loss: 1.0041193962097168\n",
      "[step: 7315] loss: 1.1422804594039917\n",
      "[step: 7316] loss: 1.301131010055542\n",
      "[step: 7317] loss: 1.5613468885421753\n",
      "[step: 7318] loss: 1.7320460081100464\n",
      "[step: 7319] loss: 1.9359347820281982\n",
      "[step: 7320] loss: 1.75967276096344\n",
      "[step: 7321] loss: 1.476195216178894\n",
      "[step: 7322] loss: 1.0313924551010132\n",
      "[step: 7323] loss: 0.7923958897590637\n",
      "[step: 7324] loss: 0.8450511693954468\n",
      "[step: 7325] loss: 1.0596988201141357\n",
      "[step: 7326] loss: 1.222334861755371\n",
      "[step: 7327] loss: 1.1280391216278076\n",
      "[step: 7328] loss: 0.9285236597061157\n",
      "[step: 7329] loss: 0.78249192237854\n",
      "[step: 7330] loss: 0.8062776327133179\n",
      "[step: 7331] loss: 0.9341609477996826\n",
      "[step: 7332] loss: 1.014031171798706\n",
      "[step: 7333] loss: 0.9900091290473938\n",
      "[step: 7334] loss: 0.8716497421264648\n",
      "[step: 7335] loss: 0.7817329168319702\n",
      "[step: 7336] loss: 0.7776645421981812\n",
      "[step: 7337] loss: 0.8380556106567383\n",
      "[step: 7338] loss: 0.8954610824584961\n",
      "[step: 7339] loss: 0.8901017904281616\n",
      "[step: 7340] loss: 0.8399766683578491\n",
      "[step: 7341] loss: 0.7828068733215332\n",
      "[step: 7342] loss: 0.7641323804855347\n",
      "[step: 7343] loss: 0.7865914106369019\n",
      "[step: 7344] loss: 0.82025146484375\n",
      "[step: 7345] loss: 0.8360376954078674\n",
      "[step: 7346] loss: 0.819625735282898\n",
      "[step: 7347] loss: 0.7898143529891968\n",
      "[step: 7348] loss: 0.7662425637245178\n",
      "[step: 7349] loss: 0.7622646689414978\n",
      "[step: 7350] loss: 0.7747882604598999\n",
      "[step: 7351] loss: 0.7911654114723206\n",
      "[step: 7352] loss: 0.8004738092422485\n",
      "[step: 7353] loss: 0.7963836789131165\n",
      "[step: 7354] loss: 0.7844884991645813\n",
      "[step: 7355] loss: 0.7703633308410645\n",
      "[step: 7356] loss: 0.7607173919677734\n",
      "[step: 7357] loss: 0.758008599281311\n",
      "[step: 7358] loss: 0.7613622546195984\n",
      "[step: 7359] loss: 0.7676941156387329\n",
      "[step: 7360] loss: 0.773607611656189\n",
      "[step: 7361] loss: 0.7772607803344727\n",
      "[step: 7362] loss: 0.7770406007766724\n",
      "[step: 7363] loss: 0.7741762399673462\n",
      "[step: 7364] loss: 0.7692418098449707\n",
      "[step: 7365] loss: 0.7640881538391113\n",
      "[step: 7366] loss: 0.7594972848892212\n",
      "[step: 7367] loss: 0.7563234567642212\n",
      "[step: 7368] loss: 0.7546160221099854\n",
      "[step: 7369] loss: 0.7542225122451782\n",
      "[step: 7370] loss: 0.7548313140869141\n",
      "[step: 7371] loss: 0.7560400366783142\n",
      "[step: 7372] loss: 0.7575943470001221\n",
      "[step: 7373] loss: 0.7593063712120056\n",
      "[step: 7374] loss: 0.7612133026123047\n",
      "[step: 7375] loss: 0.7632370591163635\n",
      "[step: 7376] loss: 0.7657864093780518\n",
      "[step: 7377] loss: 0.7688171863555908\n",
      "[step: 7378] loss: 0.7732614278793335\n",
      "[step: 7379] loss: 0.7791376113891602\n",
      "[step: 7380] loss: 0.7884646654129028\n",
      "[step: 7381] loss: 0.8013203740119934\n",
      "[step: 7382] loss: 0.8225212097167969\n",
      "[step: 7383] loss: 0.8517035245895386\n",
      "[step: 7384] loss: 0.9011052846908569\n",
      "[step: 7385] loss: 0.9663567543029785\n",
      "[step: 7386] loss: 1.0778729915618896\n",
      "[step: 7387] loss: 1.2053710222244263\n",
      "[step: 7388] loss: 1.4123692512512207\n",
      "[step: 7389] loss: 1.560756802558899\n",
      "[step: 7390] loss: 1.7495942115783691\n",
      "[step: 7391] loss: 1.662505865097046\n",
      "[step: 7392] loss: 1.4915920495986938\n",
      "[step: 7393] loss: 1.11527681350708\n",
      "[step: 7394] loss: 0.8397188186645508\n",
      "[step: 7395] loss: 0.7700933218002319\n",
      "[step: 7396] loss: 0.8965623378753662\n",
      "[step: 7397] loss: 1.0783733129501343\n",
      "[step: 7398] loss: 1.1228175163269043\n",
      "[step: 7399] loss: 1.0361648797988892\n",
      "[step: 7400] loss: 0.8631103038787842\n",
      "[step: 7401] loss: 0.7643559575080872\n",
      "[step: 7402] loss: 0.7867215871810913\n",
      "[step: 7403] loss: 0.8799691200256348\n",
      "[step: 7404] loss: 0.9593865275382996\n",
      "[step: 7405] loss: 0.9480564594268799\n",
      "[step: 7406] loss: 0.8786479234695435\n",
      "[step: 7407] loss: 0.7940325140953064\n",
      "[step: 7408] loss: 0.7575355768203735\n",
      "[step: 7409] loss: 0.779095470905304\n",
      "[step: 7410] loss: 0.8271657228469849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7411] loss: 0.8620989918708801\n",
      "[step: 7412] loss: 0.8528006672859192\n",
      "[step: 7413] loss: 0.8161500692367554\n",
      "[step: 7414] loss: 0.7741473913192749\n",
      "[step: 7415] loss: 0.7544330358505249\n",
      "[step: 7416] loss: 0.7619279623031616\n",
      "[step: 7417] loss: 0.7844316363334656\n",
      "[step: 7418] loss: 0.8047500848770142\n",
      "[step: 7419] loss: 0.808081865310669\n",
      "[step: 7420] loss: 0.797029435634613\n",
      "[step: 7421] loss: 0.7767192125320435\n",
      "[step: 7422] loss: 0.7593327760696411\n",
      "[step: 7423] loss: 0.7505638003349304\n",
      "[step: 7424] loss: 0.7516632080078125\n",
      "[step: 7425] loss: 0.7596203684806824\n",
      "[step: 7426] loss: 0.7694326043128967\n",
      "[step: 7427] loss: 0.7773845195770264\n",
      "[step: 7428] loss: 0.7802879214286804\n",
      "[step: 7429] loss: 0.7794193029403687\n",
      "[step: 7430] loss: 0.7742736339569092\n",
      "[step: 7431] loss: 0.7677109837532043\n",
      "[step: 7432] loss: 0.7606754302978516\n",
      "[step: 7433] loss: 0.755160927772522\n",
      "[step: 7434] loss: 0.7515383958816528\n",
      "[step: 7435] loss: 0.7500614523887634\n",
      "[step: 7436] loss: 0.7506937980651855\n",
      "[step: 7437] loss: 0.753096878528595\n",
      "[step: 7438] loss: 0.7571640014648438\n",
      "[step: 7439] loss: 0.7625502347946167\n",
      "[step: 7440] loss: 0.7699291706085205\n",
      "[step: 7441] loss: 0.778887152671814\n",
      "[step: 7442] loss: 0.7912428379058838\n",
      "[step: 7443] loss: 0.8052078485488892\n",
      "[step: 7444] loss: 0.8238883018493652\n",
      "[step: 7445] loss: 0.8407554030418396\n",
      "[step: 7446] loss: 0.8595982193946838\n",
      "[step: 7447] loss: 0.8652660846710205\n",
      "[step: 7448] loss: 0.8637151122093201\n",
      "[step: 7449] loss: 0.8405150175094604\n",
      "[step: 7450] loss: 0.8108041882514954\n",
      "[step: 7451] loss: 0.778506875038147\n",
      "[step: 7452] loss: 0.7593631148338318\n",
      "[step: 7453] loss: 0.758743941783905\n",
      "[step: 7454] loss: 0.776564359664917\n",
      "[step: 7455] loss: 0.8074706792831421\n",
      "[step: 7456] loss: 0.8457911014556885\n",
      "[step: 7457] loss: 0.8932206630706787\n",
      "[step: 7458] loss: 0.9584494829177856\n",
      "[step: 7459] loss: 1.0595664978027344\n",
      "[step: 7460] loss: 1.2530815601348877\n",
      "[step: 7461] loss: 1.5198560953140259\n",
      "[step: 7462] loss: 1.9935524463653564\n",
      "[step: 7463] loss: 2.303666830062866\n",
      "[step: 7464] loss: 2.604799509048462\n",
      "[step: 7465] loss: 2.0613722801208496\n",
      "[step: 7466] loss: 1.3495491743087769\n",
      "[step: 7467] loss: 0.8119000792503357\n",
      "[step: 7468] loss: 0.9307449460029602\n",
      "[step: 7469] loss: 1.3920924663543701\n",
      "[step: 7470] loss: 1.4757519960403442\n",
      "[step: 7471] loss: 1.1479663848876953\n",
      "[step: 7472] loss: 0.7935953140258789\n",
      "[step: 7473] loss: 0.8834103941917419\n",
      "[step: 7474] loss: 1.178701400756836\n",
      "[step: 7475] loss: 1.1864311695098877\n",
      "[step: 7476] loss: 0.946607232093811\n",
      "[step: 7477] loss: 0.7662902474403381\n",
      "[step: 7478] loss: 0.868704617023468\n",
      "[step: 7479] loss: 1.0436290502548218\n",
      "[step: 7480] loss: 1.013364315032959\n",
      "[step: 7481] loss: 0.8475087881088257\n",
      "[step: 7482] loss: 0.7605235576629639\n",
      "[step: 7483] loss: 0.8500685691833496\n",
      "[step: 7484] loss: 0.9468209147453308\n",
      "[step: 7485] loss: 0.9025328159332275\n",
      "[step: 7486] loss: 0.7950664162635803\n",
      "[step: 7487] loss: 0.7580896615982056\n",
      "[step: 7488] loss: 0.8237611651420593\n",
      "[step: 7489] loss: 0.875366747379303\n",
      "[step: 7490] loss: 0.8374672532081604\n",
      "[step: 7491] loss: 0.7715981006622314\n",
      "[step: 7492] loss: 0.756355345249176\n",
      "[step: 7493] loss: 0.7994159460067749\n",
      "[step: 7494] loss: 0.8303720951080322\n",
      "[step: 7495] loss: 0.8095159530639648\n",
      "[step: 7496] loss: 0.7677022218704224\n",
      "[step: 7497] loss: 0.749413013458252\n",
      "[step: 7498] loss: 0.7700426578521729\n",
      "[step: 7499] loss: 0.7949795722961426\n",
      "[step: 7500] loss: 0.7945839166641235\n",
      "[step: 7501] loss: 0.7729219794273376\n",
      "[step: 7502] loss: 0.7506868243217468\n",
      "[step: 7503] loss: 0.7494343519210815\n",
      "[step: 7504] loss: 0.7624658346176147\n",
      "[step: 7505] loss: 0.7731066346168518\n",
      "[step: 7506] loss: 0.7712969779968262\n",
      "[step: 7507] loss: 0.7579914331436157\n",
      "[step: 7508] loss: 0.7477356791496277\n",
      "[step: 7509] loss: 0.7459761500358582\n",
      "[step: 7510] loss: 0.7510340213775635\n",
      "[step: 7511] loss: 0.7565869092941284\n",
      "[step: 7512] loss: 0.7559475898742676\n",
      "[step: 7513] loss: 0.7515497207641602\n",
      "[step: 7514] loss: 0.7463154196739197\n",
      "[step: 7515] loss: 0.7435504794120789\n",
      "[step: 7516] loss: 0.7441652417182922\n",
      "[step: 7517] loss: 0.7453400492668152\n",
      "[step: 7518] loss: 0.7460253238677979\n",
      "[step: 7519] loss: 0.7452287673950195\n",
      "[step: 7520] loss: 0.743750810623169\n",
      "[step: 7521] loss: 0.7426755428314209\n",
      "[step: 7522] loss: 0.741941511631012\n",
      "[step: 7523] loss: 0.7417111992835999\n",
      "[step: 7524] loss: 0.7410010099411011\n",
      "[step: 7525] loss: 0.7397919297218323\n",
      "[step: 7526] loss: 0.738545298576355\n",
      "[step: 7527] loss: 0.7375085353851318\n",
      "[step: 7528] loss: 0.7373791337013245\n",
      "[step: 7529] loss: 0.7378355860710144\n",
      "[step: 7530] loss: 0.7384837865829468\n",
      "[step: 7531] loss: 0.7388789653778076\n",
      "[step: 7532] loss: 0.738585352897644\n",
      "[step: 7533] loss: 0.7378110885620117\n",
      "[step: 7534] loss: 0.7367321848869324\n",
      "[step: 7535] loss: 0.7357789278030396\n",
      "[step: 7536] loss: 0.7351655960083008\n",
      "[step: 7537] loss: 0.7348368167877197\n",
      "[step: 7538] loss: 0.7347442507743835\n",
      "[step: 7539] loss: 0.7346137762069702\n",
      "[step: 7540] loss: 0.7343603372573853\n",
      "[step: 7541] loss: 0.7339789867401123\n",
      "[step: 7542] loss: 0.7335065007209778\n",
      "[step: 7543] loss: 0.7331293225288391\n",
      "[step: 7544] loss: 0.7329051494598389\n",
      "[step: 7545] loss: 0.7329335808753967\n",
      "[step: 7546] loss: 0.7332308292388916\n",
      "[step: 7547] loss: 0.7338428497314453\n",
      "[step: 7548] loss: 0.7348918318748474\n",
      "[step: 7549] loss: 0.7366636395454407\n",
      "[step: 7550] loss: 0.7396554350852966\n",
      "[step: 7551] loss: 0.7449790239334106\n",
      "[step: 7552] loss: 0.754095733165741\n",
      "[step: 7553] loss: 0.7708603143692017\n",
      "[step: 7554] loss: 0.7995308041572571\n",
      "[step: 7555] loss: 0.8537756204605103\n",
      "[step: 7556] loss: 0.9435042142868042\n",
      "[step: 7557] loss: 1.1160988807678223\n",
      "[step: 7558] loss: 1.368902325630188\n",
      "[step: 7559] loss: 1.831606149673462\n",
      "[step: 7560] loss: 2.2563412189483643\n",
      "[step: 7561] loss: 2.7988996505737305\n",
      "[step: 7562] loss: 2.4731194972991943\n",
      "[step: 7563] loss: 1.8158400058746338\n",
      "[step: 7564] loss: 0.9570966362953186\n",
      "[step: 7565] loss: 0.8335351943969727\n",
      "[step: 7566] loss: 1.3323092460632324\n",
      "[step: 7567] loss: 1.5787367820739746\n",
      "[step: 7568] loss: 1.2821608781814575\n",
      "[step: 7569] loss: 0.8207405209541321\n",
      "[step: 7570] loss: 0.8826224207878113\n",
      "[step: 7571] loss: 1.2256016731262207\n",
      "[step: 7572] loss: 1.2173250913619995\n",
      "[step: 7573] loss: 0.925114095211029\n",
      "[step: 7574] loss: 0.7589141130447388\n",
      "[step: 7575] loss: 0.9197956919670105\n",
      "[step: 7576] loss: 1.0915610790252686\n",
      "[step: 7577] loss: 0.9795010089874268\n",
      "[step: 7578] loss: 0.7900793552398682\n",
      "[step: 7579] loss: 0.7788881063461304\n",
      "[step: 7580] loss: 0.9134813547134399\n",
      "[step: 7581] loss: 0.9514882564544678\n",
      "[step: 7582] loss: 0.8302747011184692\n",
      "[step: 7583] loss: 0.7494519948959351\n",
      "[step: 7584] loss: 0.7991471290588379\n",
      "[step: 7585] loss: 0.8702865839004517\n",
      "[step: 7586] loss: 0.8451752662658691\n",
      "[step: 7587] loss: 0.7652024030685425\n",
      "[step: 7588] loss: 0.7502306699752808\n",
      "[step: 7589] loss: 0.8002805709838867\n",
      "[step: 7590] loss: 0.8248852491378784\n",
      "[step: 7591] loss: 0.7885147929191589\n",
      "[step: 7592] loss: 0.745830237865448\n",
      "[step: 7593] loss: 0.7502418756484985\n",
      "[step: 7594] loss: 0.781063437461853\n",
      "[step: 7595] loss: 0.789476752281189\n",
      "[step: 7596] loss: 0.7656861543655396\n",
      "[step: 7597] loss: 0.7411089539527893\n",
      "[step: 7598] loss: 0.7428262829780579\n",
      "[step: 7599] loss: 0.7602575421333313\n",
      "[step: 7600] loss: 0.7680588960647583\n",
      "[step: 7601] loss: 0.757220447063446\n",
      "[step: 7602] loss: 0.7408062815666199\n",
      "[step: 7603] loss: 0.735957682132721\n",
      "[step: 7604] loss: 0.7435739040374756\n",
      "[step: 7605] loss: 0.7518535852432251\n",
      "[step: 7606] loss: 0.7502515316009521\n",
      "[step: 7607] loss: 0.741000235080719\n",
      "[step: 7608] loss: 0.7337087392807007\n",
      "[step: 7609] loss: 0.7336819171905518\n",
      "[step: 7610] loss: 0.7388830184936523\n",
      "[step: 7611] loss: 0.7421395182609558\n",
      "[step: 7612] loss: 0.7399365305900574\n",
      "[step: 7613] loss: 0.7346707582473755\n",
      "[step: 7614] loss: 0.7306101322174072\n",
      "[step: 7615] loss: 0.7305499315261841\n",
      "[step: 7616] loss: 0.7330314517021179\n",
      "[step: 7617] loss: 0.7349907159805298\n",
      "[step: 7618] loss: 0.7343814373016357\n",
      "[step: 7619] loss: 0.731631875038147\n",
      "[step: 7620] loss: 0.7289196848869324\n",
      "[step: 7621] loss: 0.7276681661605835\n",
      "[step: 7622] loss: 0.7281819581985474\n",
      "[step: 7623] loss: 0.7293795943260193\n",
      "[step: 7624] loss: 0.7298859357833862\n",
      "[step: 7625] loss: 0.729243278503418\n",
      "[step: 7626] loss: 0.7277276515960693\n",
      "[step: 7627] loss: 0.7262210249900818\n",
      "[step: 7628] loss: 0.7254105806350708\n",
      "[step: 7629] loss: 0.7253866195678711\n",
      "[step: 7630] loss: 0.7257969975471497\n",
      "[step: 7631] loss: 0.7260410785675049\n",
      "[step: 7632] loss: 0.7258133888244629\n",
      "[step: 7633] loss: 0.7251077890396118\n",
      "[step: 7634] loss: 0.7241840362548828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7635] loss: 0.7234326004981995\n",
      "[step: 7636] loss: 0.7230017185211182\n",
      "[step: 7637] loss: 0.7228988409042358\n",
      "[step: 7638] loss: 0.7229436635971069\n",
      "[step: 7639] loss: 0.7229279279708862\n",
      "[step: 7640] loss: 0.7227351665496826\n",
      "[step: 7641] loss: 0.7223331332206726\n",
      "[step: 7642] loss: 0.7218272089958191\n",
      "[step: 7643] loss: 0.7213201522827148\n",
      "[step: 7644] loss: 0.7209128141403198\n",
      "[step: 7645] loss: 0.720639169216156\n",
      "[step: 7646] loss: 0.7204622030258179\n",
      "[step: 7647] loss: 0.7203384637832642\n",
      "[step: 7648] loss: 0.7201964259147644\n",
      "[step: 7649] loss: 0.719994306564331\n",
      "[step: 7650] loss: 0.719731330871582\n",
      "[step: 7651] loss: 0.7194211483001709\n",
      "[step: 7652] loss: 0.7191062569618225\n",
      "[step: 7653] loss: 0.7188189029693604\n",
      "[step: 7654] loss: 0.7185913324356079\n",
      "[step: 7655] loss: 0.7184531688690186\n",
      "[step: 7656] loss: 0.7184374332427979\n",
      "[step: 7657] loss: 0.7186082601547241\n",
      "[step: 7658] loss: 0.7191050052642822\n",
      "[step: 7659] loss: 0.7202072143554688\n",
      "[step: 7660] loss: 0.722530722618103\n",
      "[step: 7661] loss: 0.7272404432296753\n",
      "[step: 7662] loss: 0.737005889415741\n",
      "[step: 7663] loss: 0.7565305829048157\n",
      "[step: 7664] loss: 0.7977725267410278\n",
      "[step: 7665] loss: 0.8789393901824951\n",
      "[step: 7666] loss: 1.053924798965454\n",
      "[step: 7667] loss: 1.3691997528076172\n",
      "[step: 7668] loss: 2.018967866897583\n",
      "[step: 7669] loss: 2.8071179389953613\n",
      "[step: 7670] loss: 3.9128665924072266\n",
      "[step: 7671] loss: 3.4917244911193848\n",
      "[step: 7672] loss: 2.299309730529785\n",
      "[step: 7673] loss: 0.9457093477249146\n",
      "[step: 7674] loss: 1.2524213790893555\n",
      "[step: 7675] loss: 2.2508130073547363\n",
      "[step: 7676] loss: 1.852870225906372\n",
      "[step: 7677] loss: 0.9412540197372437\n",
      "[step: 7678] loss: 1.0755388736724854\n",
      "[step: 7679] loss: 1.6806824207305908\n",
      "[step: 7680] loss: 1.3988498449325562\n",
      "[step: 7681] loss: 0.8122866153717041\n",
      "[step: 7682] loss: 1.1074683666229248\n",
      "[step: 7683] loss: 1.4421508312225342\n",
      "[step: 7684] loss: 1.0062050819396973\n",
      "[step: 7685] loss: 0.7918858528137207\n",
      "[step: 7686] loss: 1.116600751876831\n",
      "[step: 7687] loss: 1.1075310707092285\n",
      "[step: 7688] loss: 0.8100787401199341\n",
      "[step: 7689] loss: 0.847867488861084\n",
      "[step: 7690] loss: 1.029097318649292\n",
      "[step: 7691] loss: 0.9265577793121338\n",
      "[step: 7692] loss: 0.7592151165008545\n",
      "[step: 7693] loss: 0.871354341506958\n",
      "[step: 7694] loss: 0.9549936056137085\n",
      "[step: 7695] loss: 0.8028246164321899\n",
      "[step: 7696] loss: 0.7670714855194092\n",
      "[step: 7697] loss: 0.8747072219848633\n",
      "[step: 7698] loss: 0.8471340537071228\n",
      "[step: 7699] loss: 0.7540876865386963\n",
      "[step: 7700] loss: 0.7823155522346497\n",
      "[step: 7701] loss: 0.8354346752166748\n",
      "[step: 7702] loss: 0.7944178581237793\n",
      "[step: 7703] loss: 0.7435817718505859\n",
      "[step: 7704] loss: 0.7753145694732666\n",
      "[step: 7705] loss: 0.8075414896011353\n",
      "[step: 7706] loss: 0.7647789716720581\n",
      "[step: 7707] loss: 0.7382080554962158\n",
      "[step: 7708] loss: 0.7656543254852295\n",
      "[step: 7709] loss: 0.77848219871521\n",
      "[step: 7710] loss: 0.752265214920044\n",
      "[step: 7711] loss: 0.7350403070449829\n",
      "[step: 7712] loss: 0.75113844871521\n",
      "[step: 7713] loss: 0.7600518465042114\n",
      "[step: 7714] loss: 0.7440513372421265\n",
      "[step: 7715] loss: 0.731361448764801\n",
      "[step: 7716] loss: 0.7396081686019897\n",
      "[step: 7717] loss: 0.7483864426612854\n",
      "[step: 7718] loss: 0.7384161949157715\n",
      "[step: 7719] loss: 0.7279137969017029\n",
      "[step: 7720] loss: 0.7322759628295898\n",
      "[step: 7721] loss: 0.738889217376709\n",
      "[step: 7722] loss: 0.7351236343383789\n",
      "[step: 7723] loss: 0.7266942262649536\n",
      "[step: 7724] loss: 0.7258455157279968\n",
      "[step: 7725] loss: 0.7311580777168274\n",
      "[step: 7726] loss: 0.7317773103713989\n",
      "[step: 7727] loss: 0.7259602546691895\n",
      "[step: 7728] loss: 0.722618579864502\n",
      "[step: 7729] loss: 0.7248640656471252\n",
      "[step: 7730] loss: 0.727103054523468\n",
      "[step: 7731] loss: 0.7252843976020813\n",
      "[step: 7732] loss: 0.7215834856033325\n",
      "[step: 7733] loss: 0.7206596732139587\n",
      "[step: 7734] loss: 0.7224686145782471\n",
      "[step: 7735] loss: 0.7231136560440063\n",
      "[step: 7736] loss: 0.7211334705352783\n",
      "[step: 7737] loss: 0.7189886569976807\n",
      "[step: 7738] loss: 0.7188632488250732\n",
      "[step: 7739] loss: 0.7197610139846802\n",
      "[step: 7740] loss: 0.719785213470459\n",
      "[step: 7741] loss: 0.7183853387832642\n",
      "[step: 7742] loss: 0.7170287370681763\n",
      "[step: 7743] loss: 0.7169894576072693\n",
      "[step: 7744] loss: 0.7174102067947388\n",
      "[step: 7745] loss: 0.7172078490257263\n",
      "[step: 7746] loss: 0.7162857055664062\n",
      "[step: 7747] loss: 0.7153945565223694\n",
      "[step: 7748] loss: 0.7151970863342285\n",
      "[step: 7749] loss: 0.7153466939926147\n",
      "[step: 7750] loss: 0.7151792049407959\n",
      "[step: 7751] loss: 0.7145166993141174\n",
      "[step: 7752] loss: 0.7138475179672241\n",
      "[step: 7753] loss: 0.7135564088821411\n",
      "[step: 7754] loss: 0.7135107517242432\n",
      "[step: 7755] loss: 0.713392972946167\n",
      "[step: 7756] loss: 0.7129682302474976\n",
      "[step: 7757] loss: 0.7124277949333191\n",
      "[step: 7758] loss: 0.7120703458786011\n",
      "[step: 7759] loss: 0.7119014263153076\n",
      "[step: 7760] loss: 0.711760401725769\n",
      "[step: 7761] loss: 0.7114849090576172\n",
      "[step: 7762] loss: 0.7110854387283325\n",
      "[step: 7763] loss: 0.7107079029083252\n",
      "[step: 7764] loss: 0.7104523181915283\n",
      "[step: 7765] loss: 0.7102795839309692\n",
      "[step: 7766] loss: 0.7100664377212524\n",
      "[step: 7767] loss: 0.7097765207290649\n",
      "[step: 7768] loss: 0.709445595741272\n",
      "[step: 7769] loss: 0.7091479897499084\n",
      "[step: 7770] loss: 0.7089207172393799\n",
      "[step: 7771] loss: 0.708717405796051\n",
      "[step: 7772] loss: 0.7084863185882568\n",
      "[step: 7773] loss: 0.7082147598266602\n",
      "[step: 7774] loss: 0.707929790019989\n",
      "[step: 7775] loss: 0.7076684236526489\n",
      "[step: 7776] loss: 0.7074422836303711\n",
      "[step: 7777] loss: 0.7072288393974304\n",
      "[step: 7778] loss: 0.7070028781890869\n",
      "[step: 7779] loss: 0.7067551612854004\n",
      "[step: 7780] loss: 0.7065001726150513\n",
      "[step: 7781] loss: 0.7062578201293945\n",
      "[step: 7782] loss: 0.706035315990448\n",
      "[step: 7783] loss: 0.7058223485946655\n",
      "[step: 7784] loss: 0.7056073546409607\n",
      "[step: 7785] loss: 0.7053862810134888\n",
      "[step: 7786] loss: 0.7051665782928467\n",
      "[step: 7787] loss: 0.704957902431488\n",
      "[step: 7788] loss: 0.7047778367996216\n",
      "[step: 7789] loss: 0.7046363353729248\n",
      "[step: 7790] loss: 0.7045525312423706\n",
      "[step: 7791] loss: 0.7045677900314331\n",
      "[step: 7792] loss: 0.7047678232192993\n",
      "[step: 7793] loss: 0.7053191661834717\n",
      "[step: 7794] loss: 0.7065569162368774\n",
      "[step: 7795] loss: 0.7090783715248108\n",
      "[step: 7796] loss: 0.7142071723937988\n",
      "[step: 7797] loss: 0.7242406606674194\n",
      "[step: 7798] loss: 0.7447423934936523\n",
      "[step: 7799] loss: 0.7844958901405334\n",
      "[step: 7800] loss: 0.8677715063095093\n",
      "[step: 7801] loss: 1.022887945175171\n",
      "[step: 7802] loss: 1.3491824865341187\n",
      "[step: 7803] loss: 1.8553954362869263\n",
      "[step: 7804] loss: 2.7878305912017822\n",
      "[step: 7805] loss: 3.3554351329803467\n",
      "[step: 7806] loss: 3.6450350284576416\n",
      "[step: 7807] loss: 2.127912998199463\n",
      "[step: 7808] loss: 0.9156594276428223\n",
      "[step: 7809] loss: 1.0948269367218018\n",
      "[step: 7810] loss: 1.910515308380127\n",
      "[step: 7811] loss: 1.8601505756378174\n",
      "[step: 7812] loss: 0.9810649156570435\n",
      "[step: 7813] loss: 0.9829260110855103\n",
      "[step: 7814] loss: 1.543936014175415\n",
      "[step: 7815] loss: 1.3023451566696167\n",
      "[step: 7816] loss: 0.8202341794967651\n",
      "[step: 7817] loss: 0.9611888527870178\n",
      "[step: 7818] loss: 1.2647957801818848\n",
      "[step: 7819] loss: 1.0785353183746338\n",
      "[step: 7820] loss: 0.7803389430046082\n",
      "[step: 7821] loss: 0.9373226165771484\n",
      "[step: 7822] loss: 1.107645034790039\n",
      "[step: 7823] loss: 0.8867651224136353\n",
      "[step: 7824] loss: 0.7541568875312805\n",
      "[step: 7825] loss: 0.9134246706962585\n",
      "[step: 7826] loss: 0.9490960836410522\n",
      "[step: 7827] loss: 0.7812165021896362\n",
      "[step: 7828] loss: 0.763198971748352\n",
      "[step: 7829] loss: 0.8773736953735352\n",
      "[step: 7830] loss: 0.844629168510437\n",
      "[step: 7831] loss: 0.7423751354217529\n",
      "[step: 7832] loss: 0.7760634422302246\n",
      "[step: 7833] loss: 0.8339093327522278\n",
      "[step: 7834] loss: 0.7783403396606445\n",
      "[step: 7835] loss: 0.732306957244873\n",
      "[step: 7836] loss: 0.7763259410858154\n",
      "[step: 7837] loss: 0.7915102243423462\n",
      "[step: 7838] loss: 0.7397946119308472\n",
      "[step: 7839] loss: 0.7326659560203552\n",
      "[step: 7840] loss: 0.7658400535583496\n",
      "[step: 7841] loss: 0.7555963397026062\n",
      "[step: 7842] loss: 0.7256892919540405\n",
      "[step: 7843] loss: 0.7318987846374512\n",
      "[step: 7844] loss: 0.7498946189880371\n",
      "[step: 7845] loss: 0.7359131574630737\n",
      "[step: 7846] loss: 0.7166706323623657\n",
      "[step: 7847] loss: 0.7274599075317383\n",
      "[step: 7848] loss: 0.7383801341056824\n",
      "[step: 7849] loss: 0.7239598035812378\n",
      "[step: 7850] loss: 0.7128609418869019\n",
      "[step: 7851] loss: 0.7215914726257324\n",
      "[step: 7852] loss: 0.7274760007858276\n",
      "[step: 7853] loss: 0.7180938720703125\n",
      "[step: 7854] loss: 0.710344135761261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 7855] loss: 0.7161798477172852\n",
      "[step: 7856] loss: 0.7202378511428833\n",
      "[step: 7857] loss: 0.7132952213287354\n",
      "[step: 7858] loss: 0.7089588642120361\n",
      "[step: 7859] loss: 0.712579607963562\n",
      "[step: 7860] loss: 0.7143642902374268\n",
      "[step: 7861] loss: 0.709983229637146\n",
      "[step: 7862] loss: 0.7068799734115601\n",
      "[step: 7863] loss: 0.7092459797859192\n",
      "[step: 7864] loss: 0.7108175158500671\n",
      "[step: 7865] loss: 0.7078364491462708\n",
      "[step: 7866] loss: 0.7051253318786621\n",
      "[step: 7867] loss: 0.7062437534332275\n",
      "[step: 7868] loss: 0.7075332999229431\n",
      "[step: 7869] loss: 0.706003725528717\n",
      "[step: 7870] loss: 0.7040517330169678\n",
      "[step: 7871] loss: 0.7041645050048828\n",
      "[step: 7872] loss: 0.704967737197876\n",
      "[step: 7873] loss: 0.7040782570838928\n",
      "[step: 7874] loss: 0.7025092244148254\n",
      "[step: 7875] loss: 0.7023654580116272\n",
      "[step: 7876] loss: 0.7029684782028198\n",
      "[step: 7877] loss: 0.702599823474884\n",
      "[step: 7878] loss: 0.7014506459236145\n",
      "[step: 7879] loss: 0.7008840441703796\n",
      "[step: 7880] loss: 0.7010842561721802\n",
      "[step: 7881] loss: 0.7010107040405273\n",
      "[step: 7882] loss: 0.7002615928649902\n",
      "[step: 7883] loss: 0.6996594667434692\n",
      "[step: 7884] loss: 0.6996583938598633\n",
      "[step: 7885] loss: 0.6996527910232544\n",
      "[step: 7886] loss: 0.699192464351654\n",
      "[step: 7887] loss: 0.6985957622528076\n",
      "[step: 7888] loss: 0.6983296275138855\n",
      "[step: 7889] loss: 0.6982786655426025\n",
      "[step: 7890] loss: 0.6980220079421997\n",
      "[step: 7891] loss: 0.6975492238998413\n",
      "[step: 7892] loss: 0.6972019672393799\n",
      "[step: 7893] loss: 0.6970778703689575\n",
      "[step: 7894] loss: 0.6969203352928162\n",
      "[step: 7895] loss: 0.6965906620025635\n",
      "[step: 7896] loss: 0.6962281465530396\n",
      "[step: 7897] loss: 0.6959910988807678\n",
      "[step: 7898] loss: 0.6958374977111816\n",
      "[step: 7899] loss: 0.6956002712249756\n",
      "[step: 7900] loss: 0.6952755451202393\n",
      "[step: 7901] loss: 0.6949909925460815\n",
      "[step: 7902] loss: 0.6947895884513855\n",
      "[step: 7903] loss: 0.6945965886116028\n",
      "[step: 7904] loss: 0.6943404674530029\n",
      "[step: 7905] loss: 0.6940568685531616\n",
      "[step: 7906] loss: 0.6938179731369019\n",
      "[step: 7907] loss: 0.6936211585998535\n",
      "[step: 7908] loss: 0.6934118270874023\n",
      "[step: 7909] loss: 0.6931650638580322\n",
      "[step: 7910] loss: 0.6929185390472412\n",
      "[step: 7911] loss: 0.6927028894424438\n",
      "[step: 7912] loss: 0.6925079226493835\n",
      "[step: 7913] loss: 0.6923052072525024\n",
      "[step: 7914] loss: 0.6920833587646484\n",
      "[step: 7915] loss: 0.691875696182251\n",
      "[step: 7916] loss: 0.6917054653167725\n",
      "[step: 7917] loss: 0.6915669441223145\n",
      "[step: 7918] loss: 0.6914612650871277\n",
      "[step: 7919] loss: 0.6914241909980774\n",
      "[step: 7920] loss: 0.6915234327316284\n",
      "[step: 7921] loss: 0.6918851733207703\n",
      "[step: 7922] loss: 0.6926953196525574\n",
      "[step: 7923] loss: 0.6943569779396057\n",
      "[step: 7924] loss: 0.6975753903388977\n",
      "[step: 7925] loss: 0.7039319276809692\n",
      "[step: 7926] loss: 0.7160238027572632\n",
      "[step: 7927] loss: 0.7402291297912598\n",
      "[step: 7928] loss: 0.785824179649353\n",
      "[step: 7929] loss: 0.8793581128120422\n",
      "[step: 7930] loss: 1.0473573207855225\n",
      "[step: 7931] loss: 1.3904236555099487\n",
      "[step: 7932] loss: 1.8915555477142334\n",
      "[step: 7933] loss: 2.760532855987549\n",
      "[step: 7934] loss: 3.1936709880828857\n",
      "[step: 7935] loss: 3.2677507400512695\n",
      "[step: 7936] loss: 1.8849823474884033\n",
      "[step: 7937] loss: 0.8409486413002014\n",
      "[step: 7938] loss: 1.0215739011764526\n",
      "[step: 7939] loss: 1.7745453119277954\n",
      "[step: 7940] loss: 1.8459327220916748\n",
      "[step: 7941] loss: 1.0064640045166016\n",
      "[step: 7942] loss: 0.8270028829574585\n",
      "[step: 7943] loss: 1.3933203220367432\n",
      "[step: 7944] loss: 1.3945395946502686\n",
      "[step: 7945] loss: 0.9031664729118347\n",
      "[step: 7946] loss: 0.783747673034668\n",
      "[step: 7947] loss: 1.1296238899230957\n",
      "[step: 7948] loss: 1.2047480344772339\n",
      "[step: 7949] loss: 0.8430439829826355\n",
      "[step: 7950] loss: 0.7622206807136536\n",
      "[step: 7951] loss: 0.9970563650131226\n",
      "[step: 7952] loss: 0.9970837831497192\n",
      "[step: 7953] loss: 0.7820841073989868\n",
      "[step: 7954] loss: 0.745259165763855\n",
      "[step: 7955] loss: 0.8939433097839355\n",
      "[step: 7956] loss: 0.8942668437957764\n",
      "[step: 7957] loss: 0.7448691129684448\n",
      "[step: 7958] loss: 0.7409913539886475\n",
      "[step: 7959] loss: 0.8421981334686279\n",
      "[step: 7960] loss: 0.8210837841033936\n",
      "[step: 7961] loss: 0.7289059162139893\n",
      "[step: 7962] loss: 0.732109546661377\n",
      "[step: 7963] loss: 0.7979671955108643\n",
      "[step: 7964] loss: 0.7887124419212341\n",
      "[step: 7965] loss: 0.7220282554626465\n",
      "[step: 7966] loss: 0.7182059288024902\n",
      "[step: 7967] loss: 0.762488067150116\n",
      "[step: 7968] loss: 0.7618732452392578\n",
      "[step: 7969] loss: 0.7213706970214844\n",
      "[step: 7970] loss: 0.706278383731842\n",
      "[step: 7971] loss: 0.7315493822097778\n",
      "[step: 7972] loss: 0.7468241453170776\n",
      "[step: 7973] loss: 0.7252601385116577\n",
      "[step: 7974] loss: 0.7034323215484619\n",
      "[step: 7975] loss: 0.7091046571731567\n",
      "[step: 7976] loss: 0.7254213094711304\n",
      "[step: 7977] loss: 0.7242984771728516\n",
      "[step: 7978] loss: 0.7074546813964844\n",
      "[step: 7979] loss: 0.699447751045227\n",
      "[step: 7980] loss: 0.707001805305481\n",
      "[step: 7981] loss: 0.7149989604949951\n",
      "[step: 7982] loss: 0.7107547521591187\n",
      "[step: 7983] loss: 0.7003689408302307\n",
      "[step: 7984] loss: 0.6972205638885498\n",
      "[step: 7985] loss: 0.702231764793396\n",
      "[step: 7986] loss: 0.706523060798645\n",
      "[step: 7987] loss: 0.7037925720214844\n",
      "[step: 7988] loss: 0.6973998546600342\n",
      "[step: 7989] loss: 0.6946489810943604\n",
      "[step: 7990] loss: 0.6968738436698914\n",
      "[step: 7991] loss: 0.6998898386955261\n",
      "[step: 7992] loss: 0.6995186805725098\n",
      "[step: 7993] loss: 0.6960008144378662\n",
      "[step: 7994] loss: 0.6929653882980347\n",
      "[step: 7995] loss: 0.6926620006561279\n",
      "[step: 7996] loss: 0.6942882537841797\n",
      "[step: 7997] loss: 0.6954734325408936\n",
      "[step: 7998] loss: 0.6947234869003296\n",
      "[step: 7999] loss: 0.6926014423370361\n",
      "[step: 8000] loss: 0.6907607913017273\n",
      "[step: 8001] loss: 0.6902928352355957\n",
      "[step: 8002] loss: 0.6909130811691284\n",
      "[step: 8003] loss: 0.6915873289108276\n",
      "[step: 8004] loss: 0.6914472579956055\n",
      "[step: 8005] loss: 0.690429151058197\n",
      "[step: 8006] loss: 0.6891902685165405\n",
      "[step: 8007] loss: 0.6883668899536133\n",
      "[step: 8008] loss: 0.6881508827209473\n",
      "[step: 8009] loss: 0.6883466839790344\n",
      "[step: 8010] loss: 0.6885356903076172\n",
      "[step: 8011] loss: 0.688396155834198\n",
      "[step: 8012] loss: 0.6878942847251892\n",
      "[step: 8013] loss: 0.687187671661377\n",
      "[step: 8014] loss: 0.6865214705467224\n",
      "[step: 8015] loss: 0.6860681772232056\n",
      "[step: 8016] loss: 0.685848593711853\n",
      "[step: 8017] loss: 0.6857845187187195\n",
      "[step: 8018] loss: 0.6857466101646423\n",
      "[step: 8019] loss: 0.6856279373168945\n",
      "[step: 8020] loss: 0.6853830218315125\n",
      "[step: 8021] loss: 0.6850308179855347\n",
      "[step: 8022] loss: 0.6846258640289307\n",
      "[step: 8023] loss: 0.6842069625854492\n",
      "[step: 8024] loss: 0.6838332414627075\n",
      "[step: 8025] loss: 0.6835170984268188\n",
      "[step: 8026] loss: 0.6832633018493652\n",
      "[step: 8027] loss: 0.6830617189407349\n",
      "[step: 8028] loss: 0.6828875541687012\n",
      "[step: 8029] loss: 0.6827210187911987\n",
      "[step: 8030] loss: 0.6825543642044067\n",
      "[step: 8031] loss: 0.6823771595954895\n",
      "[step: 8032] loss: 0.6821874976158142\n",
      "[step: 8033] loss: 0.6819875240325928\n",
      "[step: 8034] loss: 0.6817833781242371\n",
      "[step: 8035] loss: 0.6815760731697083\n",
      "[step: 8036] loss: 0.6813709139823914\n",
      "[step: 8037] loss: 0.6811766624450684\n",
      "[step: 8038] loss: 0.6809974908828735\n",
      "[step: 8039] loss: 0.6808433532714844\n",
      "[step: 8040] loss: 0.6807184219360352\n",
      "[step: 8041] loss: 0.6806451082229614\n",
      "[step: 8042] loss: 0.6806391477584839\n",
      "[step: 8043] loss: 0.6807464957237244\n",
      "[step: 8044] loss: 0.6810140609741211\n",
      "[step: 8045] loss: 0.6815566420555115\n",
      "[step: 8046] loss: 0.6825071573257446\n",
      "[step: 8047] loss: 0.6841816902160645\n",
      "[step: 8048] loss: 0.6869643330574036\n",
      "[step: 8049] loss: 0.691805899143219\n",
      "[step: 8050] loss: 0.6998187303543091\n",
      "[step: 8051] loss: 0.7140527963638306\n",
      "[step: 8052] loss: 0.7376015186309814\n",
      "[step: 8053] loss: 0.7806956768035889\n",
      "[step: 8054] loss: 0.8503748774528503\n",
      "[step: 8055] loss: 0.9803659915924072\n",
      "[step: 8056] loss: 1.1706247329711914\n",
      "[step: 8057] loss: 1.5121427774429321\n",
      "[step: 8058] loss: 1.8613638877868652\n",
      "[step: 8059] loss: 2.3425137996673584\n",
      "[step: 8060] loss: 2.2805793285369873\n",
      "[step: 8061] loss: 1.9465535879135132\n",
      "[step: 8062] loss: 1.14726984500885\n",
      "[step: 8063] loss: 0.7250142097473145\n",
      "[step: 8064] loss: 0.9090874791145325\n",
      "[step: 8065] loss: 1.3000327348709106\n",
      "[step: 8066] loss: 1.3959219455718994\n",
      "[step: 8067] loss: 1.0025634765625\n",
      "[step: 8068] loss: 0.7196330428123474\n",
      "[step: 8069] loss: 0.8365994691848755\n",
      "[step: 8070] loss: 1.0787138938903809\n",
      "[step: 8071] loss: 1.1065049171447754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8072] loss: 0.8586772680282593\n",
      "[step: 8073] loss: 0.70246821641922\n",
      "[step: 8074] loss: 0.793951153755188\n",
      "[step: 8075] loss: 0.9390019178390503\n",
      "[step: 8076] loss: 0.9369398355484009\n",
      "[step: 8077] loss: 0.7801773548126221\n",
      "[step: 8078] loss: 0.6961824893951416\n",
      "[step: 8079] loss: 0.7670412063598633\n",
      "[step: 8080] loss: 0.8493500351905823\n",
      "[step: 8081] loss: 0.8301917314529419\n",
      "[step: 8082] loss: 0.732716977596283\n",
      "[step: 8083] loss: 0.6930817365646362\n",
      "[step: 8084] loss: 0.7444585561752319\n",
      "[step: 8085] loss: 0.7902765870094299\n",
      "[step: 8086] loss: 0.7703597545623779\n",
      "[step: 8087] loss: 0.7110502123832703\n",
      "[step: 8088] loss: 0.6904593706130981\n",
      "[step: 8089] loss: 0.7221140265464783\n",
      "[step: 8090] loss: 0.7509924173355103\n",
      "[step: 8091] loss: 0.7426377534866333\n",
      "[step: 8092] loss: 0.7066075801849365\n",
      "[step: 8093] loss: 0.6856638193130493\n",
      "[step: 8094] loss: 0.697408139705658\n",
      "[step: 8095] loss: 0.7194470167160034\n",
      "[step: 8096] loss: 0.7264336347579956\n",
      "[step: 8097] loss: 0.7107586860656738\n",
      "[step: 8098] loss: 0.6900487542152405\n",
      "[step: 8099] loss: 0.6829192042350769\n",
      "[step: 8100] loss: 0.6911076307296753\n",
      "[step: 8101] loss: 0.7029388546943665\n",
      "[step: 8102] loss: 0.705561637878418\n",
      "[step: 8103] loss: 0.6972332000732422\n",
      "[step: 8104] loss: 0.6858503818511963\n",
      "[step: 8105] loss: 0.6801400184631348\n",
      "[step: 8106] loss: 0.6826452016830444\n",
      "[step: 8107] loss: 0.6890439987182617\n",
      "[step: 8108] loss: 0.692642867565155\n",
      "[step: 8109] loss: 0.690788984298706\n",
      "[step: 8110] loss: 0.6852428913116455\n",
      "[step: 8111] loss: 0.6798598766326904\n",
      "[step: 8112] loss: 0.677749514579773\n",
      "[step: 8113] loss: 0.6788918375968933\n",
      "[step: 8114] loss: 0.6817368865013123\n",
      "[step: 8115] loss: 0.6838333606719971\n",
      "[step: 8116] loss: 0.6839147806167603\n",
      "[step: 8117] loss: 0.6821282505989075\n",
      "[step: 8118] loss: 0.6793349385261536\n",
      "[step: 8119] loss: 0.6769474148750305\n",
      "[step: 8120] loss: 0.6754772067070007\n",
      "[step: 8121] loss: 0.6752392053604126\n",
      "[step: 8122] loss: 0.6758309006690979\n",
      "[step: 8123] loss: 0.676726758480072\n",
      "[step: 8124] loss: 0.6775169372558594\n",
      "[step: 8125] loss: 0.6777729988098145\n",
      "[step: 8126] loss: 0.6775389909744263\n",
      "[step: 8127] loss: 0.6767861843109131\n",
      "[step: 8128] loss: 0.6757910251617432\n",
      "[step: 8129] loss: 0.674718976020813\n",
      "[step: 8130] loss: 0.6737464666366577\n",
      "[step: 8131] loss: 0.6729774475097656\n",
      "[step: 8132] loss: 0.672391414642334\n",
      "[step: 8133] loss: 0.6719976663589478\n",
      "[step: 8134] loss: 0.671758770942688\n",
      "[step: 8135] loss: 0.6716212630271912\n",
      "[step: 8136] loss: 0.6715815663337708\n",
      "[step: 8137] loss: 0.6716017723083496\n",
      "[step: 8138] loss: 0.67168128490448\n",
      "[step: 8139] loss: 0.6718301773071289\n",
      "[step: 8140] loss: 0.6720598340034485\n",
      "[step: 8141] loss: 0.6724478006362915\n",
      "[step: 8142] loss: 0.6730372905731201\n",
      "[step: 8143] loss: 0.6739987134933472\n",
      "[step: 8144] loss: 0.6754756569862366\n",
      "[step: 8145] loss: 0.6778779029846191\n",
      "[step: 8146] loss: 0.6815796494483948\n",
      "[step: 8147] loss: 0.6876959800720215\n",
      "[step: 8148] loss: 0.697211742401123\n",
      "[step: 8149] loss: 0.7135022878646851\n",
      "[step: 8150] loss: 0.7390003800392151\n",
      "[step: 8151] loss: 0.7840046286582947\n",
      "[step: 8152] loss: 0.8524677753448486\n",
      "[step: 8153] loss: 0.9751255512237549\n",
      "[step: 8154] loss: 1.1426851749420166\n",
      "[step: 8155] loss: 1.4297447204589844\n",
      "[step: 8156] loss: 1.6990176439285278\n",
      "[step: 8157] loss: 2.050997257232666\n",
      "[step: 8158] loss: 1.984788179397583\n",
      "[step: 8159] loss: 1.7223248481750488\n",
      "[step: 8160] loss: 1.108323097229004\n",
      "[step: 8161] loss: 0.7288265824317932\n",
      "[step: 8162] loss: 0.7828499674797058\n",
      "[step: 8163] loss: 1.0828793048858643\n",
      "[step: 8164] loss: 1.268556833267212\n",
      "[step: 8165] loss: 1.068457007408142\n",
      "[step: 8166] loss: 0.7839590907096863\n",
      "[step: 8167] loss: 0.6998904943466187\n",
      "[step: 8168] loss: 0.8515415787696838\n",
      "[step: 8169] loss: 1.0184483528137207\n",
      "[step: 8170] loss: 0.9740300178527832\n",
      "[step: 8171] loss: 0.806868314743042\n",
      "[step: 8172] loss: 0.6894943714141846\n",
      "[step: 8173] loss: 0.7295738458633423\n",
      "[step: 8174] loss: 0.8415803909301758\n",
      "[step: 8175] loss: 0.8679673671722412\n",
      "[step: 8176] loss: 0.7912776470184326\n",
      "[step: 8177] loss: 0.6977789998054504\n",
      "[step: 8178] loss: 0.687692403793335\n",
      "[step: 8179] loss: 0.74606853723526\n",
      "[step: 8180] loss: 0.7869524359703064\n",
      "[step: 8181] loss: 0.7651327252388\n",
      "[step: 8182] loss: 0.7058237791061401\n",
      "[step: 8183] loss: 0.6774818301200867\n",
      "[step: 8184] loss: 0.6978062391281128\n",
      "[step: 8185] loss: 0.7320842742919922\n",
      "[step: 8186] loss: 0.7401248812675476\n",
      "[step: 8187] loss: 0.7133522033691406\n",
      "[step: 8188] loss: 0.6833459138870239\n",
      "[step: 8189] loss: 0.6743956804275513\n",
      "[step: 8190] loss: 0.6881666779518127\n",
      "[step: 8191] loss: 0.7061198949813843\n",
      "[step: 8192] loss: 0.7099628448486328\n",
      "[step: 8193] loss: 0.6991878747940063\n",
      "[step: 8194] loss: 0.6823380589485168\n",
      "[step: 8195] loss: 0.6724358797073364\n",
      "[step: 8196] loss: 0.6735923290252686\n",
      "[step: 8197] loss: 0.6816492080688477\n",
      "[step: 8198] loss: 0.6890280246734619\n",
      "[step: 8199] loss: 0.6897248029708862\n",
      "[step: 8200] loss: 0.6844660043716431\n",
      "[step: 8201] loss: 0.6764864921569824\n",
      "[step: 8202] loss: 0.6706289649009705\n",
      "[step: 8203] loss: 0.6691086888313293\n",
      "[step: 8204] loss: 0.6713443994522095\n",
      "[step: 8205] loss: 0.6749159693717957\n",
      "[step: 8206] loss: 0.6772795915603638\n",
      "[step: 8207] loss: 0.6774888634681702\n",
      "[step: 8208] loss: 0.6753870248794556\n",
      "[step: 8209] loss: 0.6722973585128784\n",
      "[step: 8210] loss: 0.6692463755607605\n",
      "[step: 8211] loss: 0.6670538187026978\n",
      "[step: 8212] loss: 0.6660991907119751\n",
      "[step: 8213] loss: 0.6662269830703735\n",
      "[step: 8214] loss: 0.6670659184455872\n",
      "[step: 8215] loss: 0.6682047247886658\n",
      "[step: 8216] loss: 0.6692831516265869\n",
      "[step: 8217] loss: 0.6699835062026978\n",
      "[step: 8218] loss: 0.6702895164489746\n",
      "[step: 8219] loss: 0.6701256036758423\n",
      "[step: 8220] loss: 0.6696559190750122\n",
      "[step: 8221] loss: 0.6689475178718567\n",
      "[step: 8222] loss: 0.6682263612747192\n",
      "[step: 8223] loss: 0.6675148606300354\n",
      "[step: 8224] loss: 0.6669610738754272\n",
      "[step: 8225] loss: 0.6664998531341553\n",
      "[step: 8226] loss: 0.6662085056304932\n",
      "[step: 8227] loss: 0.6660387516021729\n",
      "[step: 8228] loss: 0.666084885597229\n",
      "[step: 8229] loss: 0.6663504838943481\n",
      "[step: 8230] loss: 0.6670278310775757\n",
      "[step: 8231] loss: 0.6682173609733582\n",
      "[step: 8232] loss: 0.6703346967697144\n",
      "[step: 8233] loss: 0.6736948490142822\n",
      "[step: 8234] loss: 0.6793831586837769\n",
      "[step: 8235] loss: 0.6882834434509277\n",
      "[step: 8236] loss: 0.7035363912582397\n",
      "[step: 8237] loss: 0.7274599075317383\n",
      "[step: 8238] loss: 0.7696030735969543\n",
      "[step: 8239] loss: 0.8341934680938721\n",
      "[step: 8240] loss: 0.9497005343437195\n",
      "[step: 8241] loss: 1.109957218170166\n",
      "[step: 8242] loss: 1.3847544193267822\n",
      "[step: 8243] loss: 1.6548842191696167\n",
      "[step: 8244] loss: 2.013904094696045\n",
      "[step: 8245] loss: 1.988509178161621\n",
      "[step: 8246] loss: 1.7671617269515991\n",
      "[step: 8247] loss: 1.1570656299591064\n",
      "[step: 8248] loss: 0.7415926456451416\n",
      "[step: 8249] loss: 0.7508920431137085\n",
      "[step: 8250] loss: 1.0442845821380615\n",
      "[step: 8251] loss: 1.2589396238327026\n",
      "[step: 8252] loss: 1.0876743793487549\n",
      "[step: 8253] loss: 0.7985408306121826\n",
      "[step: 8254] loss: 0.6852327585220337\n",
      "[step: 8255] loss: 0.8231862783432007\n",
      "[step: 8256] loss: 1.0034343004226685\n",
      "[step: 8257] loss: 0.9812933206558228\n",
      "[step: 8258] loss: 0.8200210928916931\n",
      "[step: 8259] loss: 0.685488224029541\n",
      "[step: 8260] loss: 0.7089788913726807\n",
      "[step: 8261] loss: 0.8218276500701904\n",
      "[step: 8262] loss: 0.8627756834030151\n",
      "[step: 8263] loss: 0.7955117225646973\n",
      "[step: 8264] loss: 0.6964806318283081\n",
      "[step: 8265] loss: 0.6756231784820557\n",
      "[step: 8266] loss: 0.7303583025932312\n",
      "[step: 8267] loss: 0.7774884700775146\n",
      "[step: 8268] loss: 0.7626519203186035\n",
      "[step: 8269] loss: 0.7032570242881775\n",
      "[step: 8270] loss: 0.6693645119667053\n",
      "[step: 8271] loss: 0.6856998801231384\n",
      "[step: 8272] loss: 0.7215170860290527\n",
      "[step: 8273] loss: 0.7337737083435059\n",
      "[step: 8274] loss: 0.7093229293823242\n",
      "[step: 8275] loss: 0.6777092218399048\n",
      "[step: 8276] loss: 0.6652358770370483\n",
      "[step: 8277] loss: 0.6771441698074341\n",
      "[step: 8278] loss: 0.6962807178497314\n",
      "[step: 8279] loss: 0.7029209136962891\n",
      "[step: 8280] loss: 0.6938090324401855\n",
      "[step: 8281] loss: 0.6763898134231567\n",
      "[step: 8282] loss: 0.6644826531410217\n",
      "[step: 8283] loss: 0.6638692617416382\n",
      "[step: 8284] loss: 0.6717315912246704\n",
      "[step: 8285] loss: 0.6802458167076111\n",
      "[step: 8286] loss: 0.6823869943618774\n",
      "[step: 8287] loss: 0.6778321266174316\n",
      "[step: 8288] loss: 0.6694452166557312\n",
      "[step: 8289] loss: 0.6625444293022156\n",
      "[step: 8290] loss: 0.6601283550262451\n",
      "[step: 8291] loss: 0.662150502204895\n",
      "[step: 8292] loss: 0.6661204099655151\n",
      "[step: 8293] loss: 0.6692131757736206\n",
      "[step: 8294] loss: 0.6699848771095276\n",
      "[step: 8295] loss: 0.6680213212966919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8296] loss: 0.6647052764892578\n",
      "[step: 8297] loss: 0.6612062454223633\n",
      "[step: 8298] loss: 0.658663272857666\n",
      "[step: 8299] loss: 0.6575216054916382\n",
      "[step: 8300] loss: 0.6576343774795532\n",
      "[step: 8301] loss: 0.658570408821106\n",
      "[step: 8302] loss: 0.6598210334777832\n",
      "[step: 8303] loss: 0.6609891653060913\n",
      "[step: 8304] loss: 0.6617788076400757\n",
      "[step: 8305] loss: 0.6622195243835449\n",
      "[step: 8306] loss: 0.6622032523155212\n",
      "[step: 8307] loss: 0.6619515419006348\n",
      "[step: 8308] loss: 0.6614634394645691\n",
      "[step: 8309] loss: 0.6609535813331604\n",
      "[step: 8310] loss: 0.6604211926460266\n",
      "[step: 8311] loss: 0.6600250005722046\n",
      "[step: 8312] loss: 0.6597102880477905\n",
      "[step: 8313] loss: 0.659618616104126\n",
      "[step: 8314] loss: 0.6597276329994202\n",
      "[step: 8315] loss: 0.6602035760879517\n",
      "[step: 8316] loss: 0.6610769033432007\n",
      "[step: 8317] loss: 0.6626838445663452\n",
      "[step: 8318] loss: 0.6651982665061951\n",
      "[step: 8319] loss: 0.6694119572639465\n",
      "[step: 8320] loss: 0.6758923530578613\n",
      "[step: 8321] loss: 0.6867647171020508\n",
      "[step: 8322] loss: 0.7034787535667419\n",
      "[step: 8323] loss: 0.7321906089782715\n",
      "[step: 8324] loss: 0.7758416533470154\n",
      "[step: 8325] loss: 0.8523529171943665\n",
      "[step: 8326] loss: 0.9615943431854248\n",
      "[step: 8327] loss: 1.1503374576568604\n",
      "[step: 8328] loss: 1.3682992458343506\n",
      "[step: 8329] loss: 1.6959989070892334\n",
      "[step: 8330] loss: 1.8489289283752441\n",
      "[step: 8331] loss: 1.9212822914123535\n",
      "[step: 8332] loss: 1.516002893447876\n",
      "[step: 8333] loss: 1.0366218090057373\n",
      "[step: 8334] loss: 0.7052211761474609\n",
      "[step: 8335] loss: 0.7542036771774292\n",
      "[step: 8336] loss: 1.0300874710083008\n",
      "[step: 8337] loss: 1.165900707244873\n",
      "[step: 8338] loss: 1.0562210083007812\n",
      "[step: 8339] loss: 0.7849708795547485\n",
      "[step: 8340] loss: 0.668315052986145\n",
      "[step: 8341] loss: 0.7708704471588135\n",
      "[step: 8342] loss: 0.9259053468704224\n",
      "[step: 8343] loss: 0.9686903953552246\n",
      "[step: 8344] loss: 0.8381713032722473\n",
      "[step: 8345] loss: 0.7017030715942383\n",
      "[step: 8346] loss: 0.6755514144897461\n",
      "[step: 8347] loss: 0.7548309564590454\n",
      "[step: 8348] loss: 0.8301364183425903\n",
      "[step: 8349] loss: 0.8065577745437622\n",
      "[step: 8350] loss: 0.7252588868141174\n",
      "[step: 8351] loss: 0.6651275157928467\n",
      "[step: 8352] loss: 0.6764733791351318\n",
      "[step: 8353] loss: 0.7284305691719055\n",
      "[step: 8354] loss: 0.7531010508537292\n",
      "[step: 8355] loss: 0.7293871641159058\n",
      "[step: 8356] loss: 0.682153582572937\n",
      "[step: 8357] loss: 0.6592355966567993\n",
      "[step: 8358] loss: 0.6735464334487915\n",
      "[step: 8359] loss: 0.7011709213256836\n",
      "[step: 8360] loss: 0.7135348320007324\n",
      "[step: 8361] loss: 0.6989363431930542\n",
      "[step: 8362] loss: 0.6739307641983032\n",
      "[step: 8363] loss: 0.6570596694946289\n",
      "[step: 8364] loss: 0.65840744972229\n",
      "[step: 8365] loss: 0.6720853447914124\n",
      "[step: 8366] loss: 0.6844382286071777\n",
      "[step: 8367] loss: 0.6872237920761108\n",
      "[step: 8368] loss: 0.6785340309143066\n",
      "[step: 8369] loss: 0.6656984090805054\n",
      "[step: 8370] loss: 0.6558036804199219\n",
      "[step: 8371] loss: 0.65334153175354\n",
      "[step: 8372] loss: 0.6573604941368103\n",
      "[step: 8373] loss: 0.6637632846832275\n",
      "[step: 8374] loss: 0.6682735681533813\n",
      "[step: 8375] loss: 0.6681390404701233\n",
      "[step: 8376] loss: 0.6643556356430054\n",
      "[step: 8377] loss: 0.65860915184021\n",
      "[step: 8378] loss: 0.6535065174102783\n",
      "[step: 8379] loss: 0.6506420373916626\n",
      "[step: 8380] loss: 0.6503474712371826\n",
      "[step: 8381] loss: 0.6519145369529724\n",
      "[step: 8382] loss: 0.6542896032333374\n",
      "[step: 8383] loss: 0.6565180420875549\n",
      "[step: 8384] loss: 0.657873809337616\n",
      "[step: 8385] loss: 0.6583255529403687\n",
      "[step: 8386] loss: 0.6577292084693909\n",
      "[step: 8387] loss: 0.6565764546394348\n",
      "[step: 8388] loss: 0.6550528407096863\n",
      "[step: 8389] loss: 0.6535050868988037\n",
      "[step: 8390] loss: 0.6519966125488281\n",
      "[step: 8391] loss: 0.6507181525230408\n",
      "[step: 8392] loss: 0.6496313214302063\n",
      "[step: 8393] loss: 0.6487542390823364\n",
      "[step: 8394] loss: 0.6480557322502136\n",
      "[step: 8395] loss: 0.6475266218185425\n",
      "[step: 8396] loss: 0.6471282243728638\n",
      "[step: 8397] loss: 0.646835446357727\n",
      "[step: 8398] loss: 0.6466470956802368\n",
      "[step: 8399] loss: 0.6465826630592346\n",
      "[step: 8400] loss: 0.6466566920280457\n",
      "[step: 8401] loss: 0.6469225883483887\n",
      "[step: 8402] loss: 0.6474785208702087\n",
      "[step: 8403] loss: 0.6484911441802979\n",
      "[step: 8404] loss: 0.650181770324707\n",
      "[step: 8405] loss: 0.6530406475067139\n",
      "[step: 8406] loss: 0.6576974987983704\n",
      "[step: 8407] loss: 0.6655234098434448\n",
      "[step: 8408] loss: 0.6782195568084717\n",
      "[step: 8409] loss: 0.6997655630111694\n",
      "[step: 8410] loss: 0.7344385981559753\n",
      "[step: 8411] loss: 0.7934103012084961\n",
      "[step: 8412] loss: 0.884598970413208\n",
      "[step: 8413] loss: 1.0345916748046875\n",
      "[step: 8414] loss: 1.2398557662963867\n",
      "[step: 8415] loss: 1.548962950706482\n",
      "[step: 8416] loss: 1.8568928241729736\n",
      "[step: 8417] loss: 2.2669789791107178\n",
      "[step: 8418] loss: 2.32502818107605\n",
      "[step: 8419] loss: 2.3374030590057373\n",
      "[step: 8420] loss: 1.7873451709747314\n",
      "[step: 8421] loss: 1.2378476858139038\n",
      "[step: 8422] loss: 1.0610318183898926\n",
      "[step: 8423] loss: 1.2582643032073975\n",
      "[step: 8424] loss: 1.4822643995285034\n",
      "[step: 8425] loss: 1.229339361190796\n",
      "[step: 8426] loss: 0.8782946467399597\n",
      "[step: 8427] loss: 0.8894842863082886\n",
      "[step: 8428] loss: 1.1218910217285156\n",
      "[step: 8429] loss: 1.1920194625854492\n",
      "[step: 8430] loss: 0.9248363971710205\n",
      "[step: 8431] loss: 0.7329025268554688\n",
      "[step: 8432] loss: 0.8447645306587219\n",
      "[step: 8433] loss: 1.008795976638794\n",
      "[step: 8434] loss: 0.946092426776886\n",
      "[step: 8435] loss: 0.7222034931182861\n",
      "[step: 8436] loss: 0.7097196578979492\n",
      "[step: 8437] loss: 0.8730072975158691\n",
      "[step: 8438] loss: 0.8843852877616882\n",
      "[step: 8439] loss: 0.738757848739624\n",
      "[step: 8440] loss: 0.6662035584449768\n",
      "[step: 8441] loss: 0.7559361457824707\n",
      "[step: 8442] loss: 0.819243311882019\n",
      "[step: 8443] loss: 0.7422590851783752\n",
      "[step: 8444] loss: 0.6707158088684082\n",
      "[step: 8445] loss: 0.697627604007721\n",
      "[step: 8446] loss: 0.7454870939254761\n",
      "[step: 8447] loss: 0.7307059168815613\n",
      "[step: 8448] loss: 0.6888926029205322\n",
      "[step: 8449] loss: 0.6787607073783875\n",
      "[step: 8450] loss: 0.6881692409515381\n",
      "[step: 8451] loss: 0.6964237093925476\n",
      "[step: 8452] loss: 0.6959879398345947\n",
      "[step: 8453] loss: 0.6855553388595581\n",
      "[step: 8454] loss: 0.6700561046600342\n",
      "[step: 8455] loss: 0.6612156629562378\n",
      "[step: 8456] loss: 0.6724660396575928\n",
      "[step: 8457] loss: 0.6867144107818604\n",
      "[step: 8458] loss: 0.6789117455482483\n",
      "[step: 8459] loss: 0.6579488515853882\n",
      "[step: 8460] loss: 0.6496249437332153\n",
      "[step: 8461] loss: 0.6609141826629639\n",
      "[step: 8462] loss: 0.6708124876022339\n",
      "[step: 8463] loss: 0.6662237644195557\n",
      "[step: 8464] loss: 0.6560113430023193\n",
      "[step: 8465] loss: 0.6512028574943542\n",
      "[step: 8466] loss: 0.6527320146560669\n",
      "[step: 8467] loss: 0.6538964509963989\n",
      "[step: 8468] loss: 0.6542441248893738\n",
      "[step: 8469] loss: 0.6556378602981567\n",
      "[step: 8470] loss: 0.6545997262001038\n",
      "[step: 8471] loss: 0.6501234769821167\n",
      "[step: 8472] loss: 0.6454390287399292\n",
      "[step: 8473] loss: 0.645146369934082\n",
      "[step: 8474] loss: 0.6484712958335876\n",
      "[step: 8475] loss: 0.6505077481269836\n",
      "[step: 8476] loss: 0.649476170539856\n",
      "[step: 8477] loss: 0.6470296382904053\n",
      "[step: 8478] loss: 0.6455309987068176\n",
      "[step: 8479] loss: 0.6444939970970154\n",
      "[step: 8480] loss: 0.6431525945663452\n",
      "[step: 8481] loss: 0.6424283981323242\n",
      "[step: 8482] loss: 0.6429929733276367\n",
      "[step: 8483] loss: 0.64417964220047\n",
      "[step: 8484] loss: 0.6445072889328003\n",
      "[step: 8485] loss: 0.64363032579422\n",
      "[step: 8486] loss: 0.6424785256385803\n",
      "[step: 8487] loss: 0.6417545080184937\n",
      "[step: 8488] loss: 0.6411620378494263\n",
      "[step: 8489] loss: 0.6403225064277649\n",
      "[step: 8490] loss: 0.6395662426948547\n",
      "[step: 8491] loss: 0.6393243670463562\n",
      "[step: 8492] loss: 0.6395338177680969\n",
      "[step: 8493] loss: 0.6397117376327515\n",
      "[step: 8494] loss: 0.6396210193634033\n",
      "[step: 8495] loss: 0.6394727826118469\n",
      "[step: 8496] loss: 0.6394804120063782\n",
      "[step: 8497] loss: 0.639539361000061\n",
      "[step: 8498] loss: 0.6394269466400146\n",
      "[step: 8499] loss: 0.6391595602035522\n",
      "[step: 8500] loss: 0.6389505863189697\n",
      "[step: 8501] loss: 0.6388784050941467\n",
      "[step: 8502] loss: 0.6389118432998657\n",
      "[step: 8503] loss: 0.6389244198799133\n",
      "[step: 8504] loss: 0.6390061378479004\n",
      "[step: 8505] loss: 0.6393048763275146\n",
      "[step: 8506] loss: 0.6399712562561035\n",
      "[step: 8507] loss: 0.6410394906997681\n",
      "[step: 8508] loss: 0.6427310109138489\n",
      "[step: 8509] loss: 0.6453585624694824\n",
      "[step: 8510] loss: 0.6497616767883301\n",
      "[step: 8511] loss: 0.6566814184188843\n",
      "[step: 8512] loss: 0.6683648228645325\n",
      "[step: 8513] loss: 0.6865972280502319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8514] loss: 0.718272864818573\n",
      "[step: 8515] loss: 0.7674140930175781\n",
      "[step: 8516] loss: 0.8545235395431519\n",
      "[step: 8517] loss: 0.9810205698013306\n",
      "[step: 8518] loss: 1.199596881866455\n",
      "[step: 8519] loss: 1.4505176544189453\n",
      "[step: 8520] loss: 1.8156557083129883\n",
      "[step: 8521] loss: 1.9514899253845215\n",
      "[step: 8522] loss: 1.9501656293869019\n",
      "[step: 8523] loss: 1.4371942281723022\n",
      "[step: 8524] loss: 0.9092119336128235\n",
      "[step: 8525] loss: 0.6634295582771301\n",
      "[step: 8526] loss: 0.8328323364257812\n",
      "[step: 8527] loss: 1.1430895328521729\n",
      "[step: 8528] loss: 1.1769592761993408\n",
      "[step: 8529] loss: 0.9470030665397644\n",
      "[step: 8530] loss: 0.6905007362365723\n",
      "[step: 8531] loss: 0.6891433000564575\n",
      "[step: 8532] loss: 0.8737868070602417\n",
      "[step: 8533] loss: 0.9796160459518433\n",
      "[step: 8534] loss: 0.9021209478378296\n",
      "[step: 8535] loss: 0.7217630743980408\n",
      "[step: 8536] loss: 0.6501415967941284\n",
      "[step: 8537] loss: 0.7252100706100464\n",
      "[step: 8538] loss: 0.8220999836921692\n",
      "[step: 8539] loss: 0.8221307992935181\n",
      "[step: 8540] loss: 0.7228904366493225\n",
      "[step: 8541] loss: 0.6510606408119202\n",
      "[step: 8542] loss: 0.6662425994873047\n",
      "[step: 8543] loss: 0.726878821849823\n",
      "[step: 8544] loss: 0.7534670233726501\n",
      "[step: 8545] loss: 0.7122762799263\n",
      "[step: 8546] loss: 0.6595548987388611\n",
      "[step: 8547] loss: 0.6458665728569031\n",
      "[step: 8548] loss: 0.6733797788619995\n",
      "[step: 8549] loss: 0.7031816244125366\n",
      "[step: 8550] loss: 0.6995247006416321\n",
      "[step: 8551] loss: 0.6713895797729492\n",
      "[step: 8552] loss: 0.6451324820518494\n",
      "[step: 8553] loss: 0.6420068144798279\n",
      "[step: 8554] loss: 0.6581049561500549\n",
      "[step: 8555] loss: 0.6736665964126587\n",
      "[step: 8556] loss: 0.6752111911773682\n",
      "[step: 8557] loss: 0.6614748239517212\n",
      "[step: 8558] loss: 0.6452432870864868\n",
      "[step: 8559] loss: 0.6374247670173645\n",
      "[step: 8560] loss: 0.6408168077468872\n",
      "[step: 8561] loss: 0.649738073348999\n",
      "[step: 8562] loss: 0.6559516191482544\n",
      "[step: 8563] loss: 0.655398428440094\n",
      "[step: 8564] loss: 0.6487435102462769\n",
      "[step: 8565] loss: 0.6409416198730469\n",
      "[step: 8566] loss: 0.635800838470459\n",
      "[step: 8567] loss: 0.6352951526641846\n",
      "[step: 8568] loss: 0.638226330280304\n",
      "[step: 8569] loss: 0.6419868469238281\n",
      "[step: 8570] loss: 0.6443619728088379\n",
      "[step: 8571] loss: 0.64393150806427\n",
      "[step: 8572] loss: 0.6413542032241821\n",
      "[step: 8573] loss: 0.6377544403076172\n",
      "[step: 8574] loss: 0.6346268653869629\n",
      "[step: 8575] loss: 0.6327340006828308\n",
      "[step: 8576] loss: 0.6322342753410339\n",
      "[step: 8577] loss: 0.6327894926071167\n",
      "[step: 8578] loss: 0.6338698863983154\n",
      "[step: 8579] loss: 0.6351115107536316\n",
      "[step: 8580] loss: 0.6361175775527954\n",
      "[step: 8581] loss: 0.6367521286010742\n",
      "[step: 8582] loss: 0.6368980407714844\n",
      "[step: 8583] loss: 0.6366817951202393\n",
      "[step: 8584] loss: 0.6361449956893921\n",
      "[step: 8585] loss: 0.6355350613594055\n",
      "[step: 8586] loss: 0.6348374485969543\n",
      "[step: 8587] loss: 0.6342116594314575\n",
      "[step: 8588] loss: 0.6336129903793335\n",
      "[step: 8589] loss: 0.6331624984741211\n",
      "[step: 8590] loss: 0.632835865020752\n",
      "[step: 8591] loss: 0.6327382326126099\n",
      "[step: 8592] loss: 0.6328480243682861\n",
      "[step: 8593] loss: 0.633292019367218\n",
      "[step: 8594] loss: 0.6341326832771301\n",
      "[step: 8595] loss: 0.6356797814369202\n",
      "[step: 8596] loss: 0.6381561756134033\n",
      "[step: 8597] loss: 0.6423559784889221\n",
      "[step: 8598] loss: 0.6489355564117432\n",
      "[step: 8599] loss: 0.6601125001907349\n",
      "[step: 8600] loss: 0.6776490807533264\n",
      "[step: 8601] loss: 0.7081810832023621\n",
      "[step: 8602] loss: 0.7554786801338196\n",
      "[step: 8603] loss: 0.8395906686782837\n",
      "[step: 8604] loss: 0.9615421891212463\n",
      "[step: 8605] loss: 1.1740822792053223\n",
      "[step: 8606] loss: 1.4181694984436035\n",
      "[step: 8607] loss: 1.7787377834320068\n",
      "[step: 8608] loss: 1.9187724590301514\n",
      "[step: 8609] loss: 1.9345945119857788\n",
      "[step: 8610] loss: 1.4409750699996948\n",
      "[step: 8611] loss: 0.9194812774658203\n",
      "[step: 8612] loss: 0.6593548059463501\n",
      "[step: 8613] loss: 0.8102487325668335\n",
      "[step: 8614] loss: 1.1159954071044922\n",
      "[step: 8615] loss: 1.1655220985412598\n",
      "[step: 8616] loss: 0.9536656737327576\n",
      "[step: 8617] loss: 0.6943478584289551\n",
      "[step: 8618] loss: 0.6740052700042725\n",
      "[step: 8619] loss: 0.8473506569862366\n",
      "[step: 8620] loss: 0.9645991325378418\n",
      "[step: 8621] loss: 0.9086621999740601\n",
      "[step: 8622] loss: 0.7317099571228027\n",
      "[step: 8623] loss: 0.6420354247093201\n",
      "[step: 8624] loss: 0.7003291845321655\n",
      "[step: 8625] loss: 0.8009815216064453\n",
      "[step: 8626] loss: 0.8184360265731812\n",
      "[step: 8627] loss: 0.7285692691802979\n",
      "[step: 8628] loss: 0.6479566097259521\n",
      "[step: 8629] loss: 0.6492659449577332\n",
      "[step: 8630] loss: 0.707719624042511\n",
      "[step: 8631] loss: 0.7439744472503662\n",
      "[step: 8632] loss: 0.7122249603271484\n",
      "[step: 8633] loss: 0.6578775644302368\n",
      "[step: 8634] loss: 0.635537326335907\n",
      "[step: 8635] loss: 0.6580802202224731\n",
      "[step: 8636] loss: 0.6906524896621704\n",
      "[step: 8637] loss: 0.6940650939941406\n",
      "[step: 8638] loss: 0.6696701049804688\n",
      "[step: 8639] loss: 0.6412718892097473\n",
      "[step: 8640] loss: 0.6328452229499817\n",
      "[step: 8641] loss: 0.6451401114463806\n",
      "[step: 8642] loss: 0.6617752313613892\n",
      "[step: 8643] loss: 0.6676132082939148\n",
      "[step: 8644] loss: 0.658110499382019\n",
      "[step: 8645] loss: 0.6428712606430054\n",
      "[step: 8646] loss: 0.6319105625152588\n",
      "[step: 8647] loss: 0.6308053731918335\n",
      "[step: 8648] loss: 0.6373761296272278\n",
      "[step: 8649] loss: 0.6449019908905029\n",
      "[step: 8650] loss: 0.6478505730628967\n",
      "[step: 8651] loss: 0.6442790031433105\n",
      "[step: 8652] loss: 0.6371723413467407\n",
      "[step: 8653] loss: 0.6302682757377625\n",
      "[step: 8654] loss: 0.6271162033081055\n",
      "[step: 8655] loss: 0.6280732154846191\n",
      "[step: 8656] loss: 0.6314347982406616\n",
      "[step: 8657] loss: 0.6348448991775513\n",
      "[step: 8658] loss: 0.6362411379814148\n",
      "[step: 8659] loss: 0.6353779435157776\n",
      "[step: 8660] loss: 0.6327067613601685\n",
      "[step: 8661] loss: 0.6295870542526245\n",
      "[step: 8662] loss: 0.6268516778945923\n",
      "[step: 8663] loss: 0.6250941753387451\n",
      "[step: 8664] loss: 0.6243464350700378\n",
      "[step: 8665] loss: 0.6244498491287231\n",
      "[step: 8666] loss: 0.6251544952392578\n",
      "[step: 8667] loss: 0.6261252164840698\n",
      "[step: 8668] loss: 0.6271688938140869\n",
      "[step: 8669] loss: 0.6280513405799866\n",
      "[step: 8670] loss: 0.6287169456481934\n",
      "[step: 8671] loss: 0.6290981769561768\n",
      "[step: 8672] loss: 0.6293637752532959\n",
      "[step: 8673] loss: 0.6295011043548584\n",
      "[step: 8674] loss: 0.6297607421875\n",
      "[step: 8675] loss: 0.6300726532936096\n",
      "[step: 8676] loss: 0.6307086944580078\n",
      "[step: 8677] loss: 0.6315937042236328\n",
      "[step: 8678] loss: 0.6331654787063599\n",
      "[step: 8679] loss: 0.6354221105575562\n",
      "[step: 8680] loss: 0.6391971111297607\n",
      "[step: 8681] loss: 0.6446744203567505\n",
      "[step: 8682] loss: 0.6536328792572021\n",
      "[step: 8683] loss: 0.6666648983955383\n",
      "[step: 8684] loss: 0.6883471012115479\n",
      "[step: 8685] loss: 0.7198017835617065\n",
      "[step: 8686] loss: 0.7731571793556213\n",
      "[step: 8687] loss: 0.8469463586807251\n",
      "[step: 8688] loss: 0.9714764356613159\n",
      "[step: 8689] loss: 1.1191649436950684\n",
      "[step: 8690] loss: 1.3482671976089478\n",
      "[step: 8691] loss: 1.5087389945983887\n",
      "[step: 8692] loss: 1.670343041419983\n",
      "[step: 8693] loss: 1.5187926292419434\n",
      "[step: 8694] loss: 1.2439155578613281\n",
      "[step: 8695] loss: 0.8563103675842285\n",
      "[step: 8696] loss: 0.65183424949646\n",
      "[step: 8697] loss: 0.7050060629844666\n",
      "[step: 8698] loss: 0.8941468596458435\n",
      "[step: 8699] loss: 1.032996654510498\n",
      "[step: 8700] loss: 0.9606085419654846\n",
      "[step: 8701] loss: 0.7881721258163452\n",
      "[step: 8702] loss: 0.6506218910217285\n",
      "[step: 8703] loss: 0.6525716781616211\n",
      "[step: 8704] loss: 0.7556572556495667\n",
      "[step: 8705] loss: 0.8414022326469421\n",
      "[step: 8706] loss: 0.8444417715072632\n",
      "[step: 8707] loss: 0.7519342303276062\n",
      "[step: 8708] loss: 0.6586426496505737\n",
      "[step: 8709] loss: 0.629644513130188\n",
      "[step: 8710] loss: 0.6696153879165649\n",
      "[step: 8711] loss: 0.7273915410041809\n",
      "[step: 8712] loss: 0.7427112460136414\n",
      "[step: 8713] loss: 0.7101449966430664\n",
      "[step: 8714] loss: 0.6555230021476746\n",
      "[step: 8715] loss: 0.6263833045959473\n",
      "[step: 8716] loss: 0.6367446184158325\n",
      "[step: 8717] loss: 0.6669541597366333\n",
      "[step: 8718] loss: 0.6886081695556641\n",
      "[step: 8719] loss: 0.6827383041381836\n",
      "[step: 8720] loss: 0.6586824655532837\n",
      "[step: 8721] loss: 0.6330920457839966\n",
      "[step: 8722] loss: 0.6223896741867065\n",
      "[step: 8723] loss: 0.628605842590332\n",
      "[step: 8724] loss: 0.643103301525116\n",
      "[step: 8725] loss: 0.6552139520645142\n",
      "[step: 8726] loss: 0.6570783853530884\n",
      "[step: 8727] loss: 0.6500796675682068\n",
      "[step: 8728] loss: 0.6376597881317139\n",
      "[step: 8729] loss: 0.6266425848007202\n",
      "[step: 8730] loss: 0.6206215620040894\n",
      "[step: 8731] loss: 0.6204698085784912\n",
      "[step: 8732] loss: 0.6245390176773071\n",
      "[step: 8733] loss: 0.6299924254417419\n",
      "[step: 8734] loss: 0.6343759298324585\n",
      "[step: 8735] loss: 0.6358993053436279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8736] loss: 0.6348971128463745\n",
      "[step: 8737] loss: 0.631514310836792\n",
      "[step: 8738] loss: 0.6273313164710999\n",
      "[step: 8739] loss: 0.623123049736023\n",
      "[step: 8740] loss: 0.619807243347168\n",
      "[step: 8741] loss: 0.6176781058311462\n",
      "[step: 8742] loss: 0.6167564988136292\n",
      "[step: 8743] loss: 0.6167917251586914\n",
      "[step: 8744] loss: 0.6175276041030884\n",
      "[step: 8745] loss: 0.6187446117401123\n",
      "[step: 8746] loss: 0.6202161312103271\n",
      "[step: 8747] loss: 0.6219521760940552\n",
      "[step: 8748] loss: 0.6239293813705444\n",
      "[step: 8749] loss: 0.6264753341674805\n",
      "[step: 8750] loss: 0.6296330690383911\n",
      "[step: 8751] loss: 0.6342037320137024\n",
      "[step: 8752] loss: 0.6402801871299744\n",
      "[step: 8753] loss: 0.6495856046676636\n",
      "[step: 8754] loss: 0.6622751355171204\n",
      "[step: 8755] loss: 0.6824051141738892\n",
      "[step: 8756] loss: 0.7099261283874512\n",
      "[step: 8757] loss: 0.7548187971115112\n",
      "[step: 8758] loss: 0.8138645887374878\n",
      "[step: 8759] loss: 0.9102522134780884\n",
      "[step: 8760] loss: 1.0208227634429932\n",
      "[step: 8761] loss: 1.1897785663604736\n",
      "[step: 8762] loss: 1.3164119720458984\n",
      "[step: 8763] loss: 1.4607633352279663\n",
      "[step: 8764] loss: 1.4019041061401367\n",
      "[step: 8765] loss: 1.2573597431182861\n",
      "[step: 8766] loss: 0.9545913934707642\n",
      "[step: 8767] loss: 0.7143500447273254\n",
      "[step: 8768] loss: 0.6325230598449707\n",
      "[step: 8769] loss: 0.7160045504570007\n",
      "[step: 8770] loss: 0.8642079830169678\n",
      "[step: 8771] loss: 0.9321825504302979\n",
      "[step: 8772] loss: 0.8965361714363098\n",
      "[step: 8773] loss: 0.7651063799858093\n",
      "[step: 8774] loss: 0.655343770980835\n",
      "[step: 8775] loss: 0.6258222460746765\n",
      "[step: 8776] loss: 0.674260139465332\n",
      "[step: 8777] loss: 0.749441385269165\n",
      "[step: 8778] loss: 0.7845664024353027\n",
      "[step: 8779] loss: 0.7668420076370239\n",
      "[step: 8780] loss: 0.7010548710823059\n",
      "[step: 8781] loss: 0.641592264175415\n",
      "[step: 8782] loss: 0.6199043393135071\n",
      "[step: 8783] loss: 0.6396182775497437\n",
      "[step: 8784] loss: 0.6768593788146973\n",
      "[step: 8785] loss: 0.698630690574646\n",
      "[step: 8786] loss: 0.694572925567627\n",
      "[step: 8787] loss: 0.6660668253898621\n",
      "[step: 8788] loss: 0.6357589364051819\n",
      "[step: 8789] loss: 0.6185833811759949\n",
      "[step: 8790] loss: 0.6205254793167114\n",
      "[step: 8791] loss: 0.6354445219039917\n",
      "[step: 8792] loss: 0.6512603759765625\n",
      "[step: 8793] loss: 0.6602457761764526\n",
      "[step: 8794] loss: 0.6575728058815002\n",
      "[step: 8795] loss: 0.6477248072624207\n",
      "[step: 8796] loss: 0.6337578296661377\n",
      "[step: 8797] loss: 0.6219122409820557\n",
      "[step: 8798] loss: 0.615193784236908\n",
      "[step: 8799] loss: 0.6141688823699951\n",
      "[step: 8800] loss: 0.6173569560050964\n",
      "[step: 8801] loss: 0.6227586269378662\n",
      "[step: 8802] loss: 0.6286318302154541\n",
      "[step: 8803] loss: 0.6329449415206909\n",
      "[step: 8804] loss: 0.635429859161377\n",
      "[step: 8805] loss: 0.6353621482849121\n",
      "[step: 8806] loss: 0.6339795589447021\n",
      "[step: 8807] loss: 0.6310687065124512\n",
      "[step: 8808] loss: 0.6278203725814819\n",
      "[step: 8809] loss: 0.6242930889129639\n",
      "[step: 8810] loss: 0.6211856603622437\n",
      "[step: 8811] loss: 0.6183798313140869\n",
      "[step: 8812] loss: 0.6161594986915588\n",
      "[step: 8813] loss: 0.6144505739212036\n",
      "[step: 8814] loss: 0.6132577061653137\n",
      "[step: 8815] loss: 0.612409234046936\n",
      "[step: 8816] loss: 0.6119056940078735\n",
      "[step: 8817] loss: 0.6116757988929749\n",
      "[step: 8818] loss: 0.6117372512817383\n",
      "[step: 8819] loss: 0.61209636926651\n",
      "[step: 8820] loss: 0.6129708290100098\n",
      "[step: 8821] loss: 0.6145740151405334\n",
      "[step: 8822] loss: 0.6174911260604858\n",
      "[step: 8823] loss: 0.6224064826965332\n",
      "[step: 8824] loss: 0.631188690662384\n",
      "[step: 8825] loss: 0.645836591720581\n",
      "[step: 8826] loss: 0.6723416447639465\n",
      "[step: 8827] loss: 0.7161734104156494\n",
      "[step: 8828] loss: 0.7975020408630371\n",
      "[step: 8829] loss: 0.925711989402771\n",
      "[step: 8830] loss: 1.1612560749053955\n",
      "[step: 8831] loss: 1.465497612953186\n",
      "[step: 8832] loss: 1.9442112445831299\n",
      "[step: 8833] loss: 2.193756341934204\n",
      "[step: 8834] loss: 2.278263807296753\n",
      "[step: 8835] loss: 1.6342061758041382\n",
      "[step: 8836] loss: 0.9413227438926697\n",
      "[step: 8837] loss: 0.6780065894126892\n",
      "[step: 8838] loss: 0.9762306809425354\n",
      "[step: 8839] loss: 1.3481701612472534\n",
      "[step: 8840] loss: 1.1976757049560547\n",
      "[step: 8841] loss: 0.8051018714904785\n",
      "[step: 8842] loss: 0.6475380659103394\n",
      "[step: 8843] loss: 0.8644357919692993\n",
      "[step: 8844] loss: 1.096003532409668\n",
      "[step: 8845] loss: 0.9763606786727905\n",
      "[step: 8846] loss: 0.7245273590087891\n",
      "[step: 8847] loss: 0.6421065330505371\n",
      "[step: 8848] loss: 0.7783066034317017\n",
      "[step: 8849] loss: 0.8982495069503784\n",
      "[step: 8850] loss: 0.8051928281784058\n",
      "[step: 8851] loss: 0.6568990349769592\n",
      "[step: 8852] loss: 0.6449244618415833\n",
      "[step: 8853] loss: 0.7432966232299805\n",
      "[step: 8854] loss: 0.7895995378494263\n",
      "[step: 8855] loss: 0.708236575126648\n",
      "[step: 8856] loss: 0.6281046867370605\n",
      "[step: 8857] loss: 0.645430326461792\n",
      "[step: 8858] loss: 0.7067791223526001\n",
      "[step: 8859] loss: 0.7181916236877441\n",
      "[step: 8860] loss: 0.6624079942703247\n",
      "[step: 8861] loss: 0.6220782995223999\n",
      "[step: 8862] loss: 0.6376790404319763\n",
      "[step: 8863] loss: 0.673575222492218\n",
      "[step: 8864] loss: 0.6796364784240723\n",
      "[step: 8865] loss: 0.6467635035514832\n",
      "[step: 8866] loss: 0.6189194917678833\n",
      "[step: 8867] loss: 0.6220686435699463\n",
      "[step: 8868] loss: 0.6443588733673096\n",
      "[step: 8869] loss: 0.6564289331436157\n",
      "[step: 8870] loss: 0.6446212530136108\n",
      "[step: 8871] loss: 0.6239898800849915\n",
      "[step: 8872] loss: 0.6131432056427002\n",
      "[step: 8873] loss: 0.6189101338386536\n",
      "[step: 8874] loss: 0.630842387676239\n",
      "[step: 8875] loss: 0.6354048252105713\n",
      "[step: 8876] loss: 0.6289341449737549\n",
      "[step: 8877] loss: 0.6175971031188965\n",
      "[step: 8878] loss: 0.6110418438911438\n",
      "[step: 8879] loss: 0.6126280426979065\n",
      "[step: 8880] loss: 0.6186332702636719\n",
      "[step: 8881] loss: 0.6227937936782837\n",
      "[step: 8882] loss: 0.6216814517974854\n",
      "[step: 8883] loss: 0.6164830327033997\n",
      "[step: 8884] loss: 0.6109892725944519\n",
      "[step: 8885] loss: 0.6082945466041565\n",
      "[step: 8886] loss: 0.6089876294136047\n",
      "[step: 8887] loss: 0.61163330078125\n",
      "[step: 8888] loss: 0.61399245262146\n",
      "[step: 8889] loss: 0.6146355271339417\n",
      "[step: 8890] loss: 0.6133233308792114\n",
      "[step: 8891] loss: 0.6107828617095947\n",
      "[step: 8892] loss: 0.6081522703170776\n",
      "[step: 8893] loss: 0.6062629222869873\n",
      "[step: 8894] loss: 0.6055194139480591\n",
      "[step: 8895] loss: 0.6057337522506714\n",
      "[step: 8896] loss: 0.6065309643745422\n",
      "[step: 8897] loss: 0.6073630452156067\n",
      "[step: 8898] loss: 0.6078965663909912\n",
      "[step: 8899] loss: 0.6079892516136169\n",
      "[step: 8900] loss: 0.6076213717460632\n",
      "[step: 8901] loss: 0.6069505214691162\n",
      "[step: 8902] loss: 0.6061006188392639\n",
      "[step: 8903] loss: 0.6052601337432861\n",
      "[step: 8904] loss: 0.6044809818267822\n",
      "[step: 8905] loss: 0.6038227081298828\n",
      "[step: 8906] loss: 0.6032825708389282\n",
      "[step: 8907] loss: 0.6028600335121155\n",
      "[step: 8908] loss: 0.6025065183639526\n",
      "[step: 8909] loss: 0.6022167801856995\n",
      "[step: 8910] loss: 0.6019535660743713\n",
      "[step: 8911] loss: 0.6017099618911743\n",
      "[step: 8912] loss: 0.601473331451416\n",
      "[step: 8913] loss: 0.6012382507324219\n",
      "[step: 8914] loss: 0.6010074019432068\n",
      "[step: 8915] loss: 0.6007770895957947\n",
      "[step: 8916] loss: 0.6005507707595825\n",
      "[step: 8917] loss: 0.600326418876648\n",
      "[step: 8918] loss: 0.6001138091087341\n",
      "[step: 8919] loss: 0.599909782409668\n",
      "[step: 8920] loss: 0.5997194051742554\n",
      "[step: 8921] loss: 0.599553108215332\n",
      "[step: 8922] loss: 0.5994191765785217\n",
      "[step: 8923] loss: 0.5993444323539734\n",
      "[step: 8924] loss: 0.5993685722351074\n",
      "[step: 8925] loss: 0.5995757579803467\n",
      "[step: 8926] loss: 0.6001259684562683\n",
      "[step: 8927] loss: 0.6013187170028687\n",
      "[step: 8928] loss: 0.6038151979446411\n",
      "[step: 8929] loss: 0.6087902188301086\n",
      "[step: 8930] loss: 0.6190714836120605\n",
      "[step: 8931] loss: 0.6394731998443604\n",
      "[step: 8932] loss: 0.682652473449707\n",
      "[step: 8933] loss: 0.7673168182373047\n",
      "[step: 8934] loss: 0.9498785734176636\n",
      "[step: 8935] loss: 1.2780671119689941\n",
      "[step: 8936] loss: 1.9428365230560303\n",
      "[step: 8937] loss: 2.7312254905700684\n",
      "[step: 8938] loss: 3.7014665603637695\n",
      "[step: 8939] loss: 3.1906182765960693\n",
      "[step: 8940] loss: 1.864654302597046\n",
      "[step: 8941] loss: 0.859976053237915\n",
      "[step: 8942] loss: 1.400364875793457\n",
      "[step: 8943] loss: 2.212231397628784\n",
      "[step: 8944] loss: 1.4858070611953735\n",
      "[step: 8945] loss: 0.7125744819641113\n",
      "[step: 8946] loss: 1.1650065183639526\n",
      "[step: 8947] loss: 1.5706819295883179\n",
      "[step: 8948] loss: 1.085342526435852\n",
      "[step: 8949] loss: 0.6915321350097656\n",
      "[step: 8950] loss: 1.0666754245758057\n",
      "[step: 8951] loss: 1.254664421081543\n",
      "[step: 8952] loss: 0.8133063912391663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 8953] loss: 0.7409800291061401\n",
      "[step: 8954] loss: 1.0424765348434448\n",
      "[step: 8955] loss: 0.9475566744804382\n",
      "[step: 8956] loss: 0.6587207317352295\n",
      "[step: 8957] loss: 0.7629567980766296\n",
      "[step: 8958] loss: 0.919592559337616\n",
      "[step: 8959] loss: 0.7578368186950684\n",
      "[step: 8960] loss: 0.6602717638015747\n",
      "[step: 8961] loss: 0.7778018116950989\n",
      "[step: 8962] loss: 0.7955297231674194\n",
      "[step: 8963] loss: 0.6640523076057434\n",
      "[step: 8964] loss: 0.6834192276000977\n",
      "[step: 8965] loss: 0.7745276093482971\n",
      "[step: 8966] loss: 0.69769287109375\n",
      "[step: 8967] loss: 0.6298375129699707\n",
      "[step: 8968] loss: 0.6871163845062256\n",
      "[step: 8969] loss: 0.714320957660675\n",
      "[step: 8970] loss: 0.6576002836227417\n",
      "[step: 8971] loss: 0.6263376474380493\n",
      "[step: 8972] loss: 0.6692683696746826\n",
      "[step: 8973] loss: 0.6811866164207458\n",
      "[step: 8974] loss: 0.6365232467651367\n",
      "[step: 8975] loss: 0.6267260313034058\n",
      "[step: 8976] loss: 0.6531093120574951\n",
      "[step: 8977] loss: 0.6549124717712402\n",
      "[step: 8978] loss: 0.6273072957992554\n",
      "[step: 8979] loss: 0.6180488467216492\n",
      "[step: 8980] loss: 0.6377260684967041\n",
      "[step: 8981] loss: 0.6425432562828064\n",
      "[step: 8982] loss: 0.6229170560836792\n",
      "[step: 8983] loss: 0.6130658388137817\n",
      "[step: 8984] loss: 0.623788058757782\n",
      "[step: 8985] loss: 0.6314482092857361\n",
      "[step: 8986] loss: 0.6217232346534729\n",
      "[step: 8987] loss: 0.6111764907836914\n",
      "[step: 8988] loss: 0.6143061518669128\n",
      "[step: 8989] loss: 0.6212427020072937\n",
      "[step: 8990] loss: 0.6190286874771118\n",
      "[step: 8991] loss: 0.6111821532249451\n",
      "[step: 8992] loss: 0.6085060834884644\n",
      "[step: 8993] loss: 0.6125901341438293\n",
      "[step: 8994] loss: 0.615013062953949\n",
      "[step: 8995] loss: 0.6115150451660156\n",
      "[step: 8996] loss: 0.6067907810211182\n",
      "[step: 8997] loss: 0.6059727668762207\n",
      "[step: 8998] loss: 0.6087246537208557\n",
      "[step: 8999] loss: 0.609946072101593\n",
      "[step: 9000] loss: 0.6075455546379089\n",
      "[step: 9001] loss: 0.6044072508811951\n",
      "[step: 9002] loss: 0.6033818125724792\n",
      "[step: 9003] loss: 0.604754626750946\n",
      "[step: 9004] loss: 0.6058263182640076\n",
      "[step: 9005] loss: 0.6048955917358398\n",
      "[step: 9006] loss: 0.6028410196304321\n",
      "[step: 9007] loss: 0.601445198059082\n",
      "[step: 9008] loss: 0.6016164422035217\n",
      "[step: 9009] loss: 0.6023634672164917\n",
      "[step: 9010] loss: 0.6024397611618042\n",
      "[step: 9011] loss: 0.6015129089355469\n",
      "[step: 9012] loss: 0.6002523303031921\n",
      "[step: 9013] loss: 0.5994853973388672\n",
      "[step: 9014] loss: 0.5994551777839661\n",
      "[step: 9015] loss: 0.5997421145439148\n",
      "[step: 9016] loss: 0.5997264981269836\n",
      "[step: 9017] loss: 0.5992223024368286\n",
      "[step: 9018] loss: 0.598429799079895\n",
      "[step: 9019] loss: 0.5977758765220642\n",
      "[step: 9020] loss: 0.597452700138092\n",
      "[step: 9021] loss: 0.5973994731903076\n",
      "[step: 9022] loss: 0.5974187850952148\n",
      "[step: 9023] loss: 0.5972393751144409\n",
      "[step: 9024] loss: 0.5968652963638306\n",
      "[step: 9025] loss: 0.5963724255561829\n",
      "[step: 9026] loss: 0.5958979725837708\n",
      "[step: 9027] loss: 0.5955700874328613\n",
      "[step: 9028] loss: 0.5953696370124817\n",
      "[step: 9029] loss: 0.5952444672584534\n",
      "[step: 9030] loss: 0.5951076745986938\n",
      "[step: 9031] loss: 0.5949040055274963\n",
      "[step: 9032] loss: 0.5946304798126221\n",
      "[step: 9033] loss: 0.5943091511726379\n",
      "[step: 9034] loss: 0.5939855575561523\n",
      "[step: 9035] loss: 0.5936884880065918\n",
      "[step: 9036] loss: 0.593429446220398\n",
      "[step: 9037] loss: 0.5932044386863708\n",
      "[step: 9038] loss: 0.5930015444755554\n",
      "[step: 9039] loss: 0.5928077697753906\n",
      "[step: 9040] loss: 0.5926083326339722\n",
      "[step: 9041] loss: 0.5924052000045776\n",
      "[step: 9042] loss: 0.5921986103057861\n",
      "[step: 9043] loss: 0.59199059009552\n",
      "[step: 9044] loss: 0.5917825102806091\n",
      "[step: 9045] loss: 0.5915750861167908\n",
      "[step: 9046] loss: 0.5913674831390381\n",
      "[step: 9047] loss: 0.5911586284637451\n",
      "[step: 9048] loss: 0.5909509658813477\n",
      "[step: 9049] loss: 0.5907421112060547\n",
      "[step: 9050] loss: 0.5905356407165527\n",
      "[step: 9051] loss: 0.5903350114822388\n",
      "[step: 9052] loss: 0.590142011642456\n",
      "[step: 9053] loss: 0.5899597406387329\n",
      "[step: 9054] loss: 0.5897929072380066\n",
      "[step: 9055] loss: 0.5896449089050293\n",
      "[step: 9056] loss: 0.5895273089408875\n",
      "[step: 9057] loss: 0.5894509553909302\n",
      "[step: 9058] loss: 0.5894390940666199\n",
      "[step: 9059] loss: 0.589535117149353\n",
      "[step: 9060] loss: 0.5898008942604065\n",
      "[step: 9061] loss: 0.5903708338737488\n",
      "[step: 9062] loss: 0.5914351940155029\n",
      "[step: 9063] loss: 0.5934040546417236\n",
      "[step: 9064] loss: 0.5969052910804749\n",
      "[step: 9065] loss: 0.6033100485801697\n",
      "[step: 9066] loss: 0.6146546602249146\n",
      "[step: 9067] loss: 0.6358298659324646\n",
      "[step: 9068] loss: 0.6733220219612122\n",
      "[step: 9069] loss: 0.7448945045471191\n",
      "[step: 9070] loss: 0.8672178983688354\n",
      "[step: 9071] loss: 1.0986554622650146\n",
      "[step: 9072] loss: 1.4356988668441772\n",
      "[step: 9073] loss: 1.9865107536315918\n",
      "[step: 9074] loss: 2.383516311645508\n",
      "[step: 9075] loss: 2.5925605297088623\n",
      "[step: 9076] loss: 1.8881418704986572\n",
      "[step: 9077] loss: 1.0127265453338623\n",
      "[step: 9078] loss: 0.668189287185669\n",
      "[step: 9079] loss: 1.0630202293395996\n",
      "[step: 9080] loss: 1.5045713186264038\n",
      "[step: 9081] loss: 1.2148517370224\n",
      "[step: 9082] loss: 0.7217277884483337\n",
      "[step: 9083] loss: 0.7038763761520386\n",
      "[step: 9084] loss: 1.0508055686950684\n",
      "[step: 9085] loss: 1.1747575998306274\n",
      "[step: 9086] loss: 0.840096116065979\n",
      "[step: 9087] loss: 0.6224706172943115\n",
      "[step: 9088] loss: 0.7759819030761719\n",
      "[step: 9089] loss: 0.9465603828430176\n",
      "[step: 9090] loss: 0.8612900972366333\n",
      "[step: 9091] loss: 0.6494287848472595\n",
      "[step: 9092] loss: 0.6588044166564941\n",
      "[step: 9093] loss: 0.8116263151168823\n",
      "[step: 9094] loss: 0.806078314781189\n",
      "[step: 9095] loss: 0.6681716442108154\n",
      "[step: 9096] loss: 0.6118371486663818\n",
      "[step: 9097] loss: 0.7047024965286255\n",
      "[step: 9098] loss: 0.7554661631584167\n",
      "[step: 9099] loss: 0.6769647598266602\n",
      "[step: 9100] loss: 0.6097205877304077\n",
      "[step: 9101] loss: 0.6445097923278809\n",
      "[step: 9102] loss: 0.7002699375152588\n",
      "[step: 9103] loss: 0.6782327890396118\n",
      "[step: 9104] loss: 0.617979884147644\n",
      "[step: 9105] loss: 0.6066938042640686\n",
      "[step: 9106] loss: 0.646658718585968\n",
      "[step: 9107] loss: 0.664563775062561\n",
      "[step: 9108] loss: 0.6359857320785522\n",
      "[step: 9109] loss: 0.6029973030090332\n",
      "[step: 9110] loss: 0.6055779457092285\n",
      "[step: 9111] loss: 0.6313024759292603\n",
      "[step: 9112] loss: 0.6376786231994629\n",
      "[step: 9113] loss: 0.6189471483230591\n",
      "[step: 9114] loss: 0.5991090536117554\n",
      "[step: 9115] loss: 0.6001116037368774\n",
      "[step: 9116] loss: 0.6151379942893982\n",
      "[step: 9117] loss: 0.62034010887146\n",
      "[step: 9118] loss: 0.610496997833252\n",
      "[step: 9119] loss: 0.5968998670578003\n",
      "[step: 9120] loss: 0.594393253326416\n",
      "[step: 9121] loss: 0.602463960647583\n",
      "[step: 9122] loss: 0.6085571646690369\n",
      "[step: 9123] loss: 0.606277346611023\n",
      "[step: 9124] loss: 0.5975830554962158\n",
      "[step: 9125] loss: 0.5919517874717712\n",
      "[step: 9126] loss: 0.5927835702896118\n",
      "[step: 9127] loss: 0.5972240567207336\n",
      "[step: 9128] loss: 0.600206732749939\n",
      "[step: 9129] loss: 0.5982103943824768\n",
      "[step: 9130] loss: 0.593876838684082\n",
      "[step: 9131] loss: 0.5900633335113525\n",
      "[step: 9132] loss: 0.5895105600357056\n",
      "[step: 9133] loss: 0.591251015663147\n",
      "[step: 9134] loss: 0.5932300090789795\n",
      "[step: 9135] loss: 0.5937732458114624\n",
      "[step: 9136] loss: 0.5921800136566162\n",
      "[step: 9137] loss: 0.589867353439331\n",
      "[step: 9138] loss: 0.5877988934516907\n",
      "[step: 9139] loss: 0.5871672630310059\n",
      "[step: 9140] loss: 0.5876018404960632\n",
      "[step: 9141] loss: 0.588517427444458\n",
      "[step: 9142] loss: 0.5891223549842834\n",
      "[step: 9143] loss: 0.5889273881912231\n",
      "[step: 9144] loss: 0.5881311297416687\n",
      "[step: 9145] loss: 0.5869277119636536\n",
      "[step: 9146] loss: 0.5858944654464722\n",
      "[step: 9147] loss: 0.5851589441299438\n",
      "[step: 9148] loss: 0.584884524345398\n",
      "[step: 9149] loss: 0.5849115252494812\n",
      "[step: 9150] loss: 0.5850814580917358\n",
      "[step: 9151] loss: 0.585243821144104\n",
      "[step: 9152] loss: 0.5852406024932861\n",
      "[step: 9153] loss: 0.5850961208343506\n",
      "[step: 9154] loss: 0.584780216217041\n",
      "[step: 9155] loss: 0.584389865398407\n",
      "[step: 9156] loss: 0.5839458703994751\n",
      "[step: 9157] loss: 0.5835187435150146\n",
      "[step: 9158] loss: 0.5831327438354492\n",
      "[step: 9159] loss: 0.5827934741973877\n",
      "[step: 9160] loss: 0.5825173854827881\n",
      "[step: 9161] loss: 0.5822746157646179\n",
      "[step: 9162] loss: 0.5820740461349487\n",
      "[step: 9163] loss: 0.5818881392478943\n",
      "[step: 9164] loss: 0.581717848777771\n",
      "[step: 9165] loss: 0.5815572142601013\n",
      "[step: 9166] loss: 0.5813995599746704\n",
      "[step: 9167] loss: 0.5812521576881409\n",
      "[step: 9168] loss: 0.5811107158660889\n",
      "[step: 9169] loss: 0.5809898972511292\n",
      "[step: 9170] loss: 0.5808929204940796\n",
      "[step: 9171] loss: 0.5808377265930176\n",
      "[step: 9172] loss: 0.5808524489402771\n",
      "[step: 9173] loss: 0.5809693932533264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9174] loss: 0.5812680721282959\n",
      "[step: 9175] loss: 0.5818567276000977\n",
      "[step: 9176] loss: 0.5829439163208008\n",
      "[step: 9177] loss: 0.5849195718765259\n",
      "[step: 9178] loss: 0.5884189009666443\n",
      "[step: 9179] loss: 0.5948472023010254\n",
      "[step: 9180] loss: 0.6062188744544983\n",
      "[step: 9181] loss: 0.6279040575027466\n",
      "[step: 9182] loss: 0.6661179065704346\n",
      "[step: 9183] loss: 0.7416736483573914\n",
      "[step: 9184] loss: 0.8679962754249573\n",
      "[step: 9185] loss: 1.119349479675293\n",
      "[step: 9186] loss: 1.4604332447052002\n",
      "[step: 9187] loss: 2.055992841720581\n",
      "[step: 9188] loss: 2.3661673069000244\n",
      "[step: 9189] loss: 2.534182071685791\n",
      "[step: 9190] loss: 1.7942821979522705\n",
      "[step: 9191] loss: 1.1213035583496094\n",
      "[step: 9192] loss: 1.0706061124801636\n",
      "[step: 9193] loss: 1.503730297088623\n",
      "[step: 9194] loss: 1.5267413854599\n",
      "[step: 9195] loss: 0.970035195350647\n",
      "[step: 9196] loss: 0.7279158234596252\n",
      "[step: 9197] loss: 1.0342934131622314\n",
      "[step: 9198] loss: 1.2389326095581055\n",
      "[step: 9199] loss: 0.9559256434440613\n",
      "[step: 9200] loss: 0.6792460680007935\n",
      "[step: 9201] loss: 0.8429210782051086\n",
      "[step: 9202] loss: 1.009635329246521\n",
      "[step: 9203] loss: 0.8167538046836853\n",
      "[step: 9204] loss: 0.6479898691177368\n",
      "[step: 9205] loss: 0.7687671780586243\n",
      "[step: 9206] loss: 0.8708603382110596\n",
      "[step: 9207] loss: 0.7055578827857971\n",
      "[step: 9208] loss: 0.6180592179298401\n",
      "[step: 9209] loss: 0.7463134527206421\n",
      "[step: 9210] loss: 0.7561358213424683\n",
      "[step: 9211] loss: 0.6262694001197815\n",
      "[step: 9212] loss: 0.6185508370399475\n",
      "[step: 9213] loss: 0.707751452922821\n",
      "[step: 9214] loss: 0.6820919513702393\n",
      "[step: 9215] loss: 0.5984307527542114\n",
      "[step: 9216] loss: 0.6316037178039551\n",
      "[step: 9217] loss: 0.6754465699195862\n",
      "[step: 9218] loss: 0.6214354038238525\n",
      "[step: 9219] loss: 0.5938934087753296\n",
      "[step: 9220] loss: 0.6351003646850586\n",
      "[step: 9221] loss: 0.6370065808296204\n",
      "[step: 9222] loss: 0.5945950746536255\n",
      "[step: 9223] loss: 0.5957684516906738\n",
      "[step: 9224] loss: 0.6242780089378357\n",
      "[step: 9225] loss: 0.6121711730957031\n",
      "[step: 9226] loss: 0.5871932506561279\n",
      "[step: 9227] loss: 0.5977656841278076\n",
      "[step: 9228] loss: 0.6110109090805054\n",
      "[step: 9229] loss: 0.594740629196167\n",
      "[step: 9230] loss: 0.5840586423873901\n",
      "[step: 9231] loss: 0.5965932607650757\n",
      "[step: 9232] loss: 0.5999738574028015\n",
      "[step: 9233] loss: 0.5866936445236206\n",
      "[step: 9234] loss: 0.5831058025360107\n",
      "[step: 9235] loss: 0.5919527411460876\n",
      "[step: 9236] loss: 0.591262936592102\n",
      "[step: 9237] loss: 0.5822573900222778\n",
      "[step: 9238] loss: 0.5828222632408142\n",
      "[step: 9239] loss: 0.5885442495346069\n",
      "[step: 9240] loss: 0.5859615802764893\n",
      "[step: 9241] loss: 0.5800267457962036\n",
      "[step: 9242] loss: 0.581131100654602\n",
      "[step: 9243] loss: 0.584358811378479\n",
      "[step: 9244] loss: 0.5818021297454834\n",
      "[step: 9245] loss: 0.5784003734588623\n",
      "[step: 9246] loss: 0.5798589587211609\n",
      "[step: 9247] loss: 0.581774115562439\n",
      "[step: 9248] loss: 0.579710841178894\n",
      "[step: 9249] loss: 0.5774842500686646\n",
      "[step: 9250] loss: 0.5784999132156372\n",
      "[step: 9251] loss: 0.5793888568878174\n",
      "[step: 9252] loss: 0.577731728553772\n",
      "[step: 9253] loss: 0.576281726360321\n",
      "[step: 9254] loss: 0.5769162178039551\n",
      "[step: 9255] loss: 0.5774232149124146\n",
      "[step: 9256] loss: 0.5762438774108887\n",
      "[step: 9257] loss: 0.575280487537384\n",
      "[step: 9258] loss: 0.5757009983062744\n",
      "[step: 9259] loss: 0.5759774446487427\n",
      "[step: 9260] loss: 0.5751821994781494\n",
      "[step: 9261] loss: 0.574486255645752\n",
      "[step: 9262] loss: 0.5746923685073853\n",
      "[step: 9263] loss: 0.574867844581604\n",
      "[step: 9264] loss: 0.574343204498291\n",
      "[step: 9265] loss: 0.5738377571105957\n",
      "[step: 9266] loss: 0.57395339012146\n",
      "[step: 9267] loss: 0.5741551518440247\n",
      "[step: 9268] loss: 0.5739548802375793\n",
      "[step: 9269] loss: 0.5737993121147156\n",
      "[step: 9270] loss: 0.5741984844207764\n",
      "[step: 9271] loss: 0.5749812126159668\n",
      "[step: 9272] loss: 0.5759856700897217\n",
      "[step: 9273] loss: 0.5777171850204468\n",
      "[step: 9274] loss: 0.5811923742294312\n",
      "[step: 9275] loss: 0.5873212814331055\n",
      "[step: 9276] loss: 0.5980780720710754\n",
      "[step: 9277] loss: 0.6168213486671448\n",
      "[step: 9278] loss: 0.6517454385757446\n",
      "[step: 9279] loss: 0.7122601270675659\n",
      "[step: 9280] loss: 0.8264393210411072\n",
      "[step: 9281] loss: 1.0102360248565674\n",
      "[step: 9282] loss: 1.3411147594451904\n",
      "[step: 9283] loss: 1.74972403049469\n",
      "[step: 9284] loss: 2.2950406074523926\n",
      "[step: 9285] loss: 2.3915181159973145\n",
      "[step: 9286] loss: 2.062406539916992\n",
      "[step: 9287] loss: 1.1628856658935547\n",
      "[step: 9288] loss: 0.6322857141494751\n",
      "[step: 9289] loss: 0.8415758609771729\n",
      "[step: 9290] loss: 1.3059468269348145\n",
      "[step: 9291] loss: 1.3509694337844849\n",
      "[step: 9292] loss: 0.8643790483474731\n",
      "[step: 9293] loss: 0.6079029440879822\n",
      "[step: 9294] loss: 0.8497856855392456\n",
      "[step: 9295] loss: 1.0956971645355225\n",
      "[step: 9296] loss: 0.97600919008255\n",
      "[step: 9297] loss: 0.6620206236839294\n",
      "[step: 9298] loss: 0.6333082914352417\n",
      "[step: 9299] loss: 0.8410189747810364\n",
      "[step: 9300] loss: 0.9040595889091492\n",
      "[step: 9301] loss: 0.7482914924621582\n",
      "[step: 9302] loss: 0.6013836860656738\n",
      "[step: 9303] loss: 0.673715353012085\n",
      "[step: 9304] loss: 0.7906812429428101\n",
      "[step: 9305] loss: 0.7459208965301514\n",
      "[step: 9306] loss: 0.6267154216766357\n",
      "[step: 9307] loss: 0.6030839681625366\n",
      "[step: 9308] loss: 0.6889547109603882\n",
      "[step: 9309] loss: 0.7208693623542786\n",
      "[step: 9310] loss: 0.6504424810409546\n",
      "[step: 9311] loss: 0.5916017889976501\n",
      "[step: 9312] loss: 0.6186044216156006\n",
      "[step: 9313] loss: 0.6713470220565796\n",
      "[step: 9314] loss: 0.6615809202194214\n",
      "[step: 9315] loss: 0.6079069375991821\n",
      "[step: 9316] loss: 0.584434986114502\n",
      "[step: 9317] loss: 0.6133634448051453\n",
      "[step: 9318] loss: 0.6408365368843079\n",
      "[step: 9319] loss: 0.6278849840164185\n",
      "[step: 9320] loss: 0.59467613697052\n",
      "[step: 9321] loss: 0.5815462470054626\n",
      "[step: 9322] loss: 0.598983883857727\n",
      "[step: 9323] loss: 0.6157857179641724\n",
      "[step: 9324] loss: 0.6112300157546997\n",
      "[step: 9325] loss: 0.5919238924980164\n",
      "[step: 9326] loss: 0.5790218710899353\n",
      "[step: 9327] loss: 0.5847939252853394\n",
      "[step: 9328] loss: 0.5965182185173035\n",
      "[step: 9329] loss: 0.599877119064331\n",
      "[step: 9330] loss: 0.590920627117157\n",
      "[step: 9331] loss: 0.5792526006698608\n",
      "[step: 9332] loss: 0.5768000483512878\n",
      "[step: 9333] loss: 0.5821649432182312\n",
      "[step: 9334] loss: 0.5884793400764465\n",
      "[step: 9335] loss: 0.588294506072998\n",
      "[step: 9336] loss: 0.5822331309318542\n",
      "[step: 9337] loss: 0.5764255523681641\n",
      "[step: 9338] loss: 0.5741506218910217\n",
      "[step: 9339] loss: 0.5764303207397461\n",
      "[step: 9340] loss: 0.5796232223510742\n",
      "[step: 9341] loss: 0.5810104012489319\n",
      "[step: 9342] loss: 0.5795626044273376\n",
      "[step: 9343] loss: 0.5761400461196899\n",
      "[step: 9344] loss: 0.5732883214950562\n",
      "[step: 9345] loss: 0.5719817280769348\n",
      "[step: 9346] loss: 0.572716236114502\n",
      "[step: 9347] loss: 0.5741545557975769\n",
      "[step: 9348] loss: 0.5751327276229858\n",
      "[step: 9349] loss: 0.5751011371612549\n",
      "[step: 9350] loss: 0.5739253759384155\n",
      "[step: 9351] loss: 0.5723835229873657\n",
      "[step: 9352] loss: 0.5708648562431335\n",
      "[step: 9353] loss: 0.5700141787528992\n",
      "[step: 9354] loss: 0.5698548555374146\n",
      "[step: 9355] loss: 0.5701426863670349\n",
      "[step: 9356] loss: 0.5706210732460022\n",
      "[step: 9357] loss: 0.5709052681922913\n",
      "[step: 9358] loss: 0.5709811449050903\n",
      "[step: 9359] loss: 0.5707268714904785\n",
      "[step: 9360] loss: 0.5702328681945801\n",
      "[step: 9361] loss: 0.5696038007736206\n",
      "[step: 9362] loss: 0.5689290761947632\n",
      "[step: 9363] loss: 0.5683499574661255\n",
      "[step: 9364] loss: 0.5678276419639587\n",
      "[step: 9365] loss: 0.5674242377281189\n",
      "[step: 9366] loss: 0.5671062469482422\n",
      "[step: 9367] loss: 0.5668857097625732\n",
      "[step: 9368] loss: 0.5667312145233154\n",
      "[step: 9369] loss: 0.5666057467460632\n",
      "[step: 9370] loss: 0.5665264129638672\n",
      "[step: 9371] loss: 0.5664675831794739\n",
      "[step: 9372] loss: 0.5664572715759277\n",
      "[step: 9373] loss: 0.5664843320846558\n",
      "[step: 9374] loss: 0.5665781497955322\n",
      "[step: 9375] loss: 0.5667780041694641\n",
      "[step: 9376] loss: 0.5671429634094238\n",
      "[step: 9377] loss: 0.5677666068077087\n",
      "[step: 9378] loss: 0.5688047409057617\n",
      "[step: 9379] loss: 0.5704826712608337\n",
      "[step: 9380] loss: 0.5732969641685486\n",
      "[step: 9381] loss: 0.577838659286499\n",
      "[step: 9382] loss: 0.5855921506881714\n",
      "[step: 9383] loss: 0.5981752872467041\n",
      "[step: 9384] loss: 0.6203328967094421\n",
      "[step: 9385] loss: 0.6563968062400818\n",
      "[step: 9386] loss: 0.7214834094047546\n",
      "[step: 9387] loss: 0.8228927850723267\n",
      "[step: 9388] loss: 1.0042238235473633\n",
      "[step: 9389] loss: 1.2440202236175537\n",
      "[step: 9390] loss: 1.6226441860198975\n",
      "[step: 9391] loss: 1.8815455436706543\n",
      "[step: 9392] loss: 2.058955669403076\n",
      "[step: 9393] loss: 1.6517820358276367\n",
      "[step: 9394] loss: 1.06699800491333\n",
      "[step: 9395] loss: 0.6349533200263977\n",
      "[step: 9396] loss: 0.6965956687927246\n",
      "[step: 9397] loss: 1.045131802558899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9398] loss: 1.1766334772109985\n",
      "[step: 9399] loss: 0.9634092450141907\n",
      "[step: 9400] loss: 0.6478879451751709\n",
      "[step: 9401] loss: 0.6148038506507874\n",
      "[step: 9402] loss: 0.8236967325210571\n",
      "[step: 9403] loss: 0.954246461391449\n",
      "[step: 9404] loss: 0.8569086790084839\n",
      "[step: 9405] loss: 0.6491775512695312\n",
      "[step: 9406] loss: 0.5887237191200256\n",
      "[step: 9407] loss: 0.6931015849113464\n",
      "[step: 9408] loss: 0.7890018224716187\n",
      "[step: 9409] loss: 0.7490699291229248\n",
      "[step: 9410] loss: 0.6267533898353577\n",
      "[step: 9411] loss: 0.5834608674049377\n",
      "[step: 9412] loss: 0.6387061476707458\n",
      "[step: 9413] loss: 0.6962896585464478\n",
      "[step: 9414] loss: 0.6781880259513855\n",
      "[step: 9415] loss: 0.609027624130249\n",
      "[step: 9416] loss: 0.5791321992874146\n",
      "[step: 9417] loss: 0.6065092086791992\n",
      "[step: 9418] loss: 0.6432356834411621\n",
      "[step: 9419] loss: 0.641323447227478\n",
      "[step: 9420] loss: 0.603064775466919\n",
      "[step: 9421] loss: 0.5749159455299377\n",
      "[step: 9422] loss: 0.5799968242645264\n",
      "[step: 9423] loss: 0.6043304800987244\n",
      "[step: 9424] loss: 0.6166729927062988\n",
      "[step: 9425] loss: 0.6031141877174377\n",
      "[step: 9426] loss: 0.5806863307952881\n",
      "[step: 9427] loss: 0.5694857835769653\n",
      "[step: 9428] loss: 0.5762585401535034\n",
      "[step: 9429] loss: 0.5892703533172607\n",
      "[step: 9430] loss: 0.594179630279541\n",
      "[step: 9431] loss: 0.5880520343780518\n",
      "[step: 9432] loss: 0.5764549374580383\n",
      "[step: 9433] loss: 0.5687252283096313\n",
      "[step: 9434] loss: 0.5687932968139648\n",
      "[step: 9435] loss: 0.5740921497344971\n",
      "[step: 9436] loss: 0.5792747735977173\n",
      "[step: 9437] loss: 0.5801494121551514\n",
      "[step: 9438] loss: 0.5761966109275818\n",
      "[step: 9439] loss: 0.5701518058776855\n",
      "[step: 9440] loss: 0.5658167004585266\n",
      "[step: 9441] loss: 0.5651438236236572\n",
      "[step: 9442] loss: 0.5674630403518677\n",
      "[step: 9443] loss: 0.5703600645065308\n",
      "[step: 9444] loss: 0.5719181299209595\n",
      "[step: 9445] loss: 0.5715146660804749\n",
      "[step: 9446] loss: 0.5695852041244507\n",
      "[step: 9447] loss: 0.5670374631881714\n",
      "[step: 9448] loss: 0.5646251440048218\n",
      "[step: 9449] loss: 0.5630728602409363\n",
      "[step: 9450] loss: 0.5625702142715454\n",
      "[step: 9451] loss: 0.5630418062210083\n",
      "[step: 9452] loss: 0.5639314651489258\n",
      "[step: 9453] loss: 0.5647749304771423\n",
      "[step: 9454] loss: 0.5653183460235596\n",
      "[step: 9455] loss: 0.5654434561729431\n",
      "[step: 9456] loss: 0.5652725100517273\n",
      "[step: 9457] loss: 0.5647621154785156\n",
      "[step: 9458] loss: 0.5640603303909302\n",
      "[step: 9459] loss: 0.5632268190383911\n",
      "[step: 9460] loss: 0.5624552965164185\n",
      "[step: 9461] loss: 0.5617785453796387\n",
      "[step: 9462] loss: 0.5612064003944397\n",
      "[step: 9463] loss: 0.5607007145881653\n",
      "[step: 9464] loss: 0.5602571964263916\n",
      "[step: 9465] loss: 0.5598998069763184\n",
      "[step: 9466] loss: 0.5596206188201904\n",
      "[step: 9467] loss: 0.5593945384025574\n",
      "[step: 9468] loss: 0.5592012405395508\n",
      "[step: 9469] loss: 0.5590430498123169\n",
      "[step: 9470] loss: 0.5589429140090942\n",
      "[step: 9471] loss: 0.5589252710342407\n",
      "[step: 9472] loss: 0.5590389966964722\n",
      "[step: 9473] loss: 0.5593273043632507\n",
      "[step: 9474] loss: 0.559924304485321\n",
      "[step: 9475] loss: 0.5610301494598389\n",
      "[step: 9476] loss: 0.563075065612793\n",
      "[step: 9477] loss: 0.5666693449020386\n",
      "[step: 9478] loss: 0.5732185244560242\n",
      "[step: 9479] loss: 0.5847077369689941\n",
      "[step: 9480] loss: 0.6061554551124573\n",
      "[step: 9481] loss: 0.6437231302261353\n",
      "[step: 9482] loss: 0.7158353328704834\n",
      "[step: 9483] loss: 0.8369110822677612\n",
      "[step: 9484] loss: 1.06807279586792\n",
      "[step: 9485] loss: 1.3927857875823975\n",
      "[step: 9486] loss: 1.9302303791046143\n",
      "[step: 9487] loss: 2.2745251655578613\n",
      "[step: 9488] loss: 2.4451050758361816\n",
      "[step: 9489] loss: 1.737715721130371\n",
      "[step: 9490] loss: 0.9218568205833435\n",
      "[step: 9491] loss: 0.6286426782608032\n",
      "[step: 9492] loss: 1.0008307695388794\n",
      "[step: 9493] loss: 1.404089093208313\n",
      "[step: 9494] loss: 1.1694846153259277\n",
      "[step: 9495] loss: 0.7143170237541199\n",
      "[step: 9496] loss: 0.6548497676849365\n",
      "[step: 9497] loss: 0.952677845954895\n",
      "[step: 9498] loss: 1.108328938484192\n",
      "[step: 9499] loss: 0.8489985466003418\n",
      "[step: 9500] loss: 0.6010147333145142\n",
      "[step: 9501] loss: 0.6855607032775879\n",
      "[step: 9502] loss: 0.8761135935783386\n",
      "[step: 9503] loss: 0.867811918258667\n",
      "[step: 9504] loss: 0.6590288877487183\n",
      "[step: 9505] loss: 0.5892990827560425\n",
      "[step: 9506] loss: 0.7203899621963501\n",
      "[step: 9507] loss: 0.7803635597229004\n",
      "[step: 9508] loss: 0.6841995120048523\n",
      "[step: 9509] loss: 0.5791009664535522\n",
      "[step: 9510] loss: 0.6274973154067993\n",
      "[step: 9511] loss: 0.7128152847290039\n",
      "[step: 9512] loss: 0.6818310022354126\n",
      "[step: 9513] loss: 0.5971479415893555\n",
      "[step: 9514] loss: 0.5802218914031982\n",
      "[step: 9515] loss: 0.6420965790748596\n",
      "[step: 9516] loss: 0.6670878529548645\n",
      "[step: 9517] loss: 0.6211203336715698\n",
      "[step: 9518] loss: 0.573457658290863\n",
      "[step: 9519] loss: 0.5838145017623901\n",
      "[step: 9520] loss: 0.6233400106430054\n",
      "[step: 9521] loss: 0.6280159950256348\n",
      "[step: 9522] loss: 0.5960820913314819\n",
      "[step: 9523] loss: 0.5687716007232666\n",
      "[step: 9524] loss: 0.5774452686309814\n",
      "[step: 9525] loss: 0.5996031761169434\n",
      "[step: 9526] loss: 0.6043004989624023\n",
      "[step: 9527] loss: 0.5865429639816284\n",
      "[step: 9528] loss: 0.5675939321517944\n",
      "[step: 9529] loss: 0.5687482357025146\n",
      "[step: 9530] loss: 0.5805240869522095\n",
      "[step: 9531] loss: 0.5877081155776978\n",
      "[step: 9532] loss: 0.5809783339500427\n",
      "[step: 9533] loss: 0.5688124895095825\n",
      "[step: 9534] loss: 0.5637438297271729\n",
      "[step: 9535] loss: 0.5668033957481384\n",
      "[step: 9536] loss: 0.573583722114563\n",
      "[step: 9537] loss: 0.5750371217727661\n",
      "[step: 9538] loss: 0.5708562135696411\n",
      "[step: 9539] loss: 0.5644418001174927\n",
      "[step: 9540] loss: 0.560512900352478\n",
      "[step: 9541] loss: 0.5616278052330017\n",
      "[step: 9542] loss: 0.5647660493850708\n",
      "[step: 9543] loss: 0.5673891305923462\n",
      "[step: 9544] loss: 0.5664590001106262\n",
      "[step: 9545] loss: 0.5632268190383911\n",
      "[step: 9546] loss: 0.559683084487915\n",
      "[step: 9547] loss: 0.5578931570053101\n",
      "[step: 9548] loss: 0.5585458278656006\n",
      "[step: 9549] loss: 0.5600160360336304\n",
      "[step: 9550] loss: 0.5612784028053284\n",
      "[step: 9551] loss: 0.5610442161560059\n",
      "[step: 9552] loss: 0.5599139928817749\n",
      "[step: 9553] loss: 0.5583306550979614\n",
      "[step: 9554] loss: 0.5569376349449158\n",
      "[step: 9555] loss: 0.5561068654060364\n",
      "[step: 9556] loss: 0.5557451844215393\n",
      "[step: 9557] loss: 0.5559938549995422\n",
      "[step: 9558] loss: 0.5564028024673462\n",
      "[step: 9559] loss: 0.5568456053733826\n",
      "[step: 9560] loss: 0.5569226741790771\n",
      "[step: 9561] loss: 0.5566264390945435\n",
      "[step: 9562] loss: 0.5560410618782043\n",
      "[step: 9563] loss: 0.5553299784660339\n",
      "[step: 9564] loss: 0.5546935796737671\n",
      "[step: 9565] loss: 0.554092288017273\n",
      "[step: 9566] loss: 0.5536055564880371\n",
      "[step: 9567] loss: 0.5531768202781677\n",
      "[step: 9568] loss: 0.5528630018234253\n",
      "[step: 9569] loss: 0.5526582598686218\n",
      "[step: 9570] loss: 0.552538275718689\n",
      "[step: 9571] loss: 0.5524811148643494\n",
      "[step: 9572] loss: 0.5524239540100098\n",
      "[step: 9573] loss: 0.5523964166641235\n",
      "[step: 9574] loss: 0.5523937940597534\n",
      "[step: 9575] loss: 0.5524660348892212\n",
      "[step: 9576] loss: 0.5526366829872131\n",
      "[step: 9577] loss: 0.5529324412345886\n",
      "[step: 9578] loss: 0.5534343123435974\n",
      "[step: 9579] loss: 0.5542274713516235\n",
      "[step: 9580] loss: 0.5555611848831177\n",
      "[step: 9581] loss: 0.5576874017715454\n",
      "[step: 9582] loss: 0.5612611770629883\n",
      "[step: 9583] loss: 0.5669335126876831\n",
      "[step: 9584] loss: 0.5766633749008179\n",
      "[step: 9585] loss: 0.5923134684562683\n",
      "[step: 9586] loss: 0.620104968547821\n",
      "[step: 9587] loss: 0.6644102334976196\n",
      "[step: 9588] loss: 0.7446291446685791\n",
      "[step: 9589] loss: 0.8646901249885559\n",
      "[step: 9590] loss: 1.0782053470611572\n",
      "[step: 9591] loss: 1.3319287300109863\n",
      "[step: 9592] loss: 1.7091723680496216\n",
      "[step: 9593] loss: 1.8559565544128418\n",
      "[step: 9594] loss: 1.848705530166626\n",
      "[step: 9595] loss: 1.3270659446716309\n",
      "[step: 9596] loss: 0.7976003289222717\n",
      "[step: 9597] loss: 0.5963999629020691\n",
      "[step: 9598] loss: 0.8009152412414551\n",
      "[step: 9599] loss: 1.0960276126861572\n",
      "[step: 9600] loss: 1.066880464553833\n",
      "[step: 9601] loss: 0.804078221321106\n",
      "[step: 9602] loss: 0.5946805477142334\n",
      "[step: 9603] loss: 0.6498499512672424\n",
      "[step: 9604] loss: 0.8459614515304565\n",
      "[step: 9605] loss: 0.9054917693138123\n",
      "[step: 9606] loss: 0.7857144474983215\n",
      "[step: 9607] loss: 0.612305223941803\n",
      "[step: 9608] loss: 0.578607439994812\n",
      "[step: 9609] loss: 0.6815799474716187\n",
      "[step: 9610] loss: 0.7554103136062622\n",
      "[step: 9611] loss: 0.7112169861793518\n",
      "[step: 9612] loss: 0.6043825149536133\n",
      "[step: 9613] loss: 0.565637469291687\n",
      "[step: 9614] loss: 0.6166554689407349\n",
      "[step: 9615] loss: 0.6700176000595093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9616] loss: 0.6573196649551392\n",
      "[step: 9617] loss: 0.5953351259231567\n",
      "[step: 9618] loss: 0.560896098613739\n",
      "[step: 9619] loss: 0.580822765827179\n",
      "[step: 9620] loss: 0.6176297068595886\n",
      "[step: 9621] loss: 0.6258547902107239\n",
      "[step: 9622] loss: 0.5961662530899048\n",
      "[step: 9623] loss: 0.5642507076263428\n",
      "[step: 9624] loss: 0.5578452348709106\n",
      "[step: 9625] loss: 0.5752526521682739\n",
      "[step: 9626] loss: 0.5939469933509827\n",
      "[step: 9627] loss: 0.59371018409729\n",
      "[step: 9628] loss: 0.5777368545532227\n",
      "[step: 9629] loss: 0.5600507259368896\n",
      "[step: 9630] loss: 0.5540930032730103\n",
      "[step: 9631] loss: 0.5610803365707397\n",
      "[step: 9632] loss: 0.5716452598571777\n",
      "[step: 9633] loss: 0.5760761499404907\n",
      "[step: 9634] loss: 0.5708631873130798\n",
      "[step: 9635] loss: 0.5612872242927551\n",
      "[step: 9636] loss: 0.5536233186721802\n",
      "[step: 9637] loss: 0.5521158576011658\n",
      "[step: 9638] loss: 0.555864155292511\n",
      "[step: 9639] loss: 0.5607281923294067\n",
      "[step: 9640] loss: 0.5633448362350464\n",
      "[step: 9641] loss: 0.5620094537734985\n",
      "[step: 9642] loss: 0.5580638647079468\n",
      "[step: 9643] loss: 0.5535773634910583\n",
      "[step: 9644] loss: 0.5505253672599792\n",
      "[step: 9645] loss: 0.5496197938919067\n",
      "[step: 9646] loss: 0.5506260991096497\n",
      "[step: 9647] loss: 0.5525206327438354\n",
      "[step: 9648] loss: 0.5542234182357788\n",
      "[step: 9649] loss: 0.5551977157592773\n",
      "[step: 9650] loss: 0.555020809173584\n",
      "[step: 9651] loss: 0.5540003776550293\n",
      "[step: 9652] loss: 0.5523848533630371\n",
      "[step: 9653] loss: 0.5506547093391418\n",
      "[step: 9654] loss: 0.5490897297859192\n",
      "[step: 9655] loss: 0.5479159951210022\n",
      "[step: 9656] loss: 0.5471422672271729\n",
      "[step: 9657] loss: 0.5467403531074524\n",
      "[step: 9658] loss: 0.546633780002594\n",
      "[step: 9659] loss: 0.546718955039978\n",
      "[step: 9660] loss: 0.5469586253166199\n",
      "[step: 9661] loss: 0.547308087348938\n",
      "[step: 9662] loss: 0.5477566719055176\n",
      "[step: 9663] loss: 0.5483212471008301\n",
      "[step: 9664] loss: 0.549071192741394\n",
      "[step: 9665] loss: 0.5500684380531311\n",
      "[step: 9666] loss: 0.5515449047088623\n",
      "[step: 9667] loss: 0.5536563396453857\n",
      "[step: 9668] loss: 0.556955873966217\n",
      "[step: 9669] loss: 0.5618102550506592\n",
      "[step: 9670] loss: 0.5696370601654053\n",
      "[step: 9671] loss: 0.5813018679618835\n",
      "[step: 9672] loss: 0.6008455753326416\n",
      "[step: 9673] loss: 0.6300992369651794\n",
      "[step: 9674] loss: 0.6804015040397644\n",
      "[step: 9675] loss: 0.7527796626091003\n",
      "[step: 9676] loss: 0.8768921494483948\n",
      "[step: 9677] loss: 1.0318228006362915\n",
      "[step: 9678] loss: 1.274775505065918\n",
      "[step: 9679] loss: 1.4582613706588745\n",
      "[step: 9680] loss: 1.6368322372436523\n",
      "[step: 9681] loss: 1.4814233779907227\n",
      "[step: 9682] loss: 1.1712169647216797\n",
      "[step: 9683] loss: 0.7689675092697144\n",
      "[step: 9684] loss: 0.5823055505752563\n",
      "[step: 9685] loss: 0.6760916709899902\n",
      "[step: 9686] loss: 0.8773576021194458\n",
      "[step: 9687] loss: 0.984926700592041\n",
      "[step: 9688] loss: 0.8572825193405151\n",
      "[step: 9689] loss: 0.6641482710838318\n",
      "[step: 9690] loss: 0.5668718814849854\n",
      "[step: 9691] loss: 0.6233513355255127\n",
      "[step: 9692] loss: 0.748052179813385\n",
      "[step: 9693] loss: 0.7962877750396729\n",
      "[step: 9694] loss: 0.7421680688858032\n",
      "[step: 9695] loss: 0.6236035227775574\n",
      "[step: 9696] loss: 0.5586050748825073\n",
      "[step: 9697] loss: 0.5853598117828369\n",
      "[step: 9698] loss: 0.6505725383758545\n",
      "[step: 9699] loss: 0.6839303970336914\n",
      "[step: 9700] loss: 0.6457460522651672\n",
      "[step: 9701] loss: 0.5838220715522766\n",
      "[step: 9702] loss: 0.5528175234794617\n",
      "[step: 9703] loss: 0.5718978643417358\n",
      "[step: 9704] loss: 0.6102752685546875\n",
      "[step: 9705] loss: 0.6252448558807373\n",
      "[step: 9706] loss: 0.6087290644645691\n",
      "[step: 9707] loss: 0.5735699534416199\n",
      "[step: 9708] loss: 0.5508739948272705\n",
      "[step: 9709] loss: 0.553152859210968\n",
      "[step: 9710] loss: 0.5714736580848694\n",
      "[step: 9711] loss: 0.5886808633804321\n",
      "[step: 9712] loss: 0.5903317332267761\n",
      "[step: 9713] loss: 0.5783988833427429\n",
      "[step: 9714] loss: 0.5605080127716064\n",
      "[step: 9715] loss: 0.5485587120056152\n",
      "[step: 9716] loss: 0.5469252467155457\n",
      "[step: 9717] loss: 0.5534601211547852\n",
      "[step: 9718] loss: 0.5624701380729675\n",
      "[step: 9719] loss: 0.5676285028457642\n",
      "[step: 9720] loss: 0.5670545101165771\n",
      "[step: 9721] loss: 0.5610572099685669\n",
      "[step: 9722] loss: 0.5535496473312378\n",
      "[step: 9723] loss: 0.5471912622451782\n",
      "[step: 9724] loss: 0.544037401676178\n",
      "[step: 9725] loss: 0.5442925691604614\n",
      "[step: 9726] loss: 0.5467954874038696\n",
      "[step: 9727] loss: 0.5501185059547424\n",
      "[step: 9728] loss: 0.5528830885887146\n",
      "[step: 9729] loss: 0.5545181632041931\n",
      "[step: 9730] loss: 0.5546230673789978\n",
      "[step: 9731] loss: 0.5537375211715698\n",
      "[step: 9732] loss: 0.55191969871521\n",
      "[step: 9733] loss: 0.5498606562614441\n",
      "[step: 9734] loss: 0.5477185249328613\n",
      "[step: 9735] loss: 0.5457949638366699\n",
      "[step: 9736] loss: 0.5441683530807495\n",
      "[step: 9737] loss: 0.5429010987281799\n",
      "[step: 9738] loss: 0.541898250579834\n",
      "[step: 9739] loss: 0.5411608219146729\n",
      "[step: 9740] loss: 0.5406166315078735\n",
      "[step: 9741] loss: 0.5401841402053833\n",
      "[step: 9742] loss: 0.5398445129394531\n",
      "[step: 9743] loss: 0.5395649671554565\n",
      "[step: 9744] loss: 0.5393080711364746\n",
      "[step: 9745] loss: 0.5390793085098267\n",
      "[step: 9746] loss: 0.5388709306716919\n",
      "[step: 9747] loss: 0.5386654138565063\n",
      "[step: 9748] loss: 0.5384723544120789\n",
      "[step: 9749] loss: 0.5382857322692871\n",
      "[step: 9750] loss: 0.5380969047546387\n",
      "[step: 9751] loss: 0.5379127264022827\n",
      "[step: 9752] loss: 0.5377311706542969\n",
      "[step: 9753] loss: 0.5375484228134155\n",
      "[step: 9754] loss: 0.5373648405075073\n",
      "[step: 9755] loss: 0.5371872186660767\n",
      "[step: 9756] loss: 0.5370087623596191\n",
      "[step: 9757] loss: 0.5368301868438721\n",
      "[step: 9758] loss: 0.5366564393043518\n",
      "[step: 9759] loss: 0.5364854335784912\n",
      "[step: 9760] loss: 0.5363146662712097\n",
      "[step: 9761] loss: 0.5361467003822327\n",
      "[step: 9762] loss: 0.5359815359115601\n",
      "[step: 9763] loss: 0.5358160734176636\n",
      "[step: 9764] loss: 0.535656213760376\n",
      "[step: 9765] loss: 0.5354987382888794\n",
      "[step: 9766] loss: 0.5353522896766663\n",
      "[step: 9767] loss: 0.535228431224823\n",
      "[step: 9768] loss: 0.5351523756980896\n",
      "[step: 9769] loss: 0.5351865291595459\n",
      "[step: 9770] loss: 0.5354838371276855\n",
      "[step: 9771] loss: 0.5364084243774414\n",
      "[step: 9772] loss: 0.538814902305603\n",
      "[step: 9773] loss: 0.5449067950248718\n",
      "[step: 9774] loss: 0.5598520636558533\n",
      "[step: 9775] loss: 0.5975668430328369\n",
      "[step: 9776] loss: 0.688792884349823\n",
      "[step: 9777] loss: 0.9223973751068115\n",
      "[step: 9778] loss: 1.4407662153244019\n",
      "[step: 9779] loss: 2.664011001586914\n",
      "[step: 9780] loss: 4.244890213012695\n",
      "[step: 9781] loss: 5.846653461456299\n",
      "[step: 9782] loss: 3.7593724727630615\n",
      "[step: 9783] loss: 1.3707873821258545\n",
      "[step: 9784] loss: 2.079944372177124\n",
      "[step: 9785] loss: 3.0159308910369873\n",
      "[step: 9786] loss: 1.6531639099121094\n",
      "[step: 9787] loss: 1.0769151449203491\n",
      "[step: 9788] loss: 2.330639123916626\n",
      "[step: 9789] loss: 1.5406827926635742\n",
      "[step: 9790] loss: 0.7644896507263184\n",
      "[step: 9791] loss: 1.7960363626480103\n",
      "[step: 9792] loss: 1.4743722677230835\n",
      "[step: 9793] loss: 0.7467504143714905\n",
      "[step: 9794] loss: 1.472350835800171\n",
      "[step: 9795] loss: 1.4269506931304932\n",
      "[step: 9796] loss: 0.719645082950592\n",
      "[step: 9797] loss: 1.3861122131347656\n",
      "[step: 9798] loss: 1.2032670974731445\n",
      "[step: 9799] loss: 0.7089369297027588\n",
      "[step: 9800] loss: 1.31666898727417\n",
      "[step: 9801] loss: 0.9264400005340576\n",
      "[step: 9802] loss: 0.801658034324646\n",
      "[step: 9803] loss: 1.1758766174316406\n",
      "[step: 9804] loss: 0.782945990562439\n",
      "[step: 9805] loss: 0.7876816391944885\n",
      "[step: 9806] loss: 1.0027072429656982\n",
      "[step: 9807] loss: 0.6861451864242554\n",
      "[step: 9808] loss: 0.7845416069030762\n",
      "[step: 9809] loss: 0.8577223420143127\n",
      "[step: 9810] loss: 0.635758638381958\n",
      "[step: 9811] loss: 0.7724202275276184\n",
      "[step: 9812] loss: 0.7215101718902588\n",
      "[step: 9813] loss: 0.6356717348098755\n",
      "[step: 9814] loss: 0.7153233289718628\n",
      "[step: 9815] loss: 0.6764311790466309\n",
      "[step: 9816] loss: 0.6107175350189209\n",
      "[step: 9817] loss: 0.6781139969825745\n",
      "[step: 9818] loss: 0.6511597037315369\n",
      "[step: 9819] loss: 0.5883404612541199\n",
      "[step: 9820] loss: 0.6571169495582581\n",
      "[step: 9821] loss: 0.6224933862686157\n",
      "[step: 9822] loss: 0.5881861448287964\n",
      "[step: 9823] loss: 0.6265103816986084\n",
      "[step: 9824] loss: 0.6132656931877136\n",
      "[step: 9825] loss: 0.5771435499191284\n",
      "[step: 9826] loss: 0.6112658977508545\n",
      "[step: 9827] loss: 0.5988377332687378\n",
      "[step: 9828] loss: 0.5738006830215454\n",
      "[step: 9829] loss: 0.5964869856834412\n",
      "[step: 9830] loss: 0.5879607796669006\n",
      "[step: 9831] loss: 0.5743676424026489\n",
      "[step: 9832] loss: 0.5794333219528198\n",
      "[step: 9833] loss: 0.5853707194328308\n",
      "[step: 9834] loss: 0.569510817527771\n",
      "[step: 9835] loss: 0.5698248147964478\n",
      "[step: 9836] loss: 0.5779150724411011\n",
      "[step: 9837] loss: 0.5694842338562012\n",
      "[step: 9838] loss: 0.5619714260101318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 9839] loss: 0.570468544960022\n",
      "[step: 9840] loss: 0.5687870979309082\n",
      "[step: 9841] loss: 0.5590540766716003\n",
      "[step: 9842] loss: 0.5634669065475464\n",
      "[step: 9843] loss: 0.56431645154953\n",
      "[step: 9844] loss: 0.5601334571838379\n",
      "[step: 9845] loss: 0.5571587085723877\n",
      "[step: 9846] loss: 0.5599157810211182\n",
      "[step: 9847] loss: 0.5588443875312805\n",
      "[step: 9848] loss: 0.5550299882888794\n",
      "[step: 9849] loss: 0.5556666851043701\n",
      "[step: 9850] loss: 0.5563287734985352\n",
      "[step: 9851] loss: 0.5538739562034607\n",
      "[step: 9852] loss: 0.5528254508972168\n",
      "[step: 9853] loss: 0.5537058115005493\n",
      "[step: 9854] loss: 0.5521097779273987\n",
      "[step: 9855] loss: 0.5512374639511108\n",
      "[step: 9856] loss: 0.5508812665939331\n",
      "[step: 9857] loss: 0.5507606863975525\n",
      "[step: 9858] loss: 0.5495458245277405\n",
      "[step: 9859] loss: 0.5487160682678223\n",
      "[step: 9860] loss: 0.5491049885749817\n",
      "[step: 9861] loss: 0.5483695268630981\n",
      "[step: 9862] loss: 0.5472426414489746\n",
      "[step: 9863] loss: 0.5470129251480103\n",
      "[step: 9864] loss: 0.5471649169921875\n",
      "[step: 9865] loss: 0.546291708946228\n",
      "[step: 9866] loss: 0.5455388426780701\n",
      "[step: 9867] loss: 0.5453113317489624\n",
      "[step: 9868] loss: 0.5452805757522583\n",
      "[step: 9869] loss: 0.5446751117706299\n",
      "[step: 9870] loss: 0.5439231395721436\n",
      "[step: 9871] loss: 0.5438641309738159\n",
      "[step: 9872] loss: 0.5436369180679321\n",
      "[step: 9873] loss: 0.5431153774261475\n",
      "[step: 9874] loss: 0.542626678943634\n",
      "[step: 9875] loss: 0.5424153208732605\n",
      "[step: 9876] loss: 0.5421672463417053\n",
      "[step: 9877] loss: 0.5417380928993225\n",
      "[step: 9878] loss: 0.5413273572921753\n",
      "[step: 9879] loss: 0.5411208868026733\n",
      "[step: 9880] loss: 0.5408765077590942\n",
      "[step: 9881] loss: 0.5404436588287354\n",
      "[step: 9882] loss: 0.5401409864425659\n",
      "[step: 9883] loss: 0.5399038195610046\n",
      "[step: 9884] loss: 0.5396581292152405\n",
      "[step: 9885] loss: 0.5393142700195312\n",
      "[step: 9886] loss: 0.5389823913574219\n",
      "[step: 9887] loss: 0.5387665033340454\n",
      "[step: 9888] loss: 0.538527250289917\n",
      "[step: 9889] loss: 0.5382280349731445\n",
      "[step: 9890] loss: 0.5379318594932556\n",
      "[step: 9891] loss: 0.5376957058906555\n",
      "[step: 9892] loss: 0.537456750869751\n",
      "[step: 9893] loss: 0.5371953248977661\n",
      "[step: 9894] loss: 0.5369259715080261\n",
      "[step: 9895] loss: 0.5366896390914917\n",
      "[step: 9896] loss: 0.5364679098129272\n",
      "[step: 9897] loss: 0.5362107157707214\n",
      "[step: 9898] loss: 0.5359624624252319\n",
      "[step: 9899] loss: 0.5357333421707153\n",
      "[step: 9900] loss: 0.5355172157287598\n",
      "[step: 9901] loss: 0.5352880954742432\n",
      "[step: 9902] loss: 0.5350480675697327\n",
      "[step: 9903] loss: 0.5348213911056519\n",
      "[step: 9904] loss: 0.534609317779541\n",
      "[step: 9905] loss: 0.5343940258026123\n",
      "[step: 9906] loss: 0.5341693758964539\n",
      "[step: 9907] loss: 0.5339549779891968\n",
      "[step: 9908] loss: 0.5337464809417725\n",
      "[step: 9909] loss: 0.5335392951965332\n",
      "[step: 9910] loss: 0.5333285927772522\n",
      "[step: 9911] loss: 0.533117413520813\n",
      "[step: 9912] loss: 0.5329164862632751\n",
      "[step: 9913] loss: 0.5327177047729492\n",
      "[step: 9914] loss: 0.5325168371200562\n",
      "[step: 9915] loss: 0.5323176383972168\n",
      "[step: 9916] loss: 0.5321201682090759\n",
      "[step: 9917] loss: 0.5319262742996216\n",
      "[step: 9918] loss: 0.531735360622406\n",
      "[step: 9919] loss: 0.5315413475036621\n",
      "[step: 9920] loss: 0.5313490629196167\n",
      "[step: 9921] loss: 0.531163215637207\n",
      "[step: 9922] loss: 0.5309771299362183\n",
      "[step: 9923] loss: 0.5307920575141907\n",
      "[step: 9924] loss: 0.5306081771850586\n",
      "[step: 9925] loss: 0.5304257869720459\n",
      "[step: 9926] loss: 0.5302459597587585\n",
      "[step: 9927] loss: 0.5300670862197876\n",
      "[step: 9928] loss: 0.529888391494751\n",
      "[step: 9929] loss: 0.5297108888626099\n",
      "[step: 9930] loss: 0.5295358300209045\n",
      "[step: 9931] loss: 0.5293622016906738\n",
      "[step: 9932] loss: 0.5291893482208252\n",
      "[step: 9933] loss: 0.5290173292160034\n",
      "[step: 9934] loss: 0.528847336769104\n",
      "[step: 9935] loss: 0.5286780595779419\n",
      "[step: 9936] loss: 0.5285114049911499\n",
      "[step: 9937] loss: 0.5283442735671997\n",
      "[step: 9938] loss: 0.5281782150268555\n",
      "[step: 9939] loss: 0.5280137062072754\n",
      "[step: 9940] loss: 0.5278501510620117\n",
      "[step: 9941] loss: 0.5276877880096436\n",
      "[step: 9942] loss: 0.5275256633758545\n",
      "[step: 9943] loss: 0.5273659229278564\n",
      "[step: 9944] loss: 0.527206301689148\n",
      "[step: 9945] loss: 0.5270477533340454\n",
      "[step: 9946] loss: 0.5268911719322205\n",
      "[step: 9947] loss: 0.526733934879303\n",
      "[step: 9948] loss: 0.5265809297561646\n",
      "[step: 9949] loss: 0.5264252424240112\n",
      "[step: 9950] loss: 0.5262740254402161\n",
      "[step: 9951] loss: 0.5261216163635254\n",
      "[step: 9952] loss: 0.525972843170166\n",
      "[step: 9953] loss: 0.5258261561393738\n",
      "[step: 9954] loss: 0.5256820917129517\n",
      "[step: 9955] loss: 0.5255443453788757\n",
      "[step: 9956] loss: 0.5254156589508057\n",
      "[step: 9957] loss: 0.5253028869628906\n",
      "[step: 9958] loss: 0.5252178311347961\n",
      "[step: 9959] loss: 0.5251840353012085\n",
      "[step: 9960] loss: 0.5252425074577332\n",
      "[step: 9961] loss: 0.5254794359207153\n",
      "[step: 9962] loss: 0.5260538458824158\n",
      "[step: 9963] loss: 0.527288556098938\n",
      "[step: 9964] loss: 0.5298053026199341\n",
      "[step: 9965] loss: 0.5349137783050537\n",
      "[step: 9966] loss: 0.5451188087463379\n",
      "[step: 9967] loss: 0.5659855008125305\n",
      "[step: 9968] loss: 0.6076425313949585\n",
      "[step: 9969] loss: 0.6939151287078857\n",
      "[step: 9970] loss: 0.8610985279083252\n",
      "[step: 9971] loss: 1.1977108716964722\n",
      "[step: 9972] loss: 1.74996018409729\n",
      "[step: 9973] loss: 2.624918222427368\n",
      "[step: 9974] loss: 3.2091546058654785\n",
      "[step: 9975] loss: 3.0439810752868652\n",
      "[step: 9976] loss: 1.6422791481018066\n",
      "[step: 9977] loss: 0.6481690406799316\n",
      "[step: 9978] loss: 1.085507869720459\n",
      "[step: 9979] loss: 1.8026719093322754\n",
      "[step: 9980] loss: 1.4644427299499512\n",
      "[step: 9981] loss: 0.6794137954711914\n",
      "[step: 9982] loss: 0.8901818990707397\n",
      "[step: 9983] loss: 1.4266183376312256\n",
      "[step: 9984] loss: 1.0658156871795654\n",
      "[step: 9985] loss: 0.6077890396118164\n",
      "[step: 9986] loss: 0.8459184169769287\n",
      "[step: 9987] loss: 1.1194941997528076\n",
      "[step: 9988] loss: 0.8161247968673706\n",
      "[step: 9989] loss: 0.5717189311981201\n",
      "[step: 9990] loss: 0.8116240501403809\n",
      "[step: 9991] loss: 0.9037712812423706\n",
      "[step: 9992] loss: 0.6408466100692749\n",
      "[step: 9993] loss: 0.5944388508796692\n",
      "[step: 9994] loss: 0.7780608534812927\n",
      "[step: 9995] loss: 0.7202098965644836\n",
      "[step: 9996] loss: 0.5611538887023926\n",
      "[step: 9997] loss: 0.6328867673873901\n",
      "[step: 9998] loss: 0.7168737053871155\n",
      "[step: 9999] loss: 0.6182600259780884\n",
      "[step: 10000] loss: 0.5580279231071472\n",
      "[step: 10001] loss: 0.6392103433609009\n",
      "[step: 10002] loss: 0.6556008458137512\n",
      "[step: 10003] loss: 0.5689109563827515\n",
      "[step: 10004] loss: 0.5609509944915771\n",
      "[step: 10005] loss: 0.6196490526199341\n",
      "[step: 10006] loss: 0.6045594811439514\n",
      "[step: 10007] loss: 0.551232635974884\n",
      "[step: 10008] loss: 0.5604666471481323\n",
      "[step: 10009] loss: 0.5957274436950684\n",
      "[step: 10010] loss: 0.5804222822189331\n",
      "[step: 10011] loss: 0.5459549427032471\n",
      "[step: 10012] loss: 0.553520679473877\n",
      "[step: 10013] loss: 0.5755354762077332\n",
      "[step: 10014] loss: 0.5654782056808472\n",
      "[step: 10015] loss: 0.54350346326828\n",
      "[step: 10016] loss: 0.5456517934799194\n",
      "[step: 10017] loss: 0.5602045059204102\n",
      "[step: 10018] loss: 0.5561758279800415\n",
      "[step: 10019] loss: 0.5417991876602173\n",
      "[step: 10020] loss: 0.5404528379440308\n",
      "[step: 10021] loss: 0.5485476851463318\n",
      "[step: 10022] loss: 0.548933207988739\n",
      "[step: 10023] loss: 0.5407320261001587\n",
      "[step: 10024] loss: 0.5370279550552368\n",
      "[step: 10025] loss: 0.5402889251708984\n",
      "[step: 10026] loss: 0.5425527095794678\n",
      "[step: 10027] loss: 0.5395162105560303\n",
      "[step: 10028] loss: 0.5355482697486877\n",
      "[step: 10029] loss: 0.5351349711418152\n",
      "[step: 10030] loss: 0.5363537073135376\n",
      "[step: 10031] loss: 0.536523163318634\n",
      "[step: 10032] loss: 0.5350881814956665\n",
      "[step: 10033] loss: 0.5333156585693359\n",
      "[step: 10034] loss: 0.5323820114135742\n",
      "[step: 10035] loss: 0.5323314070701599\n",
      "[step: 10036] loss: 0.53278648853302\n",
      "[step: 10037] loss: 0.5325461626052856\n",
      "[step: 10038] loss: 0.5312173366546631\n",
      "[step: 10039] loss: 0.5297722816467285\n",
      "[step: 10040] loss: 0.529431939125061\n",
      "[step: 10041] loss: 0.5301666259765625\n",
      "[step: 10042] loss: 0.5304148197174072\n",
      "[step: 10043] loss: 0.5293902158737183\n",
      "[step: 10044] loss: 0.5279751420021057\n",
      "[step: 10045] loss: 0.527445912361145\n",
      "[step: 10046] loss: 0.5279040336608887\n",
      "[step: 10047] loss: 0.5282297730445862\n",
      "[step: 10048] loss: 0.5277502536773682\n",
      "[step: 10049] loss: 0.526823878288269\n",
      "[step: 10050] loss: 0.5261983275413513\n",
      "[step: 10051] loss: 0.5260895490646362\n",
      "[step: 10052] loss: 0.5261006355285645\n",
      "[step: 10053] loss: 0.5259464979171753\n",
      "[step: 10054] loss: 0.5256338119506836\n",
      "[step: 10055] loss: 0.5253164768218994\n",
      "[step: 10056] loss: 0.5249990224838257\n",
      "[step: 10057] loss: 0.5246289968490601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10058] loss: 0.5242949724197388\n",
      "[step: 10059] loss: 0.5241091251373291\n",
      "[step: 10060] loss: 0.5240563154220581\n",
      "[step: 10061] loss: 0.523959219455719\n",
      "[step: 10062] loss: 0.5236974358558655\n",
      "[step: 10063] loss: 0.5233309268951416\n",
      "[step: 10064] loss: 0.5230026245117188\n",
      "[step: 10065] loss: 0.522786021232605\n",
      "[step: 10066] loss: 0.5226246118545532\n",
      "[step: 10067] loss: 0.5224564671516418\n",
      "[step: 10068] loss: 0.522274374961853\n",
      "[step: 10069] loss: 0.5220963954925537\n",
      "[step: 10070] loss: 0.5219311714172363\n",
      "[step: 10071] loss: 0.5217398405075073\n",
      "[step: 10072] loss: 0.5215061902999878\n",
      "[step: 10073] loss: 0.5212598443031311\n",
      "[step: 10074] loss: 0.521038293838501\n",
      "[step: 10075] loss: 0.520854651927948\n",
      "[step: 10076] loss: 0.5206883549690247\n",
      "[step: 10077] loss: 0.5205186605453491\n",
      "[step: 10078] loss: 0.5203431844711304\n",
      "[step: 10079] loss: 0.5201756954193115\n",
      "[step: 10080] loss: 0.5200217962265015\n",
      "[step: 10081] loss: 0.5198668241500854\n",
      "[step: 10082] loss: 0.5197010040283203\n",
      "[step: 10083] loss: 0.5195261240005493\n",
      "[step: 10084] loss: 0.519352376461029\n",
      "[step: 10085] loss: 0.5191829204559326\n",
      "[step: 10086] loss: 0.5190200209617615\n",
      "[step: 10087] loss: 0.5188553333282471\n",
      "[step: 10088] loss: 0.5186883211135864\n",
      "[step: 10089] loss: 0.5185233354568481\n",
      "[step: 10090] loss: 0.5183663368225098\n",
      "[step: 10091] loss: 0.5182154178619385\n",
      "[step: 10092] loss: 0.518071174621582\n",
      "[step: 10093] loss: 0.5179309844970703\n",
      "[step: 10094] loss: 0.5177993774414062\n",
      "[step: 10095] loss: 0.5176855325698853\n",
      "[step: 10096] loss: 0.517595648765564\n",
      "[step: 10097] loss: 0.5175436735153198\n",
      "[step: 10098] loss: 0.517551064491272\n",
      "[step: 10099] loss: 0.5176478624343872\n",
      "[step: 10100] loss: 0.5179070234298706\n",
      "[step: 10101] loss: 0.518430233001709\n",
      "[step: 10102] loss: 0.5194256901741028\n",
      "[step: 10103] loss: 0.5212104320526123\n",
      "[step: 10104] loss: 0.5244683027267456\n",
      "[step: 10105] loss: 0.5302175879478455\n",
      "[step: 10106] loss: 0.540860652923584\n",
      "[step: 10107] loss: 0.5596405267715454\n",
      "[step: 10108] loss: 0.5953360795974731\n",
      "[step: 10109] loss: 0.6571961641311646\n",
      "[step: 10110] loss: 0.7767637372016907\n",
      "[step: 10111] loss: 0.9663819074630737\n",
      "[step: 10112] loss: 1.3157751560211182\n",
      "[step: 10113] loss: 1.7095530033111572\n",
      "[step: 10114] loss: 2.2317981719970703\n",
      "[step: 10115] loss: 2.18143367767334\n",
      "[step: 10116] loss: 1.7280535697937012\n",
      "[step: 10117] loss: 0.9139931201934814\n",
      "[step: 10118] loss: 0.5941042304039001\n",
      "[step: 10119] loss: 0.9168421030044556\n",
      "[step: 10120] loss: 1.2513362169265747\n",
      "[step: 10121] loss: 1.1438794136047363\n",
      "[step: 10122] loss: 0.7075329422950745\n",
      "[step: 10123] loss: 0.6057793498039246\n",
      "[step: 10124] loss: 0.8836908936500549\n",
      "[step: 10125] loss: 1.02494478225708\n",
      "[step: 10126] loss: 0.8542980551719666\n",
      "[step: 10127] loss: 0.5871288776397705\n",
      "[step: 10128] loss: 0.6069451570510864\n",
      "[step: 10129] loss: 0.8018931150436401\n",
      "[step: 10130] loss: 0.8217012882232666\n",
      "[step: 10131] loss: 0.6573396921157837\n",
      "[step: 10132] loss: 0.5422194004058838\n",
      "[step: 10133] loss: 0.6339613795280457\n",
      "[step: 10134] loss: 0.7318576574325562\n",
      "[step: 10135] loss: 0.6611311435699463\n",
      "[step: 10136] loss: 0.5525724291801453\n",
      "[step: 10137] loss: 0.5601465702056885\n",
      "[step: 10138] loss: 0.6436637043952942\n",
      "[step: 10139] loss: 0.6512824296951294\n",
      "[step: 10140] loss: 0.5744361877441406\n",
      "[step: 10141] loss: 0.533726692199707\n",
      "[step: 10142] loss: 0.5743955373764038\n",
      "[step: 10143] loss: 0.6155733466148376\n",
      "[step: 10144] loss: 0.5932547450065613\n",
      "[step: 10145] loss: 0.5433950424194336\n",
      "[step: 10146] loss: 0.5309127569198608\n",
      "[step: 10147] loss: 0.563029944896698\n",
      "[step: 10148] loss: 0.5830425024032593\n",
      "[step: 10149] loss: 0.5650092363357544\n",
      "[step: 10150] loss: 0.5344237089157104\n",
      "[step: 10151] loss: 0.5266552567481995\n",
      "[step: 10152] loss: 0.5456945896148682\n",
      "[step: 10153] loss: 0.5591033697128296\n",
      "[step: 10154] loss: 0.551463782787323\n",
      "[step: 10155] loss: 0.5320068597793579\n",
      "[step: 10156] loss: 0.5228797197341919\n",
      "[step: 10157] loss: 0.5315215587615967\n",
      "[step: 10158] loss: 0.5419042110443115\n",
      "[step: 10159] loss: 0.5426548719406128\n",
      "[step: 10160] loss: 0.5320394039154053\n",
      "[step: 10161] loss: 0.522308886051178\n",
      "[step: 10162] loss: 0.5222351551055908\n",
      "[step: 10163] loss: 0.5281717777252197\n",
      "[step: 10164] loss: 0.5334306955337524\n",
      "[step: 10165] loss: 0.5315495133399963\n",
      "[step: 10166] loss: 0.5255070924758911\n",
      "[step: 10167] loss: 0.5202163457870483\n",
      "[step: 10168] loss: 0.5191667079925537\n",
      "[step: 10169] loss: 0.5220723152160645\n",
      "[step: 10170] loss: 0.5249141454696655\n",
      "[step: 10171] loss: 0.5257319211959839\n",
      "[step: 10172] loss: 0.5234593152999878\n",
      "[step: 10173] loss: 0.5201598405838013\n",
      "[step: 10174] loss: 0.5177443623542786\n",
      "[step: 10175] loss: 0.5171309113502502\n",
      "[step: 10176] loss: 0.5182780623435974\n",
      "[step: 10177] loss: 0.5196213722229004\n",
      "[step: 10178] loss: 0.5203793048858643\n",
      "[step: 10179] loss: 0.5198296308517456\n",
      "[step: 10180] loss: 0.5184122323989868\n",
      "[step: 10181] loss: 0.5168309211730957\n",
      "[step: 10182] loss: 0.5156072378158569\n",
      "[step: 10183] loss: 0.5151814222335815\n",
      "[step: 10184] loss: 0.5152996182441711\n",
      "[step: 10185] loss: 0.5157848596572876\n",
      "[step: 10186] loss: 0.5162174701690674\n",
      "[step: 10187] loss: 0.5163848400115967\n",
      "[step: 10188] loss: 0.5162431001663208\n",
      "[step: 10189] loss: 0.5157719850540161\n",
      "[step: 10190] loss: 0.5151618123054504\n",
      "[step: 10191] loss: 0.5144829750061035\n",
      "[step: 10192] loss: 0.5138683319091797\n",
      "[step: 10193] loss: 0.5133622884750366\n",
      "[step: 10194] loss: 0.5129834413528442\n",
      "[step: 10195] loss: 0.5127520561218262\n",
      "[step: 10196] loss: 0.5125980377197266\n",
      "[step: 10197] loss: 0.5125390887260437\n",
      "[step: 10198] loss: 0.5125077962875366\n",
      "[step: 10199] loss: 0.5125098824501038\n",
      "[step: 10200] loss: 0.5125365853309631\n",
      "[step: 10201] loss: 0.5125796794891357\n",
      "[step: 10202] loss: 0.512673020362854\n",
      "[step: 10203] loss: 0.5128127932548523\n",
      "[step: 10204] loss: 0.5130585432052612\n",
      "[step: 10205] loss: 0.5134363174438477\n",
      "[step: 10206] loss: 0.5140590667724609\n",
      "[step: 10207] loss: 0.5150125622749329\n",
      "[step: 10208] loss: 0.5165464282035828\n",
      "[step: 10209] loss: 0.5189066529273987\n",
      "[step: 10210] loss: 0.5227608680725098\n",
      "[step: 10211] loss: 0.528756320476532\n",
      "[step: 10212] loss: 0.5388015508651733\n",
      "[step: 10213] loss: 0.5545965433120728\n",
      "[step: 10214] loss: 0.5819566249847412\n",
      "[step: 10215] loss: 0.6246095895767212\n",
      "[step: 10216] loss: 0.6996618509292603\n",
      "[step: 10217] loss: 0.8091998100280762\n",
      "[step: 10218] loss: 0.9969454407691956\n",
      "[step: 10219] loss: 1.216353416442871\n",
      "[step: 10220] loss: 1.5274884700775146\n",
      "[step: 10221] loss: 1.656362771987915\n",
      "[step: 10222] loss: 1.6440101861953735\n",
      "[step: 10223] loss: 1.230718970298767\n",
      "[step: 10224] loss: 0.7741128206253052\n",
      "[step: 10225] loss: 0.5540941953659058\n",
      "[step: 10226] loss: 0.6745973229408264\n",
      "[step: 10227] loss: 0.9310722947120667\n",
      "[step: 10228] loss: 0.9846494197845459\n",
      "[step: 10229] loss: 0.8159340620040894\n",
      "[step: 10230] loss: 0.5941365957260132\n",
      "[step: 10231] loss: 0.5494009256362915\n",
      "[step: 10232] loss: 0.6782994866371155\n",
      "[step: 10233] loss: 0.7981306314468384\n",
      "[step: 10234] loss: 0.7912073135375977\n",
      "[step: 10235] loss: 0.649327278137207\n",
      "[step: 10236] loss: 0.5372400879859924\n",
      "[step: 10237] loss: 0.5499054193496704\n",
      "[step: 10238] loss: 0.6373627185821533\n",
      "[step: 10239] loss: 0.6847892999649048\n",
      "[step: 10240] loss: 0.631006121635437\n",
      "[step: 10241] loss: 0.5506262183189392\n",
      "[step: 10242] loss: 0.5229965448379517\n",
      "[step: 10243] loss: 0.5630984306335449\n",
      "[step: 10244] loss: 0.6108455657958984\n",
      "[step: 10245] loss: 0.6043059229850769\n",
      "[step: 10246] loss: 0.5604429244995117\n",
      "[step: 10247] loss: 0.5227476954460144\n",
      "[step: 10248] loss: 0.5240051746368408\n",
      "[step: 10249] loss: 0.5532320737838745\n",
      "[step: 10250] loss: 0.5734195709228516\n",
      "[step: 10251] loss: 0.5673051476478577\n",
      "[step: 10252] loss: 0.5408388376235962\n",
      "[step: 10253] loss: 0.5189648270606995\n",
      "[step: 10254] loss: 0.5165788531303406\n",
      "[step: 10255] loss: 0.5304592847824097\n",
      "[step: 10256] loss: 0.5452003479003906\n",
      "[step: 10257] loss: 0.5473806858062744\n",
      "[step: 10258] loss: 0.5375268459320068\n",
      "[step: 10259] loss: 0.5226622223854065\n",
      "[step: 10260] loss: 0.5135394930839539\n",
      "[step: 10261] loss: 0.5140866637229919\n",
      "[step: 10262] loss: 0.5211272239685059\n",
      "[step: 10263] loss: 0.528429388999939\n",
      "[step: 10264] loss: 0.5304003953933716\n",
      "[step: 10265] loss: 0.5267268419265747\n",
      "[step: 10266] loss: 0.5197147130966187\n",
      "[step: 10267] loss: 0.5135095715522766\n",
      "[step: 10268] loss: 0.51064532995224\n",
      "[step: 10269] loss: 0.5114961266517639\n",
      "[step: 10270] loss: 0.5144593119621277\n",
      "[step: 10271] loss: 0.5176485776901245\n",
      "[step: 10272] loss: 0.5196387767791748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10273] loss: 0.519478976726532\n",
      "[step: 10274] loss: 0.5178686380386353\n",
      "[step: 10275] loss: 0.5152580738067627\n",
      "[step: 10276] loss: 0.5124727487564087\n",
      "[step: 10277] loss: 0.5101875066757202\n",
      "[step: 10278] loss: 0.5087588429450989\n",
      "[step: 10279] loss: 0.5080921649932861\n",
      "[step: 10280] loss: 0.508164644241333\n",
      "[step: 10281] loss: 0.5087305903434753\n",
      "[step: 10282] loss: 0.5094795227050781\n",
      "[step: 10283] loss: 0.5103459358215332\n",
      "[step: 10284] loss: 0.5111620426177979\n",
      "[step: 10285] loss: 0.5119165182113647\n",
      "[step: 10286] loss: 0.5126229524612427\n",
      "[step: 10287] loss: 0.5134240984916687\n",
      "[step: 10288] loss: 0.5142856240272522\n",
      "[step: 10289] loss: 0.5155327320098877\n",
      "[step: 10290] loss: 0.5171446204185486\n",
      "[step: 10291] loss: 0.5195831060409546\n",
      "[step: 10292] loss: 0.5229611396789551\n",
      "[step: 10293] loss: 0.5282723307609558\n",
      "[step: 10294] loss: 0.5357677340507507\n",
      "[step: 10295] loss: 0.5478038787841797\n",
      "[step: 10296] loss: 0.5648787617683411\n",
      "[step: 10297] loss: 0.5927813053131104\n",
      "[step: 10298] loss: 0.6315509080886841\n",
      "[step: 10299] loss: 0.6954907774925232\n",
      "[step: 10300] loss: 0.7780933380126953\n",
      "[step: 10301] loss: 0.909875750541687\n",
      "[step: 10302] loss: 1.0451781749725342\n",
      "[step: 10303] loss: 1.2250295877456665\n",
      "[step: 10304] loss: 1.291577935218811\n",
      "[step: 10305] loss: 1.2897133827209473\n",
      "[step: 10306] loss: 1.0706578493118286\n",
      "[step: 10307] loss: 0.796480119228363\n",
      "[step: 10308] loss: 0.5813373327255249\n",
      "[step: 10309] loss: 0.5381035804748535\n",
      "[step: 10310] loss: 0.6445655226707458\n",
      "[step: 10311] loss: 0.7724231481552124\n",
      "[step: 10312] loss: 0.8289895057678223\n",
      "[step: 10313] loss: 0.7477753162384033\n",
      "[step: 10314] loss: 0.6211676597595215\n",
      "[step: 10315] loss: 0.5314034223556519\n",
      "[step: 10316] loss: 0.5275073647499084\n",
      "[step: 10317] loss: 0.5893131494522095\n",
      "[step: 10318] loss: 0.6522294878959656\n",
      "[step: 10319] loss: 0.6725203990936279\n",
      "[step: 10320] loss: 0.6261332631111145\n",
      "[step: 10321] loss: 0.5601302981376648\n",
      "[step: 10322] loss: 0.5170067548751831\n",
      "[step: 10323] loss: 0.5194321870803833\n",
      "[step: 10324] loss: 0.5534158945083618\n",
      "[step: 10325] loss: 0.5835965871810913\n",
      "[step: 10326] loss: 0.5888462066650391\n",
      "[step: 10327] loss: 0.5637762546539307\n",
      "[step: 10328] loss: 0.5319610834121704\n",
      "[step: 10329] loss: 0.5116369724273682\n",
      "[step: 10330] loss: 0.5116070508956909\n",
      "[step: 10331] loss: 0.5266867876052856\n",
      "[step: 10332] loss: 0.5434536933898926\n",
      "[step: 10333] loss: 0.5523538589477539\n",
      "[step: 10334] loss: 0.5478723645210266\n",
      "[step: 10335] loss: 0.5356904864311218\n",
      "[step: 10336] loss: 0.5206842422485352\n",
      "[step: 10337] loss: 0.5098166465759277\n",
      "[step: 10338] loss: 0.5060587525367737\n",
      "[step: 10339] loss: 0.5089250802993774\n",
      "[step: 10340] loss: 0.515510082244873\n",
      "[step: 10341] loss: 0.5222489833831787\n",
      "[step: 10342] loss: 0.5270528793334961\n",
      "[step: 10343] loss: 0.5280517339706421\n",
      "[step: 10344] loss: 0.5260450839996338\n",
      "[step: 10345] loss: 0.5213156938552856\n",
      "[step: 10346] loss: 0.5160385966300964\n",
      "[step: 10347] loss: 0.5109657049179077\n",
      "[step: 10348] loss: 0.5070497989654541\n",
      "[step: 10349] loss: 0.5045462846755981\n",
      "[step: 10350] loss: 0.5034692287445068\n",
      "[step: 10351] loss: 0.5034853219985962\n",
      "[step: 10352] loss: 0.5042813420295715\n",
      "[step: 10353] loss: 0.5056403279304504\n",
      "[step: 10354] loss: 0.5073899030685425\n",
      "[step: 10355] loss: 0.5095402002334595\n",
      "[step: 10356] loss: 0.512081503868103\n",
      "[step: 10357] loss: 0.5154218673706055\n",
      "[step: 10358] loss: 0.5196058750152588\n",
      "[step: 10359] loss: 0.525656521320343\n",
      "[step: 10360] loss: 0.5336703062057495\n",
      "[step: 10361] loss: 0.5461229681968689\n",
      "[step: 10362] loss: 0.5630062818527222\n",
      "[step: 10363] loss: 0.5900627374649048\n",
      "[step: 10364] loss: 0.6262210607528687\n",
      "[step: 10365] loss: 0.6846899390220642\n",
      "[step: 10366] loss: 0.7573251724243164\n",
      "[step: 10367] loss: 0.8706535696983337\n",
      "[step: 10368] loss: 0.9842875003814697\n",
      "[step: 10369] loss: 1.1349480152130127\n",
      "[step: 10370] loss: 1.1994065046310425\n",
      "[step: 10371] loss: 1.2152585983276367\n",
      "[step: 10372] loss: 1.0549041032791138\n",
      "[step: 10373] loss: 0.8308037519454956\n",
      "[step: 10374] loss: 0.6209828853607178\n",
      "[step: 10375] loss: 0.5331888198852539\n",
      "[step: 10376] loss: 0.5825594663619995\n",
      "[step: 10377] loss: 0.6934814453125\n",
      "[step: 10378] loss: 0.7852919101715088\n",
      "[step: 10379] loss: 0.7654706239700317\n",
      "[step: 10380] loss: 0.681855320930481\n",
      "[step: 10381] loss: 0.5751398801803589\n",
      "[step: 10382] loss: 0.5164061784744263\n",
      "[step: 10383] loss: 0.5250558257102966\n",
      "[step: 10384] loss: 0.5768117904663086\n",
      "[step: 10385] loss: 0.630027174949646\n",
      "[step: 10386] loss: 0.6375628709793091\n",
      "[step: 10387] loss: 0.6075425148010254\n",
      "[step: 10388] loss: 0.5527695417404175\n",
      "[step: 10389] loss: 0.5136457681655884\n",
      "[step: 10390] loss: 0.5071957111358643\n",
      "[step: 10391] loss: 0.528343677520752\n",
      "[step: 10392] loss: 0.5571478605270386\n",
      "[step: 10393] loss: 0.5707208514213562\n",
      "[step: 10394] loss: 0.566213846206665\n",
      "[step: 10395] loss: 0.5442097187042236\n",
      "[step: 10396] loss: 0.5209217071533203\n",
      "[step: 10397] loss: 0.505750298500061\n",
      "[step: 10398] loss: 0.5035731196403503\n",
      "[step: 10399] loss: 0.5116326212882996\n",
      "[step: 10400] loss: 0.5235545039176941\n",
      "[step: 10401] loss: 0.5340341329574585\n",
      "[step: 10402] loss: 0.5378267765045166\n",
      "[step: 10403] loss: 0.5360482931137085\n",
      "[step: 10404] loss: 0.5283291339874268\n",
      "[step: 10405] loss: 0.5192713737487793\n",
      "[step: 10406] loss: 0.5104721784591675\n",
      "[step: 10407] loss: 0.5041980147361755\n",
      "[step: 10408] loss: 0.5009366273880005\n",
      "[step: 10409] loss: 0.5005652904510498\n",
      "[step: 10410] loss: 0.5023155212402344\n",
      "[step: 10411] loss: 0.5052226781845093\n",
      "[step: 10412] loss: 0.5086658000946045\n",
      "[step: 10413] loss: 0.511948823928833\n",
      "[step: 10414] loss: 0.5151822566986084\n",
      "[step: 10415] loss: 0.517818808555603\n",
      "[step: 10416] loss: 0.5207449197769165\n",
      "[step: 10417] loss: 0.5233793258666992\n",
      "[step: 10418] loss: 0.5270665884017944\n",
      "[step: 10419] loss: 0.53114914894104\n",
      "[step: 10420] loss: 0.5376279950141907\n",
      "[step: 10421] loss: 0.5457876920700073\n",
      "[step: 10422] loss: 0.558673083782196\n",
      "[step: 10423] loss: 0.5754284858703613\n",
      "[step: 10424] loss: 0.601470947265625\n",
      "[step: 10425] loss: 0.6349042654037476\n",
      "[step: 10426] loss: 0.685900866985321\n",
      "[step: 10427] loss: 0.7472009658813477\n",
      "[step: 10428] loss: 0.8353522419929504\n",
      "[step: 10429] loss: 0.9214989542961121\n",
      "[step: 10430] loss: 1.022673487663269\n",
      "[step: 10431] loss: 1.0642330646514893\n",
      "[step: 10432] loss: 1.061289668083191\n",
      "[step: 10433] loss: 0.9470827579498291\n",
      "[step: 10434] loss: 0.7833455801010132\n",
      "[step: 10435] loss: 0.6187347173690796\n",
      "[step: 10436] loss: 0.5249500870704651\n",
      "[step: 10437] loss: 0.5268493890762329\n",
      "[step: 10438] loss: 0.5960992574691772\n",
      "[step: 10439] loss: 0.6834776997566223\n",
      "[step: 10440] loss: 0.7230173349380493\n",
      "[step: 10441] loss: 0.715150773525238\n",
      "[step: 10442] loss: 0.6481955051422119\n",
      "[step: 10443] loss: 0.5737689733505249\n",
      "[step: 10444] loss: 0.5176801085472107\n",
      "[step: 10445] loss: 0.5015718936920166\n",
      "[step: 10446] loss: 0.5213001370429993\n",
      "[step: 10447] loss: 0.5567631125450134\n",
      "[step: 10448] loss: 0.5861648917198181\n",
      "[step: 10449] loss: 0.5908078551292419\n",
      "[step: 10450] loss: 0.5749242305755615\n",
      "[step: 10451] loss: 0.5443925857543945\n",
      "[step: 10452] loss: 0.5171935558319092\n",
      "[step: 10453] loss: 0.5019660592079163\n",
      "[step: 10454] loss: 0.5017561316490173\n",
      "[step: 10455] loss: 0.5124316215515137\n",
      "[step: 10456] loss: 0.5267503261566162\n",
      "[step: 10457] loss: 0.538324236869812\n",
      "[step: 10458] loss: 0.5427558422088623\n",
      "[step: 10459] loss: 0.5407853126525879\n",
      "[step: 10460] loss: 0.5329029560089111\n",
      "[step: 10461] loss: 0.5234131813049316\n",
      "[step: 10462] loss: 0.5137149095535278\n",
      "[step: 10463] loss: 0.5064507722854614\n",
      "[step: 10464] loss: 0.5018985271453857\n",
      "[step: 10465] loss: 0.5002188086509705\n",
      "[step: 10466] loss: 0.5007964968681335\n",
      "[step: 10467] loss: 0.5027884244918823\n",
      "[step: 10468] loss: 0.5054813027381897\n",
      "[step: 10469] loss: 0.5083551406860352\n",
      "[step: 10470] loss: 0.5110434293746948\n",
      "[step: 10471] loss: 0.5136193037033081\n",
      "[step: 10472] loss: 0.5160017609596252\n",
      "[step: 10473] loss: 0.5186828970909119\n",
      "[step: 10474] loss: 0.5219882726669312\n",
      "[step: 10475] loss: 0.5266309976577759\n",
      "[step: 10476] loss: 0.5337291359901428\n",
      "[step: 10477] loss: 0.5439456701278687\n",
      "[step: 10478] loss: 0.5605236291885376\n",
      "[step: 10479] loss: 0.5835033655166626\n",
      "[step: 10480] loss: 0.6218689680099487\n",
      "[step: 10481] loss: 0.6715525388717651\n",
      "[step: 10482] loss: 0.7552222013473511\n",
      "[step: 10483] loss: 0.8466769456863403\n",
      "[step: 10484] loss: 0.9925805926322937\n",
      "[step: 10485] loss: 1.0865824222564697\n",
      "[step: 10486] loss: 1.1999506950378418\n",
      "[step: 10487] loss: 1.1248656511306763\n",
      "[step: 10488] loss: 0.9923937320709229\n",
      "[step: 10489] loss: 0.7568320035934448\n",
      "[step: 10490] loss: 0.5901614427566528\n",
      "[step: 10491] loss: 0.5578311681747437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10492] loss: 0.6455898880958557\n",
      "[step: 10493] loss: 0.768722653388977\n",
      "[step: 10494] loss: 0.8117956519126892\n",
      "[step: 10495] loss: 0.7736253142356873\n",
      "[step: 10496] loss: 0.6461936235427856\n",
      "[step: 10497] loss: 0.540268063545227\n",
      "[step: 10498] loss: 0.5016179084777832\n",
      "[step: 10499] loss: 0.5359435677528381\n",
      "[step: 10500] loss: 0.6020165681838989\n",
      "[step: 10501] loss: 0.640580415725708\n",
      "[step: 10502] loss: 0.6308760643005371\n",
      "[step: 10503] loss: 0.5738027691841125\n",
      "[step: 10504] loss: 0.5195985436439514\n",
      "[step: 10505] loss: 0.4984959661960602\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17da0e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer=LSTM(txs,7,'WeekNumber_Month_Season_Year' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict=answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real=answer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20050.502, 18498.662, 19718.93, 22545.387, 24069.215, 24375.484, 26333.574]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19616.21999997,  19251.49999997,  18947.80999997,  21904.46999996,\n",
       "        22764.00999996,  24185.26999996,  27390.80999995])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812.25033149110504"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(predict,real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestPredictY=[item for sublist in predict for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalizedTestPredictY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalizedTestY=originalSales[train_size+seq_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sales=noOutlierSales(sales)\n",
    "tempxy=[list(txs['season']),list(txs['year']),list(txs['month']),list(txs['week_number']),sales]\n",
    "# tempxy=[list(txs['season']),list(txs['day_of_week01']),list(txs['week_number']),sales]\n",
    "xy=np.array(tempxy).transpose().astype(np.float)\n",
    "originalxy=np.array(tempxy).transpose().astype(np.float)\n",
    "xy=minMaxNormalizer(xy)\n",
    "\n",
    "#data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "data_dim=len(tempxy)\n",
    "#data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "seq_length=5\n",
    "#output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "\n",
    "output_dim=forecastDay\n",
    "#hidden_dim은 정말 임의로 설정\n",
    "hidden_dim=100\n",
    "#learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "learning_rate=0.01\n",
    "#iterations는 반복 횟수\n",
    "iterations=1000\n",
    "x=xy\n",
    "y=xy[:,[-1]]\n",
    "\n",
    "#build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0, len(y)-seq_length - forecastDay):\n",
    "    _x=x[i:i+seq_length]\n",
    "    _y = y[i + seq_length:i + seq_length + forecastDay]\n",
    "    _y = np.reshape(_y, (forecastDay))\n",
    "#     print(_x,\"->\",_y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "    train_size = int(len(dataY) * 0.7)\n",
    "    \n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "X=tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y=tf.placeholder(tf.float32, [None, forecastDay])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn= None) \n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "denormalizedTestY=originalSales[train_size+seq_length:]\n",
    "# denormalizedTestY_original=sales[train_size+seq_length:]\n",
    "denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])\n",
    "\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "with tf.Session() as sess:\n",
    "    #초기화\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalxy)\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(denormalizedTestY_feed) #실제 sales 파란색\n",
    "    plt.plot(test_predict)           #예측 sales 주황색\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(i for i in list(test_predict[-1]    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17721.039]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_predict[  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestPredictY=[item for sublist in test_predict for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a5cffacf1166>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrootMeanSquaredError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdenormalizedTestY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdenormalizedTestPredictY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-a9533e569715>\u001b[0m in \u001b[0;36mrootMeanSquaredError\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0msum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0msum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rootMeanSquaredError(denormalizedTestY,denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
