{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "tf.set_random_seed(77)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecastDays=7\n",
    "rawArrayDatas=[[\"2017-08-02\",\n",
    "\"2017-08-03\",\n",
    "\"2017-08-04\",\n",
    "\"2017-08-05\",\n",
    "\"2017-08-06\",\n",
    "\"2017-08-07\",\n",
    "\"2017-08-08\",\n",
    "\"2017-08-09\",\n",
    "\"2017-08-10\",\n",
    "\"2017-08-11\",\n",
    "\"2017-08-12\",\n",
    "\"2017-08-13\",\n",
    "\"2017-08-14\",\n",
    "\"2017-08-15\",\n",
    "\"2017-08-16\",\n",
    "\"2017-08-17\",\n",
    "\"2017-08-18\",\n",
    "\"2017-08-19\",\n",
    "\"2017-08-20\",\n",
    "\"2017-08-21\",\n",
    "\"2017-08-22\",\n",
    "\"2017-08-23\",\n",
    "\"2017-08-24\",\n",
    "\"2017-08-25\",\n",
    "\"2017-08-26\",\n",
    "\"2017-08-27\",\n",
    "\"2017-08-28\",\n",
    "\"2017-08-29\",\n",
    "\"2017-08-30\",\n",
    "\"2017-08-31\",\n",
    "\"2017-09-01\",\n",
    "\"2017-09-02\",\n",
    "\"2017-09-03\",\n",
    "\"2017-09-04\",\n",
    "\"2017-09-05\",\n",
    "\"2017-09-06\",\n",
    "\"2017-09-07\",\n",
    "\"2017-09-08\",\n",
    "\"2017-09-09\",\n",
    "\"2017-09-10\",\n",
    "\"2017-09-11\",\n",
    "\"2017-09-12\",\n",
    "\"2017-09-13\",\n",
    "\"2017-09-14\",\n",
    "\"2017-09-15\",\n",
    "\"2017-09-16\",\n",
    "\"2017-09-17\",\n",
    "\"2017-09-18\",\n",
    "\"2017-09-19\",\n",
    "\"2017-09-20\",\n",
    "\"2017-09-21\",\n",
    "\"2017-09-22\",\n",
    "\"2017-09-23\",\n",
    "\"2017-09-24\",\n",
    "\"2017-09-25\",\n",
    "\"2017-09-26\",\n",
    "\"2017-09-27\",\n",
    "\"2017-09-28\",\n",
    "\"2017-09-29\",\n",
    "\"2017-09-30\",\n",
    "\"2017-10-01\",\n",
    "\"2017-10-02\",\n",
    "\"2017-10-03\",\n",
    "\"2017-10-04\",\n",
    "\"2017-10-05\",\n",
    "\"2017-10-06\",\n",
    "\"2017-10-07\",\n",
    "\"2017-10-08\",\n",
    "\"2017-10-09\",\n",
    "\"2017-10-10\",\n",
    "\"2017-10-11\",\n",
    "\"2017-10-12\",\n",
    "\"2017-10-13\",\n",
    "\"2017-10-14\",\n",
    "\"2017-10-15\",\n",
    "\"2017-10-16\",\n",
    "\"2017-10-17\",\n",
    "\"2017-10-18\",\n",
    "\"2017-10-19\",\n",
    "\"2017-10-20\",\n",
    "\"2017-10-21\",\n",
    "\"2017-10-22\",\n",
    "\"2017-10-23\"],[34,\n",
    "41,\n",
    "54,\n",
    "41,\n",
    "35,\n",
    "44,\n",
    "50,\n",
    "42,\n",
    "42,\n",
    "66,\n",
    "50,\n",
    "55,\n",
    "56,\n",
    "53,\n",
    "44,\n",
    "54,\n",
    "54,\n",
    "50,\n",
    "40,\n",
    "49,\n",
    "28,\n",
    "72,\n",
    "71,\n",
    "53,\n",
    "43,\n",
    "38,\n",
    "55,\n",
    "49,\n",
    "43,\n",
    "49,\n",
    "49,\n",
    "44,\n",
    "39,\n",
    "52,\n",
    "45,\n",
    "33,\n",
    "43,\n",
    "40,\n",
    "46,\n",
    "49,\n",
    "50,\n",
    "37,\n",
    "37,\n",
    "45,\n",
    "48,\n",
    "48,\n",
    "38,\n",
    "60,\n",
    "31,\n",
    "35,\n",
    "53,\n",
    "70,\n",
    "62,\n",
    "48,\n",
    "51,\n",
    "49,\n",
    "38,\n",
    "32,\n",
    "39,\n",
    "35,\n",
    "30,\n",
    "36,\n",
    "31,\n",
    "31,\n",
    "44,\n",
    "41,\n",
    "41,\n",
    "45,\n",
    "46,\n",
    "45,\n",
    "41,\n",
    "47,\n",
    "48,\n",
    "40,\n",
    "42,\n",
    "38,\n",
    "38,\n",
    "45,\n",
    "48,\n",
    "62,\n",
    "46,\n",
    "38,\n",
    "62,\n",
    "81]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)\n",
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02777778  0.02777778  0.43055555  0.47222222]\n",
      " [ 0.02777778  0.04166667  0.43055555  0.56944444]\n",
      " [ 0.02777778  0.05555556  0.43055555  0.75      ]\n",
      " [ 0.02777778  0.06944444  0.43055555  0.56944444]\n",
      " [ 0.02777778  0.08333333  0.43055555  0.48611111]] -> [ 0.61111111  0.69444444  0.58333333  0.58333333  0.91666667  0.69444444\n",
      "  0.76388889]\n",
      "[[ 0.02777778  0.04166667  0.43055555  0.56944444]\n",
      " [ 0.02777778  0.05555556  0.43055555  0.75      ]\n",
      " [ 0.02777778  0.06944444  0.43055555  0.56944444]\n",
      " [ 0.02777778  0.08333333  0.43055555  0.48611111]\n",
      " [ 0.02777778  0.          0.44444444  0.61111111]] -> [ 0.69444444  0.58333333  0.58333333  0.91666667  0.69444444  0.76388889\n",
      "  0.77777778]\n",
      "[[ 0.02777778  0.05555556  0.43055555  0.75      ]\n",
      " [ 0.02777778  0.06944444  0.43055555  0.56944444]\n",
      " [ 0.02777778  0.08333333  0.43055555  0.48611111]\n",
      " [ 0.02777778  0.          0.44444444  0.61111111]\n",
      " [ 0.02777778  0.01388889  0.44444444  0.69444444]] -> [ 0.58333333  0.58333333  0.91666667  0.69444444  0.76388889  0.77777778\n",
      "  0.73611111]\n",
      "[[ 0.02777778  0.06944444  0.43055555  0.56944444]\n",
      " [ 0.02777778  0.08333333  0.43055555  0.48611111]\n",
      " [ 0.02777778  0.          0.44444444  0.61111111]\n",
      " [ 0.02777778  0.01388889  0.44444444  0.69444444]\n",
      " [ 0.02777778  0.02777778  0.44444444  0.58333333]] -> [ 0.58333333  0.91666667  0.69444444  0.76388889  0.77777778  0.73611111\n",
      "  0.61111111]\n",
      "[[ 0.02777778  0.08333333  0.43055555  0.48611111]\n",
      " [ 0.02777778  0.          0.44444444  0.61111111]\n",
      " [ 0.02777778  0.01388889  0.44444444  0.69444444]\n",
      " [ 0.02777778  0.02777778  0.44444444  0.58333333]\n",
      " [ 0.02777778  0.04166667  0.44444444  0.58333333]] -> [ 0.91666667  0.69444444  0.76388889  0.77777778  0.73611111  0.61111111\n",
      "  0.75      ]\n",
      "[[ 0.02777778  0.          0.44444444  0.61111111]\n",
      " [ 0.02777778  0.01388889  0.44444444  0.69444444]\n",
      " [ 0.02777778  0.02777778  0.44444444  0.58333333]\n",
      " [ 0.02777778  0.04166667  0.44444444  0.58333333]\n",
      " [ 0.02777778  0.05555556  0.44444444  0.91666667]] -> [ 0.69444444  0.76388889  0.77777778  0.73611111  0.61111111  0.75        0.75      ]\n",
      "[[ 0.02777778  0.01388889  0.44444444  0.69444444]\n",
      " [ 0.02777778  0.02777778  0.44444444  0.58333333]\n",
      " [ 0.02777778  0.04166667  0.44444444  0.58333333]\n",
      " [ 0.02777778  0.05555556  0.44444444  0.91666667]\n",
      " [ 0.02777778  0.06944444  0.44444444  0.69444444]] -> [ 0.76388889  0.77777778  0.73611111  0.61111111  0.75        0.75\n",
      "  0.69444444]\n",
      "[[ 0.02777778  0.02777778  0.44444444  0.58333333]\n",
      " [ 0.02777778  0.04166667  0.44444444  0.58333333]\n",
      " [ 0.02777778  0.05555556  0.44444444  0.91666667]\n",
      " [ 0.02777778  0.06944444  0.44444444  0.69444444]\n",
      " [ 0.02777778  0.08333333  0.44444444  0.76388889]] -> [ 0.77777778  0.73611111  0.61111111  0.75        0.75        0.69444444\n",
      "  0.55555555]\n",
      "[[ 0.02777778  0.04166667  0.44444444  0.58333333]\n",
      " [ 0.02777778  0.05555556  0.44444444  0.91666667]\n",
      " [ 0.02777778  0.06944444  0.44444444  0.69444444]\n",
      " [ 0.02777778  0.08333333  0.44444444  0.76388889]\n",
      " [ 0.02777778  0.          0.45833333  0.77777778]] -> [ 0.73611111  0.61111111  0.75        0.75        0.69444444  0.55555555\n",
      "  0.68055555]\n",
      "[[ 0.02777778  0.05555556  0.44444444  0.91666667]\n",
      " [ 0.02777778  0.06944444  0.44444444  0.69444444]\n",
      " [ 0.02777778  0.08333333  0.44444444  0.76388889]\n",
      " [ 0.02777778  0.          0.45833333  0.77777778]\n",
      " [ 0.02777778  0.01388889  0.45833333  0.73611111]] -> [ 0.61111111  0.75        0.75        0.69444444  0.55555555  0.68055555\n",
      "  0.38888889]\n",
      "[[ 0.02777778  0.06944444  0.44444444  0.69444444]\n",
      " [ 0.02777778  0.08333333  0.44444444  0.76388889]\n",
      " [ 0.02777778  0.          0.45833333  0.77777778]\n",
      " [ 0.02777778  0.01388889  0.45833333  0.73611111]\n",
      " [ 0.02777778  0.02777778  0.45833333  0.61111111]] -> [ 0.75        0.75        0.69444444  0.55555555  0.68055555  0.38888889\n",
      "  1.        ]\n",
      "[[ 0.02777778  0.08333333  0.44444444  0.76388889]\n",
      " [ 0.02777778  0.          0.45833333  0.77777778]\n",
      " [ 0.02777778  0.01388889  0.45833333  0.73611111]\n",
      " [ 0.02777778  0.02777778  0.45833333  0.61111111]\n",
      " [ 0.02777778  0.04166667  0.45833333  0.75      ]] -> [ 0.75        0.69444444  0.55555555  0.68055555  0.38888889  1.\n",
      "  0.98611111]\n",
      "[[ 0.02777778  0.          0.45833333  0.77777778]\n",
      " [ 0.02777778  0.01388889  0.45833333  0.73611111]\n",
      " [ 0.02777778  0.02777778  0.45833333  0.61111111]\n",
      " [ 0.02777778  0.04166667  0.45833333  0.75      ]\n",
      " [ 0.02777778  0.05555556  0.45833333  0.75      ]] -> [ 0.69444444  0.55555555  0.68055555  0.38888889  1.          0.98611111\n",
      "  0.73611111]\n",
      "[[ 0.02777778  0.01388889  0.45833333  0.73611111]\n",
      " [ 0.02777778  0.02777778  0.45833333  0.61111111]\n",
      " [ 0.02777778  0.04166667  0.45833333  0.75      ]\n",
      " [ 0.02777778  0.05555556  0.45833333  0.75      ]\n",
      " [ 0.02777778  0.06944444  0.45833333  0.69444444]] -> [ 0.55555555  0.68055555  0.38888889  1.          0.98611111  0.73611111\n",
      "  0.59722222]\n",
      "[[ 0.02777778  0.02777778  0.45833333  0.61111111]\n",
      " [ 0.02777778  0.04166667  0.45833333  0.75      ]\n",
      " [ 0.02777778  0.05555556  0.45833333  0.75      ]\n",
      " [ 0.02777778  0.06944444  0.45833333  0.69444444]\n",
      " [ 0.02777778  0.08333333  0.45833333  0.55555555]] -> [ 0.68055555  0.38888889  1.          0.98611111  0.73611111  0.59722222\n",
      "  0.52777778]\n",
      "[[ 0.02777778  0.04166667  0.45833333  0.75      ]\n",
      " [ 0.02777778  0.05555556  0.45833333  0.75      ]\n",
      " [ 0.02777778  0.06944444  0.45833333  0.69444444]\n",
      " [ 0.02777778  0.08333333  0.45833333  0.55555555]\n",
      " [ 0.02777778  0.          0.47222222  0.68055555]] -> [ 0.38888889  1.          0.98611111  0.73611111  0.59722222  0.52777778\n",
      "  0.76388889]\n",
      "[[ 0.02777778  0.05555556  0.45833333  0.75      ]\n",
      " [ 0.02777778  0.06944444  0.45833333  0.69444444]\n",
      " [ 0.02777778  0.08333333  0.45833333  0.55555555]\n",
      " [ 0.02777778  0.          0.47222222  0.68055555]\n",
      " [ 0.02777778  0.01388889  0.47222222  0.38888889]] -> [ 1.          0.98611111  0.73611111  0.59722222  0.52777778  0.76388889\n",
      "  0.68055555]\n",
      "[[ 0.02777778  0.06944444  0.45833333  0.69444444]\n",
      " [ 0.02777778  0.08333333  0.45833333  0.55555555]\n",
      " [ 0.02777778  0.          0.47222222  0.68055555]\n",
      " [ 0.02777778  0.01388889  0.47222222  0.38888889]\n",
      " [ 0.02777778  0.02777778  0.47222222  1.        ]] -> [ 0.98611111  0.73611111  0.59722222  0.52777778  0.76388889  0.68055555\n",
      "  0.59722222]\n",
      "[[ 0.02777778  0.08333333  0.45833333  0.55555555]\n",
      " [ 0.02777778  0.          0.47222222  0.68055555]\n",
      " [ 0.02777778  0.01388889  0.47222222  0.38888889]\n",
      " [ 0.02777778  0.02777778  0.47222222  1.        ]\n",
      " [ 0.02777778  0.04166667  0.47222222  0.98611111]] -> [ 0.73611111  0.59722222  0.52777778  0.76388889  0.68055555  0.59722222\n",
      "  0.68055555]\n",
      "[[ 0.02777778  0.          0.47222222  0.68055555]\n",
      " [ 0.02777778  0.01388889  0.47222222  0.38888889]\n",
      " [ 0.02777778  0.02777778  0.47222222  1.        ]\n",
      " [ 0.02777778  0.04166667  0.47222222  0.98611111]\n",
      " [ 0.02777778  0.05555556  0.47222222  0.73611111]] -> [ 0.59722222  0.52777778  0.76388889  0.68055555  0.59722222  0.68055555\n",
      "  0.68055555]\n",
      "[[ 0.02777778  0.01388889  0.47222222  0.38888889]\n",
      " [ 0.02777778  0.02777778  0.47222222  1.        ]\n",
      " [ 0.02777778  0.04166667  0.47222222  0.98611111]\n",
      " [ 0.02777778  0.05555556  0.47222222  0.73611111]\n",
      " [ 0.02777778  0.06944444  0.47222222  0.59722222]] -> [ 0.52777778  0.76388889  0.68055555  0.59722222  0.68055555  0.68055555\n",
      "  0.61111111]\n",
      "[[ 0.02777778  0.02777778  0.47222222  1.        ]\n",
      " [ 0.02777778  0.04166667  0.47222222  0.98611111]\n",
      " [ 0.02777778  0.05555556  0.47222222  0.73611111]\n",
      " [ 0.02777778  0.06944444  0.47222222  0.59722222]\n",
      " [ 0.02777778  0.08333333  0.47222222  0.52777778]] -> [ 0.76388889  0.68055555  0.59722222  0.68055555  0.68055555  0.61111111\n",
      "  0.54166667]\n",
      "[[ 0.02777778  0.04166667  0.47222222  0.98611111]\n",
      " [ 0.02777778  0.05555556  0.47222222  0.73611111]\n",
      " [ 0.02777778  0.06944444  0.47222222  0.59722222]\n",
      " [ 0.02777778  0.08333333  0.47222222  0.52777778]\n",
      " [ 0.02777778  0.          0.48611111  0.76388889]] -> [ 0.68055555  0.59722222  0.68055555  0.68055555  0.61111111  0.54166667\n",
      "  0.72222222]\n",
      "[[ 0.02777778  0.05555556  0.47222222  0.73611111]\n",
      " [ 0.02777778  0.06944444  0.47222222  0.59722222]\n",
      " [ 0.02777778  0.08333333  0.47222222  0.52777778]\n",
      " [ 0.02777778  0.          0.48611111  0.76388889]\n",
      " [ 0.02777778  0.01388889  0.48611111  0.68055555]] -> [ 0.59722222  0.68055555  0.68055555  0.61111111  0.54166667  0.72222222\n",
      "  0.625     ]\n",
      "[[ 0.02777778  0.06944444  0.47222222  0.59722222]\n",
      " [ 0.02777778  0.08333333  0.47222222  0.52777778]\n",
      " [ 0.02777778  0.          0.48611111  0.76388889]\n",
      " [ 0.02777778  0.01388889  0.48611111  0.68055555]\n",
      " [ 0.02777778  0.02777778  0.48611111  0.59722222]] -> [ 0.68055555  0.68055555  0.61111111  0.54166667  0.72222222  0.625\n",
      "  0.45833333]\n",
      "[[ 0.02777778  0.08333333  0.47222222  0.52777778]\n",
      " [ 0.02777778  0.          0.48611111  0.76388889]\n",
      " [ 0.02777778  0.01388889  0.48611111  0.68055555]\n",
      " [ 0.02777778  0.02777778  0.48611111  0.59722222]\n",
      " [ 0.02777778  0.04166667  0.48611111  0.68055555]] -> [ 0.68055555  0.61111111  0.54166667  0.72222222  0.625       0.45833333\n",
      "  0.59722222]\n",
      "[[ 0.02777778  0.          0.48611111  0.76388889]\n",
      " [ 0.02777778  0.01388889  0.48611111  0.68055555]\n",
      " [ 0.02777778  0.02777778  0.48611111  0.59722222]\n",
      " [ 0.02777778  0.04166667  0.48611111  0.68055555]\n",
      " [ 0.04166667  0.05555556  0.48611111  0.68055555]] -> [ 0.61111111  0.54166667  0.72222222  0.625       0.45833333  0.59722222\n",
      "  0.55555555]\n",
      "[[ 0.02777778  0.01388889  0.48611111  0.68055555]\n",
      " [ 0.02777778  0.02777778  0.48611111  0.59722222]\n",
      " [ 0.02777778  0.04166667  0.48611111  0.68055555]\n",
      " [ 0.04166667  0.05555556  0.48611111  0.68055555]\n",
      " [ 0.04166667  0.06944444  0.48611111  0.61111111]] -> [ 0.54166667  0.72222222  0.625       0.45833333  0.59722222  0.55555555\n",
      "  0.63888889]\n",
      "[[ 0.02777778  0.02777778  0.48611111  0.59722222]\n",
      " [ 0.02777778  0.04166667  0.48611111  0.68055555]\n",
      " [ 0.04166667  0.05555556  0.48611111  0.68055555]\n",
      " [ 0.04166667  0.06944444  0.48611111  0.61111111]\n",
      " [ 0.04166667  0.08333333  0.48611111  0.54166667]] -> [ 0.72222222  0.625       0.45833333  0.59722222  0.55555555  0.63888889\n",
      "  0.68055555]\n",
      "[[ 0.02777778  0.04166667  0.48611111  0.68055555]\n",
      " [ 0.04166667  0.05555556  0.48611111  0.68055555]\n",
      " [ 0.04166667  0.06944444  0.48611111  0.61111111]\n",
      " [ 0.04166667  0.08333333  0.48611111  0.54166667]\n",
      " [ 0.04166667  0.          0.5         0.72222222]] -> [ 0.625       0.45833333  0.59722222  0.55555555  0.63888889  0.68055555\n",
      "  0.69444444]\n",
      "[[ 0.04166667  0.05555556  0.48611111  0.68055555]\n",
      " [ 0.04166667  0.06944444  0.48611111  0.61111111]\n",
      " [ 0.04166667  0.08333333  0.48611111  0.54166667]\n",
      " [ 0.04166667  0.          0.5         0.72222222]\n",
      " [ 0.04166667  0.01388889  0.5         0.625     ]] -> [ 0.45833333  0.59722222  0.55555555  0.63888889  0.68055555  0.69444444\n",
      "  0.51388889]\n",
      "[[ 0.04166667  0.06944444  0.48611111  0.61111111]\n",
      " [ 0.04166667  0.08333333  0.48611111  0.54166667]\n",
      " [ 0.04166667  0.          0.5         0.72222222]\n",
      " [ 0.04166667  0.01388889  0.5         0.625     ]\n",
      " [ 0.04166667  0.02777778  0.5         0.45833333]] -> [ 0.59722222  0.55555555  0.63888889  0.68055555  0.69444444  0.51388889\n",
      "  0.51388889]\n",
      "[[ 0.04166667  0.08333333  0.48611111  0.54166667]\n",
      " [ 0.04166667  0.          0.5         0.72222222]\n",
      " [ 0.04166667  0.01388889  0.5         0.625     ]\n",
      " [ 0.04166667  0.02777778  0.5         0.45833333]\n",
      " [ 0.04166667  0.04166667  0.5         0.59722222]] -> [ 0.55555555  0.63888889  0.68055555  0.69444444  0.51388889  0.51388889\n",
      "  0.625     ]\n",
      "[[ 0.04166667  0.          0.5         0.72222222]\n",
      " [ 0.04166667  0.01388889  0.5         0.625     ]\n",
      " [ 0.04166667  0.02777778  0.5         0.45833333]\n",
      " [ 0.04166667  0.04166667  0.5         0.59722222]\n",
      " [ 0.04166667  0.05555556  0.5         0.55555555]] -> [ 0.63888889  0.68055555  0.69444444  0.51388889  0.51388889  0.625\n",
      "  0.66666667]\n",
      "[[ 0.04166667  0.01388889  0.5         0.625     ]\n",
      " [ 0.04166667  0.02777778  0.5         0.45833333]\n",
      " [ 0.04166667  0.04166667  0.5         0.59722222]\n",
      " [ 0.04166667  0.05555556  0.5         0.55555555]\n",
      " [ 0.04166667  0.06944444  0.5         0.63888889]] -> [ 0.68055555  0.69444444  0.51388889  0.51388889  0.625       0.66666667\n",
      "  0.66666667]\n",
      "[[ 0.04166667  0.02777778  0.5         0.45833333]\n",
      " [ 0.04166667  0.04166667  0.5         0.59722222]\n",
      " [ 0.04166667  0.05555556  0.5         0.55555555]\n",
      " [ 0.04166667  0.06944444  0.5         0.63888889]\n",
      " [ 0.04166667  0.08333333  0.5         0.68055555]] -> [ 0.69444444  0.51388889  0.51388889  0.625       0.66666667  0.66666667\n",
      "  0.52777778]\n",
      "[[ 0.04166667  0.04166667  0.5         0.59722222]\n",
      " [ 0.04166667  0.05555556  0.5         0.55555555]\n",
      " [ 0.04166667  0.06944444  0.5         0.63888889]\n",
      " [ 0.04166667  0.08333333  0.5         0.68055555]\n",
      " [ 0.04166667  0.          0.51388889  0.69444444]] -> [ 0.51388889  0.51388889  0.625       0.66666667  0.66666667  0.52777778\n",
      "  0.83333333]\n",
      "[[ 0.04166667  0.05555556  0.5         0.55555555]\n",
      " [ 0.04166667  0.06944444  0.5         0.63888889]\n",
      " [ 0.04166667  0.08333333  0.5         0.68055555]\n",
      " [ 0.04166667  0.          0.51388889  0.69444444]\n",
      " [ 0.04166667  0.01388889  0.51388889  0.51388889]] -> [ 0.51388889  0.625       0.66666667  0.66666667  0.52777778  0.83333333\n",
      "  0.43055555]\n",
      "[[ 0.04166667  0.06944444  0.5         0.63888889]\n",
      " [ 0.04166667  0.08333333  0.5         0.68055555]\n",
      " [ 0.04166667  0.          0.51388889  0.69444444]\n",
      " [ 0.04166667  0.01388889  0.51388889  0.51388889]\n",
      " [ 0.04166667  0.02777778  0.51388889  0.51388889]] -> [ 0.625       0.66666667  0.66666667  0.52777778  0.83333333  0.43055555\n",
      "  0.48611111]\n",
      "[[ 0.04166667  0.08333333  0.5         0.68055555]\n",
      " [ 0.04166667  0.          0.51388889  0.69444444]\n",
      " [ 0.04166667  0.01388889  0.51388889  0.51388889]\n",
      " [ 0.04166667  0.02777778  0.51388889  0.51388889]\n",
      " [ 0.04166667  0.04166667  0.51388889  0.625     ]] -> [ 0.66666667  0.66666667  0.52777778  0.83333333  0.43055555  0.48611111\n",
      "  0.73611111]\n",
      "[[ 0.04166667  0.          0.51388889  0.69444444]\n",
      " [ 0.04166667  0.01388889  0.51388889  0.51388889]\n",
      " [ 0.04166667  0.02777778  0.51388889  0.51388889]\n",
      " [ 0.04166667  0.04166667  0.51388889  0.625     ]\n",
      " [ 0.04166667  0.05555556  0.51388889  0.66666667]] -> [ 0.66666667  0.52777778  0.83333333  0.43055555  0.48611111  0.73611111\n",
      "  0.97222222]\n",
      "[[ 0.04166667  0.01388889  0.51388889  0.51388889]\n",
      " [ 0.04166667  0.02777778  0.51388889  0.51388889]\n",
      " [ 0.04166667  0.04166667  0.51388889  0.625     ]\n",
      " [ 0.04166667  0.05555556  0.51388889  0.66666667]\n",
      " [ 0.04166667  0.06944444  0.51388889  0.66666667]] -> [ 0.52777778  0.83333333  0.43055555  0.48611111  0.73611111  0.97222222\n",
      "  0.86111111]\n",
      "[[ 0.04166667  0.02777778  0.51388889  0.51388889]\n",
      " [ 0.04166667  0.04166667  0.51388889  0.625     ]\n",
      " [ 0.04166667  0.05555556  0.51388889  0.66666667]\n",
      " [ 0.04166667  0.06944444  0.51388889  0.66666667]\n",
      " [ 0.04166667  0.08333333  0.51388889  0.52777778]] -> [ 0.83333333  0.43055555  0.48611111  0.73611111  0.97222222  0.86111111\n",
      "  0.66666667]\n",
      "[[ 0.04166667  0.04166667  0.51388889  0.625     ]\n",
      " [ 0.04166667  0.05555556  0.51388889  0.66666667]\n",
      " [ 0.04166667  0.06944444  0.51388889  0.66666667]\n",
      " [ 0.04166667  0.08333333  0.51388889  0.52777778]\n",
      " [ 0.04166667  0.          0.52777778  0.83333333]] -> [ 0.43055555  0.48611111  0.73611111  0.97222222  0.86111111  0.66666667\n",
      "  0.70833333]\n",
      "[[ 0.04166667  0.05555556  0.51388889  0.66666667]\n",
      " [ 0.04166667  0.06944444  0.51388889  0.66666667]\n",
      " [ 0.04166667  0.08333333  0.51388889  0.52777778]\n",
      " [ 0.04166667  0.          0.52777778  0.83333333]\n",
      " [ 0.04166667  0.01388889  0.52777778  0.43055555]] -> [ 0.48611111  0.73611111  0.97222222  0.86111111  0.66666667  0.70833333\n",
      "  0.68055555]\n",
      "[[ 0.04166667  0.06944444  0.51388889  0.66666667]\n",
      " [ 0.04166667  0.08333333  0.51388889  0.52777778]\n",
      " [ 0.04166667  0.          0.52777778  0.83333333]\n",
      " [ 0.04166667  0.01388889  0.52777778  0.43055555]\n",
      " [ 0.04166667  0.02777778  0.52777778  0.48611111]] -> [ 0.73611111  0.97222222  0.86111111  0.66666667  0.70833333  0.68055555\n",
      "  0.52777778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 105.85023498535156\n",
      "[step: 1] loss: 101.60631561279297\n",
      "[step: 2] loss: 97.79518127441406\n",
      "[step: 3] loss: 94.26548767089844\n",
      "[step: 4] loss: 90.87113952636719\n",
      "[step: 5] loss: 87.54103088378906\n",
      "[step: 6] loss: 84.23419189453125\n",
      "[step: 7] loss: 80.91807556152344\n",
      "[step: 8] loss: 77.562744140625\n",
      "[step: 9] loss: 74.14495086669922\n",
      "[step: 10] loss: 70.65010833740234\n",
      "[step: 11] loss: 67.0784912109375\n",
      "[step: 12] loss: 63.44300842285156\n",
      "[step: 13] loss: 59.774208068847656\n",
      "[step: 14] loss: 56.12092208862305\n",
      "[step: 15] loss: 52.34928894042969\n",
      "[step: 16] loss: 48.40460968017578\n",
      "[step: 17] loss: 44.49124526977539\n",
      "[step: 18] loss: 40.89198303222656\n",
      "[step: 19] loss: 37.76025390625\n",
      "[step: 20] loss: 34.94670867919922\n",
      "[step: 21] loss: 32.00694274902344\n",
      "[step: 22] loss: 28.73839569091797\n",
      "[step: 23] loss: 25.232728958129883\n",
      "[step: 24] loss: 21.719409942626953\n",
      "[step: 25] loss: 18.401546478271484\n",
      "[step: 26] loss: 15.382979393005371\n",
      "[step: 27] loss: 12.695536613464355\n",
      "[step: 28] loss: 10.351354598999023\n",
      "[step: 29] loss: 8.38929557800293\n",
      "[step: 30] loss: 6.908931732177734\n",
      "[step: 31] loss: 6.011526107788086\n",
      "[step: 32] loss: 5.64517879486084\n",
      "[step: 33] loss: 5.609988212585449\n",
      "[step: 34] loss: 5.659836292266846\n",
      "[step: 35] loss: 5.648432731628418\n",
      "[step: 36] loss: 5.5434250831604\n",
      "[step: 37] loss: 5.395279884338379\n",
      "[step: 38] loss: 5.222311973571777\n",
      "[step: 39] loss: 5.0463480949401855\n",
      "[step: 40] loss: 4.862923622131348\n",
      "[step: 41] loss: 4.693858623504639\n",
      "[step: 42] loss: 4.565739631652832\n",
      "[step: 43] loss: 4.513187408447266\n",
      "[step: 44] loss: 4.499303340911865\n",
      "[step: 45] loss: 4.43287467956543\n",
      "[step: 46] loss: 4.327342987060547\n",
      "[step: 47] loss: 4.245838642120361\n",
      "[step: 48] loss: 4.2173309326171875\n",
      "[step: 49] loss: 4.220731735229492\n",
      "[step: 50] loss: 4.219547271728516\n",
      "[step: 51] loss: 4.193967819213867\n",
      "[step: 52] loss: 4.146999835968018\n",
      "[step: 53] loss: 4.093734264373779\n",
      "[step: 54] loss: 4.049712181091309\n",
      "[step: 55] loss: 4.024087429046631\n",
      "[step: 56] loss: 4.017677307128906\n",
      "[step: 57] loss: 4.02503776550293\n",
      "[step: 58] loss: 4.0363359451293945\n",
      "[step: 59] loss: 4.041525363922119\n",
      "[step: 60] loss: 4.034634590148926\n",
      "[step: 61] loss: 4.016035556793213\n",
      "[step: 62] loss: 3.9911930561065674\n",
      "[step: 63] loss: 3.9671337604522705\n",
      "[step: 64] loss: 3.9491162300109863\n",
      "[step: 65] loss: 3.9391613006591797\n",
      "[step: 66] loss: 3.936344623565674\n",
      "[step: 67] loss: 3.938467502593994\n",
      "[step: 68] loss: 3.942474842071533\n",
      "[step: 69] loss: 3.9459004402160645\n",
      "[step: 70] loss: 3.947146415710449\n",
      "[step: 71] loss: 3.9456777572631836\n",
      "[step: 72] loss: 3.9417214393615723\n",
      "[step: 73] loss: 3.935969114303589\n",
      "[step: 74] loss: 3.9293289184570312\n",
      "[step: 75] loss: 3.9227447509765625\n",
      "[step: 76] loss: 3.9170308113098145\n",
      "[step: 77] loss: 3.912717342376709\n",
      "[step: 78] loss: 3.909956216812134\n",
      "[step: 79] loss: 3.9085402488708496\n",
      "[step: 80] loss: 3.908027172088623\n",
      "[step: 81] loss: 3.9079129695892334\n",
      "[step: 82] loss: 3.9077601432800293\n",
      "[step: 83] loss: 3.9072742462158203\n",
      "[step: 84] loss: 3.9062631130218506\n",
      "[step: 85] loss: 3.9047131538391113\n",
      "[step: 86] loss: 3.9027464389801025\n",
      "[step: 87] loss: 3.900589942932129\n",
      "[step: 88] loss: 3.8985090255737305\n",
      "[step: 89] loss: 3.896714687347412\n",
      "[step: 90] loss: 3.8952832221984863\n",
      "[step: 91] loss: 3.8941457271575928\n",
      "[step: 92] loss: 3.893160104751587\n",
      "[step: 93] loss: 3.8921103477478027\n",
      "[step: 94] loss: 3.8908989429473877\n",
      "[step: 95] loss: 3.8895459175109863\n",
      "[step: 96] loss: 3.8881630897521973\n",
      "[step: 97] loss: 3.886880874633789\n",
      "[step: 98] loss: 3.885779619216919\n",
      "[step: 99] loss: 3.8848509788513184\n",
      "[step: 100] loss: 3.8840172290802\n",
      "[step: 101] loss: 3.8831751346588135\n",
      "[step: 102] loss: 3.882234573364258\n",
      "[step: 103] loss: 3.8811745643615723\n",
      "[step: 104] loss: 3.8800048828125\n",
      "[step: 105] loss: 3.8787732124328613\n",
      "[step: 106] loss: 3.877542495727539\n",
      "[step: 107] loss: 3.87636661529541\n",
      "[step: 108] loss: 3.875281810760498\n",
      "[step: 109] loss: 3.874286413192749\n",
      "[step: 110] loss: 3.8733441829681396\n",
      "[step: 111] loss: 3.8724067211151123\n",
      "[step: 112] loss: 3.871431350708008\n",
      "[step: 113] loss: 3.870401382446289\n",
      "[step: 114] loss: 3.8693270683288574\n",
      "[step: 115] loss: 3.8682241439819336\n",
      "[step: 116] loss: 3.867114782333374\n",
      "[step: 117] loss: 3.8660178184509277\n",
      "[step: 118] loss: 3.8649394512176514\n",
      "[step: 119] loss: 3.863879680633545\n",
      "[step: 120] loss: 3.8628103733062744\n",
      "[step: 121] loss: 3.8617396354675293\n",
      "[step: 122] loss: 3.860661506652832\n",
      "[step: 123] loss: 3.8595752716064453\n",
      "[step: 124] loss: 3.85849666595459\n",
      "[step: 125] loss: 3.857347011566162\n",
      "[step: 126] loss: 3.856168031692505\n",
      "[step: 127] loss: 3.8550009727478027\n",
      "[step: 128] loss: 3.8538153171539307\n",
      "[step: 129] loss: 3.8526127338409424\n",
      "[step: 130] loss: 3.8514294624328613\n",
      "[step: 131] loss: 3.8502187728881836\n",
      "[step: 132] loss: 3.8489933013916016\n",
      "[step: 133] loss: 3.8477096557617188\n",
      "[step: 134] loss: 3.8463759422302246\n",
      "[step: 135] loss: 3.8449952602386475\n",
      "[step: 136] loss: 3.843627452850342\n",
      "[step: 137] loss: 3.8422956466674805\n",
      "[step: 138] loss: 3.8409831523895264\n",
      "[step: 139] loss: 3.8396944999694824\n",
      "[step: 140] loss: 3.838411808013916\n",
      "[step: 141] loss: 3.8371291160583496\n",
      "[step: 142] loss: 3.8358802795410156\n",
      "[step: 143] loss: 3.8346238136291504\n",
      "[step: 144] loss: 3.833357334136963\n",
      "[step: 145] loss: 3.832080364227295\n",
      "[step: 146] loss: 3.8307948112487793\n",
      "[step: 147] loss: 3.829500436782837\n",
      "[step: 148] loss: 3.828195571899414\n",
      "[step: 149] loss: 3.826878547668457\n",
      "[step: 150] loss: 3.825549602508545\n",
      "[step: 151] loss: 3.8242084980010986\n",
      "[step: 152] loss: 3.822854518890381\n",
      "[step: 153] loss: 3.8214855194091797\n",
      "[step: 154] loss: 3.820098876953125\n",
      "[step: 155] loss: 3.818695068359375\n",
      "[step: 156] loss: 3.8172760009765625\n",
      "[step: 157] loss: 3.815840482711792\n",
      "[step: 158] loss: 3.814389705657959\n",
      "[step: 159] loss: 3.8129215240478516\n",
      "[step: 160] loss: 3.811434030532837\n",
      "[step: 161] loss: 3.809927225112915\n",
      "[step: 162] loss: 3.8083996772766113\n",
      "[step: 163] loss: 3.806849956512451\n",
      "[step: 164] loss: 3.8052778244018555\n",
      "[step: 165] loss: 3.803682804107666\n",
      "[step: 166] loss: 3.802064895629883\n",
      "[step: 167] loss: 3.800421714782715\n",
      "[step: 168] loss: 3.798752546310425\n",
      "[step: 169] loss: 3.797055721282959\n",
      "[step: 170] loss: 3.7953290939331055\n",
      "[step: 171] loss: 3.793572187423706\n",
      "[step: 172] loss: 3.791783094406128\n",
      "[step: 173] loss: 3.7899603843688965\n",
      "[step: 174] loss: 3.7881019115448\n",
      "[step: 175] loss: 3.7862071990966797\n",
      "[step: 176] loss: 3.784273147583008\n",
      "[step: 177] loss: 3.7822980880737305\n",
      "[step: 178] loss: 3.7802798748016357\n",
      "[step: 179] loss: 3.7782187461853027\n",
      "[step: 180] loss: 3.776113510131836\n",
      "[step: 181] loss: 3.773958444595337\n",
      "[step: 182] loss: 3.771796464920044\n",
      "[step: 183] loss: 3.769589424133301\n",
      "[step: 184] loss: 3.7673332691192627\n",
      "[step: 185] loss: 3.765018939971924\n",
      "[step: 186] loss: 3.762661933898926\n",
      "[step: 187] loss: 3.7602553367614746\n",
      "[step: 188] loss: 3.757781505584717\n",
      "[step: 189] loss: 3.7552409172058105\n",
      "[step: 190] loss: 3.7526369094848633\n",
      "[step: 191] loss: 3.7499594688415527\n",
      "[step: 192] loss: 3.747204065322876\n",
      "[step: 193] loss: 3.7443718910217285\n",
      "[step: 194] loss: 3.741466522216797\n",
      "[step: 195] loss: 3.738459348678589\n",
      "[step: 196] loss: 3.73537540435791\n",
      "[step: 197] loss: 3.7321834564208984\n",
      "[step: 198] loss: 3.7288923263549805\n",
      "[step: 199] loss: 3.7255072593688965\n",
      "[step: 200] loss: 3.7220101356506348\n",
      "[step: 201] loss: 3.7184133529663086\n",
      "[step: 202] loss: 3.71466064453125\n",
      "[step: 203] loss: 3.7107725143432617\n",
      "[step: 204] loss: 3.7068276405334473\n",
      "[step: 205] loss: 3.7026143074035645\n",
      "[step: 206] loss: 3.698385000228882\n",
      "[step: 207] loss: 3.693894386291504\n",
      "[step: 208] loss: 3.68927001953125\n",
      "[step: 209] loss: 3.684495449066162\n",
      "[step: 210] loss: 3.6794166564941406\n",
      "[step: 211] loss: 3.674213409423828\n",
      "[step: 212] loss: 3.668837785720825\n",
      "[step: 213] loss: 3.6631784439086914\n",
      "[step: 214] loss: 3.657210350036621\n",
      "[step: 215] loss: 3.6511831283569336\n",
      "[step: 216] loss: 3.6452746391296387\n",
      "[step: 217] loss: 3.638030529022217\n",
      "[step: 218] loss: 3.6317548751831055\n",
      "[step: 219] loss: 3.624033212661743\n",
      "[step: 220] loss: 3.6163713932037354\n",
      "[step: 221] loss: 3.6090478897094727\n",
      "[step: 222] loss: 3.600005626678467\n",
      "[step: 223] loss: 3.5917437076568604\n",
      "[step: 224] loss: 3.5831875801086426\n",
      "[step: 225] loss: 3.574639320373535\n",
      "[step: 226] loss: 3.564531087875366\n",
      "[step: 227] loss: 3.553819179534912\n",
      "[step: 228] loss: 3.543256998062134\n",
      "[step: 229] loss: 3.5325212478637695\n",
      "[step: 230] loss: 3.521592378616333\n",
      "[step: 231] loss: 3.5105676651000977\n",
      "[step: 232] loss: 3.5009613037109375\n",
      "[step: 233] loss: 3.5235204696655273\n",
      "[step: 234] loss: 3.746896743774414\n",
      "[step: 235] loss: 3.770843267440796\n",
      "[step: 236] loss: 3.6008777618408203\n",
      "[step: 237] loss: 3.7048683166503906\n",
      "[step: 238] loss: 3.506542205810547\n",
      "[step: 239] loss: 3.618319511413574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 240] loss: 3.5612549781799316\n",
      "[step: 241] loss: 3.5151517391204834\n",
      "[step: 242] loss: 3.5705647468566895\n",
      "[step: 243] loss: 3.4742648601531982\n",
      "[step: 244] loss: 3.5700173377990723\n",
      "[step: 245] loss: 3.447044849395752\n",
      "[step: 246] loss: 3.5488760471343994\n",
      "[step: 247] loss: 3.4509432315826416\n",
      "[step: 248] loss: 3.5017240047454834\n",
      "[step: 249] loss: 3.4733166694641113\n",
      "[step: 250] loss: 3.4558932781219482\n",
      "[step: 251] loss: 3.479680299758911\n",
      "[step: 252] loss: 3.433781623840332\n",
      "[step: 253] loss: 3.4740262031555176\n",
      "[step: 254] loss: 3.420133352279663\n",
      "[step: 255] loss: 3.461965560913086\n",
      "[step: 256] loss: 3.4146759510040283\n",
      "[step: 257] loss: 3.444821357727051\n",
      "[step: 258] loss: 3.412489414215088\n",
      "[step: 259] loss: 3.4303977489471436\n",
      "[step: 260] loss: 3.4059746265411377\n",
      "[step: 261] loss: 3.4185614585876465\n",
      "[step: 262] loss: 3.40097713470459\n",
      "[step: 263] loss: 3.4106974601745605\n",
      "[step: 264] loss: 3.3926661014556885\n",
      "[step: 265] loss: 3.4040818214416504\n",
      "[step: 266] loss: 3.3863930702209473\n",
      "[step: 267] loss: 3.3996081352233887\n",
      "[step: 268] loss: 3.379611015319824\n",
      "[step: 269] loss: 3.390937566757202\n",
      "[step: 270] loss: 3.3758370876312256\n",
      "[step: 271] loss: 3.382734537124634\n",
      "[step: 272] loss: 3.3758561611175537\n",
      "[step: 273] loss: 3.3734493255615234\n",
      "[step: 274] loss: 3.3760592937469482\n",
      "[step: 275] loss: 3.366398334503174\n",
      "[step: 276] loss: 3.3709821701049805\n",
      "[step: 277] loss: 3.3653604984283447\n",
      "[step: 278] loss: 3.3638808727264404\n",
      "[step: 279] loss: 3.3656258583068848\n",
      "[step: 280] loss: 3.3599464893341064\n",
      "[step: 281] loss: 3.361110210418701\n",
      "[step: 282] loss: 3.360293388366699\n",
      "[step: 283] loss: 3.3564343452453613\n",
      "[step: 284] loss: 3.358168125152588\n",
      "[step: 285] loss: 3.356107711791992\n",
      "[step: 286] loss: 3.353583812713623\n",
      "[step: 287] loss: 3.3548736572265625\n",
      "[step: 288] loss: 3.352764844894409\n",
      "[step: 289] loss: 3.350804567337036\n",
      "[step: 290] loss: 3.351640224456787\n",
      "[step: 291] loss: 3.349942207336426\n",
      "[step: 292] loss: 3.3480753898620605\n",
      "[step: 293] loss: 3.3485145568847656\n",
      "[step: 294] loss: 3.3473446369171143\n",
      "[step: 295] loss: 3.34550142288208\n",
      "[step: 296] loss: 3.345538854598999\n",
      "[step: 297] loss: 3.3448333740234375\n",
      "[step: 298] loss: 3.3431129455566406\n",
      "[step: 299] loss: 3.3427371978759766\n",
      "[step: 300] loss: 3.342339515686035\n",
      "[step: 301] loss: 3.340878963470459\n",
      "[step: 302] loss: 3.340146780014038\n",
      "[step: 303] loss: 3.339857816696167\n",
      "[step: 304] loss: 3.3387389183044434\n",
      "[step: 305] loss: 3.337782859802246\n",
      "[step: 306] loss: 3.33742618560791\n",
      "[step: 307] loss: 3.3366308212280273\n",
      "[step: 308] loss: 3.335620403289795\n",
      "[step: 309] loss: 3.3350956439971924\n",
      "[step: 310] loss: 3.3345181941986084\n",
      "[step: 311] loss: 3.3335981369018555\n",
      "[step: 312] loss: 3.332902193069458\n",
      "[step: 313] loss: 3.3323984146118164\n",
      "[step: 314] loss: 3.3316409587860107\n",
      "[step: 315] loss: 3.33085298538208\n",
      "[step: 316] loss: 3.3302981853485107\n",
      "[step: 317] loss: 3.3296854496002197\n",
      "[step: 318] loss: 3.328916072845459\n",
      "[step: 319] loss: 3.3282651901245117\n",
      "[step: 320] loss: 3.3277082443237305\n",
      "[step: 321] loss: 3.3270325660705566\n",
      "[step: 322] loss: 3.3263297080993652\n",
      "[step: 323] loss: 3.325737953186035\n",
      "[step: 324] loss: 3.3251471519470215\n",
      "[step: 325] loss: 3.3244752883911133\n",
      "[step: 326] loss: 3.3238277435302734\n",
      "[step: 327] loss: 3.3232483863830566\n",
      "[step: 328] loss: 3.3226447105407715\n",
      "[step: 329] loss: 3.321995973587036\n",
      "[step: 330] loss: 3.3213791847229004\n",
      "[step: 331] loss: 3.3208022117614746\n",
      "[step: 332] loss: 3.320201873779297\n",
      "[step: 333] loss: 3.319577217102051\n",
      "[step: 334] loss: 3.3189756870269775\n",
      "[step: 335] loss: 3.3184008598327637\n",
      "[step: 336] loss: 3.3178114891052246\n",
      "[step: 337] loss: 3.317207098007202\n",
      "[step: 338] loss: 3.3166146278381348\n",
      "[step: 339] loss: 3.3160412311553955\n",
      "[step: 340] loss: 3.315464735031128\n",
      "[step: 341] loss: 3.314875841140747\n",
      "[step: 342] loss: 3.3142900466918945\n",
      "[step: 343] loss: 3.3137192726135254\n",
      "[step: 344] loss: 3.3132195472717285\n",
      "[step: 345] loss: 3.312864303588867\n",
      "[step: 346] loss: 3.312708854675293\n",
      "[step: 347] loss: 3.312389850616455\n",
      "[step: 348] loss: 3.311691999435425\n",
      "[step: 349] loss: 3.3109378814697266\n",
      "[step: 350] loss: 3.309985876083374\n",
      "[step: 351] loss: 3.309283494949341\n",
      "[step: 352] loss: 3.3089537620544434\n",
      "[step: 353] loss: 3.308587074279785\n",
      "[step: 354] loss: 3.307899236679077\n",
      "[step: 355] loss: 3.3071184158325195\n",
      "[step: 356] loss: 3.3065431118011475\n",
      "[step: 357] loss: 3.3061509132385254\n",
      "[step: 358] loss: 3.305691957473755\n",
      "[step: 359] loss: 3.3050425052642822\n",
      "[step: 360] loss: 3.30434513092041\n",
      "[step: 361] loss: 3.3037662506103516\n",
      "[step: 362] loss: 3.3033061027526855\n",
      "[step: 363] loss: 3.302830934524536\n",
      "[step: 364] loss: 3.3022501468658447\n",
      "[step: 365] loss: 3.3016552925109863\n",
      "[step: 366] loss: 3.301274538040161\n",
      "[step: 367] loss: 3.3009910583496094\n",
      "[step: 368] loss: 3.300955295562744\n",
      "[step: 369] loss: 3.3007609844207764\n",
      "[step: 370] loss: 3.3006324768066406\n",
      "[step: 371] loss: 3.2997922897338867\n",
      "[step: 372] loss: 3.2993078231811523\n",
      "[step: 373] loss: 3.2981698513031006\n",
      "[step: 374] loss: 3.2969489097595215\n",
      "[step: 375] loss: 3.296262741088867\n",
      "[step: 376] loss: 3.296093225479126\n",
      "[step: 377] loss: 3.2959675788879395\n",
      "[step: 378] loss: 3.2955105304718018\n",
      "[step: 379] loss: 3.2946529388427734\n",
      "[step: 380] loss: 3.29372501373291\n",
      "[step: 381] loss: 3.2930054664611816\n",
      "[step: 382] loss: 3.292565107345581\n",
      "[step: 383] loss: 3.292252540588379\n",
      "[step: 384] loss: 3.2918572425842285\n",
      "[step: 385] loss: 3.2912895679473877\n",
      "[step: 386] loss: 3.2905683517456055\n",
      "[step: 387] loss: 3.289825439453125\n",
      "[step: 388] loss: 3.2891571521759033\n",
      "[step: 389] loss: 3.288602352142334\n",
      "[step: 390] loss: 3.2881295680999756\n",
      "[step: 391] loss: 3.287679672241211\n",
      "[step: 392] loss: 3.287234306335449\n",
      "[step: 393] loss: 3.287487745285034\n",
      "[step: 394] loss: 3.2881827354431152\n",
      "[step: 395] loss: 3.289836883544922\n",
      "[step: 396] loss: 3.2919018268585205\n",
      "[step: 397] loss: 3.2920360565185547\n",
      "[step: 398] loss: 3.2917320728302\n",
      "[step: 399] loss: 3.2891764640808105\n",
      "[step: 400] loss: 3.2864153385162354\n",
      "[step: 401] loss: 3.2833549976348877\n",
      "[step: 402] loss: 3.281538963317871\n",
      "[step: 403] loss: 3.281055450439453\n",
      "[step: 404] loss: 3.2815070152282715\n",
      "[step: 405] loss: 3.2823143005371094\n",
      "[step: 406] loss: 3.282698154449463\n",
      "[step: 407] loss: 3.282608985900879\n",
      "[step: 408] loss: 3.281510829925537\n",
      "[step: 409] loss: 3.2800610065460205\n",
      "[step: 410] loss: 3.2783045768737793\n",
      "[step: 411] loss: 3.276824474334717\n",
      "[step: 412] loss: 3.2757794857025146\n",
      "[step: 413] loss: 3.2752158641815186\n",
      "[step: 414] loss: 3.275002956390381\n",
      "[step: 415] loss: 3.2749385833740234\n",
      "[step: 416] loss: 3.2748911380767822\n",
      "[step: 417] loss: 3.2746613025665283\n",
      "[step: 418] loss: 3.2742724418640137\n",
      "[step: 419] loss: 3.2729763984680176\n",
      "[step: 420] loss: 3.2717907428741455\n",
      "[step: 421] loss: 3.2705416679382324\n",
      "[step: 422] loss: 3.2698726654052734\n",
      "[step: 423] loss: 3.2695775032043457\n",
      "[step: 424] loss: 3.2692363262176514\n",
      "[step: 425] loss: 3.268700122833252\n",
      "[step: 426] loss: 3.268028736114502\n",
      "[step: 427] loss: 3.267070770263672\n",
      "[step: 428] loss: 3.26621675491333\n",
      "[step: 429] loss: 3.2654552459716797\n",
      "[step: 430] loss: 3.2647483348846436\n",
      "[step: 431] loss: 3.264075517654419\n",
      "[step: 432] loss: 3.263425350189209\n",
      "[step: 433] loss: 3.2627954483032227\n",
      "[step: 434] loss: 3.262197971343994\n",
      "[step: 435] loss: 3.2616565227508545\n",
      "[step: 436] loss: 3.2612380981445312\n",
      "[step: 437] loss: 3.261051893234253\n",
      "[step: 438] loss: 3.2616164684295654\n",
      "[step: 439] loss: 3.2650492191314697\n",
      "[step: 440] loss: 3.2747018337249756\n",
      "[step: 441] loss: 3.293877601623535\n",
      "[step: 442] loss: 3.334597110748291\n",
      "[step: 443] loss: 3.351271867752075\n",
      "[step: 444] loss: 3.3613030910491943\n",
      "[step: 445] loss: 3.2854418754577637\n",
      "[step: 446] loss: 3.255073070526123\n",
      "[step: 447] loss: 3.2894551753997803\n",
      "[step: 448] loss: 3.3036813735961914\n",
      "[step: 449] loss: 3.273120880126953\n",
      "[step: 450] loss: 3.2536888122558594\n",
      "[step: 451] loss: 3.280832290649414\n",
      "[step: 452] loss: 3.283341646194458\n",
      "[step: 453] loss: 3.252406120300293\n",
      "[step: 454] loss: 3.265561103820801\n",
      "[step: 455] loss: 3.279470682144165\n",
      "[step: 456] loss: 3.25321364402771\n",
      "[step: 457] loss: 3.2568864822387695\n",
      "[step: 458] loss: 3.271397590637207\n",
      "[step: 459] loss: 3.2522778511047363\n",
      "[step: 460] loss: 3.2509584426879883\n",
      "[step: 461] loss: 3.2631704807281494\n",
      "[step: 462] loss: 3.250601053237915\n",
      "[step: 463] loss: 3.246180534362793\n",
      "[step: 464] loss: 3.255434513092041\n",
      "[step: 465] loss: 3.2487974166870117\n",
      "[step: 466] loss: 3.242434501647949\n",
      "[step: 467] loss: 3.2481284141540527\n",
      "[step: 468] loss: 3.24678111076355\n",
      "[step: 469] loss: 3.240262985229492\n",
      "[step: 470] loss: 3.2415761947631836\n",
      "[step: 471] loss: 3.2438206672668457\n",
      "[step: 472] loss: 3.239792823791504\n",
      "[step: 473] loss: 3.2369868755340576\n",
      "[step: 474] loss: 3.239238739013672\n",
      "[step: 475] loss: 3.2392337322235107\n",
      "[step: 476] loss: 3.2353875637054443\n",
      "[step: 477] loss: 3.2343695163726807\n",
      "[step: 478] loss: 3.2359278202056885\n",
      "[step: 479] loss: 3.234942674636841\n",
      "[step: 480] loss: 3.2322258949279785\n",
      "[step: 481] loss: 3.2313520908355713\n",
      "[step: 482] loss: 3.232093572616577\n",
      "[step: 483] loss: 3.2318291664123535\n",
      "[step: 484] loss: 3.230262517929077\n",
      "[step: 485] loss: 3.2284111976623535\n",
      "[step: 486] loss: 3.227787494659424\n",
      "[step: 487] loss: 3.2279164791107178\n",
      "[step: 488] loss: 3.227409839630127\n",
      "[step: 489] loss: 3.226499080657959\n",
      "[step: 490] loss: 3.225008249282837\n",
      "[step: 491] loss: 3.223966360092163\n",
      "[step: 492] loss: 3.223675489425659\n",
      "[step: 493] loss: 3.223409652709961\n",
      "[step: 494] loss: 3.2230563163757324\n",
      "[step: 495] loss: 3.2219552993774414\n",
      "[step: 496] loss: 3.2208189964294434\n",
      "[step: 497] loss: 3.2196717262268066\n",
      "[step: 498] loss: 3.2189383506774902\n",
      "[step: 499] loss: 3.218505859375\n",
      "[step: 500] loss: 3.2178709506988525\n",
      "[step: 501] loss: 3.217048406600952\n",
      "[step: 502] loss: 3.2159171104431152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 503] loss: 3.2150206565856934\n",
      "[step: 504] loss: 3.2142322063446045\n",
      "[step: 505] loss: 3.213468551635742\n",
      "[step: 506] loss: 3.212902069091797\n",
      "[step: 507] loss: 3.212348461151123\n",
      "[step: 508] loss: 3.211907386779785\n",
      "[step: 509] loss: 3.2109415531158447\n",
      "[step: 510] loss: 3.210082530975342\n",
      "[step: 511] loss: 3.2088422775268555\n",
      "[step: 512] loss: 3.2077527046203613\n",
      "[step: 513] loss: 3.2070670127868652\n",
      "[step: 514] loss: 3.206475257873535\n",
      "[step: 515] loss: 3.205622673034668\n",
      "[step: 516] loss: 3.2045137882232666\n",
      "[step: 517] loss: 3.203467845916748\n",
      "[step: 518] loss: 3.2026495933532715\n",
      "[step: 519] loss: 3.202004909515381\n",
      "[step: 520] loss: 3.2017247676849365\n",
      "[step: 521] loss: 3.2013511657714844\n",
      "[step: 522] loss: 3.200840473175049\n",
      "[step: 523] loss: 3.199831008911133\n",
      "[step: 524] loss: 3.1986184120178223\n",
      "[step: 525] loss: 3.197298288345337\n",
      "[step: 526] loss: 3.1960606575012207\n",
      "[step: 527] loss: 3.1949758529663086\n",
      "[step: 528] loss: 3.194030284881592\n",
      "[step: 529] loss: 3.1931533813476562\n",
      "[step: 530] loss: 3.192490577697754\n",
      "[step: 531] loss: 3.1918270587921143\n",
      "[step: 532] loss: 3.1914944648742676\n",
      "[step: 533] loss: 3.1909916400909424\n",
      "[step: 534] loss: 3.1909027099609375\n",
      "[step: 535] loss: 3.1914963722229004\n",
      "[step: 536] loss: 3.1928162574768066\n",
      "[step: 537] loss: 3.1927566528320312\n",
      "[step: 538] loss: 3.193288564682007\n",
      "[step: 539] loss: 3.191324234008789\n",
      "[step: 540] loss: 3.19010066986084\n",
      "[step: 541] loss: 3.1869568824768066\n",
      "[step: 542] loss: 3.1855313777923584\n",
      "[step: 543] loss: 3.183014392852783\n",
      "[step: 544] loss: 3.1809494495391846\n",
      "[step: 545] loss: 3.178974151611328\n",
      "[step: 546] loss: 3.177565574645996\n",
      "[step: 547] loss: 3.1764650344848633\n",
      "[step: 548] loss: 3.1754608154296875\n",
      "[step: 549] loss: 3.174487352371216\n",
      "[step: 550] loss: 3.1735641956329346\n",
      "[step: 551] loss: 3.172677516937256\n",
      "[step: 552] loss: 3.1718051433563232\n",
      "[step: 553] loss: 3.1709351539611816\n",
      "[step: 554] loss: 3.1700501441955566\n",
      "[step: 555] loss: 3.16914701461792\n",
      "[step: 556] loss: 3.1682231426239014\n",
      "[step: 557] loss: 3.1672563552856445\n",
      "[step: 558] loss: 3.1662099361419678\n",
      "[step: 559] loss: 3.1651110649108887\n",
      "[step: 560] loss: 3.1639227867126465\n",
      "[step: 561] loss: 3.1627655029296875\n",
      "[step: 562] loss: 3.161623954772949\n",
      "[step: 563] loss: 3.1604983806610107\n",
      "[step: 564] loss: 3.1594228744506836\n",
      "[step: 565] loss: 3.1584200859069824\n",
      "[step: 566] loss: 3.157622814178467\n",
      "[step: 567] loss: 3.1571784019470215\n",
      "[step: 568] loss: 3.157954692840576\n",
      "[step: 569] loss: 3.162559986114502\n",
      "[step: 570] loss: 3.1761226654052734\n",
      "[step: 571] loss: 3.1938533782958984\n",
      "[step: 572] loss: 3.2335410118103027\n",
      "[step: 573] loss: 3.2265422344207764\n",
      "[step: 574] loss: 3.22497296333313\n",
      "[step: 575] loss: 3.166185140609741\n",
      "[step: 576] loss: 3.1472175121307373\n",
      "[step: 577] loss: 3.1690077781677246\n",
      "[step: 578] loss: 3.180713176727295\n",
      "[step: 579] loss: 3.1755642890930176\n",
      "[step: 580] loss: 3.1456968784332275\n",
      "[step: 581] loss: 3.1512367725372314\n",
      "[step: 582] loss: 3.1802215576171875\n",
      "[step: 583] loss: 3.1615653038024902\n",
      "[step: 584] loss: 3.1424965858459473\n",
      "[step: 585] loss: 3.1419644355773926\n",
      "[step: 586] loss: 3.1533961296081543\n",
      "[step: 587] loss: 3.1524105072021484\n",
      "[step: 588] loss: 3.1369597911834717\n",
      "[step: 589] loss: 3.139803886413574\n",
      "[step: 590] loss: 3.1502137184143066\n",
      "[step: 591] loss: 3.1406707763671875\n",
      "[step: 592] loss: 3.131978988647461\n",
      "[step: 593] loss: 3.135072946548462\n",
      "[step: 594] loss: 3.138458251953125\n",
      "[step: 595] loss: 3.1369121074676514\n",
      "[step: 596] loss: 3.1291003227233887\n",
      "[step: 597] loss: 3.1277966499328613\n",
      "[step: 598] loss: 3.1316497325897217\n",
      "[step: 599] loss: 3.1305527687072754\n",
      "[step: 600] loss: 3.1270811557769775\n",
      "[step: 601] loss: 3.1229686737060547\n",
      "[step: 602] loss: 3.1234312057495117\n",
      "[step: 603] loss: 3.1252963542938232\n",
      "[step: 604] loss: 3.1235294342041016\n",
      "[step: 605] loss: 3.12062931060791\n",
      "[step: 606] loss: 3.1180033683776855\n",
      "[step: 607] loss: 3.1175384521484375\n",
      "[step: 608] loss: 3.1183433532714844\n",
      "[step: 609] loss: 3.118224620819092\n",
      "[step: 610] loss: 3.1182851791381836\n",
      "[step: 611] loss: 3.116157054901123\n",
      "[step: 612] loss: 3.1131927967071533\n",
      "[step: 613] loss: 3.1110427379608154\n",
      "[step: 614] loss: 3.11053729057312\n",
      "[step: 615] loss: 3.1108896732330322\n",
      "[step: 616] loss: 3.110792636871338\n",
      "[step: 617] loss: 3.1098270416259766\n",
      "[step: 618] loss: 3.1080191135406494\n",
      "[step: 619] loss: 3.1060128211975098\n",
      "[step: 620] loss: 3.104240894317627\n",
      "[step: 621] loss: 3.103142023086548\n",
      "[step: 622] loss: 3.1026740074157715\n",
      "[step: 623] loss: 3.1024861335754395\n",
      "[step: 624] loss: 3.1019387245178223\n",
      "[step: 625] loss: 3.10032320022583\n",
      "[step: 626] loss: 3.0979459285736084\n",
      "[step: 627] loss: 3.0964229106903076\n",
      "[step: 628] loss: 3.0953831672668457\n",
      "[step: 629] loss: 3.0952701568603516\n",
      "[step: 630] loss: 3.095684051513672\n",
      "[step: 631] loss: 3.0992679595947266\n",
      "[step: 632] loss: 3.1022891998291016\n",
      "[step: 633] loss: 3.1103177070617676\n",
      "[step: 634] loss: 3.10689377784729\n",
      "[step: 635] loss: 3.108738422393799\n",
      "[step: 636] loss: 3.0955810546875\n",
      "[step: 637] loss: 3.0891847610473633\n",
      "[step: 638] loss: 3.0863022804260254\n",
      "[step: 639] loss: 3.088092565536499\n",
      "[step: 640] loss: 3.090994358062744\n",
      "[step: 641] loss: 3.0901379585266113\n",
      "[step: 642] loss: 3.0876963138580322\n",
      "[step: 643] loss: 3.0832719802856445\n",
      "[step: 644] loss: 3.080186367034912\n",
      "[step: 645] loss: 3.078937530517578\n",
      "[step: 646] loss: 3.0790343284606934\n",
      "[step: 647] loss: 3.080195188522339\n",
      "[step: 648] loss: 3.081648349761963\n",
      "[step: 649] loss: 3.0901594161987305\n",
      "[step: 650] loss: 3.1004881858825684\n",
      "[step: 651] loss: 3.1389997005462646\n",
      "[step: 652] loss: 3.1532158851623535\n",
      "[step: 653] loss: 3.1997299194335938\n",
      "[step: 654] loss: 3.1260361671447754\n",
      "[step: 655] loss: 3.0882256031036377\n",
      "[step: 656] loss: 3.070585012435913\n",
      "[step: 657] loss: 3.0934524536132812\n",
      "[step: 658] loss: 3.1270108222961426\n",
      "[step: 659] loss: 3.0984303951263428\n",
      "[step: 660] loss: 3.077148914337158\n",
      "[step: 661] loss: 3.0659995079040527\n",
      "[step: 662] loss: 3.081475019454956\n",
      "[step: 663] loss: 3.116920232772827\n",
      "[step: 664] loss: 3.0982067584991455\n",
      "[step: 665] loss: 3.080841541290283\n",
      "[step: 666] loss: 3.0627574920654297\n",
      "[step: 667] loss: 3.064504623413086\n",
      "[step: 668] loss: 3.079056978225708\n",
      "[step: 669] loss: 3.0766661167144775\n",
      "[step: 670] loss: 3.0704658031463623\n",
      "[step: 671] loss: 3.057826519012451\n",
      "[step: 672] loss: 3.0597033500671387\n",
      "[step: 673] loss: 3.070399284362793\n",
      "[step: 674] loss: 3.068277597427368\n",
      "[step: 675] loss: 3.0619611740112305\n",
      "[step: 676] loss: 3.053117275238037\n",
      "[step: 677] loss: 3.0561180114746094\n",
      "[step: 678] loss: 3.0668113231658936\n",
      "[step: 679] loss: 3.0701866149902344\n",
      "[step: 680] loss: 3.073871612548828\n",
      "[step: 681] loss: 3.060321092605591\n",
      "[step: 682] loss: 3.049571990966797\n",
      "[step: 683] loss: 3.0483171939849854\n",
      "[step: 684] loss: 3.0553650856018066\n",
      "[step: 685] loss: 3.0638370513916016\n",
      "[step: 686] loss: 3.0600814819335938\n",
      "[step: 687] loss: 3.0536959171295166\n",
      "[step: 688] loss: 3.0436224937438965\n",
      "[step: 689] loss: 3.043424129486084\n",
      "[step: 690] loss: 3.050900459289551\n",
      "[step: 691] loss: 3.052128791809082\n",
      "[step: 692] loss: 3.049412727355957\n",
      "[step: 693] loss: 3.0440917015075684\n",
      "[step: 694] loss: 3.0390827655792236\n",
      "[step: 695] loss: 3.0372118949890137\n",
      "[step: 696] loss: 3.039623260498047\n",
      "[step: 697] loss: 3.0438857078552246\n",
      "[step: 698] loss: 3.0466504096984863\n",
      "[step: 699] loss: 3.0507214069366455\n",
      "[step: 700] loss: 3.0470309257507324\n",
      "[step: 701] loss: 3.0447263717651367\n",
      "[step: 702] loss: 3.0374763011932373\n",
      "[step: 703] loss: 3.0334367752075195\n",
      "[step: 704] loss: 3.029731512069702\n",
      "[step: 705] loss: 3.0298900604248047\n",
      "[step: 706] loss: 3.0312507152557373\n",
      "[step: 707] loss: 3.03056001663208\n",
      "[step: 708] loss: 3.0310518741607666\n",
      "[step: 709] loss: 3.0286383628845215\n",
      "[step: 710] loss: 3.0267457962036133\n",
      "[step: 711] loss: 3.0242648124694824\n",
      "[step: 712] loss: 3.023249387741089\n",
      "[step: 713] loss: 3.02327823638916\n",
      "[step: 714] loss: 3.022956371307373\n",
      "[step: 715] loss: 3.022585391998291\n",
      "[step: 716] loss: 3.0238020420074463\n",
      "[step: 717] loss: 3.0265755653381348\n",
      "[step: 718] loss: 3.0299439430236816\n",
      "[step: 719] loss: 3.0360610485076904\n",
      "[step: 720] loss: 3.036834239959717\n",
      "[step: 721] loss: 3.0451207160949707\n",
      "[step: 722] loss: 3.0423777103424072\n",
      "[step: 723] loss: 3.0561132431030273\n",
      "[step: 724] loss: 3.0450263023376465\n",
      "[step: 725] loss: 3.0505781173706055\n",
      "[step: 726] loss: 3.027862071990967\n",
      "[step: 727] loss: 3.015981674194336\n",
      "[step: 728] loss: 3.0110344886779785\n",
      "[step: 729] loss: 3.0119216442108154\n",
      "[step: 730] loss: 3.015895366668701\n",
      "[step: 731] loss: 3.021379232406616\n",
      "[step: 732] loss: 3.0292162895202637\n",
      "[step: 733] loss: 3.0279009342193604\n",
      "[step: 734] loss: 3.0266129970550537\n",
      "[step: 735] loss: 3.0171961784362793\n",
      "[step: 736] loss: 3.011033058166504\n",
      "[step: 737] loss: 3.006354331970215\n",
      "[step: 738] loss: 3.0035338401794434\n",
      "[step: 739] loss: 3.0038294792175293\n",
      "[step: 740] loss: 3.005631446838379\n",
      "[step: 741] loss: 3.0065746307373047\n",
      "[step: 742] loss: 3.00411319732666\n",
      "[step: 743] loss: 3.0007872581481934\n",
      "[step: 744] loss: 2.9989519119262695\n",
      "[step: 745] loss: 2.9993338584899902\n",
      "[step: 746] loss: 3.0004115104675293\n",
      "[step: 747] loss: 3.0040159225463867\n",
      "[step: 748] loss: 3.0071091651916504\n",
      "[step: 749] loss: 3.0087008476257324\n",
      "[step: 750] loss: 3.0126395225524902\n",
      "[step: 751] loss: 3.011232852935791\n",
      "[step: 752] loss: 3.0119879245758057\n",
      "[step: 753] loss: 3.0058794021606445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 754] loss: 3.0035207271575928\n",
      "[step: 755] loss: 2.9984545707702637\n",
      "[step: 756] loss: 2.9927010536193848\n",
      "[step: 757] loss: 2.9908101558685303\n",
      "[step: 758] loss: 2.9889845848083496\n",
      "[step: 759] loss: 2.9881861209869385\n",
      "[step: 760] loss: 2.9880881309509277\n",
      "[step: 761] loss: 2.9880783557891846\n",
      "[step: 762] loss: 2.9872119426727295\n",
      "[step: 763] loss: 2.985820770263672\n",
      "[step: 764] loss: 2.9844374656677246\n",
      "[step: 765] loss: 2.9835169315338135\n",
      "[step: 766] loss: 2.9830079078674316\n",
      "[step: 767] loss: 2.982802391052246\n",
      "[step: 768] loss: 2.9830305576324463\n",
      "[step: 769] loss: 2.9853029251098633\n",
      "[step: 770] loss: 2.9888174533843994\n",
      "[step: 771] loss: 2.996095895767212\n",
      "[step: 772] loss: 3.0149898529052734\n",
      "[step: 773] loss: 3.0165114402770996\n",
      "[step: 774] loss: 3.0419259071350098\n",
      "[step: 775] loss: 3.0127265453338623\n",
      "[step: 776] loss: 2.9956016540527344\n",
      "[step: 777] loss: 2.9779269695281982\n",
      "[step: 778] loss: 2.976116418838501\n",
      "[step: 779] loss: 2.985877752304077\n",
      "[step: 780] loss: 2.9937338829040527\n",
      "[step: 781] loss: 3.013124465942383\n",
      "[step: 782] loss: 2.9982213973999023\n",
      "[step: 783] loss: 2.986414670944214\n",
      "[step: 784] loss: 2.972879409790039\n",
      "[step: 785] loss: 2.9712448120117188\n",
      "[step: 786] loss: 2.978693962097168\n",
      "[step: 787] loss: 2.983224391937256\n",
      "[step: 788] loss: 2.9807658195495605\n",
      "[step: 789] loss: 2.9719629287719727\n",
      "[step: 790] loss: 2.967226982116699\n",
      "[step: 791] loss: 2.9686121940612793\n",
      "[step: 792] loss: 2.9707205295562744\n",
      "[step: 793] loss: 2.9693422317504883\n",
      "[step: 794] loss: 2.9650211334228516\n",
      "[step: 795] loss: 2.964756965637207\n",
      "[step: 796] loss: 2.966925621032715\n",
      "[step: 797] loss: 2.9653477668762207\n",
      "[step: 798] loss: 2.962628126144409\n",
      "[step: 799] loss: 2.9610331058502197\n",
      "[step: 800] loss: 2.962031364440918\n",
      "[step: 801] loss: 2.9627864360809326\n",
      "[step: 802] loss: 2.9607532024383545\n",
      "[step: 803] loss: 2.9580135345458984\n",
      "[step: 804] loss: 2.957505702972412\n",
      "[step: 805] loss: 2.958383083343506\n",
      "[step: 806] loss: 2.9597115516662598\n",
      "[step: 807] loss: 2.9614081382751465\n",
      "[step: 808] loss: 2.962376117706299\n",
      "[step: 809] loss: 2.9614038467407227\n",
      "[step: 810] loss: 2.957345962524414\n",
      "[step: 811] loss: 2.954098701477051\n",
      "[step: 812] loss: 2.951423168182373\n",
      "[step: 813] loss: 2.950777053833008\n",
      "[step: 814] loss: 2.954197883605957\n",
      "[step: 815] loss: 2.960689067840576\n",
      "[step: 816] loss: 2.965312957763672\n",
      "[step: 817] loss: 2.974782705307007\n",
      "[step: 818] loss: 2.978907585144043\n",
      "[step: 819] loss: 2.981353282928467\n",
      "[step: 820] loss: 2.9729020595550537\n",
      "[step: 821] loss: 2.964874267578125\n",
      "[step: 822] loss: 2.9523367881774902\n",
      "[step: 823] loss: 2.9454550743103027\n",
      "[step: 824] loss: 2.9436194896698\n",
      "[step: 825] loss: 2.945666790008545\n",
      "[step: 826] loss: 2.946986198425293\n",
      "[step: 827] loss: 2.9452407360076904\n",
      "[step: 828] loss: 2.9419479370117188\n",
      "[step: 829] loss: 2.9396815299987793\n",
      "[step: 830] loss: 2.938889503479004\n",
      "[step: 831] loss: 2.9401423931121826\n",
      "[step: 832] loss: 2.9447128772735596\n",
      "[step: 833] loss: 2.9509336948394775\n",
      "[step: 834] loss: 2.9545843601226807\n",
      "[step: 835] loss: 2.9611101150512695\n",
      "[step: 836] loss: 2.9533064365386963\n",
      "[step: 837] loss: 2.9474096298217773\n",
      "[step: 838] loss: 2.9432528018951416\n",
      "[step: 839] loss: 2.939828395843506\n",
      "[step: 840] loss: 2.9375624656677246\n",
      "[step: 841] loss: 2.936779499053955\n",
      "[step: 842] loss: 2.9379284381866455\n",
      "[step: 843] loss: 2.9390804767608643\n",
      "[step: 844] loss: 2.9404242038726807\n",
      "[step: 845] loss: 2.939873218536377\n",
      "[step: 846] loss: 2.938295602798462\n",
      "[step: 847] loss: 2.9351205825805664\n",
      "[step: 848] loss: 2.929048776626587\n",
      "[step: 849] loss: 2.926409959793091\n",
      "[step: 850] loss: 2.927938461303711\n",
      "[step: 851] loss: 2.9364023208618164\n",
      "[step: 852] loss: 2.95371675491333\n",
      "[step: 853] loss: 2.9565491676330566\n",
      "[step: 854] loss: 2.959536075592041\n",
      "[step: 855] loss: 2.940154552459717\n",
      "[step: 856] loss: 2.9301681518554688\n",
      "[step: 857] loss: 2.921642303466797\n",
      "[step: 858] loss: 2.9270081520080566\n",
      "[step: 859] loss: 2.951362133026123\n",
      "[step: 860] loss: 2.95841646194458\n",
      "[step: 861] loss: 2.9657838344573975\n",
      "[step: 862] loss: 2.934227466583252\n",
      "[step: 863] loss: 2.920337677001953\n",
      "[step: 864] loss: 2.918788433074951\n",
      "[step: 865] loss: 2.9293744564056396\n",
      "[step: 866] loss: 2.945772409439087\n",
      "[step: 867] loss: 2.933986186981201\n",
      "[step: 868] loss: 2.9249091148376465\n",
      "[step: 869] loss: 2.914313554763794\n",
      "[step: 870] loss: 2.9180784225463867\n",
      "[step: 871] loss: 2.934980630874634\n",
      "[step: 872] loss: 2.9319558143615723\n",
      "[step: 873] loss: 2.927295207977295\n",
      "[step: 874] loss: 2.912835121154785\n",
      "[step: 875] loss: 2.9107789993286133\n",
      "[step: 876] loss: 2.9136176109313965\n",
      "[step: 877] loss: 2.9125916957855225\n",
      "[step: 878] loss: 2.9138827323913574\n",
      "[step: 879] loss: 2.909348249435425\n",
      "[step: 880] loss: 2.907747268676758\n",
      "[step: 881] loss: 2.908720016479492\n",
      "[step: 882] loss: 2.91485333442688\n",
      "[step: 883] loss: 2.9264047145843506\n",
      "[step: 884] loss: 2.9214529991149902\n",
      "[step: 885] loss: 2.9197816848754883\n",
      "[step: 886] loss: 2.9071550369262695\n",
      "[step: 887] loss: 2.9037342071533203\n",
      "[step: 888] loss: 2.9032256603240967\n",
      "[step: 889] loss: 2.909755229949951\n",
      "[step: 890] loss: 2.9185173511505127\n",
      "[step: 891] loss: 2.924787998199463\n",
      "[step: 892] loss: 2.9318456649780273\n",
      "[step: 893] loss: 2.912675380706787\n",
      "[step: 894] loss: 2.9045803546905518\n",
      "[step: 895] loss: 2.9006307125091553\n",
      "[step: 896] loss: 2.904524803161621\n",
      "[step: 897] loss: 2.916560411453247\n",
      "[step: 898] loss: 2.9110898971557617\n",
      "[step: 899] loss: 2.908752918243408\n",
      "[step: 900] loss: 2.8986308574676514\n",
      "[step: 901] loss: 2.8978867530822754\n",
      "[step: 902] loss: 2.9046900272369385\n",
      "[step: 903] loss: 2.903806209564209\n",
      "[step: 904] loss: 2.9034507274627686\n",
      "[step: 905] loss: 2.894775152206421\n",
      "[step: 906] loss: 2.8929882049560547\n",
      "[step: 907] loss: 2.904305934906006\n",
      "[step: 908] loss: 2.9089996814727783\n",
      "[step: 909] loss: 2.9163098335266113\n",
      "[step: 910] loss: 2.8991379737854004\n",
      "[step: 911] loss: 2.890475273132324\n",
      "[step: 912] loss: 2.891608238220215\n",
      "[step: 913] loss: 2.902710437774658\n",
      "[step: 914] loss: 2.917959213256836\n",
      "[step: 915] loss: 2.9034509658813477\n",
      "[step: 916] loss: 2.8964409828186035\n",
      "[step: 917] loss: 2.8871922492980957\n",
      "[step: 918] loss: 2.889307975769043\n",
      "[step: 919] loss: 2.895073890686035\n",
      "[step: 920] loss: 2.892514705657959\n",
      "[step: 921] loss: 2.8904306888580322\n",
      "[step: 922] loss: 2.884582757949829\n",
      "[step: 923] loss: 2.882394790649414\n",
      "[step: 924] loss: 2.8843917846679688\n",
      "[step: 925] loss: 2.8886311054229736\n",
      "[step: 926] loss: 2.888570785522461\n",
      "[step: 927] loss: 2.883758068084717\n",
      "[step: 928] loss: 2.881488084793091\n",
      "[step: 929] loss: 2.8782453536987305\n",
      "[step: 930] loss: 2.8780369758605957\n",
      "[step: 931] loss: 2.8793296813964844\n",
      "[step: 932] loss: 2.8787050247192383\n",
      "[step: 933] loss: 2.877202033996582\n",
      "[step: 934] loss: 2.8752870559692383\n",
      "[step: 935] loss: 2.8745148181915283\n",
      "[step: 936] loss: 2.874633312225342\n",
      "[step: 937] loss: 2.8745410442352295\n",
      "[step: 938] loss: 2.8743038177490234\n",
      "[step: 939] loss: 2.872560977935791\n",
      "[step: 940] loss: 2.8712809085845947\n",
      "[step: 941] loss: 2.8703765869140625\n",
      "[step: 942] loss: 2.8697009086608887\n",
      "[step: 943] loss: 2.8693675994873047\n",
      "[step: 944] loss: 2.8691751956939697\n",
      "[step: 945] loss: 2.868924856185913\n",
      "[step: 946] loss: 2.8688902854919434\n",
      "[step: 947] loss: 2.868821144104004\n",
      "[step: 948] loss: 2.8677330017089844\n",
      "[step: 949] loss: 2.8668811321258545\n",
      "[step: 950] loss: 2.865931987762451\n",
      "[step: 951] loss: 2.8647022247314453\n",
      "[step: 952] loss: 2.863496780395508\n",
      "[step: 953] loss: 2.8625550270080566\n",
      "[step: 954] loss: 2.8621745109558105\n",
      "[step: 955] loss: 2.862039804458618\n",
      "[step: 956] loss: 2.861955165863037\n",
      "[step: 957] loss: 2.8624486923217773\n",
      "[step: 958] loss: 2.8706393241882324\n",
      "[step: 959] loss: 2.8772518634796143\n",
      "[step: 960] loss: 2.900956869125366\n",
      "[step: 961] loss: 2.888779640197754\n",
      "[step: 962] loss: 2.8937888145446777\n",
      "[step: 963] loss: 2.8666372299194336\n",
      "[step: 964] loss: 2.857341766357422\n",
      "[step: 965] loss: 2.8677544593811035\n",
      "[step: 966] loss: 2.8844246864318848\n",
      "[step: 967] loss: 2.9220969676971436\n",
      "[step: 968] loss: 2.8804190158843994\n",
      "[step: 969] loss: 2.8563828468322754\n",
      "[step: 970] loss: 2.8578076362609863\n",
      "[step: 971] loss: 2.8822402954101562\n",
      "[step: 972] loss: 2.926450252532959\n",
      "[step: 973] loss: 2.884793519973755\n",
      "[step: 974] loss: 2.8714354038238525\n",
      "[step: 975] loss: 2.854525566101074\n",
      "[step: 976] loss: 2.861337661743164\n",
      "[step: 977] loss: 2.8762223720550537\n",
      "[step: 978] loss: 2.868378162384033\n",
      "[step: 979] loss: 2.8664472103118896\n",
      "[step: 980] loss: 2.851318836212158\n",
      "[step: 981] loss: 2.850510835647583\n",
      "[step: 982] loss: 2.859534740447998\n",
      "[step: 983] loss: 2.8528013229370117\n",
      "[step: 984] loss: 2.8458147048950195\n",
      "[step: 985] loss: 2.846616744995117\n",
      "[step: 986] loss: 2.8500280380249023\n",
      "[step: 987] loss: 2.8501710891723633\n",
      "[step: 988] loss: 2.848177671432495\n",
      "[step: 989] loss: 2.8434348106384277\n",
      "[step: 990] loss: 2.841310739517212\n",
      "[step: 991] loss: 2.843238115310669\n",
      "[step: 992] loss: 2.8456079959869385\n",
      "[step: 993] loss: 2.8434791564941406\n",
      "[step: 994] loss: 2.8456101417541504\n",
      "[step: 995] loss: 2.842170238494873\n",
      "[step: 996] loss: 2.8377816677093506\n",
      "[step: 997] loss: 2.836972236633301\n",
      "[step: 998] loss: 2.8394434452056885\n",
      "[step: 999] loss: 2.840287923812866\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w8XtiYI6pY7fwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc356898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mockForecastDictionary={}\n",
    "realForecastDictionary={}\n",
    "\n",
    "trainSize=int(len(rawArrayDatas[0])*0.7)\n",
    "testSize=len(rawArrayDatas[0])-trainSize\n",
    "\n",
    "testY= rawArrayDatas[1][trainSize:]\n",
    "ds = rawArrayDatas[0][:trainSize]\n",
    "y = list((rawArrayDatas[1][:trainSize]))\n",
    "sales = list(zip(ds, y))\n",
    "txs = pd.DataFrame(data=sales, columns=['date', 'sales'])\n",
    "\n",
    "\n",
    "year = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).year  \n",
    "day_of_week = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).weekday()\n",
    "month = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).month\n",
    "# please read docs on how week numbers are calculate\n",
    "week_number = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).strftime('%V')\n",
    "\n",
    "txs['year'] = txs['date'].map(year)\n",
    "txs['month']=txs['date'].map(month)\n",
    "txs['week_number']=txs['date'].map(week_number)\n",
    "txs['day_of_week']=txs['date'].map(day_of_week)\n",
    "\n",
    "seasons = [0,0,1,1,1,2,2,2,3,3,3,0] #dec - feb is winter, then spring, summer, fall etc\n",
    "season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\" ).month-1)]\n",
    "day_of_week01s=[0,0,0,0,0,1,1]\n",
    "day_of_week01= lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\" ).weekday())]\n",
    "txs['season']=txs['date'].map(season)\n",
    "txs['day_of_week01']=txs['date'].map(day_of_week01)\n",
    "originalSales=list(txs['sales'])\n",
    "sales=list(txs['sales'])\n",
    "\n",
    "tempxy=[list(txs['season']),list(txs['day_of_week']),list(txs['week_number']),sales]\n",
    "\n",
    "xy=np.array(tempxy).transpose().astype(np.float)\n",
    "originalXY=np.array(tempxy).transpose().astype(np.float)\n",
    "xy=minMaxNormalizer(xy)\n",
    "#data_dim y   feature +1(  +1(y))\n",
    "data_dim=4\n",
    "\n",
    "#data_dim data   seq_length input \n",
    "seq_length=5\n",
    "\n",
    "#output_dim(=forecastDays)  y_data \n",
    "forecastDays=7\n",
    "output_dim=forecastDays\n",
    "\n",
    "#hidden_dim   \n",
    "hidden_dim=10\n",
    "\n",
    "#learning rate  ( ,   )\n",
    "learning_rate=0.01\n",
    "\n",
    "#iterations  \n",
    "iterations=1000\n",
    "\n",
    "\n",
    "x=xy\n",
    "y=xy[:,[-1]]\n",
    "\n",
    "#build a series dataset(seq_length   X  forecastDays  Y)\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0, len(y)-seq_length-forecastDays):\n",
    "    _x=x[i:i+seq_length]\n",
    "    _y=y[i+seq_length:i+seq_length+forecastDays]\n",
    "    _y=np.reshape(_y,(forecastDays))\n",
    "    #     _y=Y[i+seq_length:i+seq_length+forecastDays]\n",
    "    print(_x,\"->\",_y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n",
    "\n",
    "\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])\n",
    "\n",
    "X=tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y=tf.placeholder(tf.float32, [None, forecastDays])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.nn.relu)\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn= None) \n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
    "with tf.Session() as sess:\n",
    "    #\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalXY)\n",
    "#     rmse_val = sess.run(rmse, feed_dict={targets: denormalizedTestY_feed, predictions: test_predict})\n",
    "#     print(\"RMSE: {}\".format(rmse_val))\n",
    "    plt.plot(test_predict)\n",
    "    plt.plot(testY) # sales \n",
    "    plt.plot(test_predict)           # sales \n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55555555,  0.63888889,  0.68055555,  0.69444444,  0.51388889,\n",
       "         0.51388889,  0.625     ],\n",
       "       [ 0.63888889,  0.68055555,  0.69444444,  0.51388889,  0.51388889,\n",
       "         0.625     ,  0.66666667],\n",
       "       [ 0.68055555,  0.69444444,  0.51388889,  0.51388889,  0.625     ,\n",
       "         0.66666667,  0.66666667],\n",
       "       [ 0.69444444,  0.51388889,  0.51388889,  0.625     ,  0.66666667,\n",
       "         0.66666667,  0.52777778],\n",
       "       [ 0.51388889,  0.51388889,  0.625     ,  0.66666667,  0.66666667,\n",
       "         0.52777778,  0.83333333],\n",
       "       [ 0.51388889,  0.625     ,  0.66666667,  0.66666667,  0.52777778,\n",
       "         0.83333333,  0.43055555],\n",
       "       [ 0.625     ,  0.66666667,  0.66666667,  0.52777778,  0.83333333,\n",
       "         0.43055555,  0.48611111],\n",
       "       [ 0.66666667,  0.66666667,  0.52777778,  0.83333333,  0.43055555,\n",
       "         0.48611111,  0.73611111],\n",
       "       [ 0.66666667,  0.52777778,  0.83333333,  0.43055555,  0.48611111,\n",
       "         0.73611111,  0.97222222],\n",
       "       [ 0.52777778,  0.83333333,  0.43055555,  0.48611111,  0.73611111,\n",
       "         0.97222222,  0.86111111],\n",
       "       [ 0.83333333,  0.43055555,  0.48611111,  0.73611111,  0.97222222,\n",
       "         0.86111111,  0.66666667],\n",
       "       [ 0.43055555,  0.48611111,  0.73611111,  0.97222222,  0.86111111,\n",
       "         0.66666667,  0.70833333],\n",
       "       [ 0.48611111,  0.73611111,  0.97222222,  0.86111111,  0.66666667,\n",
       "         0.70833333,  0.68055555],\n",
       "       [ 0.73611111,  0.97222222,  0.86111111,  0.66666667,  0.70833333,\n",
       "         0.68055555,  0.52777778]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43.683342, 42.836231, 41.315269, 41.802895, 42.284412, 38.266495, 39.234688]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_predict[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
