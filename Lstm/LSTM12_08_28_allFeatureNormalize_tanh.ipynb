{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과정 요약:<br> \n",
    "feature: season, day of week, week number, sales <br> \n",
    "feature engineering: normalize+ denormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과요약: seed 7: rmse 96.04 (마지막 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상세과정:\n",
    "주중(1)/주말(2) + 겨울(1)봄(2)여름(3)가을(4) // \n",
    "이미 lstm이라는 것이 sequence 개념이 있으므로 시간축(1~397)를 feature로 설정하는 것은 의미가 없을 듯 하여 LSTM 시도2에서는 제외함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가의견) 개인적으로 이상점을 제거한 후 normalize를 하면 rmse가 커질 수 밖에 없다고 생각함. 다음시도(lstm3)은 이상점을 제거하지 않고 normalize를 하고, lstm4에서는 이상점을 제거하고 normalize를 하지 않는 것을 시도하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가의견2) 처음에는 denormalize과정 없이 rmse를 구했는데, 이것보다는 denormalize를 한 후 rmse를 구하여 모델간 비교를 하는 것이 더 적절한 것 같다. normalize는 어디까지나 변환이니 항상 변환을 할때는 역변환을 하여 원본 데이터 형태m로 생각하는 것이 덜 헷갈리고, 결과값에 대한 더욱 직관적인 이해가 가능할 것 같다. 그리고, 여러 형태의 data transformation이 있는데 normalize 가장 마지막 단계의 data transformation인 것 같다. 다른 로그나 루트 변환을 한 후 normalization을 하고, 그리고 예측값이 모델을 통해 생성되면 바로 denormalize하고...예측(뉴럴) 모델의 input직전과 output직후."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set seednumber(7 or 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DATA 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns=['date','sales']\n",
    "\n",
    "txs=pd.read_table('./lstmData/lstmPrac2.csv', sep=',',header=None,names=columns )\n",
    "txs.info()\n",
    "txs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales=list(txs['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 기본 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ds-y'의 ds로부터 api로 얻을 수 있는 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year, day of week, month, week number를 기본 feature로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).year  \n",
    "day_of_week = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).weekday()\n",
    "month = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).month\n",
    "# please read docs on how week numbers are calculate\n",
    "week_number = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).strftime('%V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txs['year'] = txs['date'].map(year)\n",
    "txs['month']=txs['date'].map(month)\n",
    "txs['week_number']=txs['date'].map(week_number)\n",
    "txs['day_of_week']=txs['date'].map(day_of_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 추가 feature + 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ds-y'의 ds로부터 api로 얻을 수 없는 값: 본 feature를 가공한 feature + ds와 무관한 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = [0,0,1,1,1,2,2,2,3,3,3,0] #dec - feb is winter, then spring, summer, fall etc\n",
    "season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\" ).month-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주중/주말(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_of_week01s=[0,0,0,0,0,1,1]\n",
    "day_of_week01= lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\" ).weekday())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txs['season']=txs['date'].map(season)\n",
    "txs['day_of_week01']=txs['date'].map(day_of_week01)\n",
    "txs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas를 통해 구한 각 feature는 list()로 우리의 기준type인 list로 변경이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 추가 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 y의 추가 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막rmse를 계산할 때 원본 데이터를 복원하기 위해 저장해놓음(아무런 가공되지 않은 원본 sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "originalSales=sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가공을 하는 순서도 중요한데, 이상점 제거-> log or sqrt -> normalization이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상점 제거, bucketization 을 하여 새로운 열을 생성하는 방향으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이상점 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상점 제거를 위해 평균과 표준편차를 구한다. 이상점의 기준은 일단 평균+-2*sd로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noOutlierSales(sales):\n",
    "    mean=np.mean(sales)\n",
    "    std=np.std(sales)\n",
    "    for i in range(len(sales)):\n",
    "        if (sales[i]<mean-2*std or sales[i]>mean+2*std):\n",
    "             sales[i]=int(mean)\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logSales(sales):\n",
    "    for i in range(len(sales)):\n",
    "        if sales[i] is 0:\n",
    "            sales[i]=1\n",
    "    return np.log(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqrt(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sqrtSales(sales):\n",
    "    return np.sqrt(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 x의 추가 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 합친 후 (필요 시 normalize하여) 최종 data 생성: XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계절(0~3) + 요일(0~6) + 주수(1~53) -> 판매량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempxy=[list(txs['season']),list(txs['day_of_week']),list(txs['week_number']),sales]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계절(0~3) + 요일(0,1) + 주수(1~53) -> 판매량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempxy=[list(txs['season']),list(txs['day_of_week01']),list(txs['week_number']),sales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy=np.array(tempxy).transpose().astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization이 필요한 열을 normalize시킴(현재는 sales에 해당하는 마지막 열만 normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy=minMaxNormalizer(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xy[:,-1]=minMaxNormalizer(xy[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 열 ex) XY[:,-3]=minMaxNormalizer(XY[:,-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측모델을 통해 얻은 sales결과를 denormalize시켜 기존 단위로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수를 마지막 rmse구하기 전에 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODEL 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 사용 model 정의: RNN LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 해당 model의 train parameters 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "data_dim=4\n",
    "\n",
    "#data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "seq_length=5\n",
    "\n",
    "#output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "forecastDays=1\n",
    "output_dim=forecastDays\n",
    "\n",
    "#hidden_dim은 정말 임의로 설정\n",
    "hidden_dim=10\n",
    "\n",
    "#learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "learning_rate=0.01\n",
    "\n",
    "#iterations는 반복 횟수\n",
    "iterations=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 사용 model, train parameter에 맞추어 dataset(XY) 변환: dataX, dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=xy\n",
    "y=xy[:,[-1]]\n",
    "\n",
    "#build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0, len(y)-seq_length):\n",
    "    _x=x[i:i+seq_length]\n",
    "    _y=y[i+seq_length]\n",
    "    #     _y=Y[i+seq_length:i+seq_length+forecastDays]\n",
    "    print(_x,\"->\",_y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 input place holders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X의 경우 input type과 [batch size, sequence length, input data dimension(feature+1))]\n",
    "\n",
    "Y의 경우 input type과 [batch size, 원하는 output 의 개수(forecastDays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y=tf.placeholder(tf.float32, [None, forecastDays])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 build LSTM network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm network의 \n",
    "\n",
    "    기본단위 cell, \n",
    "    \n",
    "    사용 driver, \n",
    "    \n",
    "    예측 y 산출방식, \n",
    "    \n",
    "    loss 함수, \n",
    "    \n",
    "    사용 optimizer 정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### lstm의 한 기본단위인 cell을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell의 결과값을 fully connected layer로 한 번 더 가공할 것이기 때문에 cell의 output dimension인 num_units=hidden_dim로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic rnn이라는 driver를 가동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.dynamic_rnn의 input은 cell, input, input type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.dynamic_rnn의 output은 outputs와 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마지막 cell의 output과 fully connected layer를  이용하여 Y_pred 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs[:, -1]: cell의 outputs 중 마지막 하나만 이용(we use the last cell's output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_dim: fully connected의 최종출력개수는 output_dim(=forecastDays) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation_fn= None: 분류 문제가 아니라 회귀 문제이므로 activation_fn은 none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn= None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss를 줄이는 방향으로 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model 평가 with RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestY=originalSales[train_size+seq_length:]\n",
    "denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rootMeanSquaredError(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt( sum/len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #초기화\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalSales)\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: denormalizedTestY_feed, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(denormalizedTestY_feed)\n",
    "    plt.plot(test_predict)\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestPredictY=[item for sublist in test_predict for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootMeanSquaredError(denormalizedTestY,denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
