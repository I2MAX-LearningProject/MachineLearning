{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과정 요약:<br> \n",
    "feature: season, day of week, week number, sales <br> \n",
    "feature engineering: normalize+ denormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과요약: seed 7: rmse 96.04 (마지막 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상세과정:\n",
    "주중(1)/주말(2) + 겨울(1)봄(2)여름(3)가을(4) // \n",
    "이미 lstm이라는 것이 sequence 개념이 있으므로 시간축(1~397)를 feature로 설정하는 것은 의미가 없을 듯 하여 LSTM 시도2에서는 제외함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가의견) 개인적으로 이상점을 제거한 후 normalize를 하면 rmse가 커질 수 밖에 없다고 생각함. 다음시도(lstm3)은 이상점을 제거하지 않고 normalize를 하고, lstm4에서는 이상점을 제거하고 normalize를 하지 않는 것을 시도하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가의견2) 처음에는 denormalize과정 없이 rmse를 구했는데, 이것보다는 denormalize를 한 후 rmse를 구하여 모델간 비교를 하는 것이 더 적절한 것 같다. normalize는 어디까지나 변환이니 항상 변환을 할때는 역변환을 하여 원본 데이터 형태m로 생각하는 것이 덜 헷갈리고, 결과값에 대한 더욱 직관적인 이해가 가능할 것 같다. 그리고, 여러 형태의 data transformation이 있는데 normalize 가장 마지막 단계의 data transformation인 것 같다. 다른 로그나 루트 변환을 한 후 normalization을 하고, 그리고 예측값이 모델을 통해 생성되면 바로 denormalize하고...예측(뉴럴) 모델의 input직전과 output직후."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set seednumber(7 or 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DATA 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 397 entries, 0 to 396\n",
      "Data columns (total 2 columns):\n",
      "date     397 non-null object\n",
      "sales    397 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  sales\n",
       "0  2016-01-01     34\n",
       "1  2016-01-02     41\n",
       "2  2016-01-03     54\n",
       "3  2016-01-04     41\n",
       "4  2016-01-05     35"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['date','sales']\n",
    "\n",
    "txs=pd.read_table('./lstmData/lstmPrac2.csv', sep=',',header=None,names=columns )\n",
    "txs.info()\n",
    "txs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales=list(txs['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34,\n",
       " 41,\n",
       " 54,\n",
       " 41,\n",
       " 35,\n",
       " 44,\n",
       " 50,\n",
       " 42,\n",
       " 42,\n",
       " 66,\n",
       " 50,\n",
       " 55,\n",
       " 56,\n",
       " 53,\n",
       " 44,\n",
       " 54,\n",
       " 54,\n",
       " 50,\n",
       " 40,\n",
       " 49,\n",
       " 28,\n",
       " 72,\n",
       " 71,\n",
       " 53,\n",
       " 43,\n",
       " 38,\n",
       " 55,\n",
       " 49,\n",
       " 43,\n",
       " 49,\n",
       " 49,\n",
       " 44,\n",
       " 39,\n",
       " 52,\n",
       " 45,\n",
       " 33,\n",
       " 43,\n",
       " 40,\n",
       " 46,\n",
       " 49,\n",
       " 50,\n",
       " 37,\n",
       " 37,\n",
       " 45,\n",
       " 48,\n",
       " 48,\n",
       " 38,\n",
       " 60,\n",
       " 31,\n",
       " 35,\n",
       " 53,\n",
       " 70,\n",
       " 62,\n",
       " 48,\n",
       " 51,\n",
       " 49,\n",
       " 38,\n",
       " 32,\n",
       " 39,\n",
       " 35,\n",
       " 30,\n",
       " 36,\n",
       " 31,\n",
       " 31,\n",
       " 44,\n",
       " 41,\n",
       " 41,\n",
       " 45,\n",
       " 46,\n",
       " 45,\n",
       " 41,\n",
       " 47,\n",
       " 48,\n",
       " 40,\n",
       " 42,\n",
       " 38,\n",
       " 38,\n",
       " 45,\n",
       " 48,\n",
       " 62,\n",
       " 46,\n",
       " 38,\n",
       " 62,\n",
       " 81,\n",
       " 40,\n",
       " 45,\n",
       " 42,\n",
       " 53,\n",
       " 53,\n",
       " 56,\n",
       " 53,\n",
       " 47,\n",
       " 61,\n",
       " 64,\n",
       " 62,\n",
       " 37,\n",
       " 65,\n",
       " 54,\n",
       " 44,\n",
       " 46,\n",
       " 50,\n",
       " 43,\n",
       " 53,\n",
       " 63,\n",
       " 52,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 72,\n",
       " 59,\n",
       " 75,\n",
       " 47,\n",
       " 44,\n",
       " 77,\n",
       " 90,\n",
       " 93,\n",
       " 47,\n",
       " 61,\n",
       " 77,\n",
       " 282,\n",
       " 50,\n",
       " 58,\n",
       " 33,\n",
       " 41,\n",
       " 36,\n",
       " 64,\n",
       " 49,\n",
       " 53,\n",
       " 60,\n",
       " 43,\n",
       " 41,\n",
       " 55,\n",
       " 45,\n",
       " 18,\n",
       " 60,\n",
       " 69,\n",
       " 54,\n",
       " 39,\n",
       " 50,\n",
       " 44,\n",
       " 54,\n",
       " 57,\n",
       " 82,\n",
       " 57,\n",
       " 44,\n",
       " 56,\n",
       " 51,\n",
       " 55,\n",
       " 28,\n",
       " 56,\n",
       " 54,\n",
       " 36,\n",
       " 12,\n",
       " 25,\n",
       " 41,\n",
       " 11,\n",
       " 6,\n",
       " 9,\n",
       " 230,\n",
       " 29,\n",
       " 24,\n",
       " 23,\n",
       " 14,\n",
       " 21,\n",
       " 20,\n",
       " 25,\n",
       " 23,\n",
       " 27,\n",
       " 31,\n",
       " 16,\n",
       " 14,\n",
       " 30,\n",
       " 32,\n",
       " 75,\n",
       " 35,\n",
       " 26,\n",
       " 12,\n",
       " 21,\n",
       " 23,\n",
       " 28,\n",
       " 25,\n",
       " 31,\n",
       " 21,\n",
       " 17,\n",
       " 10,\n",
       " 29,\n",
       " 34,\n",
       " 28,\n",
       " 20,\n",
       " 36,\n",
       " 23,\n",
       " 15,\n",
       " 42,\n",
       " 28,\n",
       " 24,\n",
       " 29,\n",
       " 20,\n",
       " 14,\n",
       " 18,\n",
       " 27,\n",
       " 22,\n",
       " 31,\n",
       " 24,\n",
       " 39,\n",
       " 31,\n",
       " 17,\n",
       " 42,\n",
       " 29,\n",
       " 33,\n",
       " 22,\n",
       " 39,\n",
       " 25,\n",
       " 23,\n",
       " 25,\n",
       " 21,\n",
       " 18,\n",
       " 24,\n",
       " 24,\n",
       " 20,\n",
       " 9,\n",
       " 31,\n",
       " 33,\n",
       " 33,\n",
       " 31,\n",
       " 27,\n",
       " 570,\n",
       " 15,\n",
       " 25,\n",
       " 32,\n",
       " 29,\n",
       " 32,\n",
       " 21,\n",
       " 92,\n",
       " 19,\n",
       " 37,\n",
       " 29,\n",
       " 33,\n",
       " 16,\n",
       " 22,\n",
       " 25,\n",
       " 21,\n",
       " 46,\n",
       " 34,\n",
       " 31,\n",
       " 22,\n",
       " 30,\n",
       " 28,\n",
       " 14,\n",
       " 25,\n",
       " 33,\n",
       " 28,\n",
       " 24,\n",
       " 42,\n",
       " 26,\n",
       " 19,\n",
       " 41,\n",
       " 34,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 11,\n",
       " 16,\n",
       " 27,\n",
       " 27,\n",
       " 21,\n",
       " 37,\n",
       " 41,\n",
       " 23,\n",
       " 30,\n",
       " 25,\n",
       " 22,\n",
       " 26,\n",
       " 29,\n",
       " 31,\n",
       " 29,\n",
       " 12,\n",
       " 15,\n",
       " 48,\n",
       " 18,\n",
       " 25,\n",
       " 27,\n",
       " 25,\n",
       " 21,\n",
       " 28,\n",
       " 24,\n",
       " 21,\n",
       " 29,\n",
       " 121,\n",
       " 0,\n",
       " 7,\n",
       " 32,\n",
       " 19,\n",
       " 34,\n",
       " 28,\n",
       " 23,\n",
       " 15,\n",
       " 10,\n",
       " 15,\n",
       " 9,\n",
       " 78,\n",
       " 59,\n",
       " 29,\n",
       " 23,\n",
       " 12,\n",
       " 36,\n",
       " 13,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 14,\n",
       " 8,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 20,\n",
       " 24,\n",
       " 8,\n",
       " 15,\n",
       " 17,\n",
       " 9,\n",
       " 17,\n",
       " 13,\n",
       " 19,\n",
       " 8,\n",
       " 14,\n",
       " 18,\n",
       " 24,\n",
       " 16,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 16,\n",
       " 11,\n",
       " 22,\n",
       " 15,\n",
       " 18,\n",
       " 11,\n",
       " 14,\n",
       " 34,\n",
       " 10,\n",
       " 13,\n",
       " 21,\n",
       " 35,\n",
       " 15,\n",
       " 6,\n",
       " 23,\n",
       " 22,\n",
       " 19,\n",
       " 20,\n",
       " 13,\n",
       " 21,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 16,\n",
       " 12,\n",
       " 15,\n",
       " 8,\n",
       " 0,\n",
       " 24,\n",
       " 19,\n",
       " 22,\n",
       " 13,\n",
       " 12,\n",
       " 16,\n",
       " 2,\n",
       " 16,\n",
       " 20,\n",
       " 12,\n",
       " 11,\n",
       " 22,\n",
       " 20,\n",
       " 16,\n",
       " 17,\n",
       " 97,\n",
       " 0,\n",
       " 23,\n",
       " 100,\n",
       " 18,\n",
       " 13,\n",
       " 21,\n",
       " 24,\n",
       " 16,\n",
       " 18,\n",
       " 22,\n",
       " 18,\n",
       " 11,\n",
       " 24,\n",
       " 22,\n",
       " 20,\n",
       " 22,\n",
       " 26,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 31]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 기본 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ds-y'의 ds로부터 api로 얻을 수 있는 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year, day of week, month, week number를 기본 feature로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).year  \n",
    "day_of_week = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).weekday()\n",
    "month = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).month\n",
    "# please read docs on how week numbers are calculate\n",
    "week_number = lambda x: datetime.strptime(x, \"%Y-%m-%d\" ).strftime('%V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txs['year'] = txs['date'].map(year)\n",
    "txs['month']=txs['date'].map(month)\n",
    "txs['week_number']=txs['date'].map(week_number)\n",
    "txs['day_of_week']=txs['date'].map(day_of_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 추가 feature + 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ds-y'의 ds로부터 api로 얻을 수 없는 값: 본 feature를 가공한 feature + ds와 무관한 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons = [0,0,1,1,1,2,2,2,3,3,3,0] #dec - feb is winter, then spring, summer, fall etc\n",
    "season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d\" ).month-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주중/주말(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_of_week01s=[0,0,0,0,0,1,1]\n",
    "day_of_week01= lambda x: day_of_week01s[(datetime.strptime(x, \"%Y-%m-%d\" ).weekday())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week_number</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>season</th>\n",
       "      <th>day_of_week01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>34</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>41</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>54</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>41</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>35</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>44</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>50</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>42</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>42</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>66</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016-01-11</td>\n",
       "      <td>50</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-01-12</td>\n",
       "      <td>55</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>56</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>53</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>44</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>54</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-01-17</td>\n",
       "      <td>54</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>50</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-01-19</td>\n",
       "      <td>40</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>49</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-01-21</td>\n",
       "      <td>28</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>72</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-01-23</td>\n",
       "      <td>71</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>53</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>43</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>38</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>55</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2016-01-28</td>\n",
       "      <td>49</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2016-01-29</td>\n",
       "      <td>43</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2016-01-30</td>\n",
       "      <td>49</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>16</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>20</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>22</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>2017-01-07</td>\n",
       "      <td>20</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>16</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>17</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>97</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>2017-01-12</td>\n",
       "      <td>23</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>100</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>2017-01-14</td>\n",
       "      <td>18</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>2017-01-15</td>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>02</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>2017-01-16</td>\n",
       "      <td>21</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>2017-01-17</td>\n",
       "      <td>24</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>2017-01-18</td>\n",
       "      <td>16</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>2017-01-19</td>\n",
       "      <td>18</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>2017-01-20</td>\n",
       "      <td>22</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>2017-01-21</td>\n",
       "      <td>18</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>03</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>24</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>2017-01-24</td>\n",
       "      <td>22</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>2017-01-25</td>\n",
       "      <td>20</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>2017-01-26</td>\n",
       "      <td>22</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>26</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>2017-01-28</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>04</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>31</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  sales  year  month week_number  day_of_week  season  \\\n",
       "0    2016-01-01     34  2016      1          53            4       0   \n",
       "1    2016-01-02     41  2016      1          53            5       0   \n",
       "2    2016-01-03     54  2016      1          53            6       0   \n",
       "3    2016-01-04     41  2016      1          01            0       0   \n",
       "4    2016-01-05     35  2016      1          01            1       0   \n",
       "5    2016-01-06     44  2016      1          01            2       0   \n",
       "6    2016-01-07     50  2016      1          01            3       0   \n",
       "7    2016-01-08     42  2016      1          01            4       0   \n",
       "8    2016-01-09     42  2016      1          01            5       0   \n",
       "9    2016-01-10     66  2016      1          01            6       0   \n",
       "10   2016-01-11     50  2016      1          02            0       0   \n",
       "11   2016-01-12     55  2016      1          02            1       0   \n",
       "12   2016-01-13     56  2016      1          02            2       0   \n",
       "13   2016-01-14     53  2016      1          02            3       0   \n",
       "14   2016-01-15     44  2016      1          02            4       0   \n",
       "15   2016-01-16     54  2016      1          02            5       0   \n",
       "16   2016-01-17     54  2016      1          02            6       0   \n",
       "17   2016-01-18     50  2016      1          03            0       0   \n",
       "18   2016-01-19     40  2016      1          03            1       0   \n",
       "19   2016-01-20     49  2016      1          03            2       0   \n",
       "20   2016-01-21     28  2016      1          03            3       0   \n",
       "21   2016-01-22     72  2016      1          03            4       0   \n",
       "22   2016-01-23     71  2016      1          03            5       0   \n",
       "23   2016-01-24     53  2016      1          03            6       0   \n",
       "24   2016-01-25     43  2016      1          04            0       0   \n",
       "25   2016-01-26     38  2016      1          04            1       0   \n",
       "26   2016-01-27     55  2016      1          04            2       0   \n",
       "27   2016-01-28     49  2016      1          04            3       0   \n",
       "28   2016-01-29     43  2016      1          04            4       0   \n",
       "29   2016-01-30     49  2016      1          04            5       0   \n",
       "..          ...    ...   ...    ...         ...          ...     ...   \n",
       "367  2017-01-02     16  2017      1          01            0       0   \n",
       "368  2017-01-03     20  2017      1          01            1       0   \n",
       "369  2017-01-04     12  2017      1          01            2       0   \n",
       "370  2017-01-05     11  2017      1          01            3       0   \n",
       "371  2017-01-06     22  2017      1          01            4       0   \n",
       "372  2017-01-07     20  2017      1          01            5       0   \n",
       "373  2017-01-08     16  2017      1          01            6       0   \n",
       "374  2017-01-09     17  2017      1          02            0       0   \n",
       "375  2017-01-10     97  2017      1          02            1       0   \n",
       "376  2017-01-11      0  2017      1          02            2       0   \n",
       "377  2017-01-12     23  2017      1          02            3       0   \n",
       "378  2017-01-13    100  2017      1          02            4       0   \n",
       "379  2017-01-14     18  2017      1          02            5       0   \n",
       "380  2017-01-15     13  2017      1          02            6       0   \n",
       "381  2017-01-16     21  2017      1          03            0       0   \n",
       "382  2017-01-17     24  2017      1          03            1       0   \n",
       "383  2017-01-18     16  2017      1          03            2       0   \n",
       "384  2017-01-19     18  2017      1          03            3       0   \n",
       "385  2017-01-20     22  2017      1          03            4       0   \n",
       "386  2017-01-21     18  2017      1          03            5       0   \n",
       "387  2017-01-22     11  2017      1          03            6       0   \n",
       "388  2017-01-23     24  2017      1          04            0       0   \n",
       "389  2017-01-24     22  2017      1          04            1       0   \n",
       "390  2017-01-25     20  2017      1          04            2       0   \n",
       "391  2017-01-26     22  2017      1          04            3       0   \n",
       "392  2017-01-27     26  2017      1          04            4       0   \n",
       "393  2017-01-28      0  2017      1          04            5       0   \n",
       "394  2017-01-29      0  2017      1          04            6       0   \n",
       "395  2017-01-30      5  2017      1          05            0       0   \n",
       "396  2017-01-31     31  2017      1          05            1       0   \n",
       "\n",
       "     day_of_week01  \n",
       "0                0  \n",
       "1                1  \n",
       "2                1  \n",
       "3                0  \n",
       "4                0  \n",
       "5                0  \n",
       "6                0  \n",
       "7                0  \n",
       "8                1  \n",
       "9                1  \n",
       "10               0  \n",
       "11               0  \n",
       "12               0  \n",
       "13               0  \n",
       "14               0  \n",
       "15               1  \n",
       "16               1  \n",
       "17               0  \n",
       "18               0  \n",
       "19               0  \n",
       "20               0  \n",
       "21               0  \n",
       "22               1  \n",
       "23               1  \n",
       "24               0  \n",
       "25               0  \n",
       "26               0  \n",
       "27               0  \n",
       "28               0  \n",
       "29               1  \n",
       "..             ...  \n",
       "367              0  \n",
       "368              0  \n",
       "369              0  \n",
       "370              0  \n",
       "371              0  \n",
       "372              1  \n",
       "373              1  \n",
       "374              0  \n",
       "375              0  \n",
       "376              0  \n",
       "377              0  \n",
       "378              0  \n",
       "379              1  \n",
       "380              1  \n",
       "381              0  \n",
       "382              0  \n",
       "383              0  \n",
       "384              0  \n",
       "385              0  \n",
       "386              1  \n",
       "387              1  \n",
       "388              0  \n",
       "389              0  \n",
       "390              0  \n",
       "391              0  \n",
       "392              0  \n",
       "393              1  \n",
       "394              1  \n",
       "395              0  \n",
       "396              0  \n",
       "\n",
       "[397 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txs['season']=txs['date'].map(season)\n",
    "txs['day_of_week01']=txs['date'].map(day_of_week01)\n",
    "txs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas를 통해 구한 각 feature는 list()로 우리의 기준type인 list로 변경이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 추가 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 y의 추가 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막rmse를 계산할 때 원본 데이터를 복원하기 위해 저장해놓음(아무런 가공되지 않은 원본 sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "originalSales=list(txs['sales'])\n",
    "sales=list(txs['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "282 in originalSales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가공을 하는 순서도 중요한데, 이상점 제거-> log or sqrt -> normalization이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상점 제거, bucketization 을 하여 새로운 열을 생성하는 방향으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이상점 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상점 제거를 위해 평균과 표준편차를 구한다. 이상점의 기준은 일단 평균+-2*sd로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noOutlierSales(sales):\n",
    "    mean=np.mean(sales)\n",
    "    std=np.std(sales)\n",
    "    for i in range(len(sales)):\n",
    "        if (sales[i]<mean-2*std or sales[i]>mean+2*std):\n",
    "             sales[i]=int(mean)\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logSales(sales):\n",
    "    for i in range(len(sales)):\n",
    "        if sales[i] is 0:\n",
    "            sales[i]=1\n",
    "    return np.log(sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqrt(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sqrtSales(sales):\n",
    "    return np.sqrt(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales=noOutlierSales(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "282 in sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "282 in originalSales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 x의 추가 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 합친 후 (필요 시 normalize하여) 최종 data 생성: XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계절(0~3) + 요일(0~6) + 주수(1~53) -> 판매량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tempxy=[list(txs['season']),list(txs['day_of_week']),list(txs['week_number']),sales]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계절(0~3) + 요일(0,1) + 주수(1~53) -> 판매량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tempxy=[list(txs['season']),list(txs['day_of_week01']),list(txs['week_number']),sales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy=np.array(tempxy).transpose().astype(np.float)\n",
    "originalXY=np.array(tempxy).transpose().astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minMaxNormalizer(data):\n",
    "    numerator=data-np.min(data)\n",
    "    denominator=np.max(data)-np.min(data)\n",
    "    return numerator/(denominator+1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization이 필요한 열을 normalize시킴(현재는 sales에 해당하는 마지막 열만 normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy=minMaxNormalizer(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.04,  0.53,  0.34],\n",
       "       [ 0.  ,  0.05,  0.53,  0.41],\n",
       "       [ 0.  ,  0.06,  0.53,  0.54],\n",
       "       ..., \n",
       "       [ 0.  ,  0.06,  0.04,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.05,  0.05],\n",
       "       [ 0.  ,  0.01,  0.05,  0.31]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xy[:,-1]=minMaxNormalizer(xy[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 열 ex) XY[:,-3]=minMaxNormalizer(XY[:,-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측모델을 통해 얻은 sales결과를 denormalize시켜 기존 단위로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minMaxDeNormalizer(data, originalData):\n",
    "    shift=np.min(originalData)\n",
    "    multiplier=np.max(originalData)-np.min(originalData)\n",
    "    return (data+shift)*multiplier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 함수를 마지막 rmse구하기 전에 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MODEL 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 사용 model 정의: RNN LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 해당 model의 train parameters 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_dim은 y값 도출을 위한 feature 가지수+1(독립변수 가지수 +1(y포함))\n",
    "data_dim=4\n",
    "\n",
    "#data_dim크기의 data 한 묶음이 seq_length만큼 input으로 들어가\n",
    "seq_length=5\n",
    "\n",
    "#output_dim(=forecastDays)만큼의 다음날 y_data를 예측\n",
    "forecastDays=1\n",
    "output_dim=forecastDays\n",
    "\n",
    "#hidden_dim은 정말 임의로 설정\n",
    "hidden_dim=10\n",
    "\n",
    "#learning rate은 배우는 속도(너무 크지도, 작지도 않게 설정)\n",
    "learning_rate=0.01\n",
    "\n",
    "#iterations는 반복 횟수\n",
    "iterations=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 사용 model, train parameter에 맞추어 dataset(XY) 변환: dataX, dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.04  0.53  0.34]\n",
      " [ 0.    0.05  0.53  0.41]\n",
      " [ 0.    0.06  0.53  0.54]\n",
      " [ 0.    0.    0.01  0.41]\n",
      " [ 0.    0.01  0.01  0.35]] -> [ 0.44]\n",
      "[[ 0.    0.05  0.53  0.41]\n",
      " [ 0.    0.06  0.53  0.54]\n",
      " [ 0.    0.    0.01  0.41]\n",
      " [ 0.    0.01  0.01  0.35]\n",
      " [ 0.    0.02  0.01  0.44]] -> [ 0.5]\n",
      "[[ 0.    0.06  0.53  0.54]\n",
      " [ 0.    0.    0.01  0.41]\n",
      " [ 0.    0.01  0.01  0.35]\n",
      " [ 0.    0.02  0.01  0.44]\n",
      " [ 0.    0.03  0.01  0.5 ]] -> [ 0.42]\n",
      "[[ 0.    0.    0.01  0.41]\n",
      " [ 0.    0.01  0.01  0.35]\n",
      " [ 0.    0.02  0.01  0.44]\n",
      " [ 0.    0.03  0.01  0.5 ]\n",
      " [ 0.    0.04  0.01  0.42]] -> [ 0.42]\n",
      "[[ 0.    0.01  0.01  0.35]\n",
      " [ 0.    0.02  0.01  0.44]\n",
      " [ 0.    0.03  0.01  0.5 ]\n",
      " [ 0.    0.04  0.01  0.42]\n",
      " [ 0.    0.05  0.01  0.42]] -> [ 0.66]\n",
      "[[ 0.    0.02  0.01  0.44]\n",
      " [ 0.    0.03  0.01  0.5 ]\n",
      " [ 0.    0.04  0.01  0.42]\n",
      " [ 0.    0.05  0.01  0.42]\n",
      " [ 0.    0.06  0.01  0.66]] -> [ 0.5]\n",
      "[[ 0.    0.03  0.01  0.5 ]\n",
      " [ 0.    0.04  0.01  0.42]\n",
      " [ 0.    0.05  0.01  0.42]\n",
      " [ 0.    0.06  0.01  0.66]\n",
      " [ 0.    0.    0.02  0.5 ]] -> [ 0.55]\n",
      "[[ 0.    0.04  0.01  0.42]\n",
      " [ 0.    0.05  0.01  0.42]\n",
      " [ 0.    0.06  0.01  0.66]\n",
      " [ 0.    0.    0.02  0.5 ]\n",
      " [ 0.    0.01  0.02  0.55]] -> [ 0.56]\n",
      "[[ 0.    0.05  0.01  0.42]\n",
      " [ 0.    0.06  0.01  0.66]\n",
      " [ 0.    0.    0.02  0.5 ]\n",
      " [ 0.    0.01  0.02  0.55]\n",
      " [ 0.    0.02  0.02  0.56]] -> [ 0.53]\n",
      "[[ 0.    0.06  0.01  0.66]\n",
      " [ 0.    0.    0.02  0.5 ]\n",
      " [ 0.    0.01  0.02  0.55]\n",
      " [ 0.    0.02  0.02  0.56]\n",
      " [ 0.    0.03  0.02  0.53]] -> [ 0.44]\n",
      "[[ 0.    0.    0.02  0.5 ]\n",
      " [ 0.    0.01  0.02  0.55]\n",
      " [ 0.    0.02  0.02  0.56]\n",
      " [ 0.    0.03  0.02  0.53]\n",
      " [ 0.    0.04  0.02  0.44]] -> [ 0.54]\n",
      "[[ 0.    0.01  0.02  0.55]\n",
      " [ 0.    0.02  0.02  0.56]\n",
      " [ 0.    0.03  0.02  0.53]\n",
      " [ 0.    0.04  0.02  0.44]\n",
      " [ 0.    0.05  0.02  0.54]] -> [ 0.54]\n",
      "[[ 0.    0.02  0.02  0.56]\n",
      " [ 0.    0.03  0.02  0.53]\n",
      " [ 0.    0.04  0.02  0.44]\n",
      " [ 0.    0.05  0.02  0.54]\n",
      " [ 0.    0.06  0.02  0.54]] -> [ 0.5]\n",
      "[[ 0.    0.03  0.02  0.53]\n",
      " [ 0.    0.04  0.02  0.44]\n",
      " [ 0.    0.05  0.02  0.54]\n",
      " [ 0.    0.06  0.02  0.54]\n",
      " [ 0.    0.    0.03  0.5 ]] -> [ 0.4]\n",
      "[[ 0.    0.04  0.02  0.44]\n",
      " [ 0.    0.05  0.02  0.54]\n",
      " [ 0.    0.06  0.02  0.54]\n",
      " [ 0.    0.    0.03  0.5 ]\n",
      " [ 0.    0.01  0.03  0.4 ]] -> [ 0.49]\n",
      "[[ 0.    0.05  0.02  0.54]\n",
      " [ 0.    0.06  0.02  0.54]\n",
      " [ 0.    0.    0.03  0.5 ]\n",
      " [ 0.    0.01  0.03  0.4 ]\n",
      " [ 0.    0.02  0.03  0.49]] -> [ 0.28]\n",
      "[[ 0.    0.06  0.02  0.54]\n",
      " [ 0.    0.    0.03  0.5 ]\n",
      " [ 0.    0.01  0.03  0.4 ]\n",
      " [ 0.    0.02  0.03  0.49]\n",
      " [ 0.    0.03  0.03  0.28]] -> [ 0.72]\n",
      "[[ 0.    0.    0.03  0.5 ]\n",
      " [ 0.    0.01  0.03  0.4 ]\n",
      " [ 0.    0.02  0.03  0.49]\n",
      " [ 0.    0.03  0.03  0.28]\n",
      " [ 0.    0.04  0.03  0.72]] -> [ 0.71]\n",
      "[[ 0.    0.01  0.03  0.4 ]\n",
      " [ 0.    0.02  0.03  0.49]\n",
      " [ 0.    0.03  0.03  0.28]\n",
      " [ 0.    0.04  0.03  0.72]\n",
      " [ 0.    0.05  0.03  0.71]] -> [ 0.53]\n",
      "[[ 0.    0.02  0.03  0.49]\n",
      " [ 0.    0.03  0.03  0.28]\n",
      " [ 0.    0.04  0.03  0.72]\n",
      " [ 0.    0.05  0.03  0.71]\n",
      " [ 0.    0.06  0.03  0.53]] -> [ 0.43]\n",
      "[[ 0.    0.03  0.03  0.28]\n",
      " [ 0.    0.04  0.03  0.72]\n",
      " [ 0.    0.05  0.03  0.71]\n",
      " [ 0.    0.06  0.03  0.53]\n",
      " [ 0.    0.    0.04  0.43]] -> [ 0.38]\n",
      "[[ 0.    0.04  0.03  0.72]\n",
      " [ 0.    0.05  0.03  0.71]\n",
      " [ 0.    0.06  0.03  0.53]\n",
      " [ 0.    0.    0.04  0.43]\n",
      " [ 0.    0.01  0.04  0.38]] -> [ 0.55]\n",
      "[[ 0.    0.05  0.03  0.71]\n",
      " [ 0.    0.06  0.03  0.53]\n",
      " [ 0.    0.    0.04  0.43]\n",
      " [ 0.    0.01  0.04  0.38]\n",
      " [ 0.    0.02  0.04  0.55]] -> [ 0.49]\n",
      "[[ 0.    0.06  0.03  0.53]\n",
      " [ 0.    0.    0.04  0.43]\n",
      " [ 0.    0.01  0.04  0.38]\n",
      " [ 0.    0.02  0.04  0.55]\n",
      " [ 0.    0.03  0.04  0.49]] -> [ 0.43]\n",
      "[[ 0.    0.    0.04  0.43]\n",
      " [ 0.    0.01  0.04  0.38]\n",
      " [ 0.    0.02  0.04  0.55]\n",
      " [ 0.    0.03  0.04  0.49]\n",
      " [ 0.    0.04  0.04  0.43]] -> [ 0.49]\n",
      "[[ 0.    0.01  0.04  0.38]\n",
      " [ 0.    0.02  0.04  0.55]\n",
      " [ 0.    0.03  0.04  0.49]\n",
      " [ 0.    0.04  0.04  0.43]\n",
      " [ 0.    0.05  0.04  0.49]] -> [ 0.49]\n",
      "[[ 0.    0.02  0.04  0.55]\n",
      " [ 0.    0.03  0.04  0.49]\n",
      " [ 0.    0.04  0.04  0.43]\n",
      " [ 0.    0.05  0.04  0.49]\n",
      " [ 0.    0.06  0.04  0.49]] -> [ 0.44]\n",
      "[[ 0.    0.03  0.04  0.49]\n",
      " [ 0.    0.04  0.04  0.43]\n",
      " [ 0.    0.05  0.04  0.49]\n",
      " [ 0.    0.06  0.04  0.49]\n",
      " [ 0.    0.    0.05  0.44]] -> [ 0.39]\n",
      "[[ 0.    0.04  0.04  0.43]\n",
      " [ 0.    0.05  0.04  0.49]\n",
      " [ 0.    0.06  0.04  0.49]\n",
      " [ 0.    0.    0.05  0.44]\n",
      " [ 0.    0.01  0.05  0.39]] -> [ 0.52]\n",
      "[[ 0.    0.05  0.04  0.49]\n",
      " [ 0.    0.06  0.04  0.49]\n",
      " [ 0.    0.    0.05  0.44]\n",
      " [ 0.    0.01  0.05  0.39]\n",
      " [ 0.    0.02  0.05  0.52]] -> [ 0.45]\n",
      "[[ 0.    0.06  0.04  0.49]\n",
      " [ 0.    0.    0.05  0.44]\n",
      " [ 0.    0.01  0.05  0.39]\n",
      " [ 0.    0.02  0.05  0.52]\n",
      " [ 0.    0.03  0.05  0.45]] -> [ 0.33]\n",
      "[[ 0.    0.    0.05  0.44]\n",
      " [ 0.    0.01  0.05  0.39]\n",
      " [ 0.    0.02  0.05  0.52]\n",
      " [ 0.    0.03  0.05  0.45]\n",
      " [ 0.    0.04  0.05  0.33]] -> [ 0.43]\n",
      "[[ 0.    0.01  0.05  0.39]\n",
      " [ 0.    0.02  0.05  0.52]\n",
      " [ 0.    0.03  0.05  0.45]\n",
      " [ 0.    0.04  0.05  0.33]\n",
      " [ 0.    0.05  0.05  0.43]] -> [ 0.4]\n",
      "[[ 0.    0.02  0.05  0.52]\n",
      " [ 0.    0.03  0.05  0.45]\n",
      " [ 0.    0.04  0.05  0.33]\n",
      " [ 0.    0.05  0.05  0.43]\n",
      " [ 0.    0.06  0.05  0.4 ]] -> [ 0.46]\n",
      "[[ 0.    0.03  0.05  0.45]\n",
      " [ 0.    0.04  0.05  0.33]\n",
      " [ 0.    0.05  0.05  0.43]\n",
      " [ 0.    0.06  0.05  0.4 ]\n",
      " [ 0.    0.    0.06  0.46]] -> [ 0.49]\n",
      "[[ 0.    0.04  0.05  0.33]\n",
      " [ 0.    0.05  0.05  0.43]\n",
      " [ 0.    0.06  0.05  0.4 ]\n",
      " [ 0.    0.    0.06  0.46]\n",
      " [ 0.    0.01  0.06  0.49]] -> [ 0.5]\n",
      "[[ 0.    0.05  0.05  0.43]\n",
      " [ 0.    0.06  0.05  0.4 ]\n",
      " [ 0.    0.    0.06  0.46]\n",
      " [ 0.    0.01  0.06  0.49]\n",
      " [ 0.    0.02  0.06  0.5 ]] -> [ 0.37]\n",
      "[[ 0.    0.06  0.05  0.4 ]\n",
      " [ 0.    0.    0.06  0.46]\n",
      " [ 0.    0.01  0.06  0.49]\n",
      " [ 0.    0.02  0.06  0.5 ]\n",
      " [ 0.    0.03  0.06  0.37]] -> [ 0.37]\n",
      "[[ 0.    0.    0.06  0.46]\n",
      " [ 0.    0.01  0.06  0.49]\n",
      " [ 0.    0.02  0.06  0.5 ]\n",
      " [ 0.    0.03  0.06  0.37]\n",
      " [ 0.    0.04  0.06  0.37]] -> [ 0.45]\n",
      "[[ 0.    0.01  0.06  0.49]\n",
      " [ 0.    0.02  0.06  0.5 ]\n",
      " [ 0.    0.03  0.06  0.37]\n",
      " [ 0.    0.04  0.06  0.37]\n",
      " [ 0.    0.05  0.06  0.45]] -> [ 0.48]\n",
      "[[ 0.    0.02  0.06  0.5 ]\n",
      " [ 0.    0.03  0.06  0.37]\n",
      " [ 0.    0.04  0.06  0.37]\n",
      " [ 0.    0.05  0.06  0.45]\n",
      " [ 0.    0.06  0.06  0.48]] -> [ 0.48]\n",
      "[[ 0.    0.03  0.06  0.37]\n",
      " [ 0.    0.04  0.06  0.37]\n",
      " [ 0.    0.05  0.06  0.45]\n",
      " [ 0.    0.06  0.06  0.48]\n",
      " [ 0.    0.    0.07  0.48]] -> [ 0.38]\n",
      "[[ 0.    0.04  0.06  0.37]\n",
      " [ 0.    0.05  0.06  0.45]\n",
      " [ 0.    0.06  0.06  0.48]\n",
      " [ 0.    0.    0.07  0.48]\n",
      " [ 0.    0.01  0.07  0.38]] -> [ 0.6]\n",
      "[[ 0.    0.05  0.06  0.45]\n",
      " [ 0.    0.06  0.06  0.48]\n",
      " [ 0.    0.    0.07  0.48]\n",
      " [ 0.    0.01  0.07  0.38]\n",
      " [ 0.    0.02  0.07  0.6 ]] -> [ 0.31]\n",
      "[[ 0.    0.06  0.06  0.48]\n",
      " [ 0.    0.    0.07  0.48]\n",
      " [ 0.    0.01  0.07  0.38]\n",
      " [ 0.    0.02  0.07  0.6 ]\n",
      " [ 0.    0.03  0.07  0.31]] -> [ 0.35]\n",
      "[[ 0.    0.    0.07  0.48]\n",
      " [ 0.    0.01  0.07  0.38]\n",
      " [ 0.    0.02  0.07  0.6 ]\n",
      " [ 0.    0.03  0.07  0.31]\n",
      " [ 0.    0.04  0.07  0.35]] -> [ 0.53]\n",
      "[[ 0.    0.01  0.07  0.38]\n",
      " [ 0.    0.02  0.07  0.6 ]\n",
      " [ 0.    0.03  0.07  0.31]\n",
      " [ 0.    0.04  0.07  0.35]\n",
      " [ 0.    0.05  0.07  0.53]] -> [ 0.7]\n",
      "[[ 0.    0.02  0.07  0.6 ]\n",
      " [ 0.    0.03  0.07  0.31]\n",
      " [ 0.    0.04  0.07  0.35]\n",
      " [ 0.    0.05  0.07  0.53]\n",
      " [ 0.    0.06  0.07  0.7 ]] -> [ 0.62]\n",
      "[[ 0.    0.03  0.07  0.31]\n",
      " [ 0.    0.04  0.07  0.35]\n",
      " [ 0.    0.05  0.07  0.53]\n",
      " [ 0.    0.06  0.07  0.7 ]\n",
      " [ 0.    0.    0.08  0.62]] -> [ 0.48]\n",
      "[[ 0.    0.04  0.07  0.35]\n",
      " [ 0.    0.05  0.07  0.53]\n",
      " [ 0.    0.06  0.07  0.7 ]\n",
      " [ 0.    0.    0.08  0.62]\n",
      " [ 0.    0.01  0.08  0.48]] -> [ 0.51]\n",
      "[[ 0.    0.05  0.07  0.53]\n",
      " [ 0.    0.06  0.07  0.7 ]\n",
      " [ 0.    0.    0.08  0.62]\n",
      " [ 0.    0.01  0.08  0.48]\n",
      " [ 0.    0.02  0.08  0.51]] -> [ 0.49]\n",
      "[[ 0.    0.06  0.07  0.7 ]\n",
      " [ 0.    0.    0.08  0.62]\n",
      " [ 0.    0.01  0.08  0.48]\n",
      " [ 0.    0.02  0.08  0.51]\n",
      " [ 0.    0.03  0.08  0.49]] -> [ 0.38]\n",
      "[[ 0.    0.    0.08  0.62]\n",
      " [ 0.    0.01  0.08  0.48]\n",
      " [ 0.    0.02  0.08  0.51]\n",
      " [ 0.    0.03  0.08  0.49]\n",
      " [ 0.    0.04  0.08  0.38]] -> [ 0.32]\n",
      "[[ 0.    0.01  0.08  0.48]\n",
      " [ 0.    0.02  0.08  0.51]\n",
      " [ 0.    0.03  0.08  0.49]\n",
      " [ 0.    0.04  0.08  0.38]\n",
      " [ 0.    0.05  0.08  0.32]] -> [ 0.39]\n",
      "[[ 0.    0.02  0.08  0.51]\n",
      " [ 0.    0.03  0.08  0.49]\n",
      " [ 0.    0.04  0.08  0.38]\n",
      " [ 0.    0.05  0.08  0.32]\n",
      " [ 0.    0.06  0.08  0.39]] -> [ 0.35]\n",
      "[[ 0.    0.03  0.08  0.49]\n",
      " [ 0.    0.04  0.08  0.38]\n",
      " [ 0.    0.05  0.08  0.32]\n",
      " [ 0.    0.06  0.08  0.39]\n",
      " [ 0.    0.    0.09  0.35]] -> [ 0.3]\n",
      "[[ 0.    0.04  0.08  0.38]\n",
      " [ 0.    0.05  0.08  0.32]\n",
      " [ 0.    0.06  0.08  0.39]\n",
      " [ 0.    0.    0.09  0.35]\n",
      " [ 0.01  0.01  0.09  0.3 ]] -> [ 0.36]\n",
      "[[ 0.    0.05  0.08  0.32]\n",
      " [ 0.    0.06  0.08  0.39]\n",
      " [ 0.    0.    0.09  0.35]\n",
      " [ 0.01  0.01  0.09  0.3 ]\n",
      " [ 0.01  0.02  0.09  0.36]] -> [ 0.31]\n",
      "[[ 0.    0.06  0.08  0.39]\n",
      " [ 0.    0.    0.09  0.35]\n",
      " [ 0.01  0.01  0.09  0.3 ]\n",
      " [ 0.01  0.02  0.09  0.36]\n",
      " [ 0.01  0.03  0.09  0.31]] -> [ 0.31]\n",
      "[[ 0.    0.    0.09  0.35]\n",
      " [ 0.01  0.01  0.09  0.3 ]\n",
      " [ 0.01  0.02  0.09  0.36]\n",
      " [ 0.01  0.03  0.09  0.31]\n",
      " [ 0.01  0.04  0.09  0.31]] -> [ 0.44]\n",
      "[[ 0.01  0.01  0.09  0.3 ]\n",
      " [ 0.01  0.02  0.09  0.36]\n",
      " [ 0.01  0.03  0.09  0.31]\n",
      " [ 0.01  0.04  0.09  0.31]\n",
      " [ 0.01  0.05  0.09  0.44]] -> [ 0.41]\n",
      "[[ 0.01  0.02  0.09  0.36]\n",
      " [ 0.01  0.03  0.09  0.31]\n",
      " [ 0.01  0.04  0.09  0.31]\n",
      " [ 0.01  0.05  0.09  0.44]\n",
      " [ 0.01  0.06  0.09  0.41]] -> [ 0.41]\n",
      "[[ 0.01  0.03  0.09  0.31]\n",
      " [ 0.01  0.04  0.09  0.31]\n",
      " [ 0.01  0.05  0.09  0.44]\n",
      " [ 0.01  0.06  0.09  0.41]\n",
      " [ 0.01  0.    0.1   0.41]] -> [ 0.45]\n",
      "[[ 0.01  0.04  0.09  0.31]\n",
      " [ 0.01  0.05  0.09  0.44]\n",
      " [ 0.01  0.06  0.09  0.41]\n",
      " [ 0.01  0.    0.1   0.41]\n",
      " [ 0.01  0.01  0.1   0.45]] -> [ 0.46]\n",
      "[[ 0.01  0.05  0.09  0.44]\n",
      " [ 0.01  0.06  0.09  0.41]\n",
      " [ 0.01  0.    0.1   0.41]\n",
      " [ 0.01  0.01  0.1   0.45]\n",
      " [ 0.01  0.02  0.1   0.46]] -> [ 0.45]\n",
      "[[ 0.01  0.06  0.09  0.41]\n",
      " [ 0.01  0.    0.1   0.41]\n",
      " [ 0.01  0.01  0.1   0.45]\n",
      " [ 0.01  0.02  0.1   0.46]\n",
      " [ 0.01  0.03  0.1   0.45]] -> [ 0.41]\n",
      "[[ 0.01  0.    0.1   0.41]\n",
      " [ 0.01  0.01  0.1   0.45]\n",
      " [ 0.01  0.02  0.1   0.46]\n",
      " [ 0.01  0.03  0.1   0.45]\n",
      " [ 0.01  0.04  0.1   0.41]] -> [ 0.47]\n",
      "[[ 0.01  0.01  0.1   0.45]\n",
      " [ 0.01  0.02  0.1   0.46]\n",
      " [ 0.01  0.03  0.1   0.45]\n",
      " [ 0.01  0.04  0.1   0.41]\n",
      " [ 0.01  0.05  0.1   0.47]] -> [ 0.48]\n",
      "[[ 0.01  0.02  0.1   0.46]\n",
      " [ 0.01  0.03  0.1   0.45]\n",
      " [ 0.01  0.04  0.1   0.41]\n",
      " [ 0.01  0.05  0.1   0.47]\n",
      " [ 0.01  0.06  0.1   0.48]] -> [ 0.4]\n",
      "[[ 0.01  0.03  0.1   0.45]\n",
      " [ 0.01  0.04  0.1   0.41]\n",
      " [ 0.01  0.05  0.1   0.47]\n",
      " [ 0.01  0.06  0.1   0.48]\n",
      " [ 0.01  0.    0.11  0.4 ]] -> [ 0.42]\n",
      "[[ 0.01  0.04  0.1   0.41]\n",
      " [ 0.01  0.05  0.1   0.47]\n",
      " [ 0.01  0.06  0.1   0.48]\n",
      " [ 0.01  0.    0.11  0.4 ]\n",
      " [ 0.01  0.01  0.11  0.42]] -> [ 0.38]\n",
      "[[ 0.01  0.05  0.1   0.47]\n",
      " [ 0.01  0.06  0.1   0.48]\n",
      " [ 0.01  0.    0.11  0.4 ]\n",
      " [ 0.01  0.01  0.11  0.42]\n",
      " [ 0.01  0.02  0.11  0.38]] -> [ 0.38]\n",
      "[[ 0.01  0.06  0.1   0.48]\n",
      " [ 0.01  0.    0.11  0.4 ]\n",
      " [ 0.01  0.01  0.11  0.42]\n",
      " [ 0.01  0.02  0.11  0.38]\n",
      " [ 0.01  0.03  0.11  0.38]] -> [ 0.45]\n",
      "[[ 0.01  0.    0.11  0.4 ]\n",
      " [ 0.01  0.01  0.11  0.42]\n",
      " [ 0.01  0.02  0.11  0.38]\n",
      " [ 0.01  0.03  0.11  0.38]\n",
      " [ 0.01  0.04  0.11  0.45]] -> [ 0.48]\n",
      "[[ 0.01  0.01  0.11  0.42]\n",
      " [ 0.01  0.02  0.11  0.38]\n",
      " [ 0.01  0.03  0.11  0.38]\n",
      " [ 0.01  0.04  0.11  0.45]\n",
      " [ 0.01  0.05  0.11  0.48]] -> [ 0.62]\n",
      "[[ 0.01  0.02  0.11  0.38]\n",
      " [ 0.01  0.03  0.11  0.38]\n",
      " [ 0.01  0.04  0.11  0.45]\n",
      " [ 0.01  0.05  0.11  0.48]\n",
      " [ 0.01  0.06  0.11  0.62]] -> [ 0.46]\n",
      "[[ 0.01  0.03  0.11  0.38]\n",
      " [ 0.01  0.04  0.11  0.45]\n",
      " [ 0.01  0.05  0.11  0.48]\n",
      " [ 0.01  0.06  0.11  0.62]\n",
      " [ 0.01  0.    0.12  0.46]] -> [ 0.38]\n",
      "[[ 0.01  0.04  0.11  0.45]\n",
      " [ 0.01  0.05  0.11  0.48]\n",
      " [ 0.01  0.06  0.11  0.62]\n",
      " [ 0.01  0.    0.12  0.46]\n",
      " [ 0.01  0.01  0.12  0.38]] -> [ 0.62]\n",
      "[[ 0.01  0.05  0.11  0.48]\n",
      " [ 0.01  0.06  0.11  0.62]\n",
      " [ 0.01  0.    0.12  0.46]\n",
      " [ 0.01  0.01  0.12  0.38]\n",
      " [ 0.01  0.02  0.12  0.62]] -> [ 0.81]\n",
      "[[ 0.01  0.06  0.11  0.62]\n",
      " [ 0.01  0.    0.12  0.46]\n",
      " [ 0.01  0.01  0.12  0.38]\n",
      " [ 0.01  0.02  0.12  0.62]\n",
      " [ 0.01  0.03  0.12  0.81]] -> [ 0.4]\n",
      "[[ 0.01  0.    0.12  0.46]\n",
      " [ 0.01  0.01  0.12  0.38]\n",
      " [ 0.01  0.02  0.12  0.62]\n",
      " [ 0.01  0.03  0.12  0.81]\n",
      " [ 0.01  0.04  0.12  0.4 ]] -> [ 0.45]\n",
      "[[ 0.01  0.01  0.12  0.38]\n",
      " [ 0.01  0.02  0.12  0.62]\n",
      " [ 0.01  0.03  0.12  0.81]\n",
      " [ 0.01  0.04  0.12  0.4 ]\n",
      " [ 0.01  0.05  0.12  0.45]] -> [ 0.42]\n",
      "[[ 0.01  0.02  0.12  0.62]\n",
      " [ 0.01  0.03  0.12  0.81]\n",
      " [ 0.01  0.04  0.12  0.4 ]\n",
      " [ 0.01  0.05  0.12  0.45]\n",
      " [ 0.01  0.06  0.12  0.42]] -> [ 0.53]\n",
      "[[ 0.01  0.03  0.12  0.81]\n",
      " [ 0.01  0.04  0.12  0.4 ]\n",
      " [ 0.01  0.05  0.12  0.45]\n",
      " [ 0.01  0.06  0.12  0.42]\n",
      " [ 0.01  0.    0.13  0.53]] -> [ 0.53]\n",
      "[[ 0.01  0.04  0.12  0.4 ]\n",
      " [ 0.01  0.05  0.12  0.45]\n",
      " [ 0.01  0.06  0.12  0.42]\n",
      " [ 0.01  0.    0.13  0.53]\n",
      " [ 0.01  0.01  0.13  0.53]] -> [ 0.56]\n",
      "[[ 0.01  0.05  0.12  0.45]\n",
      " [ 0.01  0.06  0.12  0.42]\n",
      " [ 0.01  0.    0.13  0.53]\n",
      " [ 0.01  0.01  0.13  0.53]\n",
      " [ 0.01  0.02  0.13  0.56]] -> [ 0.53]\n",
      "[[ 0.01  0.06  0.12  0.42]\n",
      " [ 0.01  0.    0.13  0.53]\n",
      " [ 0.01  0.01  0.13  0.53]\n",
      " [ 0.01  0.02  0.13  0.56]\n",
      " [ 0.01  0.03  0.13  0.53]] -> [ 0.47]\n",
      "[[ 0.01  0.    0.13  0.53]\n",
      " [ 0.01  0.01  0.13  0.53]\n",
      " [ 0.01  0.02  0.13  0.56]\n",
      " [ 0.01  0.03  0.13  0.53]\n",
      " [ 0.01  0.04  0.13  0.47]] -> [ 0.61]\n",
      "[[ 0.01  0.01  0.13  0.53]\n",
      " [ 0.01  0.02  0.13  0.56]\n",
      " [ 0.01  0.03  0.13  0.53]\n",
      " [ 0.01  0.04  0.13  0.47]\n",
      " [ 0.01  0.05  0.13  0.61]] -> [ 0.64]\n",
      "[[ 0.01  0.02  0.13  0.56]\n",
      " [ 0.01  0.03  0.13  0.53]\n",
      " [ 0.01  0.04  0.13  0.47]\n",
      " [ 0.01  0.05  0.13  0.61]\n",
      " [ 0.01  0.06  0.13  0.64]] -> [ 0.62]\n",
      "[[ 0.01  0.03  0.13  0.53]\n",
      " [ 0.01  0.04  0.13  0.47]\n",
      " [ 0.01  0.05  0.13  0.61]\n",
      " [ 0.01  0.06  0.13  0.64]\n",
      " [ 0.01  0.    0.14  0.62]] -> [ 0.37]\n",
      "[[ 0.01  0.04  0.13  0.47]\n",
      " [ 0.01  0.05  0.13  0.61]\n",
      " [ 0.01  0.06  0.13  0.64]\n",
      " [ 0.01  0.    0.14  0.62]\n",
      " [ 0.01  0.01  0.14  0.37]] -> [ 0.65]\n",
      "[[ 0.01  0.05  0.13  0.61]\n",
      " [ 0.01  0.06  0.13  0.64]\n",
      " [ 0.01  0.    0.14  0.62]\n",
      " [ 0.01  0.01  0.14  0.37]\n",
      " [ 0.01  0.02  0.14  0.65]] -> [ 0.54]\n",
      "[[ 0.01  0.06  0.13  0.64]\n",
      " [ 0.01  0.    0.14  0.62]\n",
      " [ 0.01  0.01  0.14  0.37]\n",
      " [ 0.01  0.02  0.14  0.65]\n",
      " [ 0.01  0.03  0.14  0.54]] -> [ 0.44]\n",
      "[[ 0.01  0.    0.14  0.62]\n",
      " [ 0.01  0.01  0.14  0.37]\n",
      " [ 0.01  0.02  0.14  0.65]\n",
      " [ 0.01  0.03  0.14  0.54]\n",
      " [ 0.01  0.04  0.14  0.44]] -> [ 0.46]\n",
      "[[ 0.01  0.01  0.14  0.37]\n",
      " [ 0.01  0.02  0.14  0.65]\n",
      " [ 0.01  0.03  0.14  0.54]\n",
      " [ 0.01  0.04  0.14  0.44]\n",
      " [ 0.01  0.05  0.14  0.46]] -> [ 0.5]\n",
      "[[ 0.01  0.02  0.14  0.65]\n",
      " [ 0.01  0.03  0.14  0.54]\n",
      " [ 0.01  0.04  0.14  0.44]\n",
      " [ 0.01  0.05  0.14  0.46]\n",
      " [ 0.01  0.06  0.14  0.5 ]] -> [ 0.43]\n",
      "[[ 0.01  0.03  0.14  0.54]\n",
      " [ 0.01  0.04  0.14  0.44]\n",
      " [ 0.01  0.05  0.14  0.46]\n",
      " [ 0.01  0.06  0.14  0.5 ]\n",
      " [ 0.01  0.    0.15  0.43]] -> [ 0.53]\n",
      "[[ 0.01  0.04  0.14  0.44]\n",
      " [ 0.01  0.05  0.14  0.46]\n",
      " [ 0.01  0.06  0.14  0.5 ]\n",
      " [ 0.01  0.    0.15  0.43]\n",
      " [ 0.01  0.01  0.15  0.53]] -> [ 0.63]\n",
      "[[ 0.01  0.05  0.14  0.46]\n",
      " [ 0.01  0.06  0.14  0.5 ]\n",
      " [ 0.01  0.    0.15  0.43]\n",
      " [ 0.01  0.01  0.15  0.53]\n",
      " [ 0.01  0.02  0.15  0.63]] -> [ 0.52]\n",
      "[[ 0.01  0.06  0.14  0.5 ]\n",
      " [ 0.01  0.    0.15  0.43]\n",
      " [ 0.01  0.01  0.15  0.53]\n",
      " [ 0.01  0.02  0.15  0.63]\n",
      " [ 0.01  0.03  0.15  0.52]] -> [ 0.02]\n",
      "[[ 0.01  0.    0.15  0.43]\n",
      " [ 0.01  0.01  0.15  0.53]\n",
      " [ 0.01  0.02  0.15  0.63]\n",
      " [ 0.01  0.03  0.15  0.52]\n",
      " [ 0.01  0.04  0.15  0.02]] -> [ 0.]\n",
      "[[ 0.01  0.01  0.15  0.53]\n",
      " [ 0.01  0.02  0.15  0.63]\n",
      " [ 0.01  0.03  0.15  0.52]\n",
      " [ 0.01  0.04  0.15  0.02]\n",
      " [ 0.01  0.05  0.15  0.  ]] -> [ 0.]\n",
      "[[ 0.01  0.02  0.15  0.63]\n",
      " [ 0.01  0.03  0.15  0.52]\n",
      " [ 0.01  0.04  0.15  0.02]\n",
      " [ 0.01  0.05  0.15  0.  ]\n",
      " [ 0.01  0.06  0.15  0.  ]] -> [ 0.72]\n",
      "[[ 0.01  0.03  0.15  0.52]\n",
      " [ 0.01  0.04  0.15  0.02]\n",
      " [ 0.01  0.05  0.15  0.  ]\n",
      " [ 0.01  0.06  0.15  0.  ]\n",
      " [ 0.01  0.    0.16  0.72]] -> [ 0.59]\n",
      "[[ 0.01  0.04  0.15  0.02]\n",
      " [ 0.01  0.05  0.15  0.  ]\n",
      " [ 0.01  0.06  0.15  0.  ]\n",
      " [ 0.01  0.    0.16  0.72]\n",
      " [ 0.01  0.01  0.16  0.59]] -> [ 0.75]\n",
      "[[ 0.01  0.05  0.15  0.  ]\n",
      " [ 0.01  0.06  0.15  0.  ]\n",
      " [ 0.01  0.    0.16  0.72]\n",
      " [ 0.01  0.01  0.16  0.59]\n",
      " [ 0.01  0.02  0.16  0.75]] -> [ 0.47]\n",
      "[[ 0.01  0.06  0.15  0.  ]\n",
      " [ 0.01  0.    0.16  0.72]\n",
      " [ 0.01  0.01  0.16  0.59]\n",
      " [ 0.01  0.02  0.16  0.75]\n",
      " [ 0.01  0.03  0.16  0.47]] -> [ 0.44]\n",
      "[[ 0.01  0.    0.16  0.72]\n",
      " [ 0.01  0.01  0.16  0.59]\n",
      " [ 0.01  0.02  0.16  0.75]\n",
      " [ 0.01  0.03  0.16  0.47]\n",
      " [ 0.01  0.04  0.16  0.44]] -> [ 0.77]\n",
      "[[ 0.01  0.01  0.16  0.59]\n",
      " [ 0.01  0.02  0.16  0.75]\n",
      " [ 0.01  0.03  0.16  0.47]\n",
      " [ 0.01  0.04  0.16  0.44]\n",
      " [ 0.01  0.05  0.16  0.77]] -> [ 0.9]\n",
      "[[ 0.01  0.02  0.16  0.75]\n",
      " [ 0.01  0.03  0.16  0.47]\n",
      " [ 0.01  0.04  0.16  0.44]\n",
      " [ 0.01  0.05  0.16  0.77]\n",
      " [ 0.01  0.06  0.16  0.9 ]] -> [ 0.93]\n",
      "[[ 0.01  0.03  0.16  0.47]\n",
      " [ 0.01  0.04  0.16  0.44]\n",
      " [ 0.01  0.05  0.16  0.77]\n",
      " [ 0.01  0.06  0.16  0.9 ]\n",
      " [ 0.01  0.    0.17  0.93]] -> [ 0.47]\n",
      "[[ 0.01  0.04  0.16  0.44]\n",
      " [ 0.01  0.05  0.16  0.77]\n",
      " [ 0.01  0.06  0.16  0.9 ]\n",
      " [ 0.01  0.    0.17  0.93]\n",
      " [ 0.01  0.01  0.17  0.47]] -> [ 0.61]\n",
      "[[ 0.01  0.05  0.16  0.77]\n",
      " [ 0.01  0.06  0.16  0.9 ]\n",
      " [ 0.01  0.    0.17  0.93]\n",
      " [ 0.01  0.01  0.17  0.47]\n",
      " [ 0.01  0.02  0.17  0.61]] -> [ 0.77]\n",
      "[[ 0.01  0.06  0.16  0.9 ]\n",
      " [ 0.01  0.    0.17  0.93]\n",
      " [ 0.01  0.01  0.17  0.47]\n",
      " [ 0.01  0.02  0.17  0.61]\n",
      " [ 0.01  0.03  0.17  0.77]] -> [ 0.35]\n",
      "[[ 0.01  0.    0.17  0.93]\n",
      " [ 0.01  0.01  0.17  0.47]\n",
      " [ 0.01  0.02  0.17  0.61]\n",
      " [ 0.01  0.03  0.17  0.77]\n",
      " [ 0.01  0.04  0.17  0.35]] -> [ 0.5]\n",
      "[[ 0.01  0.01  0.17  0.47]\n",
      " [ 0.01  0.02  0.17  0.61]\n",
      " [ 0.01  0.03  0.17  0.77]\n",
      " [ 0.01  0.04  0.17  0.35]\n",
      " [ 0.01  0.05  0.17  0.5 ]] -> [ 0.58]\n",
      "[[ 0.01  0.02  0.17  0.61]\n",
      " [ 0.01  0.03  0.17  0.77]\n",
      " [ 0.01  0.04  0.17  0.35]\n",
      " [ 0.01  0.05  0.17  0.5 ]\n",
      " [ 0.01  0.06  0.17  0.58]] -> [ 0.33]\n",
      "[[ 0.01  0.03  0.17  0.77]\n",
      " [ 0.01  0.04  0.17  0.35]\n",
      " [ 0.01  0.05  0.17  0.5 ]\n",
      " [ 0.01  0.06  0.17  0.58]\n",
      " [ 0.01  0.    0.18  0.33]] -> [ 0.41]\n",
      "[[ 0.01  0.04  0.17  0.35]\n",
      " [ 0.01  0.05  0.17  0.5 ]\n",
      " [ 0.01  0.06  0.17  0.58]\n",
      " [ 0.01  0.    0.18  0.33]\n",
      " [ 0.01  0.01  0.18  0.41]] -> [ 0.36]\n",
      "[[ 0.01  0.05  0.17  0.5 ]\n",
      " [ 0.01  0.06  0.17  0.58]\n",
      " [ 0.01  0.    0.18  0.33]\n",
      " [ 0.01  0.01  0.18  0.41]\n",
      " [ 0.01  0.02  0.18  0.36]] -> [ 0.64]\n",
      "[[ 0.01  0.06  0.17  0.58]\n",
      " [ 0.01  0.    0.18  0.33]\n",
      " [ 0.01  0.01  0.18  0.41]\n",
      " [ 0.01  0.02  0.18  0.36]\n",
      " [ 0.01  0.03  0.18  0.64]] -> [ 0.49]\n",
      "[[ 0.01  0.    0.18  0.33]\n",
      " [ 0.01  0.01  0.18  0.41]\n",
      " [ 0.01  0.02  0.18  0.36]\n",
      " [ 0.01  0.03  0.18  0.64]\n",
      " [ 0.01  0.04  0.18  0.49]] -> [ 0.53]\n",
      "[[ 0.01  0.01  0.18  0.41]\n",
      " [ 0.01  0.02  0.18  0.36]\n",
      " [ 0.01  0.03  0.18  0.64]\n",
      " [ 0.01  0.04  0.18  0.49]\n",
      " [ 0.01  0.05  0.18  0.53]] -> [ 0.6]\n",
      "[[ 0.01  0.02  0.18  0.36]\n",
      " [ 0.01  0.03  0.18  0.64]\n",
      " [ 0.01  0.04  0.18  0.49]\n",
      " [ 0.01  0.05  0.18  0.53]\n",
      " [ 0.01  0.06  0.18  0.6 ]] -> [ 0.43]\n",
      "[[ 0.01  0.03  0.18  0.64]\n",
      " [ 0.01  0.04  0.18  0.49]\n",
      " [ 0.01  0.05  0.18  0.53]\n",
      " [ 0.01  0.06  0.18  0.6 ]\n",
      " [ 0.01  0.    0.19  0.43]] -> [ 0.41]\n",
      "[[ 0.01  0.04  0.18  0.49]\n",
      " [ 0.01  0.05  0.18  0.53]\n",
      " [ 0.01  0.06  0.18  0.6 ]\n",
      " [ 0.01  0.    0.19  0.43]\n",
      " [ 0.01  0.01  0.19  0.41]] -> [ 0.55]\n",
      "[[ 0.01  0.05  0.18  0.53]\n",
      " [ 0.01  0.06  0.18  0.6 ]\n",
      " [ 0.01  0.    0.19  0.43]\n",
      " [ 0.01  0.01  0.19  0.41]\n",
      " [ 0.01  0.02  0.19  0.55]] -> [ 0.45]\n",
      "[[ 0.01  0.06  0.18  0.6 ]\n",
      " [ 0.01  0.    0.19  0.43]\n",
      " [ 0.01  0.01  0.19  0.41]\n",
      " [ 0.01  0.02  0.19  0.55]\n",
      " [ 0.01  0.03  0.19  0.45]] -> [ 0.18]\n",
      "[[ 0.01  0.    0.19  0.43]\n",
      " [ 0.01  0.01  0.19  0.41]\n",
      " [ 0.01  0.02  0.19  0.55]\n",
      " [ 0.01  0.03  0.19  0.45]\n",
      " [ 0.01  0.04  0.19  0.18]] -> [ 0.6]\n",
      "[[ 0.01  0.01  0.19  0.41]\n",
      " [ 0.01  0.02  0.19  0.55]\n",
      " [ 0.01  0.03  0.19  0.45]\n",
      " [ 0.01  0.04  0.19  0.18]\n",
      " [ 0.01  0.05  0.19  0.6 ]] -> [ 0.69]\n",
      "[[ 0.01  0.02  0.19  0.55]\n",
      " [ 0.01  0.03  0.19  0.45]\n",
      " [ 0.01  0.04  0.19  0.18]\n",
      " [ 0.01  0.05  0.19  0.6 ]\n",
      " [ 0.01  0.06  0.19  0.69]] -> [ 0.54]\n",
      "[[ 0.01  0.03  0.19  0.45]\n",
      " [ 0.01  0.04  0.19  0.18]\n",
      " [ 0.01  0.05  0.19  0.6 ]\n",
      " [ 0.01  0.06  0.19  0.69]\n",
      " [ 0.01  0.    0.2   0.54]] -> [ 0.39]\n",
      "[[ 0.01  0.04  0.19  0.18]\n",
      " [ 0.01  0.05  0.19  0.6 ]\n",
      " [ 0.01  0.06  0.19  0.69]\n",
      " [ 0.01  0.    0.2   0.54]\n",
      " [ 0.01  0.01  0.2   0.39]] -> [ 0.5]\n",
      "[[ 0.01  0.05  0.19  0.6 ]\n",
      " [ 0.01  0.06  0.19  0.69]\n",
      " [ 0.01  0.    0.2   0.54]\n",
      " [ 0.01  0.01  0.2   0.39]\n",
      " [ 0.01  0.02  0.2   0.5 ]] -> [ 0.44]\n",
      "[[ 0.01  0.06  0.19  0.69]\n",
      " [ 0.01  0.    0.2   0.54]\n",
      " [ 0.01  0.01  0.2   0.39]\n",
      " [ 0.01  0.02  0.2   0.5 ]\n",
      " [ 0.01  0.03  0.2   0.44]] -> [ 0.54]\n",
      "[[ 0.01  0.    0.2   0.54]\n",
      " [ 0.01  0.01  0.2   0.39]\n",
      " [ 0.01  0.02  0.2   0.5 ]\n",
      " [ 0.01  0.03  0.2   0.44]\n",
      " [ 0.01  0.04  0.2   0.54]] -> [ 0.57]\n",
      "[[ 0.01  0.01  0.2   0.39]\n",
      " [ 0.01  0.02  0.2   0.5 ]\n",
      " [ 0.01  0.03  0.2   0.44]\n",
      " [ 0.01  0.04  0.2   0.54]\n",
      " [ 0.01  0.05  0.2   0.57]] -> [ 0.82]\n",
      "[[ 0.01  0.02  0.2   0.5 ]\n",
      " [ 0.01  0.03  0.2   0.44]\n",
      " [ 0.01  0.04  0.2   0.54]\n",
      " [ 0.01  0.05  0.2   0.57]\n",
      " [ 0.01  0.06  0.2   0.82]] -> [ 0.57]\n",
      "[[ 0.01  0.03  0.2   0.44]\n",
      " [ 0.01  0.04  0.2   0.54]\n",
      " [ 0.01  0.05  0.2   0.57]\n",
      " [ 0.01  0.06  0.2   0.82]\n",
      " [ 0.01  0.    0.21  0.57]] -> [ 0.44]\n",
      "[[ 0.01  0.04  0.2   0.54]\n",
      " [ 0.01  0.05  0.2   0.57]\n",
      " [ 0.01  0.06  0.2   0.82]\n",
      " [ 0.01  0.    0.21  0.57]\n",
      " [ 0.01  0.01  0.21  0.44]] -> [ 0.56]\n",
      "[[ 0.01  0.05  0.2   0.57]\n",
      " [ 0.01  0.06  0.2   0.82]\n",
      " [ 0.01  0.    0.21  0.57]\n",
      " [ 0.01  0.01  0.21  0.44]\n",
      " [ 0.01  0.02  0.21  0.56]] -> [ 0.51]\n",
      "[[ 0.01  0.06  0.2   0.82]\n",
      " [ 0.01  0.    0.21  0.57]\n",
      " [ 0.01  0.01  0.21  0.44]\n",
      " [ 0.01  0.02  0.21  0.56]\n",
      " [ 0.01  0.03  0.21  0.51]] -> [ 0.55]\n",
      "[[ 0.01  0.    0.21  0.57]\n",
      " [ 0.01  0.01  0.21  0.44]\n",
      " [ 0.01  0.02  0.21  0.56]\n",
      " [ 0.01  0.03  0.21  0.51]\n",
      " [ 0.01  0.04  0.21  0.55]] -> [ 0.28]\n",
      "[[ 0.01  0.01  0.21  0.44]\n",
      " [ 0.01  0.02  0.21  0.56]\n",
      " [ 0.01  0.03  0.21  0.51]\n",
      " [ 0.01  0.04  0.21  0.55]\n",
      " [ 0.01  0.05  0.21  0.28]] -> [ 0.56]\n",
      "[[ 0.01  0.02  0.21  0.56]\n",
      " [ 0.01  0.03  0.21  0.51]\n",
      " [ 0.01  0.04  0.21  0.55]\n",
      " [ 0.01  0.05  0.21  0.28]\n",
      " [ 0.01  0.06  0.21  0.56]] -> [ 0.54]\n",
      "[[ 0.01  0.03  0.21  0.51]\n",
      " [ 0.01  0.04  0.21  0.55]\n",
      " [ 0.01  0.05  0.21  0.28]\n",
      " [ 0.01  0.06  0.21  0.56]\n",
      " [ 0.01  0.    0.22  0.54]] -> [ 0.36]\n",
      "[[ 0.01  0.04  0.21  0.55]\n",
      " [ 0.01  0.05  0.21  0.28]\n",
      " [ 0.01  0.06  0.21  0.56]\n",
      " [ 0.01  0.    0.22  0.54]\n",
      " [ 0.01  0.01  0.22  0.36]] -> [ 0.12]\n",
      "[[ 0.01  0.05  0.21  0.28]\n",
      " [ 0.01  0.06  0.21  0.56]\n",
      " [ 0.01  0.    0.22  0.54]\n",
      " [ 0.01  0.01  0.22  0.36]\n",
      " [ 0.02  0.02  0.22  0.12]] -> [ 0.25]\n",
      "[[ 0.01  0.06  0.21  0.56]\n",
      " [ 0.01  0.    0.22  0.54]\n",
      " [ 0.01  0.01  0.22  0.36]\n",
      " [ 0.02  0.02  0.22  0.12]\n",
      " [ 0.02  0.03  0.22  0.25]] -> [ 0.41]\n",
      "[[ 0.01  0.    0.22  0.54]\n",
      " [ 0.01  0.01  0.22  0.36]\n",
      " [ 0.02  0.02  0.22  0.12]\n",
      " [ 0.02  0.03  0.22  0.25]\n",
      " [ 0.02  0.04  0.22  0.41]] -> [ 0.11]\n",
      "[[ 0.01  0.01  0.22  0.36]\n",
      " [ 0.02  0.02  0.22  0.12]\n",
      " [ 0.02  0.03  0.22  0.25]\n",
      " [ 0.02  0.04  0.22  0.41]\n",
      " [ 0.02  0.05  0.22  0.11]] -> [ 0.06]\n",
      "[[ 0.02  0.02  0.22  0.12]\n",
      " [ 0.02  0.03  0.22  0.25]\n",
      " [ 0.02  0.04  0.22  0.41]\n",
      " [ 0.02  0.05  0.22  0.11]\n",
      " [ 0.02  0.06  0.22  0.06]] -> [ 0.09]\n",
      "[[ 0.02  0.03  0.22  0.25]\n",
      " [ 0.02  0.04  0.22  0.41]\n",
      " [ 0.02  0.05  0.22  0.11]\n",
      " [ 0.02  0.06  0.22  0.06]\n",
      " [ 0.02  0.    0.23  0.09]] -> [ 0.35]\n",
      "[[ 0.02  0.04  0.22  0.41]\n",
      " [ 0.02  0.05  0.22  0.11]\n",
      " [ 0.02  0.06  0.22  0.06]\n",
      " [ 0.02  0.    0.23  0.09]\n",
      " [ 0.02  0.01  0.23  0.35]] -> [ 0.29]\n",
      "[[ 0.02  0.05  0.22  0.11]\n",
      " [ 0.02  0.06  0.22  0.06]\n",
      " [ 0.02  0.    0.23  0.09]\n",
      " [ 0.02  0.01  0.23  0.35]\n",
      " [ 0.02  0.02  0.23  0.29]] -> [ 0.24]\n",
      "[[ 0.02  0.06  0.22  0.06]\n",
      " [ 0.02  0.    0.23  0.09]\n",
      " [ 0.02  0.01  0.23  0.35]\n",
      " [ 0.02  0.02  0.23  0.29]\n",
      " [ 0.02  0.03  0.23  0.24]] -> [ 0.23]\n",
      "[[ 0.02  0.    0.23  0.09]\n",
      " [ 0.02  0.01  0.23  0.35]\n",
      " [ 0.02  0.02  0.23  0.29]\n",
      " [ 0.02  0.03  0.23  0.24]\n",
      " [ 0.02  0.04  0.23  0.23]] -> [ 0.14]\n",
      "[[ 0.02  0.01  0.23  0.35]\n",
      " [ 0.02  0.02  0.23  0.29]\n",
      " [ 0.02  0.03  0.23  0.24]\n",
      " [ 0.02  0.04  0.23  0.23]\n",
      " [ 0.02  0.05  0.23  0.14]] -> [ 0.21]\n",
      "[[ 0.02  0.02  0.23  0.29]\n",
      " [ 0.02  0.03  0.23  0.24]\n",
      " [ 0.02  0.04  0.23  0.23]\n",
      " [ 0.02  0.05  0.23  0.14]\n",
      " [ 0.02  0.06  0.23  0.21]] -> [ 0.2]\n",
      "[[ 0.02  0.03  0.23  0.24]\n",
      " [ 0.02  0.04  0.23  0.23]\n",
      " [ 0.02  0.05  0.23  0.14]\n",
      " [ 0.02  0.06  0.23  0.21]\n",
      " [ 0.02  0.    0.24  0.2 ]] -> [ 0.25]\n",
      "[[ 0.02  0.04  0.23  0.23]\n",
      " [ 0.02  0.05  0.23  0.14]\n",
      " [ 0.02  0.06  0.23  0.21]\n",
      " [ 0.02  0.    0.24  0.2 ]\n",
      " [ 0.02  0.01  0.24  0.25]] -> [ 0.23]\n",
      "[[ 0.02  0.05  0.23  0.14]\n",
      " [ 0.02  0.06  0.23  0.21]\n",
      " [ 0.02  0.    0.24  0.2 ]\n",
      " [ 0.02  0.01  0.24  0.25]\n",
      " [ 0.02  0.02  0.24  0.23]] -> [ 0.27]\n",
      "[[ 0.02  0.06  0.23  0.21]\n",
      " [ 0.02  0.    0.24  0.2 ]\n",
      " [ 0.02  0.01  0.24  0.25]\n",
      " [ 0.02  0.02  0.24  0.23]\n",
      " [ 0.02  0.03  0.24  0.27]] -> [ 0.31]\n",
      "[[ 0.02  0.    0.24  0.2 ]\n",
      " [ 0.02  0.01  0.24  0.25]\n",
      " [ 0.02  0.02  0.24  0.23]\n",
      " [ 0.02  0.03  0.24  0.27]\n",
      " [ 0.02  0.04  0.24  0.31]] -> [ 0.16]\n",
      "[[ 0.02  0.01  0.24  0.25]\n",
      " [ 0.02  0.02  0.24  0.23]\n",
      " [ 0.02  0.03  0.24  0.27]\n",
      " [ 0.02  0.04  0.24  0.31]\n",
      " [ 0.02  0.05  0.24  0.16]] -> [ 0.14]\n",
      "[[ 0.02  0.02  0.24  0.23]\n",
      " [ 0.02  0.03  0.24  0.27]\n",
      " [ 0.02  0.04  0.24  0.31]\n",
      " [ 0.02  0.05  0.24  0.16]\n",
      " [ 0.02  0.06  0.24  0.14]] -> [ 0.3]\n",
      "[[ 0.02  0.03  0.24  0.27]\n",
      " [ 0.02  0.04  0.24  0.31]\n",
      " [ 0.02  0.05  0.24  0.16]\n",
      " [ 0.02  0.06  0.24  0.14]\n",
      " [ 0.02  0.    0.25  0.3 ]] -> [ 0.32]\n",
      "[[ 0.02  0.04  0.24  0.31]\n",
      " [ 0.02  0.05  0.24  0.16]\n",
      " [ 0.02  0.06  0.24  0.14]\n",
      " [ 0.02  0.    0.25  0.3 ]\n",
      " [ 0.02  0.01  0.25  0.32]] -> [ 0.75]\n",
      "[[ 0.02  0.05  0.24  0.16]\n",
      " [ 0.02  0.06  0.24  0.14]\n",
      " [ 0.02  0.    0.25  0.3 ]\n",
      " [ 0.02  0.01  0.25  0.32]\n",
      " [ 0.02  0.02  0.25  0.75]] -> [ 0.35]\n",
      "[[ 0.02  0.06  0.24  0.14]\n",
      " [ 0.02  0.    0.25  0.3 ]\n",
      " [ 0.02  0.01  0.25  0.32]\n",
      " [ 0.02  0.02  0.25  0.75]\n",
      " [ 0.02  0.03  0.25  0.35]] -> [ 0.26]\n",
      "[[ 0.02  0.    0.25  0.3 ]\n",
      " [ 0.02  0.01  0.25  0.32]\n",
      " [ 0.02  0.02  0.25  0.75]\n",
      " [ 0.02  0.03  0.25  0.35]\n",
      " [ 0.02  0.04  0.25  0.26]] -> [ 0.12]\n",
      "[[ 0.02  0.01  0.25  0.32]\n",
      " [ 0.02  0.02  0.25  0.75]\n",
      " [ 0.02  0.03  0.25  0.35]\n",
      " [ 0.02  0.04  0.25  0.26]\n",
      " [ 0.02  0.05  0.25  0.12]] -> [ 0.21]\n",
      "[[ 0.02  0.02  0.25  0.75]\n",
      " [ 0.02  0.03  0.25  0.35]\n",
      " [ 0.02  0.04  0.25  0.26]\n",
      " [ 0.02  0.05  0.25  0.12]\n",
      " [ 0.02  0.06  0.25  0.21]] -> [ 0.23]\n",
      "[[ 0.02  0.03  0.25  0.35]\n",
      " [ 0.02  0.04  0.25  0.26]\n",
      " [ 0.02  0.05  0.25  0.12]\n",
      " [ 0.02  0.06  0.25  0.21]\n",
      " [ 0.02  0.    0.26  0.23]] -> [ 0.28]\n",
      "[[ 0.02  0.04  0.25  0.26]\n",
      " [ 0.02  0.05  0.25  0.12]\n",
      " [ 0.02  0.06  0.25  0.21]\n",
      " [ 0.02  0.    0.26  0.23]\n",
      " [ 0.02  0.01  0.26  0.28]] -> [ 0.25]\n",
      "[[ 0.02  0.05  0.25  0.12]\n",
      " [ 0.02  0.06  0.25  0.21]\n",
      " [ 0.02  0.    0.26  0.23]\n",
      " [ 0.02  0.01  0.26  0.28]\n",
      " [ 0.02  0.02  0.26  0.25]] -> [ 0.31]\n",
      "[[ 0.02  0.06  0.25  0.21]\n",
      " [ 0.02  0.    0.26  0.23]\n",
      " [ 0.02  0.01  0.26  0.28]\n",
      " [ 0.02  0.02  0.26  0.25]\n",
      " [ 0.02  0.03  0.26  0.31]] -> [ 0.21]\n",
      "[[ 0.02  0.    0.26  0.23]\n",
      " [ 0.02  0.01  0.26  0.28]\n",
      " [ 0.02  0.02  0.26  0.25]\n",
      " [ 0.02  0.03  0.26  0.31]\n",
      " [ 0.02  0.04  0.26  0.21]] -> [ 0.17]\n",
      "[[ 0.02  0.01  0.26  0.28]\n",
      " [ 0.02  0.02  0.26  0.25]\n",
      " [ 0.02  0.03  0.26  0.31]\n",
      " [ 0.02  0.04  0.26  0.21]\n",
      " [ 0.02  0.05  0.26  0.17]] -> [ 0.1]\n",
      "[[ 0.02  0.02  0.26  0.25]\n",
      " [ 0.02  0.03  0.26  0.31]\n",
      " [ 0.02  0.04  0.26  0.21]\n",
      " [ 0.02  0.05  0.26  0.17]\n",
      " [ 0.02  0.06  0.26  0.1 ]] -> [ 0.29]\n",
      "[[ 0.02  0.03  0.26  0.31]\n",
      " [ 0.02  0.04  0.26  0.21]\n",
      " [ 0.02  0.05  0.26  0.17]\n",
      " [ 0.02  0.06  0.26  0.1 ]\n",
      " [ 0.02  0.    0.27  0.29]] -> [ 0.34]\n",
      "[[ 0.02  0.04  0.26  0.21]\n",
      " [ 0.02  0.05  0.26  0.17]\n",
      " [ 0.02  0.06  0.26  0.1 ]\n",
      " [ 0.02  0.    0.27  0.29]\n",
      " [ 0.02  0.01  0.27  0.34]] -> [ 0.28]\n",
      "[[ 0.02  0.05  0.26  0.17]\n",
      " [ 0.02  0.06  0.26  0.1 ]\n",
      " [ 0.02  0.    0.27  0.29]\n",
      " [ 0.02  0.01  0.27  0.34]\n",
      " [ 0.02  0.02  0.27  0.28]] -> [ 0.2]\n",
      "[[ 0.02  0.06  0.26  0.1 ]\n",
      " [ 0.02  0.    0.27  0.29]\n",
      " [ 0.02  0.01  0.27  0.34]\n",
      " [ 0.02  0.02  0.27  0.28]\n",
      " [ 0.02  0.03  0.27  0.2 ]] -> [ 0.36]\n",
      "[[ 0.02  0.    0.27  0.29]\n",
      " [ 0.02  0.01  0.27  0.34]\n",
      " [ 0.02  0.02  0.27  0.28]\n",
      " [ 0.02  0.03  0.27  0.2 ]\n",
      " [ 0.02  0.04  0.27  0.36]] -> [ 0.23]\n",
      "[[ 0.02  0.01  0.27  0.34]\n",
      " [ 0.02  0.02  0.27  0.28]\n",
      " [ 0.02  0.03  0.27  0.2 ]\n",
      " [ 0.02  0.04  0.27  0.36]\n",
      " [ 0.02  0.05  0.27  0.23]] -> [ 0.15]\n",
      "[[ 0.02  0.02  0.27  0.28]\n",
      " [ 0.02  0.03  0.27  0.2 ]\n",
      " [ 0.02  0.04  0.27  0.36]\n",
      " [ 0.02  0.05  0.27  0.23]\n",
      " [ 0.02  0.06  0.27  0.15]] -> [ 0.42]\n",
      "[[ 0.02  0.03  0.27  0.2 ]\n",
      " [ 0.02  0.04  0.27  0.36]\n",
      " [ 0.02  0.05  0.27  0.23]\n",
      " [ 0.02  0.06  0.27  0.15]\n",
      " [ 0.02  0.    0.28  0.42]] -> [ 0.28]\n",
      "[[ 0.02  0.04  0.27  0.36]\n",
      " [ 0.02  0.05  0.27  0.23]\n",
      " [ 0.02  0.06  0.27  0.15]\n",
      " [ 0.02  0.    0.28  0.42]\n",
      " [ 0.02  0.01  0.28  0.28]] -> [ 0.24]\n",
      "[[ 0.02  0.05  0.27  0.23]\n",
      " [ 0.02  0.06  0.27  0.15]\n",
      " [ 0.02  0.    0.28  0.42]\n",
      " [ 0.02  0.01  0.28  0.28]\n",
      " [ 0.02  0.02  0.28  0.24]] -> [ 0.29]\n",
      "[[ 0.02  0.06  0.27  0.15]\n",
      " [ 0.02  0.    0.28  0.42]\n",
      " [ 0.02  0.01  0.28  0.28]\n",
      " [ 0.02  0.02  0.28  0.24]\n",
      " [ 0.02  0.03  0.28  0.29]] -> [ 0.2]\n",
      "[[ 0.02  0.    0.28  0.42]\n",
      " [ 0.02  0.01  0.28  0.28]\n",
      " [ 0.02  0.02  0.28  0.24]\n",
      " [ 0.02  0.03  0.28  0.29]\n",
      " [ 0.02  0.04  0.28  0.2 ]] -> [ 0.14]\n",
      "[[ 0.02  0.01  0.28  0.28]\n",
      " [ 0.02  0.02  0.28  0.24]\n",
      " [ 0.02  0.03  0.28  0.29]\n",
      " [ 0.02  0.04  0.28  0.2 ]\n",
      " [ 0.02  0.05  0.28  0.14]] -> [ 0.18]\n",
      "[[ 0.02  0.02  0.28  0.24]\n",
      " [ 0.02  0.03  0.28  0.29]\n",
      " [ 0.02  0.04  0.28  0.2 ]\n",
      " [ 0.02  0.05  0.28  0.14]\n",
      " [ 0.02  0.06  0.28  0.18]] -> [ 0.27]\n",
      "[[ 0.02  0.03  0.28  0.29]\n",
      " [ 0.02  0.04  0.28  0.2 ]\n",
      " [ 0.02  0.05  0.28  0.14]\n",
      " [ 0.02  0.06  0.28  0.18]\n",
      " [ 0.02  0.    0.29  0.27]] -> [ 0.22]\n",
      "[[ 0.02  0.04  0.28  0.2 ]\n",
      " [ 0.02  0.05  0.28  0.14]\n",
      " [ 0.02  0.06  0.28  0.18]\n",
      " [ 0.02  0.    0.29  0.27]\n",
      " [ 0.02  0.01  0.29  0.22]] -> [ 0.31]\n",
      "[[ 0.02  0.05  0.28  0.14]\n",
      " [ 0.02  0.06  0.28  0.18]\n",
      " [ 0.02  0.    0.29  0.27]\n",
      " [ 0.02  0.01  0.29  0.22]\n",
      " [ 0.02  0.02  0.29  0.31]] -> [ 0.24]\n",
      "[[ 0.02  0.06  0.28  0.18]\n",
      " [ 0.02  0.    0.29  0.27]\n",
      " [ 0.02  0.01  0.29  0.22]\n",
      " [ 0.02  0.02  0.29  0.31]\n",
      " [ 0.02  0.03  0.29  0.24]] -> [ 0.39]\n",
      "[[ 0.02  0.    0.29  0.27]\n",
      " [ 0.02  0.01  0.29  0.22]\n",
      " [ 0.02  0.02  0.29  0.31]\n",
      " [ 0.02  0.03  0.29  0.24]\n",
      " [ 0.02  0.04  0.29  0.39]] -> [ 0.31]\n",
      "[[ 0.02  0.01  0.29  0.22]\n",
      " [ 0.02  0.02  0.29  0.31]\n",
      " [ 0.02  0.03  0.29  0.24]\n",
      " [ 0.02  0.04  0.29  0.39]\n",
      " [ 0.02  0.05  0.29  0.31]] -> [ 0.17]\n",
      "[[ 0.02  0.02  0.29  0.31]\n",
      " [ 0.02  0.03  0.29  0.24]\n",
      " [ 0.02  0.04  0.29  0.39]\n",
      " [ 0.02  0.05  0.29  0.31]\n",
      " [ 0.02  0.06  0.29  0.17]] -> [ 0.42]\n",
      "[[ 0.02  0.03  0.29  0.24]\n",
      " [ 0.02  0.04  0.29  0.39]\n",
      " [ 0.02  0.05  0.29  0.31]\n",
      " [ 0.02  0.06  0.29  0.17]\n",
      " [ 0.02  0.    0.3   0.42]] -> [ 0.29]\n",
      "[[ 0.02  0.04  0.29  0.39]\n",
      " [ 0.02  0.05  0.29  0.31]\n",
      " [ 0.02  0.06  0.29  0.17]\n",
      " [ 0.02  0.    0.3   0.42]\n",
      " [ 0.02  0.01  0.3   0.29]] -> [ 0.33]\n",
      "[[ 0.02  0.05  0.29  0.31]\n",
      " [ 0.02  0.06  0.29  0.17]\n",
      " [ 0.02  0.    0.3   0.42]\n",
      " [ 0.02  0.01  0.3   0.29]\n",
      " [ 0.02  0.02  0.3   0.33]] -> [ 0.22]\n",
      "[[ 0.02  0.06  0.29  0.17]\n",
      " [ 0.02  0.    0.3   0.42]\n",
      " [ 0.02  0.01  0.3   0.29]\n",
      " [ 0.02  0.02  0.3   0.33]\n",
      " [ 0.02  0.03  0.3   0.22]] -> [ 0.39]\n",
      "[[ 0.02  0.    0.3   0.42]\n",
      " [ 0.02  0.01  0.3   0.29]\n",
      " [ 0.02  0.02  0.3   0.33]\n",
      " [ 0.02  0.03  0.3   0.22]\n",
      " [ 0.02  0.04  0.3   0.39]] -> [ 0.25]\n",
      "[[ 0.02  0.01  0.3   0.29]\n",
      " [ 0.02  0.02  0.3   0.33]\n",
      " [ 0.02  0.03  0.3   0.22]\n",
      " [ 0.02  0.04  0.3   0.39]\n",
      " [ 0.02  0.05  0.3   0.25]] -> [ 0.23]\n",
      "[[ 0.02  0.02  0.3   0.33]\n",
      " [ 0.02  0.03  0.3   0.22]\n",
      " [ 0.02  0.04  0.3   0.39]\n",
      " [ 0.02  0.05  0.3   0.25]\n",
      " [ 0.02  0.06  0.3   0.23]] -> [ 0.25]\n",
      "[[ 0.02  0.03  0.3   0.22]\n",
      " [ 0.02  0.04  0.3   0.39]\n",
      " [ 0.02  0.05  0.3   0.25]\n",
      " [ 0.02  0.06  0.3   0.23]\n",
      " [ 0.02  0.    0.31  0.25]] -> [ 0.21]\n",
      "[[ 0.02  0.04  0.3   0.39]\n",
      " [ 0.02  0.05  0.3   0.25]\n",
      " [ 0.02  0.06  0.3   0.23]\n",
      " [ 0.02  0.    0.31  0.25]\n",
      " [ 0.02  0.01  0.31  0.21]] -> [ 0.18]\n",
      "[[ 0.02  0.05  0.3   0.25]\n",
      " [ 0.02  0.06  0.3   0.23]\n",
      " [ 0.02  0.    0.31  0.25]\n",
      " [ 0.02  0.01  0.31  0.21]\n",
      " [ 0.02  0.02  0.31  0.18]] -> [ 0.24]\n",
      "[[ 0.02  0.06  0.3   0.23]\n",
      " [ 0.02  0.    0.31  0.25]\n",
      " [ 0.02  0.01  0.31  0.21]\n",
      " [ 0.02  0.02  0.31  0.18]\n",
      " [ 0.02  0.03  0.31  0.24]] -> [ 0.24]\n",
      "[[ 0.02  0.    0.31  0.25]\n",
      " [ 0.02  0.01  0.31  0.21]\n",
      " [ 0.02  0.02  0.31  0.18]\n",
      " [ 0.02  0.03  0.31  0.24]\n",
      " [ 0.02  0.04  0.31  0.24]] -> [ 0.2]\n",
      "[[ 0.02  0.01  0.31  0.21]\n",
      " [ 0.02  0.02  0.31  0.18]\n",
      " [ 0.02  0.03  0.31  0.24]\n",
      " [ 0.02  0.04  0.31  0.24]\n",
      " [ 0.02  0.05  0.31  0.2 ]] -> [ 0.09]\n",
      "[[ 0.02  0.02  0.31  0.18]\n",
      " [ 0.02  0.03  0.31  0.24]\n",
      " [ 0.02  0.04  0.31  0.24]\n",
      " [ 0.02  0.05  0.31  0.2 ]\n",
      " [ 0.02  0.06  0.31  0.09]] -> [ 0.31]\n",
      "[[ 0.02  0.03  0.31  0.24]\n",
      " [ 0.02  0.04  0.31  0.24]\n",
      " [ 0.02  0.05  0.31  0.2 ]\n",
      " [ 0.02  0.06  0.31  0.09]\n",
      " [ 0.02  0.    0.32  0.31]] -> [ 0.33]\n",
      "[[ 0.02  0.04  0.31  0.24]\n",
      " [ 0.02  0.05  0.31  0.2 ]\n",
      " [ 0.02  0.06  0.31  0.09]\n",
      " [ 0.02  0.    0.32  0.31]\n",
      " [ 0.02  0.01  0.32  0.33]] -> [ 0.33]\n",
      "[[ 0.02  0.05  0.31  0.2 ]\n",
      " [ 0.02  0.06  0.31  0.09]\n",
      " [ 0.02  0.    0.32  0.31]\n",
      " [ 0.02  0.01  0.32  0.33]\n",
      " [ 0.02  0.02  0.32  0.33]] -> [ 0.31]\n",
      "[[ 0.02  0.06  0.31  0.09]\n",
      " [ 0.02  0.    0.32  0.31]\n",
      " [ 0.02  0.01  0.32  0.33]\n",
      " [ 0.02  0.02  0.32  0.33]\n",
      " [ 0.02  0.03  0.32  0.31]] -> [ 0.27]\n",
      "[[ 0.02  0.    0.32  0.31]\n",
      " [ 0.02  0.01  0.32  0.33]\n",
      " [ 0.02  0.02  0.32  0.33]\n",
      " [ 0.02  0.03  0.32  0.31]\n",
      " [ 0.02  0.04  0.32  0.27]] -> [ 0.35]\n",
      "[[ 0.02  0.01  0.32  0.33]\n",
      " [ 0.02  0.02  0.32  0.33]\n",
      " [ 0.02  0.03  0.32  0.31]\n",
      " [ 0.02  0.04  0.32  0.27]\n",
      " [ 0.02  0.05  0.32  0.35]] -> [ 0.15]\n",
      "[[ 0.02  0.02  0.32  0.33]\n",
      " [ 0.02  0.03  0.32  0.31]\n",
      " [ 0.02  0.04  0.32  0.27]\n",
      " [ 0.02  0.05  0.32  0.35]\n",
      " [ 0.02  0.06  0.32  0.15]] -> [ 0.25]\n",
      "[[ 0.02  0.03  0.32  0.31]\n",
      " [ 0.02  0.04  0.32  0.27]\n",
      " [ 0.02  0.05  0.32  0.35]\n",
      " [ 0.02  0.06  0.32  0.15]\n",
      " [ 0.02  0.    0.33  0.25]] -> [ 0.32]\n",
      "[[ 0.02  0.04  0.32  0.27]\n",
      " [ 0.02  0.05  0.32  0.35]\n",
      " [ 0.02  0.06  0.32  0.15]\n",
      " [ 0.02  0.    0.33  0.25]\n",
      " [ 0.02  0.01  0.33  0.32]] -> [ 0.29]\n",
      "[[ 0.02  0.05  0.32  0.35]\n",
      " [ 0.02  0.06  0.32  0.15]\n",
      " [ 0.02  0.    0.33  0.25]\n",
      " [ 0.02  0.01  0.33  0.32]\n",
      " [ 0.02  0.02  0.33  0.29]] -> [ 0.32]\n",
      "[[ 0.02  0.06  0.32  0.15]\n",
      " [ 0.02  0.    0.33  0.25]\n",
      " [ 0.02  0.01  0.33  0.32]\n",
      " [ 0.02  0.02  0.33  0.29]\n",
      " [ 0.02  0.03  0.33  0.32]] -> [ 0.21]\n",
      "[[ 0.02  0.    0.33  0.25]\n",
      " [ 0.02  0.01  0.33  0.32]\n",
      " [ 0.02  0.02  0.33  0.29]\n",
      " [ 0.02  0.03  0.33  0.32]\n",
      " [ 0.02  0.04  0.33  0.21]] -> [ 0.92]\n",
      "[[ 0.02  0.01  0.33  0.32]\n",
      " [ 0.02  0.02  0.33  0.29]\n",
      " [ 0.02  0.03  0.33  0.32]\n",
      " [ 0.02  0.04  0.33  0.21]\n",
      " [ 0.02  0.05  0.33  0.92]] -> [ 0.19]\n",
      "[[ 0.02  0.02  0.33  0.29]\n",
      " [ 0.02  0.03  0.33  0.32]\n",
      " [ 0.02  0.04  0.33  0.21]\n",
      " [ 0.02  0.05  0.33  0.92]\n",
      " [ 0.02  0.06  0.33  0.19]] -> [ 0.37]\n",
      "[[ 0.02  0.03  0.33  0.32]\n",
      " [ 0.02  0.04  0.33  0.21]\n",
      " [ 0.02  0.05  0.33  0.92]\n",
      " [ 0.02  0.06  0.33  0.19]\n",
      " [ 0.02  0.    0.34  0.37]] -> [ 0.29]\n",
      "[[ 0.02  0.04  0.33  0.21]\n",
      " [ 0.02  0.05  0.33  0.92]\n",
      " [ 0.02  0.06  0.33  0.19]\n",
      " [ 0.02  0.    0.34  0.37]\n",
      " [ 0.02  0.01  0.34  0.29]] -> [ 0.33]\n",
      "[[ 0.02  0.05  0.33  0.92]\n",
      " [ 0.02  0.06  0.33  0.19]\n",
      " [ 0.02  0.    0.34  0.37]\n",
      " [ 0.02  0.01  0.34  0.29]\n",
      " [ 0.02  0.02  0.34  0.33]] -> [ 0.16]\n",
      "[[ 0.02  0.06  0.33  0.19]\n",
      " [ 0.02  0.    0.34  0.37]\n",
      " [ 0.02  0.01  0.34  0.29]\n",
      " [ 0.02  0.02  0.34  0.33]\n",
      " [ 0.02  0.03  0.34  0.16]] -> [ 0.22]\n",
      "[[ 0.02  0.    0.34  0.37]\n",
      " [ 0.02  0.01  0.34  0.29]\n",
      " [ 0.02  0.02  0.34  0.33]\n",
      " [ 0.02  0.03  0.34  0.16]\n",
      " [ 0.02  0.04  0.34  0.22]] -> [ 0.25]\n",
      "[[ 0.02  0.01  0.34  0.29]\n",
      " [ 0.02  0.02  0.34  0.33]\n",
      " [ 0.02  0.03  0.34  0.16]\n",
      " [ 0.02  0.04  0.34  0.22]\n",
      " [ 0.02  0.05  0.34  0.25]] -> [ 0.21]\n",
      "[[ 0.02  0.02  0.34  0.33]\n",
      " [ 0.02  0.03  0.34  0.16]\n",
      " [ 0.02  0.04  0.34  0.22]\n",
      " [ 0.02  0.05  0.34  0.25]\n",
      " [ 0.02  0.06  0.34  0.21]] -> [ 0.46]\n",
      "[[ 0.02  0.03  0.34  0.16]\n",
      " [ 0.02  0.04  0.34  0.22]\n",
      " [ 0.02  0.05  0.34  0.25]\n",
      " [ 0.02  0.06  0.34  0.21]\n",
      " [ 0.02  0.    0.35  0.46]] -> [ 0.34]\n",
      "[[ 0.02  0.04  0.34  0.22]\n",
      " [ 0.02  0.05  0.34  0.25]\n",
      " [ 0.02  0.06  0.34  0.21]\n",
      " [ 0.02  0.    0.35  0.46]\n",
      " [ 0.02  0.01  0.35  0.34]] -> [ 0.31]\n",
      "[[ 0.02  0.05  0.34  0.25]\n",
      " [ 0.02  0.06  0.34  0.21]\n",
      " [ 0.02  0.    0.35  0.46]\n",
      " [ 0.02  0.01  0.35  0.34]\n",
      " [ 0.02  0.02  0.35  0.31]] -> [ 0.22]\n",
      "[[ 0.02  0.06  0.34  0.21]\n",
      " [ 0.02  0.    0.35  0.46]\n",
      " [ 0.02  0.01  0.35  0.34]\n",
      " [ 0.02  0.02  0.35  0.31]\n",
      " [ 0.03  0.03  0.35  0.22]] -> [ 0.3]\n",
      "[[ 0.02  0.    0.35  0.46]\n",
      " [ 0.02  0.01  0.35  0.34]\n",
      " [ 0.02  0.02  0.35  0.31]\n",
      " [ 0.03  0.03  0.35  0.22]\n",
      " [ 0.03  0.04  0.35  0.3 ]] -> [ 0.28]\n",
      "[[ 0.02  0.01  0.35  0.34]\n",
      " [ 0.02  0.02  0.35  0.31]\n",
      " [ 0.03  0.03  0.35  0.22]\n",
      " [ 0.03  0.04  0.35  0.3 ]\n",
      " [ 0.03  0.05  0.35  0.28]] -> [ 0.14]\n",
      "[[ 0.02  0.02  0.35  0.31]\n",
      " [ 0.03  0.03  0.35  0.22]\n",
      " [ 0.03  0.04  0.35  0.3 ]\n",
      " [ 0.03  0.05  0.35  0.28]\n",
      " [ 0.03  0.06  0.35  0.14]] -> [ 0.25]\n",
      "[[ 0.03  0.03  0.35  0.22]\n",
      " [ 0.03  0.04  0.35  0.3 ]\n",
      " [ 0.03  0.05  0.35  0.28]\n",
      " [ 0.03  0.06  0.35  0.14]\n",
      " [ 0.03  0.    0.36  0.25]] -> [ 0.33]\n",
      "[[ 0.03  0.04  0.35  0.3 ]\n",
      " [ 0.03  0.05  0.35  0.28]\n",
      " [ 0.03  0.06  0.35  0.14]\n",
      " [ 0.03  0.    0.36  0.25]\n",
      " [ 0.03  0.01  0.36  0.33]] -> [ 0.28]\n",
      "[[ 0.03  0.05  0.35  0.28]\n",
      " [ 0.03  0.06  0.35  0.14]\n",
      " [ 0.03  0.    0.36  0.25]\n",
      " [ 0.03  0.01  0.36  0.33]\n",
      " [ 0.03  0.02  0.36  0.28]] -> [ 0.24]\n",
      "[[ 0.03  0.06  0.35  0.14]\n",
      " [ 0.03  0.    0.36  0.25]\n",
      " [ 0.03  0.01  0.36  0.33]\n",
      " [ 0.03  0.02  0.36  0.28]\n",
      " [ 0.03  0.03  0.36  0.24]] -> [ 0.42]\n",
      "[[ 0.03  0.    0.36  0.25]\n",
      " [ 0.03  0.01  0.36  0.33]\n",
      " [ 0.03  0.02  0.36  0.28]\n",
      " [ 0.03  0.03  0.36  0.24]\n",
      " [ 0.03  0.04  0.36  0.42]] -> [ 0.26]\n",
      "[[ 0.03  0.01  0.36  0.33]\n",
      " [ 0.03  0.02  0.36  0.28]\n",
      " [ 0.03  0.03  0.36  0.24]\n",
      " [ 0.03  0.04  0.36  0.42]\n",
      " [ 0.03  0.05  0.36  0.26]] -> [ 0.19]\n",
      "[[ 0.03  0.02  0.36  0.28]\n",
      " [ 0.03  0.03  0.36  0.24]\n",
      " [ 0.03  0.04  0.36  0.42]\n",
      " [ 0.03  0.05  0.36  0.26]\n",
      " [ 0.03  0.06  0.36  0.19]] -> [ 0.41]\n",
      "[[ 0.03  0.03  0.36  0.24]\n",
      " [ 0.03  0.04  0.36  0.42]\n",
      " [ 0.03  0.05  0.36  0.26]\n",
      " [ 0.03  0.06  0.36  0.19]\n",
      " [ 0.03  0.    0.37  0.41]] -> [ 0.34]\n",
      "[[ 0.03  0.04  0.36  0.42]\n",
      " [ 0.03  0.05  0.36  0.26]\n",
      " [ 0.03  0.06  0.36  0.19]\n",
      " [ 0.03  0.    0.37  0.41]\n",
      " [ 0.03  0.01  0.37  0.34]] -> [ 0.08]\n",
      "[[ 0.03  0.05  0.36  0.26]\n",
      " [ 0.03  0.06  0.36  0.19]\n",
      " [ 0.03  0.    0.37  0.41]\n",
      " [ 0.03  0.01  0.37  0.34]\n",
      " [ 0.03  0.02  0.37  0.08]] -> [ 0.]\n",
      "[[ 0.03  0.06  0.36  0.19]\n",
      " [ 0.03  0.    0.37  0.41]\n",
      " [ 0.03  0.01  0.37  0.34]\n",
      " [ 0.03  0.02  0.37  0.08]\n",
      " [ 0.03  0.03  0.37  0.  ]] -> [ 0.]\n",
      "[[ 0.03  0.    0.37  0.41]\n",
      " [ 0.03  0.01  0.37  0.34]\n",
      " [ 0.03  0.02  0.37  0.08]\n",
      " [ 0.03  0.03  0.37  0.  ]\n",
      " [ 0.03  0.04  0.37  0.  ]] -> [ 0.11]\n",
      "[[ 0.03  0.01  0.37  0.34]\n",
      " [ 0.03  0.02  0.37  0.08]\n",
      " [ 0.03  0.03  0.37  0.  ]\n",
      " [ 0.03  0.04  0.37  0.  ]\n",
      " [ 0.03  0.05  0.37  0.11]] -> [ 0.16]\n",
      "[[ 0.03  0.02  0.37  0.08]\n",
      " [ 0.03  0.03  0.37  0.  ]\n",
      " [ 0.03  0.04  0.37  0.  ]\n",
      " [ 0.03  0.05  0.37  0.11]\n",
      " [ 0.03  0.06  0.37  0.16]] -> [ 0.27]\n",
      "[[ 0.03  0.03  0.37  0.  ]\n",
      " [ 0.03  0.04  0.37  0.  ]\n",
      " [ 0.03  0.05  0.37  0.11]\n",
      " [ 0.03  0.06  0.37  0.16]\n",
      " [ 0.03  0.    0.38  0.27]] -> [ 0.27]\n",
      "[[ 0.03  0.04  0.37  0.  ]\n",
      " [ 0.03  0.05  0.37  0.11]\n",
      " [ 0.03  0.06  0.37  0.16]\n",
      " [ 0.03  0.    0.38  0.27]\n",
      " [ 0.03  0.01  0.38  0.27]] -> [ 0.21]\n",
      "[[ 0.03  0.05  0.37  0.11]\n",
      " [ 0.03  0.06  0.37  0.16]\n",
      " [ 0.03  0.    0.38  0.27]\n",
      " [ 0.03  0.01  0.38  0.27]\n",
      " [ 0.03  0.02  0.38  0.21]] -> [ 0.37]\n",
      "[[ 0.03  0.06  0.37  0.16]\n",
      " [ 0.03  0.    0.38  0.27]\n",
      " [ 0.03  0.01  0.38  0.27]\n",
      " [ 0.03  0.02  0.38  0.21]\n",
      " [ 0.03  0.03  0.38  0.37]] -> [ 0.41]\n",
      "[[ 0.03  0.    0.38  0.27]\n",
      " [ 0.03  0.01  0.38  0.27]\n",
      " [ 0.03  0.02  0.38  0.21]\n",
      " [ 0.03  0.03  0.38  0.37]\n",
      " [ 0.03  0.04  0.38  0.41]] -> [ 0.23]\n",
      "[[ 0.03  0.01  0.38  0.27]\n",
      " [ 0.03  0.02  0.38  0.21]\n",
      " [ 0.03  0.03  0.38  0.37]\n",
      " [ 0.03  0.04  0.38  0.41]\n",
      " [ 0.03  0.05  0.38  0.23]] -> [ 0.3]\n",
      "[[ 0.03  0.02  0.38  0.21]\n",
      " [ 0.03  0.03  0.38  0.37]\n",
      " [ 0.03  0.04  0.38  0.41]\n",
      " [ 0.03  0.05  0.38  0.23]\n",
      " [ 0.03  0.06  0.38  0.3 ]] -> [ 0.25]\n",
      "[[ 0.03  0.03  0.38  0.37]\n",
      " [ 0.03  0.04  0.38  0.41]\n",
      " [ 0.03  0.05  0.38  0.23]\n",
      " [ 0.03  0.06  0.38  0.3 ]\n",
      " [ 0.03  0.    0.39  0.25]] -> [ 0.22]\n",
      "[[ 0.03  0.04  0.38  0.41]\n",
      " [ 0.03  0.05  0.38  0.23]\n",
      " [ 0.03  0.06  0.38  0.3 ]\n",
      " [ 0.03  0.    0.39  0.25]\n",
      " [ 0.03  0.01  0.39  0.22]] -> [ 0.26]\n",
      "[[ 0.03  0.05  0.38  0.23]\n",
      " [ 0.03  0.06  0.38  0.3 ]\n",
      " [ 0.03  0.    0.39  0.25]\n",
      " [ 0.03  0.01  0.39  0.22]\n",
      " [ 0.03  0.02  0.39  0.26]] -> [ 0.29]\n",
      "[[ 0.03  0.06  0.38  0.3 ]\n",
      " [ 0.03  0.    0.39  0.25]\n",
      " [ 0.03  0.01  0.39  0.22]\n",
      " [ 0.03  0.02  0.39  0.26]\n",
      " [ 0.03  0.03  0.39  0.29]] -> [ 0.31]\n",
      "[[ 0.03  0.    0.39  0.25]\n",
      " [ 0.03  0.01  0.39  0.22]\n",
      " [ 0.03  0.02  0.39  0.26]\n",
      " [ 0.03  0.03  0.39  0.29]\n",
      " [ 0.03  0.04  0.39  0.31]] -> [ 0.29]\n",
      "[[ 0.03  0.01  0.39  0.22]\n",
      " [ 0.03  0.02  0.39  0.26]\n",
      " [ 0.03  0.03  0.39  0.29]\n",
      " [ 0.03  0.04  0.39  0.31]\n",
      " [ 0.03  0.05  0.39  0.29]] -> [ 0.12]\n",
      "[[ 0.03  0.02  0.39  0.26]\n",
      " [ 0.03  0.03  0.39  0.29]\n",
      " [ 0.03  0.04  0.39  0.31]\n",
      " [ 0.03  0.05  0.39  0.29]\n",
      " [ 0.03  0.06  0.39  0.12]] -> [ 0.15]\n",
      "[[ 0.03  0.03  0.39  0.29]\n",
      " [ 0.03  0.04  0.39  0.31]\n",
      " [ 0.03  0.05  0.39  0.29]\n",
      " [ 0.03  0.06  0.39  0.12]\n",
      " [ 0.03  0.    0.4   0.15]] -> [ 0.48]\n",
      "[[ 0.03  0.04  0.39  0.31]\n",
      " [ 0.03  0.05  0.39  0.29]\n",
      " [ 0.03  0.06  0.39  0.12]\n",
      " [ 0.03  0.    0.4   0.15]\n",
      " [ 0.03  0.01  0.4   0.48]] -> [ 0.18]\n",
      "[[ 0.03  0.05  0.39  0.29]\n",
      " [ 0.03  0.06  0.39  0.12]\n",
      " [ 0.03  0.    0.4   0.15]\n",
      " [ 0.03  0.01  0.4   0.48]\n",
      " [ 0.03  0.02  0.4   0.18]] -> [ 0.25]\n",
      "[[ 0.03  0.06  0.39  0.12]\n",
      " [ 0.03  0.    0.4   0.15]\n",
      " [ 0.03  0.01  0.4   0.48]\n",
      " [ 0.03  0.02  0.4   0.18]\n",
      " [ 0.03  0.03  0.4   0.25]] -> [ 0.27]\n",
      "[[ 0.03  0.    0.4   0.15]\n",
      " [ 0.03  0.01  0.4   0.48]\n",
      " [ 0.03  0.02  0.4   0.18]\n",
      " [ 0.03  0.03  0.4   0.25]\n",
      " [ 0.03  0.04  0.4   0.27]] -> [ 0.25]\n",
      "[[ 0.03  0.01  0.4   0.48]\n",
      " [ 0.03  0.02  0.4   0.18]\n",
      " [ 0.03  0.03  0.4   0.25]\n",
      " [ 0.03  0.04  0.4   0.27]\n",
      " [ 0.03  0.05  0.4   0.25]] -> [ 0.21]\n",
      "[[ 0.03  0.02  0.4   0.18]\n",
      " [ 0.03  0.03  0.4   0.25]\n",
      " [ 0.03  0.04  0.4   0.27]\n",
      " [ 0.03  0.05  0.4   0.25]\n",
      " [ 0.03  0.06  0.4   0.21]] -> [ 0.28]\n",
      "[[ 0.03  0.03  0.4   0.25]\n",
      " [ 0.03  0.04  0.4   0.27]\n",
      " [ 0.03  0.05  0.4   0.25]\n",
      " [ 0.03  0.06  0.4   0.21]\n",
      " [ 0.03  0.    0.41  0.28]] -> [ 0.24]\n",
      "[[ 0.03  0.04  0.4   0.27]\n",
      " [ 0.03  0.05  0.4   0.25]\n",
      " [ 0.03  0.06  0.4   0.21]\n",
      " [ 0.03  0.    0.41  0.28]\n",
      " [ 0.03  0.01  0.41  0.24]] -> [ 0.21]\n",
      "[[ 0.03  0.05  0.4   0.25]\n",
      " [ 0.03  0.06  0.4   0.21]\n",
      " [ 0.03  0.    0.41  0.28]\n",
      " [ 0.03  0.01  0.41  0.24]\n",
      " [ 0.03  0.02  0.41  0.21]] -> [ 0.29]\n",
      "[[ 0.03  0.06  0.4   0.21]\n",
      " [ 0.03  0.    0.41  0.28]\n",
      " [ 0.03  0.01  0.41  0.24]\n",
      " [ 0.03  0.02  0.41  0.21]\n",
      " [ 0.03  0.03  0.41  0.29]] -> [ 0.35]\n",
      "[[ 0.03  0.    0.41  0.28]\n",
      " [ 0.03  0.01  0.41  0.24]\n",
      " [ 0.03  0.02  0.41  0.21]\n",
      " [ 0.03  0.03  0.41  0.29]\n",
      " [ 0.03  0.04  0.41  0.35]] -> [ 0.]\n",
      "[[ 0.03  0.01  0.41  0.24]\n",
      " [ 0.03  0.02  0.41  0.21]\n",
      " [ 0.03  0.03  0.41  0.29]\n",
      " [ 0.03  0.04  0.41  0.35]\n",
      " [ 0.03  0.05  0.41  0.  ]] -> [ 0.07]\n",
      "[[ 0.03  0.02  0.41  0.21]\n",
      " [ 0.03  0.03  0.41  0.29]\n",
      " [ 0.03  0.04  0.41  0.35]\n",
      " [ 0.03  0.05  0.41  0.  ]\n",
      " [ 0.03  0.06  0.41  0.07]] -> [ 0.32]\n",
      "[[ 0.03  0.03  0.41  0.29]\n",
      " [ 0.03  0.04  0.41  0.35]\n",
      " [ 0.03  0.05  0.41  0.  ]\n",
      " [ 0.03  0.06  0.41  0.07]\n",
      " [ 0.03  0.    0.42  0.32]] -> [ 0.19]\n",
      "[[ 0.03  0.04  0.41  0.35]\n",
      " [ 0.03  0.05  0.41  0.  ]\n",
      " [ 0.03  0.06  0.41  0.07]\n",
      " [ 0.03  0.    0.42  0.32]\n",
      " [ 0.03  0.01  0.42  0.19]] -> [ 0.34]\n",
      "[[ 0.03  0.05  0.41  0.  ]\n",
      " [ 0.03  0.06  0.41  0.07]\n",
      " [ 0.03  0.    0.42  0.32]\n",
      " [ 0.03  0.01  0.42  0.19]\n",
      " [ 0.03  0.02  0.42  0.34]] -> [ 0.28]\n",
      "[[ 0.03  0.06  0.41  0.07]\n",
      " [ 0.03  0.    0.42  0.32]\n",
      " [ 0.03  0.01  0.42  0.19]\n",
      " [ 0.03  0.02  0.42  0.34]\n",
      " [ 0.03  0.03  0.42  0.28]] -> [ 0.23]\n",
      "[[ 0.03  0.    0.42  0.32]\n",
      " [ 0.03  0.01  0.42  0.19]\n",
      " [ 0.03  0.02  0.42  0.34]\n",
      " [ 0.03  0.03  0.42  0.28]\n",
      " [ 0.03  0.04  0.42  0.23]] -> [ 0.15]\n",
      "[[ 0.03  0.01  0.42  0.19]\n",
      " [ 0.03  0.02  0.42  0.34]\n",
      " [ 0.03  0.03  0.42  0.28]\n",
      " [ 0.03  0.04  0.42  0.23]\n",
      " [ 0.03  0.05  0.42  0.15]] -> [ 0.1]\n",
      "[[ 0.03  0.02  0.42  0.34]\n",
      " [ 0.03  0.03  0.42  0.28]\n",
      " [ 0.03  0.04  0.42  0.23]\n",
      " [ 0.03  0.05  0.42  0.15]\n",
      " [ 0.03  0.06  0.42  0.1 ]] -> [ 0.15]\n",
      "[[ 0.03  0.03  0.42  0.28]\n",
      " [ 0.03  0.04  0.42  0.23]\n",
      " [ 0.03  0.05  0.42  0.15]\n",
      " [ 0.03  0.06  0.42  0.1 ]\n",
      " [ 0.03  0.    0.43  0.15]] -> [ 0.09]\n",
      "[[ 0.03  0.04  0.42  0.23]\n",
      " [ 0.03  0.05  0.42  0.15]\n",
      " [ 0.03  0.06  0.42  0.1 ]\n",
      " [ 0.03  0.    0.43  0.15]\n",
      " [ 0.03  0.01  0.43  0.09]] -> [ 0.78]\n",
      "[[ 0.03  0.05  0.42  0.15]\n",
      " [ 0.03  0.06  0.42  0.1 ]\n",
      " [ 0.03  0.    0.43  0.15]\n",
      " [ 0.03  0.01  0.43  0.09]\n",
      " [ 0.03  0.02  0.43  0.78]] -> [ 0.59]\n",
      "[[ 0.03  0.06  0.42  0.1 ]\n",
      " [ 0.03  0.    0.43  0.15]\n",
      " [ 0.03  0.01  0.43  0.09]\n",
      " [ 0.03  0.02  0.43  0.78]\n",
      " [ 0.03  0.03  0.43  0.59]] -> [ 0.29]\n",
      "[[ 0.03  0.    0.43  0.15]\n",
      " [ 0.03  0.01  0.43  0.09]\n",
      " [ 0.03  0.02  0.43  0.78]\n",
      " [ 0.03  0.03  0.43  0.59]\n",
      " [ 0.03  0.04  0.43  0.29]] -> [ 0.23]\n",
      "[[ 0.03  0.01  0.43  0.09]\n",
      " [ 0.03  0.02  0.43  0.78]\n",
      " [ 0.03  0.03  0.43  0.59]\n",
      " [ 0.03  0.04  0.43  0.29]\n",
      " [ 0.03  0.05  0.43  0.23]] -> [ 0.12]\n",
      "[[ 0.03  0.02  0.43  0.78]\n",
      " [ 0.03  0.03  0.43  0.59]\n",
      " [ 0.03  0.04  0.43  0.29]\n",
      " [ 0.03  0.05  0.43  0.23]\n",
      " [ 0.03  0.06  0.43  0.12]] -> [ 0.36]\n",
      "[[ 0.03  0.03  0.43  0.59]\n",
      " [ 0.03  0.04  0.43  0.29]\n",
      " [ 0.03  0.05  0.43  0.23]\n",
      " [ 0.03  0.06  0.43  0.12]\n",
      " [ 0.03  0.    0.44  0.36]] -> [ 0.13]\n",
      "[[ 0.03  0.04  0.43  0.29]\n",
      " [ 0.03  0.05  0.43  0.23]\n",
      " [ 0.03  0.06  0.43  0.12]\n",
      " [ 0.03  0.    0.44  0.36]\n",
      " [ 0.03  0.01  0.44  0.13]] -> [ 0.17]\n",
      "[[ 0.03  0.05  0.43  0.23]\n",
      " [ 0.03  0.06  0.43  0.12]\n",
      " [ 0.03  0.    0.44  0.36]\n",
      " [ 0.03  0.01  0.44  0.13]\n",
      " [ 0.03  0.02  0.44  0.17]] -> [ 0.17]\n",
      "[[ 0.03  0.06  0.43  0.12]\n",
      " [ 0.03  0.    0.44  0.36]\n",
      " [ 0.03  0.01  0.44  0.13]\n",
      " [ 0.03  0.02  0.44  0.17]\n",
      " [ 0.03  0.03  0.44  0.17]] -> [ 0.16]\n",
      "[[ 0.03  0.    0.44  0.36]\n",
      " [ 0.03  0.01  0.44  0.13]\n",
      " [ 0.03  0.02  0.44  0.17]\n",
      " [ 0.03  0.03  0.44  0.17]\n",
      " [ 0.03  0.04  0.44  0.16]] -> [ 0.14]\n",
      "[[ 0.03  0.01  0.44  0.13]\n",
      " [ 0.03  0.02  0.44  0.17]\n",
      " [ 0.03  0.03  0.44  0.17]\n",
      " [ 0.03  0.04  0.44  0.16]\n",
      " [ 0.03  0.05  0.44  0.14]] -> [ 0.08]\n",
      "[[ 0.03  0.02  0.44  0.17]\n",
      " [ 0.03  0.03  0.44  0.17]\n",
      " [ 0.03  0.04  0.44  0.16]\n",
      " [ 0.03  0.05  0.44  0.14]\n",
      " [ 0.03  0.06  0.44  0.08]] -> [ 0.16]\n",
      "[[ 0.03  0.03  0.44  0.17]\n",
      " [ 0.03  0.04  0.44  0.16]\n",
      " [ 0.03  0.05  0.44  0.14]\n",
      " [ 0.03  0.06  0.44  0.08]\n",
      " [ 0.03  0.    0.45  0.16]] -> [ 0.14]\n",
      "[[ 0.03  0.04  0.44  0.16]\n",
      " [ 0.03  0.05  0.44  0.14]\n",
      " [ 0.03  0.06  0.44  0.08]\n",
      " [ 0.03  0.    0.45  0.16]\n",
      " [ 0.03  0.01  0.45  0.14]] -> [ 0.14]\n",
      "[[ 0.03  0.05  0.44  0.14]\n",
      " [ 0.03  0.06  0.44  0.08]\n",
      " [ 0.03  0.    0.45  0.16]\n",
      " [ 0.03  0.01  0.45  0.14]\n",
      " [ 0.03  0.02  0.45  0.14]] -> [ 0.15]\n",
      "[[ 0.03  0.06  0.44  0.08]\n",
      " [ 0.03  0.    0.45  0.16]\n",
      " [ 0.03  0.01  0.45  0.14]\n",
      " [ 0.03  0.02  0.45  0.14]\n",
      " [ 0.03  0.03  0.45  0.15]] -> [ 0.2]\n",
      "[[ 0.03  0.    0.45  0.16]\n",
      " [ 0.03  0.01  0.45  0.14]\n",
      " [ 0.03  0.02  0.45  0.14]\n",
      " [ 0.03  0.03  0.45  0.15]\n",
      " [ 0.03  0.04  0.45  0.2 ]] -> [ 0.24]\n",
      "[[ 0.03  0.01  0.45  0.14]\n",
      " [ 0.03  0.02  0.45  0.14]\n",
      " [ 0.03  0.03  0.45  0.15]\n",
      " [ 0.03  0.04  0.45  0.2 ]\n",
      " [ 0.03  0.05  0.45  0.24]] -> [ 0.08]\n",
      "[[ 0.03  0.02  0.45  0.14]\n",
      " [ 0.03  0.03  0.45  0.15]\n",
      " [ 0.03  0.04  0.45  0.2 ]\n",
      " [ 0.03  0.05  0.45  0.24]\n",
      " [ 0.03  0.06  0.45  0.08]] -> [ 0.15]\n",
      "[[ 0.03  0.03  0.45  0.15]\n",
      " [ 0.03  0.04  0.45  0.2 ]\n",
      " [ 0.03  0.05  0.45  0.24]\n",
      " [ 0.03  0.06  0.45  0.08]\n",
      " [ 0.03  0.    0.46  0.15]] -> [ 0.17]\n",
      "[[ 0.03  0.04  0.45  0.2 ]\n",
      " [ 0.03  0.05  0.45  0.24]\n",
      " [ 0.03  0.06  0.45  0.08]\n",
      " [ 0.03  0.    0.46  0.15]\n",
      " [ 0.03  0.01  0.46  0.17]] -> [ 0.09]\n",
      "[[ 0.03  0.05  0.45  0.24]\n",
      " [ 0.03  0.06  0.45  0.08]\n",
      " [ 0.03  0.    0.46  0.15]\n",
      " [ 0.03  0.01  0.46  0.17]\n",
      " [ 0.03  0.02  0.46  0.09]] -> [ 0.17]\n",
      "[[ 0.03  0.06  0.45  0.08]\n",
      " [ 0.03  0.    0.46  0.15]\n",
      " [ 0.03  0.01  0.46  0.17]\n",
      " [ 0.03  0.02  0.46  0.09]\n",
      " [ 0.03  0.03  0.46  0.17]] -> [ 0.13]\n",
      "[[ 0.03  0.    0.46  0.15]\n",
      " [ 0.03  0.01  0.46  0.17]\n",
      " [ 0.03  0.02  0.46  0.09]\n",
      " [ 0.03  0.03  0.46  0.17]\n",
      " [ 0.03  0.04  0.46  0.13]] -> [ 0.19]\n",
      "[[ 0.03  0.01  0.46  0.17]\n",
      " [ 0.03  0.02  0.46  0.09]\n",
      " [ 0.03  0.03  0.46  0.17]\n",
      " [ 0.03  0.04  0.46  0.13]\n",
      " [ 0.03  0.05  0.46  0.19]] -> [ 0.08]\n",
      "[[ 0.03  0.02  0.46  0.09]\n",
      " [ 0.03  0.03  0.46  0.17]\n",
      " [ 0.03  0.04  0.46  0.13]\n",
      " [ 0.03  0.05  0.46  0.19]\n",
      " [ 0.03  0.06  0.46  0.08]] -> [ 0.14]\n",
      "[[ 0.03  0.03  0.46  0.17]\n",
      " [ 0.03  0.04  0.46  0.13]\n",
      " [ 0.03  0.05  0.46  0.19]\n",
      " [ 0.03  0.06  0.46  0.08]\n",
      " [ 0.03  0.    0.47  0.14]] -> [ 0.18]\n",
      "[[ 0.03  0.04  0.46  0.13]\n",
      " [ 0.03  0.05  0.46  0.19]\n",
      " [ 0.03  0.06  0.46  0.08]\n",
      " [ 0.03  0.    0.47  0.14]\n",
      " [ 0.03  0.01  0.47  0.18]] -> [ 0.24]\n",
      "[[ 0.03  0.05  0.46  0.19]\n",
      " [ 0.03  0.06  0.46  0.08]\n",
      " [ 0.03  0.    0.47  0.14]\n",
      " [ 0.03  0.01  0.47  0.18]\n",
      " [ 0.03  0.02  0.47  0.24]] -> [ 0.16]\n",
      "[[ 0.03  0.06  0.46  0.08]\n",
      " [ 0.03  0.    0.47  0.14]\n",
      " [ 0.03  0.01  0.47  0.18]\n",
      " [ 0.03  0.02  0.47  0.24]\n",
      " [ 0.03  0.03  0.47  0.16]] -> [ 0.11]\n",
      "[[ 0.03  0.    0.47  0.14]\n",
      " [ 0.03  0.01  0.47  0.18]\n",
      " [ 0.03  0.02  0.47  0.24]\n",
      " [ 0.03  0.03  0.47  0.16]\n",
      " [ 0.03  0.04  0.47  0.11]] -> [ 0.13]\n",
      "[[ 0.03  0.01  0.47  0.18]\n",
      " [ 0.03  0.02  0.47  0.24]\n",
      " [ 0.03  0.03  0.47  0.16]\n",
      " [ 0.03  0.04  0.47  0.11]\n",
      " [ 0.03  0.05  0.47  0.13]] -> [ 0.1]\n",
      "[[ 0.03  0.02  0.47  0.24]\n",
      " [ 0.03  0.03  0.47  0.16]\n",
      " [ 0.03  0.04  0.47  0.11]\n",
      " [ 0.03  0.05  0.47  0.13]\n",
      " [ 0.03  0.06  0.47  0.1 ]] -> [ 0.16]\n",
      "[[ 0.03  0.03  0.47  0.16]\n",
      " [ 0.03  0.04  0.47  0.11]\n",
      " [ 0.03  0.05  0.47  0.13]\n",
      " [ 0.03  0.06  0.47  0.1 ]\n",
      " [ 0.03  0.    0.48  0.16]] -> [ 0.11]\n",
      "[[ 0.03  0.04  0.47  0.11]\n",
      " [ 0.03  0.05  0.47  0.13]\n",
      " [ 0.03  0.06  0.47  0.1 ]\n",
      " [ 0.03  0.    0.48  0.16]\n",
      " [ 0.03  0.01  0.48  0.11]] -> [ 0.22]\n",
      "[[ 0.03  0.05  0.47  0.13]\n",
      " [ 0.03  0.06  0.47  0.1 ]\n",
      " [ 0.03  0.    0.48  0.16]\n",
      " [ 0.03  0.01  0.48  0.11]\n",
      " [ 0.03  0.02  0.48  0.22]] -> [ 0.15]\n",
      "[[ 0.03  0.06  0.47  0.1 ]\n",
      " [ 0.03  0.    0.48  0.16]\n",
      " [ 0.03  0.01  0.48  0.11]\n",
      " [ 0.03  0.02  0.48  0.22]\n",
      " [ 0.    0.03  0.48  0.15]] -> [ 0.18]\n",
      "[[ 0.03  0.    0.48  0.16]\n",
      " [ 0.03  0.01  0.48  0.11]\n",
      " [ 0.03  0.02  0.48  0.22]\n",
      " [ 0.    0.03  0.48  0.15]\n",
      " [ 0.    0.04  0.48  0.18]] -> [ 0.11]\n",
      "[[ 0.03  0.01  0.48  0.11]\n",
      " [ 0.03  0.02  0.48  0.22]\n",
      " [ 0.    0.03  0.48  0.15]\n",
      " [ 0.    0.04  0.48  0.18]\n",
      " [ 0.    0.05  0.48  0.11]] -> [ 0.14]\n",
      "[[ 0.03  0.02  0.48  0.22]\n",
      " [ 0.    0.03  0.48  0.15]\n",
      " [ 0.    0.04  0.48  0.18]\n",
      " [ 0.    0.05  0.48  0.11]\n",
      " [ 0.    0.06  0.48  0.14]] -> [ 0.34]\n",
      "[[ 0.    0.03  0.48  0.15]\n",
      " [ 0.    0.04  0.48  0.18]\n",
      " [ 0.    0.05  0.48  0.11]\n",
      " [ 0.    0.06  0.48  0.14]\n",
      " [ 0.    0.    0.49  0.34]] -> [ 0.1]\n",
      "[[ 0.    0.04  0.48  0.18]\n",
      " [ 0.    0.05  0.48  0.11]\n",
      " [ 0.    0.06  0.48  0.14]\n",
      " [ 0.    0.    0.49  0.34]\n",
      " [ 0.    0.01  0.49  0.1 ]] -> [ 0.13]\n",
      "[[ 0.    0.05  0.48  0.11]\n",
      " [ 0.    0.06  0.48  0.14]\n",
      " [ 0.    0.    0.49  0.34]\n",
      " [ 0.    0.01  0.49  0.1 ]\n",
      " [ 0.    0.02  0.49  0.13]] -> [ 0.21]\n",
      "[[ 0.    0.06  0.48  0.14]\n",
      " [ 0.    0.    0.49  0.34]\n",
      " [ 0.    0.01  0.49  0.1 ]\n",
      " [ 0.    0.02  0.49  0.13]\n",
      " [ 0.    0.03  0.49  0.21]] -> [ 0.35]\n",
      "[[ 0.    0.    0.49  0.34]\n",
      " [ 0.    0.01  0.49  0.1 ]\n",
      " [ 0.    0.02  0.49  0.13]\n",
      " [ 0.    0.03  0.49  0.21]\n",
      " [ 0.    0.04  0.49  0.35]] -> [ 0.15]\n",
      "[[ 0.    0.01  0.49  0.1 ]\n",
      " [ 0.    0.02  0.49  0.13]\n",
      " [ 0.    0.03  0.49  0.21]\n",
      " [ 0.    0.04  0.49  0.35]\n",
      " [ 0.    0.05  0.49  0.15]] -> [ 0.06]\n",
      "[[ 0.    0.02  0.49  0.13]\n",
      " [ 0.    0.03  0.49  0.21]\n",
      " [ 0.    0.04  0.49  0.35]\n",
      " [ 0.    0.05  0.49  0.15]\n",
      " [ 0.    0.06  0.49  0.06]] -> [ 0.23]\n",
      "[[ 0.    0.03  0.49  0.21]\n",
      " [ 0.    0.04  0.49  0.35]\n",
      " [ 0.    0.05  0.49  0.15]\n",
      " [ 0.    0.06  0.49  0.06]\n",
      " [ 0.    0.    0.5   0.23]] -> [ 0.22]\n",
      "[[ 0.    0.04  0.49  0.35]\n",
      " [ 0.    0.05  0.49  0.15]\n",
      " [ 0.    0.06  0.49  0.06]\n",
      " [ 0.    0.    0.5   0.23]\n",
      " [ 0.    0.01  0.5   0.22]] -> [ 0.19]\n",
      "[[ 0.    0.05  0.49  0.15]\n",
      " [ 0.    0.06  0.49  0.06]\n",
      " [ 0.    0.    0.5   0.23]\n",
      " [ 0.    0.01  0.5   0.22]\n",
      " [ 0.    0.02  0.5   0.19]] -> [ 0.2]\n",
      "[[ 0.    0.06  0.49  0.06]\n",
      " [ 0.    0.    0.5   0.23]\n",
      " [ 0.    0.01  0.5   0.22]\n",
      " [ 0.    0.02  0.5   0.19]\n",
      " [ 0.    0.03  0.5   0.2 ]] -> [ 0.13]\n",
      "[[ 0.    0.    0.5   0.23]\n",
      " [ 0.    0.01  0.5   0.22]\n",
      " [ 0.    0.02  0.5   0.19]\n",
      " [ 0.    0.03  0.5   0.2 ]\n",
      " [ 0.    0.04  0.5   0.13]] -> [ 0.21]\n",
      "[[ 0.    0.01  0.5   0.22]\n",
      " [ 0.    0.02  0.5   0.19]\n",
      " [ 0.    0.03  0.5   0.2 ]\n",
      " [ 0.    0.04  0.5   0.13]\n",
      " [ 0.    0.05  0.5   0.21]] -> [ 0.1]\n",
      "[[ 0.    0.02  0.5   0.19]\n",
      " [ 0.    0.03  0.5   0.2 ]\n",
      " [ 0.    0.04  0.5   0.13]\n",
      " [ 0.    0.05  0.5   0.21]\n",
      " [ 0.    0.06  0.5   0.1 ]] -> [ 0.11]\n",
      "[[ 0.    0.03  0.5   0.2 ]\n",
      " [ 0.    0.04  0.5   0.13]\n",
      " [ 0.    0.05  0.5   0.21]\n",
      " [ 0.    0.06  0.5   0.1 ]\n",
      " [ 0.    0.    0.51  0.11]] -> [ 0.11]\n",
      "[[ 0.    0.04  0.5   0.13]\n",
      " [ 0.    0.05  0.5   0.21]\n",
      " [ 0.    0.06  0.5   0.1 ]\n",
      " [ 0.    0.    0.51  0.11]\n",
      " [ 0.    0.01  0.51  0.11]] -> [ 0.16]\n",
      "[[ 0.    0.05  0.5   0.21]\n",
      " [ 0.    0.06  0.5   0.1 ]\n",
      " [ 0.    0.    0.51  0.11]\n",
      " [ 0.    0.01  0.51  0.11]\n",
      " [ 0.    0.02  0.51  0.16]] -> [ 0.12]\n",
      "[[ 0.    0.06  0.5   0.1 ]\n",
      " [ 0.    0.    0.51  0.11]\n",
      " [ 0.    0.01  0.51  0.11]\n",
      " [ 0.    0.02  0.51  0.16]\n",
      " [ 0.    0.03  0.51  0.12]] -> [ 0.15]\n",
      "[[ 0.    0.    0.51  0.11]\n",
      " [ 0.    0.01  0.51  0.11]\n",
      " [ 0.    0.02  0.51  0.16]\n",
      " [ 0.    0.03  0.51  0.12]\n",
      " [ 0.    0.04  0.51  0.15]] -> [ 0.08]\n",
      "[[ 0.    0.01  0.51  0.11]\n",
      " [ 0.    0.02  0.51  0.16]\n",
      " [ 0.    0.03  0.51  0.12]\n",
      " [ 0.    0.04  0.51  0.15]\n",
      " [ 0.    0.05  0.51  0.08]] -> [ 0.]\n",
      "[[ 0.    0.02  0.51  0.16]\n",
      " [ 0.    0.03  0.51  0.12]\n",
      " [ 0.    0.04  0.51  0.15]\n",
      " [ 0.    0.05  0.51  0.08]\n",
      " [ 0.    0.06  0.51  0.  ]] -> [ 0.24]\n",
      "[[ 0.    0.03  0.51  0.12]\n",
      " [ 0.    0.04  0.51  0.15]\n",
      " [ 0.    0.05  0.51  0.08]\n",
      " [ 0.    0.06  0.51  0.  ]\n",
      " [ 0.    0.    0.52  0.24]] -> [ 0.19]\n",
      "[[ 0.    0.04  0.51  0.15]\n",
      " [ 0.    0.05  0.51  0.08]\n",
      " [ 0.    0.06  0.51  0.  ]\n",
      " [ 0.    0.    0.52  0.24]\n",
      " [ 0.    0.01  0.52  0.19]] -> [ 0.22]\n",
      "[[ 0.    0.05  0.51  0.08]\n",
      " [ 0.    0.06  0.51  0.  ]\n",
      " [ 0.    0.    0.52  0.24]\n",
      " [ 0.    0.01  0.52  0.19]\n",
      " [ 0.    0.02  0.52  0.22]] -> [ 0.13]\n",
      "[[ 0.    0.06  0.51  0.  ]\n",
      " [ 0.    0.    0.52  0.24]\n",
      " [ 0.    0.01  0.52  0.19]\n",
      " [ 0.    0.02  0.52  0.22]\n",
      " [ 0.    0.03  0.52  0.13]] -> [ 0.12]\n",
      "[[ 0.    0.    0.52  0.24]\n",
      " [ 0.    0.01  0.52  0.19]\n",
      " [ 0.    0.02  0.52  0.22]\n",
      " [ 0.    0.03  0.52  0.13]\n",
      " [ 0.    0.04  0.52  0.12]] -> [ 0.16]\n",
      "[[ 0.    0.01  0.52  0.19]\n",
      " [ 0.    0.02  0.52  0.22]\n",
      " [ 0.    0.03  0.52  0.13]\n",
      " [ 0.    0.04  0.52  0.12]\n",
      " [ 0.    0.05  0.52  0.16]] -> [ 0.02]\n",
      "[[ 0.    0.02  0.52  0.22]\n",
      " [ 0.    0.03  0.52  0.13]\n",
      " [ 0.    0.04  0.52  0.12]\n",
      " [ 0.    0.05  0.52  0.16]\n",
      " [ 0.    0.06  0.52  0.02]] -> [ 0.16]\n",
      "[[ 0.    0.03  0.52  0.13]\n",
      " [ 0.    0.04  0.52  0.12]\n",
      " [ 0.    0.05  0.52  0.16]\n",
      " [ 0.    0.06  0.52  0.02]\n",
      " [ 0.    0.    0.01  0.16]] -> [ 0.2]\n",
      "[[ 0.    0.04  0.52  0.12]\n",
      " [ 0.    0.05  0.52  0.16]\n",
      " [ 0.    0.06  0.52  0.02]\n",
      " [ 0.    0.    0.01  0.16]\n",
      " [ 0.    0.01  0.01  0.2 ]] -> [ 0.12]\n",
      "[[ 0.    0.05  0.52  0.16]\n",
      " [ 0.    0.06  0.52  0.02]\n",
      " [ 0.    0.    0.01  0.16]\n",
      " [ 0.    0.01  0.01  0.2 ]\n",
      " [ 0.    0.02  0.01  0.12]] -> [ 0.11]\n",
      "[[ 0.    0.06  0.52  0.02]\n",
      " [ 0.    0.    0.01  0.16]\n",
      " [ 0.    0.01  0.01  0.2 ]\n",
      " [ 0.    0.02  0.01  0.12]\n",
      " [ 0.    0.03  0.01  0.11]] -> [ 0.22]\n",
      "[[ 0.    0.    0.01  0.16]\n",
      " [ 0.    0.01  0.01  0.2 ]\n",
      " [ 0.    0.02  0.01  0.12]\n",
      " [ 0.    0.03  0.01  0.11]\n",
      " [ 0.    0.04  0.01  0.22]] -> [ 0.2]\n",
      "[[ 0.    0.01  0.01  0.2 ]\n",
      " [ 0.    0.02  0.01  0.12]\n",
      " [ 0.    0.03  0.01  0.11]\n",
      " [ 0.    0.04  0.01  0.22]\n",
      " [ 0.    0.05  0.01  0.2 ]] -> [ 0.16]\n",
      "[[ 0.    0.02  0.01  0.12]\n",
      " [ 0.    0.03  0.01  0.11]\n",
      " [ 0.    0.04  0.01  0.22]\n",
      " [ 0.    0.05  0.01  0.2 ]\n",
      " [ 0.    0.06  0.01  0.16]] -> [ 0.17]\n",
      "[[ 0.    0.03  0.01  0.11]\n",
      " [ 0.    0.04  0.01  0.22]\n",
      " [ 0.    0.05  0.01  0.2 ]\n",
      " [ 0.    0.06  0.01  0.16]\n",
      " [ 0.    0.    0.02  0.17]] -> [ 0.97]\n",
      "[[ 0.    0.04  0.01  0.22]\n",
      " [ 0.    0.05  0.01  0.2 ]\n",
      " [ 0.    0.06  0.01  0.16]\n",
      " [ 0.    0.    0.02  0.17]\n",
      " [ 0.    0.01  0.02  0.97]] -> [ 0.]\n",
      "[[ 0.    0.05  0.01  0.2 ]\n",
      " [ 0.    0.06  0.01  0.16]\n",
      " [ 0.    0.    0.02  0.17]\n",
      " [ 0.    0.01  0.02  0.97]\n",
      " [ 0.    0.02  0.02  0.  ]] -> [ 0.23]\n",
      "[[ 0.    0.06  0.01  0.16]\n",
      " [ 0.    0.    0.02  0.17]\n",
      " [ 0.    0.01  0.02  0.97]\n",
      " [ 0.    0.02  0.02  0.  ]\n",
      " [ 0.    0.03  0.02  0.23]] -> [ 1.]\n",
      "[[ 0.    0.    0.02  0.17]\n",
      " [ 0.    0.01  0.02  0.97]\n",
      " [ 0.    0.02  0.02  0.  ]\n",
      " [ 0.    0.03  0.02  0.23]\n",
      " [ 0.    0.04  0.02  1.  ]] -> [ 0.18]\n",
      "[[ 0.    0.01  0.02  0.97]\n",
      " [ 0.    0.02  0.02  0.  ]\n",
      " [ 0.    0.03  0.02  0.23]\n",
      " [ 0.    0.04  0.02  1.  ]\n",
      " [ 0.    0.05  0.02  0.18]] -> [ 0.13]\n",
      "[[ 0.    0.02  0.02  0.  ]\n",
      " [ 0.    0.03  0.02  0.23]\n",
      " [ 0.    0.04  0.02  1.  ]\n",
      " [ 0.    0.05  0.02  0.18]\n",
      " [ 0.    0.06  0.02  0.13]] -> [ 0.21]\n",
      "[[ 0.    0.03  0.02  0.23]\n",
      " [ 0.    0.04  0.02  1.  ]\n",
      " [ 0.    0.05  0.02  0.18]\n",
      " [ 0.    0.06  0.02  0.13]\n",
      " [ 0.    0.    0.03  0.21]] -> [ 0.24]\n",
      "[[ 0.    0.04  0.02  1.  ]\n",
      " [ 0.    0.05  0.02  0.18]\n",
      " [ 0.    0.06  0.02  0.13]\n",
      " [ 0.    0.    0.03  0.21]\n",
      " [ 0.    0.01  0.03  0.24]] -> [ 0.16]\n",
      "[[ 0.    0.05  0.02  0.18]\n",
      " [ 0.    0.06  0.02  0.13]\n",
      " [ 0.    0.    0.03  0.21]\n",
      " [ 0.    0.01  0.03  0.24]\n",
      " [ 0.    0.02  0.03  0.16]] -> [ 0.18]\n",
      "[[ 0.    0.06  0.02  0.13]\n",
      " [ 0.    0.    0.03  0.21]\n",
      " [ 0.    0.01  0.03  0.24]\n",
      " [ 0.    0.02  0.03  0.16]\n",
      " [ 0.    0.03  0.03  0.18]] -> [ 0.22]\n",
      "[[ 0.    0.    0.03  0.21]\n",
      " [ 0.    0.01  0.03  0.24]\n",
      " [ 0.    0.02  0.03  0.16]\n",
      " [ 0.    0.03  0.03  0.18]\n",
      " [ 0.    0.04  0.03  0.22]] -> [ 0.18]\n",
      "[[ 0.    0.01  0.03  0.24]\n",
      " [ 0.    0.02  0.03  0.16]\n",
      " [ 0.    0.03  0.03  0.18]\n",
      " [ 0.    0.04  0.03  0.22]\n",
      " [ 0.    0.05  0.03  0.18]] -> [ 0.11]\n",
      "[[ 0.    0.02  0.03  0.16]\n",
      " [ 0.    0.03  0.03  0.18]\n",
      " [ 0.    0.04  0.03  0.22]\n",
      " [ 0.    0.05  0.03  0.18]\n",
      " [ 0.    0.06  0.03  0.11]] -> [ 0.24]\n",
      "[[ 0.    0.03  0.03  0.18]\n",
      " [ 0.    0.04  0.03  0.22]\n",
      " [ 0.    0.05  0.03  0.18]\n",
      " [ 0.    0.06  0.03  0.11]\n",
      " [ 0.    0.    0.04  0.24]] -> [ 0.22]\n",
      "[[ 0.    0.04  0.03  0.22]\n",
      " [ 0.    0.05  0.03  0.18]\n",
      " [ 0.    0.06  0.03  0.11]\n",
      " [ 0.    0.    0.04  0.24]\n",
      " [ 0.    0.01  0.04  0.22]] -> [ 0.2]\n",
      "[[ 0.    0.05  0.03  0.18]\n",
      " [ 0.    0.06  0.03  0.11]\n",
      " [ 0.    0.    0.04  0.24]\n",
      " [ 0.    0.01  0.04  0.22]\n",
      " [ 0.    0.02  0.04  0.2 ]] -> [ 0.22]\n",
      "[[ 0.    0.06  0.03  0.11]\n",
      " [ 0.    0.    0.04  0.24]\n",
      " [ 0.    0.01  0.04  0.22]\n",
      " [ 0.    0.02  0.04  0.2 ]\n",
      " [ 0.    0.03  0.04  0.22]] -> [ 0.26]\n",
      "[[ 0.    0.    0.04  0.24]\n",
      " [ 0.    0.01  0.04  0.22]\n",
      " [ 0.    0.02  0.04  0.2 ]\n",
      " [ 0.    0.03  0.04  0.22]\n",
      " [ 0.    0.04  0.04  0.26]] -> [ 0.]\n",
      "[[ 0.    0.01  0.04  0.22]\n",
      " [ 0.    0.02  0.04  0.2 ]\n",
      " [ 0.    0.03  0.04  0.22]\n",
      " [ 0.    0.04  0.04  0.26]\n",
      " [ 0.    0.05  0.04  0.  ]] -> [ 0.]\n",
      "[[ 0.    0.02  0.04  0.2 ]\n",
      " [ 0.    0.03  0.04  0.22]\n",
      " [ 0.    0.04  0.04  0.26]\n",
      " [ 0.    0.05  0.04  0.  ]\n",
      " [ 0.    0.06  0.04  0.  ]] -> [ 0.05]\n",
      "[[ 0.    0.03  0.04  0.22]\n",
      " [ 0.    0.04  0.04  0.26]\n",
      " [ 0.    0.05  0.04  0.  ]\n",
      " [ 0.    0.06  0.04  0.  ]\n",
      " [ 0.    0.    0.05  0.05]] -> [ 0.31]\n"
     ]
    }
   ],
   "source": [
    "x=xy\n",
    "y=xy[:,[-1]]\n",
    "\n",
    "#build a series dataset(seq_length에 해당하는 전날 X와 다음 forecastDays에 해당하는 Y)\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0, len(y)-seq_length):\n",
    "    _x=x[i:i+seq_length]\n",
    "    _y=y[i+seq_length]\n",
    "    #     _y=Y[i+seq_length:i+seq_length+forecastDays]\n",
    "    print(_x,\"->\",_y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size:])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 input place holders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X의 경우 input type과 [batch size, sequence length, input data dimension(feature+1))]\n",
    "\n",
    "Y의 경우 input type과 [batch size, 원하는 output 의 개수(forecastDays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y=tf.placeholder(tf.float32, [None, forecastDays])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 build LSTM network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm network의 \n",
    "\n",
    "    기본단위 cell, \n",
    "    \n",
    "    사용 driver, \n",
    "    \n",
    "    예측 y 산출방식, \n",
    "    \n",
    "    loss 함수, \n",
    "    \n",
    "    사용 optimizer 정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### lstm의 한 기본단위인 cell을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell의 결과값을 fully connected layer로 한 번 더 가공할 것이기 때문에 cell의 output dimension인 num_units=hidden_dim로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic rnn이라는 driver를 가동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.dynamic_rnn의 input은 cell, input, input type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.dynamic_rnn의 output은 outputs와 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마지막 cell의 output과 fully connected layer를  이용하여 Y_pred 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs[:, -1]: cell의 outputs 중 마지막 하나만 이용(we use the last cell's output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_dim: fully connected의 최종출력개수는 output_dim(=forecastDays) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation_fn= None: 분류 문제가 아니라 회귀 문제이므로 activation_fn은 none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn= None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss를 줄이는 방향으로 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model 평가 with RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestY=originalSales[train_size+seq_length:]\n",
    "denormalizedTestY_preprocessed=sales[train_size+seq_length:]\n",
    "denormalizedTestY_feed=np.array([[i] for i in denormalizedTestY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "121 in denormalizedTestY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rootMeanSquaredError(a,b):\n",
    "    sum=0\n",
    "    for i in range(len(a)):\n",
    "        sum=sum+(a[i]-b[i])**2\n",
    "    return np.sqrt( sum/len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 62.392765045166016\n",
      "[step: 1] loss: 56.10784149169922\n",
      "[step: 2] loss: 50.1355094909668\n",
      "[step: 3] loss: 44.490623474121094\n",
      "[step: 4] loss: 39.371864318847656\n",
      "[step: 5] loss: 34.81439208984375\n",
      "[step: 6] loss: 30.532859802246094\n",
      "[step: 7] loss: 26.204259872436523\n",
      "[step: 8] loss: 21.963151931762695\n",
      "[step: 9] loss: 18.01466178894043\n",
      "[step: 10] loss: 14.554482460021973\n",
      "[step: 11] loss: 11.874048233032227\n",
      "[step: 12] loss: 10.343114852905273\n",
      "[step: 13] loss: 10.189848899841309\n",
      "[step: 14] loss: 11.071829795837402\n",
      "[step: 15] loss: 12.152756690979004\n",
      "[step: 16] loss: 12.701696395874023\n",
      "[step: 17] loss: 12.484926223754883\n",
      "[step: 18] loss: 11.683600425720215\n",
      "[step: 19] loss: 10.626652717590332\n",
      "[step: 20] loss: 9.600470542907715\n",
      "[step: 21] loss: 8.77655029296875\n",
      "[step: 22] loss: 8.210589408874512\n",
      "[step: 23] loss: 7.880557537078857\n",
      "[step: 24] loss: 7.727231502532959\n",
      "[step: 25] loss: 7.6828508377075195\n",
      "[step: 26] loss: 7.687564373016357\n",
      "[step: 27] loss: 7.695969581604004\n",
      "[step: 28] loss: 7.6779046058654785\n",
      "[step: 29] loss: 7.616629600524902\n",
      "[step: 30] loss: 7.506274223327637\n",
      "[step: 31] loss: 7.349331378936768\n",
      "[step: 32] loss: 7.154480934143066\n",
      "[step: 33] loss: 6.934711933135986\n",
      "[step: 34] loss: 6.705580711364746\n",
      "[step: 35] loss: 6.483489990234375\n",
      "[step: 36] loss: 6.28383731842041\n",
      "[step: 37] loss: 6.119001865386963\n",
      "[step: 38] loss: 5.996306896209717\n",
      "[step: 39] loss: 5.916304111480713\n",
      "[step: 40] loss: 5.8719658851623535\n",
      "[step: 41] loss: 5.849521636962891\n",
      "[step: 42] loss: 5.831386089324951\n",
      "[step: 43] loss: 5.800797462463379\n",
      "[step: 44] loss: 5.746766567230225\n",
      "[step: 45] loss: 5.667222499847412\n",
      "[step: 46] loss: 5.568929195404053\n",
      "[step: 47] loss: 5.464289665222168\n",
      "[step: 48] loss: 5.366701126098633\n",
      "[step: 49] loss: 5.287373065948486\n",
      "[step: 50] loss: 5.229739665985107\n",
      "[step: 51] loss: 5.191730499267578\n",
      "[step: 52] loss: 5.168806552886963\n",
      "[step: 53] loss: 5.153460502624512\n",
      "[step: 54] loss: 5.1382060050964355\n",
      "[step: 55] loss: 5.118034362792969\n",
      "[step: 56] loss: 5.090978145599365\n",
      "[step: 57] loss: 5.058333873748779\n",
      "[step: 58] loss: 5.023866176605225\n",
      "[step: 59] loss: 4.992547988891602\n",
      "[step: 60] loss: 4.968814849853516\n",
      "[step: 61] loss: 4.955127716064453\n",
      "[step: 62] loss: 4.951017379760742\n",
      "[step: 63] loss: 4.953191757202148\n",
      "[step: 64] loss: 4.956847190856934\n",
      "[step: 65] loss: 4.957669258117676\n",
      "[step: 66] loss: 4.953625679016113\n",
      "[step: 67] loss: 4.945390224456787\n",
      "[step: 68] loss: 4.935756683349609\n",
      "[step: 69] loss: 4.927988052368164\n",
      "[step: 70] loss: 4.924283981323242\n",
      "[step: 71] loss: 4.924717903137207\n",
      "[step: 72] loss: 4.927757263183594\n",
      "[step: 73] loss: 4.931167125701904\n",
      "[step: 74] loss: 4.933126926422119\n",
      "[step: 75] loss: 4.932602882385254\n",
      "[step: 76] loss: 4.929782390594482\n",
      "[step: 77] loss: 4.9257097244262695\n",
      "[step: 78] loss: 4.921763896942139\n",
      "[step: 79] loss: 4.91896390914917\n",
      "[step: 80] loss: 4.917654514312744\n",
      "[step: 81] loss: 4.917364120483398\n",
      "[step: 82] loss: 4.917141914367676\n",
      "[step: 83] loss: 4.9161601066589355\n",
      "[step: 84] loss: 4.914012432098389\n",
      "[step: 85] loss: 4.910884380340576\n",
      "[step: 86] loss: 4.907374858856201\n",
      "[step: 87] loss: 4.904140472412109\n",
      "[step: 88] loss: 4.901589870452881\n",
      "[step: 89] loss: 4.899741172790527\n",
      "[step: 90] loss: 4.898303985595703\n",
      "[step: 91] loss: 4.896883487701416\n",
      "[step: 92] loss: 4.895179271697998\n",
      "[step: 93] loss: 4.893116474151611\n",
      "[step: 94] loss: 4.8908371925354\n",
      "[step: 95] loss: 4.888579845428467\n",
      "[step: 96] loss: 4.886565685272217\n",
      "[step: 97] loss: 4.88488245010376\n",
      "[step: 98] loss: 4.883511066436768\n",
      "[step: 99] loss: 4.882305145263672\n",
      "[step: 100] loss: 4.881080627441406\n",
      "[step: 101] loss: 4.879729270935059\n",
      "[step: 102] loss: 4.878250598907471\n",
      "[step: 103] loss: 4.876724720001221\n",
      "[step: 104] loss: 4.8752570152282715\n",
      "[step: 105] loss: 4.873931407928467\n",
      "[step: 106] loss: 4.872754096984863\n",
      "[step: 107] loss: 4.871668815612793\n",
      "[step: 108] loss: 4.870591163635254\n",
      "[step: 109] loss: 4.869452953338623\n",
      "[step: 110] loss: 4.868230819702148\n",
      "[step: 111] loss: 4.866942405700684\n",
      "[step: 112] loss: 4.865642070770264\n",
      "[step: 113] loss: 4.864386081695557\n",
      "[step: 114] loss: 4.863204479217529\n",
      "[step: 115] loss: 4.862072944641113\n",
      "[step: 116] loss: 4.860954284667969\n",
      "[step: 117] loss: 4.859811782836914\n",
      "[step: 118] loss: 4.8586320877075195\n",
      "[step: 119] loss: 4.857424259185791\n",
      "[step: 120] loss: 4.8562140464782715\n",
      "[step: 121] loss: 4.855026721954346\n",
      "[step: 122] loss: 4.853876113891602\n",
      "[step: 123] loss: 4.852755546569824\n",
      "[step: 124] loss: 4.851651191711426\n",
      "[step: 125] loss: 4.850545883178711\n",
      "[step: 126] loss: 4.84943151473999\n",
      "[step: 127] loss: 4.848310470581055\n",
      "[step: 128] loss: 4.847196578979492\n",
      "[step: 129] loss: 4.84609842300415\n",
      "[step: 130] loss: 4.8450236320495605\n",
      "[step: 131] loss: 4.843969345092773\n",
      "[step: 132] loss: 4.8429274559021\n",
      "[step: 133] loss: 4.841888904571533\n",
      "[step: 134] loss: 4.840851783752441\n",
      "[step: 135] loss: 4.839816570281982\n",
      "[step: 136] loss: 4.838788032531738\n",
      "[step: 137] loss: 4.837770938873291\n",
      "[step: 138] loss: 4.836768627166748\n",
      "[step: 139] loss: 4.835781097412109\n",
      "[step: 140] loss: 4.83480167388916\n",
      "[step: 141] loss: 4.833826065063477\n",
      "[step: 142] loss: 4.832854270935059\n",
      "[step: 143] loss: 4.8318867683410645\n",
      "[step: 144] loss: 4.830925941467285\n",
      "[step: 145] loss: 4.829975128173828\n",
      "[step: 146] loss: 4.829033374786377\n",
      "[step: 147] loss: 4.828099250793457\n",
      "[step: 148] loss: 4.827173233032227\n",
      "[step: 149] loss: 4.826250076293945\n",
      "[step: 150] loss: 4.8253326416015625\n",
      "[step: 151] loss: 4.824422359466553\n",
      "[step: 152] loss: 4.8235182762146\n",
      "[step: 153] loss: 4.8226213455200195\n",
      "[step: 154] loss: 4.821732521057129\n",
      "[step: 155] loss: 4.820850849151611\n",
      "[step: 156] loss: 4.819974422454834\n",
      "[step: 157] loss: 4.819105625152588\n",
      "[step: 158] loss: 4.818240165710449\n",
      "[step: 159] loss: 4.817382335662842\n",
      "[step: 160] loss: 4.816531658172607\n",
      "[step: 161] loss: 4.8156867027282715\n",
      "[step: 162] loss: 4.814850330352783\n",
      "[step: 163] loss: 4.814019680023193\n",
      "[step: 164] loss: 4.8131937980651855\n",
      "[step: 165] loss: 4.812375545501709\n",
      "[step: 166] loss: 4.8115620613098145\n",
      "[step: 167] loss: 4.810754776000977\n",
      "[step: 168] loss: 4.809952735900879\n",
      "[step: 169] loss: 4.809158802032471\n",
      "[step: 170] loss: 4.808369159698486\n",
      "[step: 171] loss: 4.807584285736084\n",
      "[step: 172] loss: 4.806807041168213\n",
      "[step: 173] loss: 4.806034088134766\n",
      "[step: 174] loss: 4.8052659034729\n",
      "[step: 175] loss: 4.80450439453125\n",
      "[step: 176] loss: 4.803746700286865\n",
      "[step: 177] loss: 4.802995681762695\n",
      "[step: 178] loss: 4.802249431610107\n",
      "[step: 179] loss: 4.801507472991943\n",
      "[step: 180] loss: 4.800772666931152\n",
      "[step: 181] loss: 4.800040245056152\n",
      "[step: 182] loss: 4.799315452575684\n",
      "[step: 183] loss: 4.798593521118164\n",
      "[step: 184] loss: 4.797879695892334\n",
      "[step: 185] loss: 4.797170639038086\n",
      "[step: 186] loss: 4.7964677810668945\n",
      "[step: 187] loss: 4.795769691467285\n",
      "[step: 188] loss: 4.7950758934021\n",
      "[step: 189] loss: 4.794386863708496\n",
      "[step: 190] loss: 4.793703556060791\n",
      "[step: 191] loss: 4.793024063110352\n",
      "[step: 192] loss: 4.792348384857178\n",
      "[step: 193] loss: 4.791676998138428\n",
      "[step: 194] loss: 4.791010856628418\n",
      "[step: 195] loss: 4.790347099304199\n",
      "[step: 196] loss: 4.789687633514404\n",
      "[step: 197] loss: 4.789034366607666\n",
      "[step: 198] loss: 4.788385391235352\n",
      "[step: 199] loss: 4.787740707397461\n",
      "[step: 200] loss: 4.787099838256836\n",
      "[step: 201] loss: 4.786462783813477\n",
      "[step: 202] loss: 4.785830497741699\n",
      "[step: 203] loss: 4.785201072692871\n",
      "[step: 204] loss: 4.784576892852783\n",
      "[step: 205] loss: 4.7839555740356445\n",
      "[step: 206] loss: 4.78333854675293\n",
      "[step: 207] loss: 4.782725811004639\n",
      "[step: 208] loss: 4.782116889953613\n",
      "[step: 209] loss: 4.781510829925537\n",
      "[step: 210] loss: 4.780908107757568\n",
      "[step: 211] loss: 4.780308723449707\n",
      "[step: 212] loss: 4.779712200164795\n",
      "[step: 213] loss: 4.779118537902832\n",
      "[step: 214] loss: 4.77852725982666\n",
      "[step: 215] loss: 4.7779388427734375\n",
      "[step: 216] loss: 4.777353763580322\n",
      "[step: 217] loss: 4.77677059173584\n",
      "[step: 218] loss: 4.776191711425781\n",
      "[step: 219] loss: 4.775620460510254\n",
      "[step: 220] loss: 4.775054454803467\n",
      "[step: 221] loss: 4.774491310119629\n",
      "[step: 222] loss: 4.773930072784424\n",
      "[step: 223] loss: 4.773373603820801\n",
      "[step: 224] loss: 4.772819519042969\n",
      "[step: 225] loss: 4.772268295288086\n",
      "[step: 226] loss: 4.7717204093933105\n",
      "[step: 227] loss: 4.771176338195801\n",
      "[step: 228] loss: 4.770634174346924\n",
      "[step: 229] loss: 4.770096302032471\n",
      "[step: 230] loss: 4.769561767578125\n",
      "[step: 231] loss: 4.7690300941467285\n",
      "[step: 232] loss: 4.768501281738281\n",
      "[step: 233] loss: 4.767976760864258\n",
      "[step: 234] loss: 4.767451286315918\n",
      "[step: 235] loss: 4.7669291496276855\n",
      "[step: 236] loss: 4.766408920288086\n",
      "[step: 237] loss: 4.7658915519714355\n",
      "[step: 238] loss: 4.765377044677734\n",
      "[step: 239] loss: 4.764864444732666\n",
      "[step: 240] loss: 4.764355659484863\n",
      "[step: 241] loss: 4.763847351074219\n",
      "[step: 242] loss: 4.763341903686523\n",
      "[step: 243] loss: 4.762839317321777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 244] loss: 4.762340068817139\n",
      "[step: 245] loss: 4.761842727661133\n",
      "[step: 246] loss: 4.761348247528076\n",
      "[step: 247] loss: 4.760851860046387\n",
      "[step: 248] loss: 4.760356426239014\n",
      "[step: 249] loss: 4.759864330291748\n",
      "[step: 250] loss: 4.759373664855957\n",
      "[step: 251] loss: 4.758882522583008\n",
      "[step: 252] loss: 4.758389949798584\n",
      "[step: 253] loss: 4.757896900177002\n",
      "[step: 254] loss: 4.757405757904053\n",
      "[step: 255] loss: 4.756916046142578\n",
      "[step: 256] loss: 4.7564263343811035\n",
      "[step: 257] loss: 4.7559380531311035\n",
      "[step: 258] loss: 4.75545072555542\n",
      "[step: 259] loss: 4.754964828491211\n",
      "[step: 260] loss: 4.754478931427002\n",
      "[step: 261] loss: 4.75399112701416\n",
      "[step: 262] loss: 4.753505229949951\n",
      "[step: 263] loss: 4.753019332885742\n",
      "[step: 264] loss: 4.752530574798584\n",
      "[step: 265] loss: 4.752039432525635\n",
      "[step: 266] loss: 4.751549243927002\n",
      "[step: 267] loss: 4.7510600090026855\n",
      "[step: 268] loss: 4.7505717277526855\n",
      "[step: 269] loss: 4.7500834465026855\n",
      "[step: 270] loss: 4.749593734741211\n",
      "[step: 271] loss: 4.749098777770996\n",
      "[step: 272] loss: 4.748602867126465\n",
      "[step: 273] loss: 4.748099327087402\n",
      "[step: 274] loss: 4.747587203979492\n",
      "[step: 275] loss: 4.747074127197266\n",
      "[step: 276] loss: 4.746560096740723\n",
      "[step: 277] loss: 4.746043682098389\n",
      "[step: 278] loss: 4.745514869689941\n",
      "[step: 279] loss: 4.744974136352539\n",
      "[step: 280] loss: 4.744431018829346\n",
      "[step: 281] loss: 4.743885517120361\n",
      "[step: 282] loss: 4.74334192276001\n",
      "[step: 283] loss: 4.742798805236816\n",
      "[step: 284] loss: 4.742255687713623\n",
      "[step: 285] loss: 4.741713523864746\n",
      "[step: 286] loss: 4.741171360015869\n",
      "[step: 287] loss: 4.740620136260986\n",
      "[step: 288] loss: 4.740054130554199\n",
      "[step: 289] loss: 4.739474773406982\n",
      "[step: 290] loss: 4.738887310028076\n",
      "[step: 291] loss: 4.738297939300537\n",
      "[step: 292] loss: 4.737703323364258\n",
      "[step: 293] loss: 4.737097263336182\n",
      "[step: 294] loss: 4.7364702224731445\n",
      "[step: 295] loss: 4.735809803009033\n",
      "[step: 296] loss: 4.735095024108887\n",
      "[step: 297] loss: 4.734325408935547\n",
      "[step: 298] loss: 4.733523368835449\n",
      "[step: 299] loss: 4.7327046394348145\n",
      "[step: 300] loss: 4.731863498687744\n",
      "[step: 301] loss: 4.731005668640137\n",
      "[step: 302] loss: 4.73013162612915\n",
      "[step: 303] loss: 4.729267120361328\n",
      "[step: 304] loss: 4.728403091430664\n",
      "[step: 305] loss: 4.727532386779785\n",
      "[step: 306] loss: 4.72669792175293\n",
      "[step: 307] loss: 4.725882530212402\n",
      "[step: 308] loss: 4.725085258483887\n",
      "[step: 309] loss: 4.724311828613281\n",
      "[step: 310] loss: 4.72352933883667\n",
      "[step: 311] loss: 4.722719192504883\n",
      "[step: 312] loss: 4.721899032592773\n",
      "[step: 313] loss: 4.7210493087768555\n",
      "[step: 314] loss: 4.720170497894287\n",
      "[step: 315] loss: 4.719325542449951\n",
      "[step: 316] loss: 4.7185258865356445\n",
      "[step: 317] loss: 4.717503547668457\n",
      "[step: 318] loss: 4.716518402099609\n",
      "[step: 319] loss: 4.715556621551514\n",
      "[step: 320] loss: 4.7146992683410645\n",
      "[step: 321] loss: 4.71394681930542\n",
      "[step: 322] loss: 4.713196277618408\n",
      "[step: 323] loss: 4.712477207183838\n",
      "[step: 324] loss: 4.711716175079346\n",
      "[step: 325] loss: 4.710968017578125\n",
      "[step: 326] loss: 4.710241794586182\n",
      "[step: 327] loss: 4.709539413452148\n",
      "[step: 328] loss: 4.7088398933410645\n",
      "[step: 329] loss: 4.708080291748047\n",
      "[step: 330] loss: 4.707329750061035\n",
      "[step: 331] loss: 4.7066650390625\n",
      "[step: 332] loss: 4.706005573272705\n",
      "[step: 333] loss: 4.705372333526611\n",
      "[step: 334] loss: 4.704718112945557\n",
      "[step: 335] loss: 4.70409631729126\n",
      "[step: 336] loss: 4.703524112701416\n",
      "[step: 337] loss: 4.70289945602417\n",
      "[step: 338] loss: 4.702188968658447\n",
      "[step: 339] loss: 4.701472282409668\n",
      "[step: 340] loss: 4.700760364532471\n",
      "[step: 341] loss: 4.700016498565674\n",
      "[step: 342] loss: 4.699307441711426\n",
      "[step: 343] loss: 4.698619842529297\n",
      "[step: 344] loss: 4.697896957397461\n",
      "[step: 345] loss: 4.697117805480957\n",
      "[step: 346] loss: 4.696452617645264\n",
      "[step: 347] loss: 4.695837497711182\n",
      "[step: 348] loss: 4.695307731628418\n",
      "[step: 349] loss: 4.694810390472412\n",
      "[step: 350] loss: 4.6942620277404785\n",
      "[step: 351] loss: 4.693719863891602\n",
      "[step: 352] loss: 4.693146705627441\n",
      "[step: 353] loss: 4.692568778991699\n",
      "[step: 354] loss: 4.692026138305664\n",
      "[step: 355] loss: 4.691539764404297\n",
      "[step: 356] loss: 4.691057205200195\n",
      "[step: 357] loss: 4.690563201904297\n",
      "[step: 358] loss: 4.690099716186523\n",
      "[step: 359] loss: 4.689644813537598\n",
      "[step: 360] loss: 4.689187049865723\n",
      "[step: 361] loss: 4.6887359619140625\n",
      "[step: 362] loss: 4.688281536102295\n",
      "[step: 363] loss: 4.6878180503845215\n",
      "[step: 364] loss: 4.687337875366211\n",
      "[step: 365] loss: 4.6868510246276855\n",
      "[step: 366] loss: 4.68635892868042\n",
      "[step: 367] loss: 4.685865879058838\n",
      "[step: 368] loss: 4.685369491577148\n",
      "[step: 369] loss: 4.684865474700928\n",
      "[step: 370] loss: 4.684353828430176\n",
      "[step: 371] loss: 4.683841228485107\n",
      "[step: 372] loss: 4.683345317840576\n",
      "[step: 373] loss: 4.682872772216797\n",
      "[step: 374] loss: 4.682397365570068\n",
      "[step: 375] loss: 4.681917667388916\n",
      "[step: 376] loss: 4.681439399719238\n",
      "[step: 377] loss: 4.680961608886719\n",
      "[step: 378] loss: 4.680487632751465\n",
      "[step: 379] loss: 4.6800103187561035\n",
      "[step: 380] loss: 4.679529190063477\n",
      "[step: 381] loss: 4.679044723510742\n",
      "[step: 382] loss: 4.678561210632324\n",
      "[step: 383] loss: 4.6780781745910645\n",
      "[step: 384] loss: 4.6775946617126465\n",
      "[step: 385] loss: 4.677117347717285\n",
      "[step: 386] loss: 4.676633358001709\n",
      "[step: 387] loss: 4.676153182983398\n",
      "[step: 388] loss: 4.675686836242676\n",
      "[step: 389] loss: 4.675233364105225\n",
      "[step: 390] loss: 4.674774169921875\n",
      "[step: 391] loss: 4.674310207366943\n",
      "[step: 392] loss: 4.67384672164917\n",
      "[step: 393] loss: 4.673385143280029\n",
      "[step: 394] loss: 4.672924041748047\n",
      "[step: 395] loss: 4.67246150970459\n",
      "[step: 396] loss: 4.671994686126709\n",
      "[step: 397] loss: 4.6715264320373535\n",
      "[step: 398] loss: 4.671057224273682\n",
      "[step: 399] loss: 4.670588970184326\n",
      "[step: 400] loss: 4.670123100280762\n",
      "[step: 401] loss: 4.669653415679932\n",
      "[step: 402] loss: 4.669179916381836\n",
      "[step: 403] loss: 4.668704032897949\n",
      "[step: 404] loss: 4.66823148727417\n",
      "[step: 405] loss: 4.667757511138916\n",
      "[step: 406] loss: 4.667282581329346\n",
      "[step: 407] loss: 4.666804790496826\n",
      "[step: 408] loss: 4.666325092315674\n",
      "[step: 409] loss: 4.66584587097168\n",
      "[step: 410] loss: 4.6653666496276855\n",
      "[step: 411] loss: 4.664885997772217\n",
      "[step: 412] loss: 4.664405345916748\n",
      "[step: 413] loss: 4.663921356201172\n",
      "[step: 414] loss: 4.663434982299805\n",
      "[step: 415] loss: 4.662950038909912\n",
      "[step: 416] loss: 4.6624627113342285\n",
      "[step: 417] loss: 4.6619744300842285\n",
      "[step: 418] loss: 4.661485195159912\n",
      "[step: 419] loss: 4.660994529724121\n",
      "[step: 420] loss: 4.660503387451172\n",
      "[step: 421] loss: 4.660009860992432\n",
      "[step: 422] loss: 4.659515857696533\n",
      "[step: 423] loss: 4.659019947052002\n",
      "[step: 424] loss: 4.658522129058838\n",
      "[step: 425] loss: 4.658022880554199\n",
      "[step: 426] loss: 4.657522678375244\n",
      "[step: 427] loss: 4.6570210456848145\n",
      "[step: 428] loss: 4.656518459320068\n",
      "[step: 429] loss: 4.6560139656066895\n",
      "[step: 430] loss: 4.655506610870361\n",
      "[step: 431] loss: 4.654998779296875\n",
      "[step: 432] loss: 4.654489517211914\n",
      "[step: 433] loss: 4.65397834777832\n",
      "[step: 434] loss: 4.653466701507568\n",
      "[step: 435] loss: 4.652952671051025\n",
      "[step: 436] loss: 4.652435779571533\n",
      "[step: 437] loss: 4.651917934417725\n",
      "[step: 438] loss: 4.651398181915283\n",
      "[step: 439] loss: 4.650876998901367\n",
      "[step: 440] loss: 4.650352954864502\n",
      "[step: 441] loss: 4.649827003479004\n",
      "[step: 442] loss: 4.649299144744873\n",
      "[step: 443] loss: 4.648769855499268\n",
      "[step: 444] loss: 4.648238658905029\n",
      "[step: 445] loss: 4.647706031799316\n",
      "[step: 446] loss: 4.6471710205078125\n",
      "[step: 447] loss: 4.646633625030518\n",
      "[step: 448] loss: 4.646093845367432\n",
      "[step: 449] loss: 4.645552158355713\n",
      "[step: 450] loss: 4.645008563995361\n",
      "[step: 451] loss: 4.644463062286377\n",
      "[step: 452] loss: 4.643913745880127\n",
      "[step: 453] loss: 4.643353462219238\n",
      "[step: 454] loss: 4.642786502838135\n",
      "[step: 455] loss: 4.642214775085449\n",
      "[step: 456] loss: 4.641637325286865\n",
      "[step: 457] loss: 4.641052722930908\n",
      "[step: 458] loss: 4.6404523849487305\n",
      "[step: 459] loss: 4.639840126037598\n",
      "[step: 460] loss: 4.639220714569092\n",
      "[step: 461] loss: 4.638592720031738\n",
      "[step: 462] loss: 4.637969017028809\n",
      "[step: 463] loss: 4.637340545654297\n",
      "[step: 464] loss: 4.636707782745361\n",
      "[step: 465] loss: 4.636070728302002\n",
      "[step: 466] loss: 4.635430335998535\n",
      "[step: 467] loss: 4.634795665740967\n",
      "[step: 468] loss: 4.634154796600342\n",
      "[step: 469] loss: 4.633454322814941\n",
      "[step: 470] loss: 4.632711410522461\n",
      "[step: 471] loss: 4.631930351257324\n",
      "[step: 472] loss: 4.631146430969238\n",
      "[step: 473] loss: 4.6303558349609375\n",
      "[step: 474] loss: 4.6295485496521\n",
      "[step: 475] loss: 4.628725528717041\n",
      "[step: 476] loss: 4.627888202667236\n",
      "[step: 477] loss: 4.627047538757324\n",
      "[step: 478] loss: 4.626236915588379\n",
      "[step: 479] loss: 4.625480651855469\n",
      "[step: 480] loss: 4.624706745147705\n",
      "[step: 481] loss: 4.623922824859619\n",
      "[step: 482] loss: 4.623159885406494\n",
      "[step: 483] loss: 4.6223955154418945\n",
      "[step: 484] loss: 4.621623516082764\n",
      "[step: 485] loss: 4.620841026306152\n",
      "[step: 486] loss: 4.620060443878174\n",
      "[step: 487] loss: 4.619280815124512\n",
      "[step: 488] loss: 4.618478298187256\n",
      "[step: 489] loss: 4.617667198181152\n",
      "[step: 490] loss: 4.616857528686523\n",
      "[step: 491] loss: 4.616043567657471\n",
      "[step: 492] loss: 4.6152262687683105\n",
      "[step: 493] loss: 4.614408493041992\n",
      "[step: 494] loss: 4.613585472106934\n",
      "[step: 495] loss: 4.6127610206604\n",
      "[step: 496] loss: 4.611938953399658\n",
      "[step: 497] loss: 4.611112594604492\n",
      "[step: 498] loss: 4.610281467437744\n",
      "[step: 499] loss: 4.609450817108154\n",
      "[step: 500] loss: 4.608617782592773\n",
      "[step: 501] loss: 4.607791900634766\n",
      "[step: 502] loss: 4.60695743560791\n",
      "[step: 503] loss: 4.606116771697998\n",
      "[step: 504] loss: 4.605281352996826\n",
      "[step: 505] loss: 4.604447364807129\n",
      "[step: 506] loss: 4.603603363037109\n",
      "[step: 507] loss: 4.602747440338135\n",
      "[step: 508] loss: 4.601881504058838\n",
      "[step: 509] loss: 4.600994110107422\n",
      "[step: 510] loss: 4.600098133087158\n",
      "[step: 511] loss: 4.599196910858154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 512] loss: 4.598294258117676\n",
      "[step: 513] loss: 4.5973896980285645\n",
      "[step: 514] loss: 4.59647798538208\n",
      "[step: 515] loss: 4.595557689666748\n",
      "[step: 516] loss: 4.594636917114258\n",
      "[step: 517] loss: 4.593710422515869\n",
      "[step: 518] loss: 4.592778205871582\n",
      "[step: 519] loss: 4.591845989227295\n",
      "[step: 520] loss: 4.590909481048584\n",
      "[step: 521] loss: 4.589970588684082\n",
      "[step: 522] loss: 4.589027404785156\n",
      "[step: 523] loss: 4.588076114654541\n",
      "[step: 524] loss: 4.587113857269287\n",
      "[step: 525] loss: 4.58614444732666\n",
      "[step: 526] loss: 4.585162162780762\n",
      "[step: 527] loss: 4.584179878234863\n",
      "[step: 528] loss: 4.583193302154541\n",
      "[step: 529] loss: 4.582205295562744\n",
      "[step: 530] loss: 4.581210613250732\n",
      "[step: 531] loss: 4.580208778381348\n",
      "[step: 532] loss: 4.579206466674805\n",
      "[step: 533] loss: 4.5782012939453125\n",
      "[step: 534] loss: 4.577193737030029\n",
      "[step: 535] loss: 4.5761799812316895\n",
      "[step: 536] loss: 4.575162410736084\n",
      "[step: 537] loss: 4.5741400718688965\n",
      "[step: 538] loss: 4.573112487792969\n",
      "[step: 539] loss: 4.572082042694092\n",
      "[step: 540] loss: 4.571044921875\n",
      "[step: 541] loss: 4.570003032684326\n",
      "[step: 542] loss: 4.568958282470703\n",
      "[step: 543] loss: 4.567909240722656\n",
      "[step: 544] loss: 4.566854953765869\n",
      "[step: 545] loss: 4.565798282623291\n",
      "[step: 546] loss: 4.5647406578063965\n",
      "[step: 547] loss: 4.5636820793151855\n",
      "[step: 548] loss: 4.562618732452393\n",
      "[step: 549] loss: 4.561551570892334\n",
      "[step: 550] loss: 4.560481071472168\n",
      "[step: 551] loss: 4.5594072341918945\n",
      "[step: 552] loss: 4.558330059051514\n",
      "[step: 553] loss: 4.557250022888184\n",
      "[step: 554] loss: 4.556166172027588\n",
      "[step: 555] loss: 4.555081367492676\n",
      "[step: 556] loss: 4.554006576538086\n",
      "[step: 557] loss: 4.552919864654541\n",
      "[step: 558] loss: 4.551828861236572\n",
      "[step: 559] loss: 4.550736904144287\n",
      "[step: 560] loss: 4.54964017868042\n",
      "[step: 561] loss: 4.548545837402344\n",
      "[step: 562] loss: 4.547452449798584\n",
      "[step: 563] loss: 4.546360492706299\n",
      "[step: 564] loss: 4.54527473449707\n",
      "[step: 565] loss: 4.544188976287842\n",
      "[step: 566] loss: 4.543102741241455\n",
      "[step: 567] loss: 4.542008399963379\n",
      "[step: 568] loss: 4.54091215133667\n",
      "[step: 569] loss: 4.539820194244385\n",
      "[step: 570] loss: 4.538727760314941\n",
      "[step: 571] loss: 4.537637233734131\n",
      "[step: 572] loss: 4.53654670715332\n",
      "[step: 573] loss: 4.535456657409668\n",
      "[step: 574] loss: 4.534372806549072\n",
      "[step: 575] loss: 4.533290863037109\n",
      "[step: 576] loss: 4.5322089195251465\n",
      "[step: 577] loss: 4.531129360198975\n",
      "[step: 578] loss: 4.530054092407227\n",
      "[step: 579] loss: 4.528981685638428\n",
      "[step: 580] loss: 4.52791166305542\n",
      "[step: 581] loss: 4.526846408843994\n",
      "[step: 582] loss: 4.525784492492676\n",
      "[step: 583] loss: 4.524725914001465\n",
      "[step: 584] loss: 4.523669719696045\n",
      "[step: 585] loss: 4.522618770599365\n",
      "[step: 586] loss: 4.5215744972229\n",
      "[step: 587] loss: 4.520536422729492\n",
      "[step: 588] loss: 4.519504070281982\n",
      "[step: 589] loss: 4.518482685089111\n",
      "[step: 590] loss: 4.517467021942139\n",
      "[step: 591] loss: 4.516457557678223\n",
      "[step: 592] loss: 4.515454292297363\n",
      "[step: 593] loss: 4.5144572257995605\n",
      "[step: 594] loss: 4.513467788696289\n",
      "[step: 595] loss: 4.512484073638916\n",
      "[step: 596] loss: 4.511505603790283\n",
      "[step: 597] loss: 4.510525703430176\n",
      "[step: 598] loss: 4.509548664093018\n",
      "[step: 599] loss: 4.508575916290283\n",
      "[step: 600] loss: 4.507608413696289\n",
      "[step: 601] loss: 4.506649017333984\n",
      "[step: 602] loss: 4.505695819854736\n",
      "[step: 603] loss: 4.504751682281494\n",
      "[step: 604] loss: 4.503815174102783\n",
      "[step: 605] loss: 4.502884864807129\n",
      "[step: 606] loss: 4.501960754394531\n",
      "[step: 607] loss: 4.50104284286499\n",
      "[step: 608] loss: 4.500137805938721\n",
      "[step: 609] loss: 4.499239444732666\n",
      "[step: 610] loss: 4.49834680557251\n",
      "[step: 611] loss: 4.497462749481201\n",
      "[step: 612] loss: 4.496586799621582\n",
      "[step: 613] loss: 4.4957170486450195\n",
      "[step: 614] loss: 4.494852542877197\n",
      "[step: 615] loss: 4.493989944458008\n",
      "[step: 616] loss: 4.493131160736084\n",
      "[step: 617] loss: 4.492275238037109\n",
      "[step: 618] loss: 4.4914231300354\n",
      "[step: 619] loss: 4.490577220916748\n",
      "[step: 620] loss: 4.489744186401367\n",
      "[step: 621] loss: 4.488919734954834\n",
      "[step: 622] loss: 4.488090991973877\n",
      "[step: 623] loss: 4.487260818481445\n",
      "[step: 624] loss: 4.486425876617432\n",
      "[step: 625] loss: 4.485591888427734\n",
      "[step: 626] loss: 4.484758377075195\n",
      "[step: 627] loss: 4.4839067459106445\n",
      "[step: 628] loss: 4.483012676239014\n",
      "[step: 629] loss: 4.482105255126953\n",
      "[step: 630] loss: 4.481184482574463\n",
      "[step: 631] loss: 4.48024320602417\n",
      "[step: 632] loss: 4.47926139831543\n",
      "[step: 633] loss: 4.478268146514893\n",
      "[step: 634] loss: 4.477249622344971\n",
      "[step: 635] loss: 4.476293087005615\n",
      "[step: 636] loss: 4.47538948059082\n",
      "[step: 637] loss: 4.474484443664551\n",
      "[step: 638] loss: 4.473580360412598\n",
      "[step: 639] loss: 4.472691059112549\n",
      "[step: 640] loss: 4.471808433532715\n",
      "[step: 641] loss: 4.47092866897583\n",
      "[step: 642] loss: 4.470073223114014\n",
      "[step: 643] loss: 4.469260215759277\n",
      "[step: 644] loss: 4.46847677230835\n",
      "[step: 645] loss: 4.467682838439941\n",
      "[step: 646] loss: 4.466885089874268\n",
      "[step: 647] loss: 4.466080665588379\n",
      "[step: 648] loss: 4.4652628898620605\n",
      "[step: 649] loss: 4.4644317626953125\n",
      "[step: 650] loss: 4.463595867156982\n",
      "[step: 651] loss: 4.462767601013184\n",
      "[step: 652] loss: 4.461946964263916\n",
      "[step: 653] loss: 4.461125373840332\n",
      "[step: 654] loss: 4.460302829742432\n",
      "[step: 655] loss: 4.45950174331665\n",
      "[step: 656] loss: 4.458705902099609\n",
      "[step: 657] loss: 4.4579176902771\n",
      "[step: 658] loss: 4.4571309089660645\n",
      "[step: 659] loss: 4.456351280212402\n",
      "[step: 660] loss: 4.455574035644531\n",
      "[step: 661] loss: 4.454801559448242\n",
      "[step: 662] loss: 4.454034328460693\n",
      "[step: 663] loss: 4.453268527984619\n",
      "[step: 664] loss: 4.45250129699707\n",
      "[step: 665] loss: 4.4517388343811035\n",
      "[step: 666] loss: 4.450978755950928\n",
      "[step: 667] loss: 4.450221538543701\n",
      "[step: 668] loss: 4.449468612670898\n",
      "[step: 669] loss: 4.44871711730957\n",
      "[step: 670] loss: 4.447965145111084\n",
      "[step: 671] loss: 4.447213172912598\n",
      "[step: 672] loss: 4.446454048156738\n",
      "[step: 673] loss: 4.445693016052246\n",
      "[step: 674] loss: 4.444931507110596\n",
      "[step: 675] loss: 4.444167613983154\n",
      "[step: 676] loss: 4.443402290344238\n",
      "[step: 677] loss: 4.442636966705322\n",
      "[step: 678] loss: 4.441872596740723\n",
      "[step: 679] loss: 4.441109657287598\n",
      "[step: 680] loss: 4.440338134765625\n",
      "[step: 681] loss: 4.439565181732178\n",
      "[step: 682] loss: 4.438793659210205\n",
      "[step: 683] loss: 4.43802547454834\n",
      "[step: 684] loss: 4.43726921081543\n",
      "[step: 685] loss: 4.436511516571045\n",
      "[step: 686] loss: 4.435758590698242\n",
      "[step: 687] loss: 4.435009002685547\n",
      "[step: 688] loss: 4.434271812438965\n",
      "[step: 689] loss: 4.433535099029541\n",
      "[step: 690] loss: 4.432798862457275\n",
      "[step: 691] loss: 4.432065963745117\n",
      "[step: 692] loss: 4.431337833404541\n",
      "[step: 693] loss: 4.430613040924072\n",
      "[step: 694] loss: 4.429887771606445\n",
      "[step: 695] loss: 4.429164886474609\n",
      "[step: 696] loss: 4.428444862365723\n",
      "[step: 697] loss: 4.427711009979248\n",
      "[step: 698] loss: 4.426976203918457\n",
      "[step: 699] loss: 4.426239490509033\n",
      "[step: 700] loss: 4.425503253936768\n",
      "[step: 701] loss: 4.424768447875977\n",
      "[step: 702] loss: 4.424055099487305\n",
      "[step: 703] loss: 4.423308849334717\n",
      "[step: 704] loss: 4.422532558441162\n",
      "[step: 705] loss: 4.421746730804443\n",
      "[step: 706] loss: 4.420953273773193\n",
      "[step: 707] loss: 4.420152187347412\n",
      "[step: 708] loss: 4.419399261474609\n",
      "[step: 709] loss: 4.41868257522583\n",
      "[step: 710] loss: 4.417961120605469\n",
      "[step: 711] loss: 4.41724157333374\n",
      "[step: 712] loss: 4.416532516479492\n",
      "[step: 713] loss: 4.415806770324707\n",
      "[step: 714] loss: 4.415089130401611\n",
      "[step: 715] loss: 4.414373397827148\n",
      "[step: 716] loss: 4.413668632507324\n",
      "[step: 717] loss: 4.412962913513184\n",
      "[step: 718] loss: 4.412257194519043\n",
      "[step: 719] loss: 4.411552429199219\n",
      "[step: 720] loss: 4.410846710205078\n",
      "[step: 721] loss: 4.410140514373779\n",
      "[step: 722] loss: 4.409451961517334\n",
      "[step: 723] loss: 4.408771514892578\n",
      "[step: 724] loss: 4.408090591430664\n",
      "[step: 725] loss: 4.407414436340332\n",
      "[step: 726] loss: 4.406754016876221\n",
      "[step: 727] loss: 4.406101703643799\n",
      "[step: 728] loss: 4.405456066131592\n",
      "[step: 729] loss: 4.404810905456543\n",
      "[step: 730] loss: 4.404164791107178\n",
      "[step: 731] loss: 4.403517723083496\n",
      "[step: 732] loss: 4.402870178222656\n",
      "[step: 733] loss: 4.4022216796875\n",
      "[step: 734] loss: 4.4015703201293945\n",
      "[step: 735] loss: 4.400918483734131\n",
      "[step: 736] loss: 4.400265693664551\n",
      "[step: 737] loss: 4.399611473083496\n",
      "[step: 738] loss: 4.398957252502441\n",
      "[step: 739] loss: 4.398303508758545\n",
      "[step: 740] loss: 4.397651195526123\n",
      "[step: 741] loss: 4.396997928619385\n",
      "[step: 742] loss: 4.3963446617126465\n",
      "[step: 743] loss: 4.395690441131592\n",
      "[step: 744] loss: 4.395035743713379\n",
      "[step: 745] loss: 4.394384860992432\n",
      "[step: 746] loss: 4.393734931945801\n",
      "[step: 747] loss: 4.393083572387695\n",
      "[step: 748] loss: 4.3924336433410645\n",
      "[step: 749] loss: 4.391782283782959\n",
      "[step: 750] loss: 4.391131401062012\n",
      "[step: 751] loss: 4.3904829025268555\n",
      "[step: 752] loss: 4.389837265014648\n",
      "[step: 753] loss: 4.389189720153809\n",
      "[step: 754] loss: 4.388543128967285\n",
      "[step: 755] loss: 4.387896537780762\n",
      "[step: 756] loss: 4.387250900268555\n",
      "[step: 757] loss: 4.3866047859191895\n",
      "[step: 758] loss: 4.385962009429932\n",
      "[step: 759] loss: 4.385318279266357\n",
      "[step: 760] loss: 4.384674549102783\n",
      "[step: 761] loss: 4.384024143218994\n",
      "[step: 762] loss: 4.383364677429199\n",
      "[step: 763] loss: 4.382700443267822\n",
      "[step: 764] loss: 4.3820366859436035\n",
      "[step: 765] loss: 4.381380081176758\n",
      "[step: 766] loss: 4.380725860595703\n",
      "[step: 767] loss: 4.380066394805908\n",
      "[step: 768] loss: 4.379401683807373\n",
      "[step: 769] loss: 4.378732204437256\n",
      "[step: 770] loss: 4.378061294555664\n",
      "[step: 771] loss: 4.3773908615112305\n",
      "[step: 772] loss: 4.376733779907227\n",
      "[step: 773] loss: 4.376064777374268\n",
      "[step: 774] loss: 4.375393390655518\n",
      "[step: 775] loss: 4.3747334480285645\n",
      "[step: 776] loss: 4.374072551727295\n",
      "[step: 777] loss: 4.373411178588867\n",
      "[step: 778] loss: 4.372751235961914\n",
      "[step: 779] loss: 4.37208890914917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 780] loss: 4.371426582336426\n",
      "[step: 781] loss: 4.370764255523682\n",
      "[step: 782] loss: 4.3700995445251465\n",
      "[step: 783] loss: 4.369442462921143\n",
      "[step: 784] loss: 4.368794918060303\n",
      "[step: 785] loss: 4.36815071105957\n",
      "[step: 786] loss: 4.367506980895996\n",
      "[step: 787] loss: 4.366868019104004\n",
      "[step: 788] loss: 4.366229057312012\n",
      "[step: 789] loss: 4.365591049194336\n",
      "[step: 790] loss: 4.364953517913818\n",
      "[step: 791] loss: 4.364314079284668\n",
      "[step: 792] loss: 4.363676071166992\n",
      "[step: 793] loss: 4.363035678863525\n",
      "[step: 794] loss: 4.362391471862793\n",
      "[step: 795] loss: 4.361749172210693\n",
      "[step: 796] loss: 4.361107349395752\n",
      "[step: 797] loss: 4.3604655265808105\n",
      "[step: 798] loss: 4.359821319580078\n",
      "[step: 799] loss: 4.359177589416504\n",
      "[step: 800] loss: 4.358534336090088\n",
      "[step: 801] loss: 4.357893466949463\n",
      "[step: 802] loss: 4.357256889343262\n",
      "[step: 803] loss: 4.356619358062744\n",
      "[step: 804] loss: 4.355981349945068\n",
      "[step: 805] loss: 4.3553361892700195\n",
      "[step: 806] loss: 4.35469913482666\n",
      "[step: 807] loss: 4.354074478149414\n",
      "[step: 808] loss: 4.35344934463501\n",
      "[step: 809] loss: 4.352827072143555\n",
      "[step: 810] loss: 4.352207660675049\n",
      "[step: 811] loss: 4.351588726043701\n",
      "[step: 812] loss: 4.350968360900879\n",
      "[step: 813] loss: 4.350348949432373\n",
      "[step: 814] loss: 4.349742412567139\n",
      "[step: 815] loss: 4.349134922027588\n",
      "[step: 816] loss: 4.348529815673828\n",
      "[step: 817] loss: 4.34792423248291\n",
      "[step: 818] loss: 4.347318172454834\n",
      "[step: 819] loss: 4.346714019775391\n",
      "[step: 820] loss: 4.346110820770264\n",
      "[step: 821] loss: 4.345509052276611\n",
      "[step: 822] loss: 4.344908714294434\n",
      "[step: 823] loss: 4.3443074226379395\n",
      "[step: 824] loss: 4.34370756149292\n",
      "[step: 825] loss: 4.343106269836426\n",
      "[step: 826] loss: 4.342504501342773\n",
      "[step: 827] loss: 4.341902732849121\n",
      "[step: 828] loss: 4.341300010681152\n",
      "[step: 829] loss: 4.340697288513184\n",
      "[step: 830] loss: 4.340087890625\n",
      "[step: 831] loss: 4.3394646644592285\n",
      "[step: 832] loss: 4.338752269744873\n",
      "[step: 833] loss: 4.338047981262207\n",
      "[step: 834] loss: 4.337399005889893\n",
      "[step: 835] loss: 4.336733341217041\n",
      "[step: 836] loss: 4.336080551147461\n",
      "[step: 837] loss: 4.335457801818848\n",
      "[step: 838] loss: 4.334881782531738\n",
      "[step: 839] loss: 4.334295272827148\n",
      "[step: 840] loss: 4.333670139312744\n",
      "[step: 841] loss: 4.333016395568848\n",
      "[step: 842] loss: 4.332365036010742\n",
      "[step: 843] loss: 4.331721782684326\n",
      "[step: 844] loss: 4.331068992614746\n",
      "[step: 845] loss: 4.330414772033691\n",
      "[step: 846] loss: 4.329795837402344\n",
      "[step: 847] loss: 4.329220294952393\n",
      "[step: 848] loss: 4.328610897064209\n",
      "[step: 849] loss: 4.327980995178223\n",
      "[step: 850] loss: 4.327359676361084\n",
      "[step: 851] loss: 4.326735496520996\n",
      "[step: 852] loss: 4.326091766357422\n",
      "[step: 853] loss: 4.325427055358887\n",
      "[step: 854] loss: 4.324779033660889\n",
      "[step: 855] loss: 4.324136257171631\n",
      "[step: 856] loss: 4.323493957519531\n",
      "[step: 857] loss: 4.322845458984375\n",
      "[step: 858] loss: 4.322184085845947\n",
      "[step: 859] loss: 4.321513652801514\n",
      "[step: 860] loss: 4.320834636688232\n",
      "[step: 861] loss: 4.320161819458008\n",
      "[step: 862] loss: 4.3194169998168945\n",
      "[step: 863] loss: 4.318647861480713\n",
      "[step: 864] loss: 4.317847728729248\n",
      "[step: 865] loss: 4.317037582397461\n",
      "[step: 866] loss: 4.316231727600098\n",
      "[step: 867] loss: 4.315415859222412\n",
      "[step: 868] loss: 4.314621448516846\n",
      "[step: 869] loss: 4.3138298988342285\n",
      "[step: 870] loss: 4.313074111938477\n",
      "[step: 871] loss: 4.312321186065674\n",
      "[step: 872] loss: 4.311540603637695\n",
      "[step: 873] loss: 4.3107123374938965\n",
      "[step: 874] loss: 4.309875011444092\n",
      "[step: 875] loss: 4.309041976928711\n",
      "[step: 876] loss: 4.30819845199585\n",
      "[step: 877] loss: 4.307316780090332\n",
      "[step: 878] loss: 4.306426048278809\n",
      "[step: 879] loss: 4.305530071258545\n",
      "[step: 880] loss: 4.304627418518066\n",
      "[step: 881] loss: 4.303715705871582\n",
      "[step: 882] loss: 4.302803039550781\n",
      "[step: 883] loss: 4.301886558532715\n",
      "[step: 884] loss: 4.300969123840332\n",
      "[step: 885] loss: 4.300046443939209\n",
      "[step: 886] loss: 4.299082279205322\n",
      "[step: 887] loss: 4.298121452331543\n",
      "[step: 888] loss: 4.297131538391113\n",
      "[step: 889] loss: 4.296148300170898\n",
      "[step: 890] loss: 4.295155048370361\n",
      "[step: 891] loss: 4.294159889221191\n",
      "[step: 892] loss: 4.293143272399902\n",
      "[step: 893] loss: 4.292253017425537\n",
      "[step: 894] loss: 4.291386127471924\n",
      "[step: 895] loss: 4.2905192375183105\n",
      "[step: 896] loss: 4.289661407470703\n",
      "[step: 897] loss: 4.288806915283203\n",
      "[step: 898] loss: 4.2879509925842285\n",
      "[step: 899] loss: 4.287103652954102\n",
      "[step: 900] loss: 4.286279201507568\n",
      "[step: 901] loss: 4.285462379455566\n",
      "[step: 902] loss: 4.2846479415893555\n",
      "[step: 903] loss: 4.28391695022583\n",
      "[step: 904] loss: 4.283204555511475\n",
      "[step: 905] loss: 4.282497406005859\n",
      "[step: 906] loss: 4.281793117523193\n",
      "[step: 907] loss: 4.28109073638916\n",
      "[step: 908] loss: 4.28039026260376\n",
      "[step: 909] loss: 4.279696464538574\n",
      "[step: 910] loss: 4.279001712799072\n",
      "[step: 911] loss: 4.27830696105957\n",
      "[step: 912] loss: 4.277613639831543\n",
      "[step: 913] loss: 4.276919841766357\n",
      "[step: 914] loss: 4.276232719421387\n",
      "[step: 915] loss: 4.275547981262207\n",
      "[step: 916] loss: 4.2748637199401855\n",
      "[step: 917] loss: 4.274176120758057\n",
      "[step: 918] loss: 4.273501396179199\n",
      "[step: 919] loss: 4.272828102111816\n",
      "[step: 920] loss: 4.272149085998535\n",
      "[step: 921] loss: 4.2714691162109375\n",
      "[step: 922] loss: 4.2708001136779785\n",
      "[step: 923] loss: 4.270129203796387\n",
      "[step: 924] loss: 4.269458770751953\n",
      "[step: 925] loss: 4.268786907196045\n",
      "[step: 926] loss: 4.2681097984313965\n",
      "[step: 927] loss: 4.267434597015381\n",
      "[step: 928] loss: 4.266754627227783\n",
      "[step: 929] loss: 4.26607084274292\n",
      "[step: 930] loss: 4.265385150909424\n",
      "[step: 931] loss: 4.264697551727295\n",
      "[step: 932] loss: 4.264009952545166\n",
      "[step: 933] loss: 4.263332366943359\n",
      "[step: 934] loss: 4.262650012969971\n",
      "[step: 935] loss: 4.261967182159424\n",
      "[step: 936] loss: 4.261282920837402\n",
      "[step: 937] loss: 4.260593891143799\n",
      "[step: 938] loss: 4.259902000427246\n",
      "[step: 939] loss: 4.259204387664795\n",
      "[step: 940] loss: 4.25849723815918\n",
      "[step: 941] loss: 4.257786273956299\n",
      "[step: 942] loss: 4.257071018218994\n",
      "[step: 943] loss: 4.256345748901367\n",
      "[step: 944] loss: 4.255619049072266\n",
      "[step: 945] loss: 4.25488805770874\n",
      "[step: 946] loss: 4.254147529602051\n",
      "[step: 947] loss: 4.25343132019043\n",
      "[step: 948] loss: 4.2527079582214355\n",
      "[step: 949] loss: 4.251969814300537\n",
      "[step: 950] loss: 4.251190662384033\n",
      "[step: 951] loss: 4.250425815582275\n",
      "[step: 952] loss: 4.2496747970581055\n",
      "[step: 953] loss: 4.248926639556885\n",
      "[step: 954] loss: 4.248215675354004\n",
      "[step: 955] loss: 4.24754524230957\n",
      "[step: 956] loss: 4.2468647956848145\n",
      "[step: 957] loss: 4.246176242828369\n",
      "[step: 958] loss: 4.245484828948975\n",
      "[step: 959] loss: 4.244792938232422\n",
      "[step: 960] loss: 4.244103908538818\n",
      "[step: 961] loss: 4.243425369262695\n",
      "[step: 962] loss: 4.2427544593811035\n",
      "[step: 963] loss: 4.242086410522461\n",
      "[step: 964] loss: 4.241422176361084\n",
      "[step: 965] loss: 4.240758419036865\n",
      "[step: 966] loss: 4.240096092224121\n",
      "[step: 967] loss: 4.239434719085693\n",
      "[step: 968] loss: 4.238792419433594\n",
      "[step: 969] loss: 4.238140106201172\n",
      "[step: 970] loss: 4.237488269805908\n",
      "[step: 971] loss: 4.236845970153809\n",
      "[step: 972] loss: 4.236178398132324\n",
      "[step: 973] loss: 4.235501766204834\n",
      "[step: 974] loss: 4.234838962554932\n",
      "[step: 975] loss: 4.234180927276611\n",
      "[step: 976] loss: 4.233525276184082\n",
      "[step: 977] loss: 4.232865810394287\n",
      "[step: 978] loss: 4.2322163581848145\n",
      "[step: 979] loss: 4.231585502624512\n",
      "[step: 980] loss: 4.230954170227051\n",
      "[step: 981] loss: 4.230318546295166\n",
      "[step: 982] loss: 4.229677677154541\n",
      "[step: 983] loss: 4.2290496826171875\n",
      "[step: 984] loss: 4.228424549102783\n",
      "[step: 985] loss: 4.2277913093566895\n",
      "[step: 986] loss: 4.227149963378906\n",
      "[step: 987] loss: 4.226502895355225\n",
      "[step: 988] loss: 4.225851535797119\n",
      "[step: 989] loss: 4.225216865539551\n",
      "[step: 990] loss: 4.224576950073242\n",
      "[step: 991] loss: 4.223930358886719\n",
      "[step: 992] loss: 4.223288536071777\n",
      "[step: 993] loss: 4.222653865814209\n",
      "[step: 994] loss: 4.222012996673584\n",
      "[step: 995] loss: 4.221365451812744\n",
      "[step: 996] loss: 4.220723628997803\n",
      "[step: 997] loss: 4.220076084136963\n",
      "[step: 998] loss: 4.219438076019287\n",
      "[step: 999] loss: 4.218795299530029\n",
      "RMSE: 20.36486053466797\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc4c9710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #초기화\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = minMaxDeNormalizer(sess.run(Y_pred, feed_dict={X: testX}),originalXY)\n",
    "    rmse_val = sess.run(rmse, feed_dict={targets: denormalizedTestY_feed, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(denormalizedTestY_feed) #실제 sales 파란색\n",
    "    plt.plot(test_predict)           #예측 sales 주황색\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denormalizedTestPredictY=[item for sublist in test_predict for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.364860324472062"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootMeanSquaredError(denormalizedTestY,denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.449244391207795"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootMeanSquaredError(denormalizedTestY_preprocessed,denormalizedTestPredictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "121 in denormalizedTestY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25,\n",
       " 27,\n",
       " 25,\n",
       " 21,\n",
       " 28,\n",
       " 24,\n",
       " 21,\n",
       " 29,\n",
       " 35,\n",
       " 0,\n",
       " 7,\n",
       " 32,\n",
       " 19,\n",
       " 34,\n",
       " 28,\n",
       " 23,\n",
       " 15,\n",
       " 10,\n",
       " 15,\n",
       " 9,\n",
       " 78,\n",
       " 59,\n",
       " 29,\n",
       " 23,\n",
       " 12,\n",
       " 36,\n",
       " 13,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 14,\n",
       " 8,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 20,\n",
       " 24,\n",
       " 8,\n",
       " 15,\n",
       " 17,\n",
       " 9,\n",
       " 17,\n",
       " 13,\n",
       " 19,\n",
       " 8,\n",
       " 14,\n",
       " 18,\n",
       " 24,\n",
       " 16,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 16,\n",
       " 11,\n",
       " 22,\n",
       " 15,\n",
       " 18,\n",
       " 11,\n",
       " 14,\n",
       " 34,\n",
       " 10,\n",
       " 13,\n",
       " 21,\n",
       " 35,\n",
       " 15,\n",
       " 6,\n",
       " 23,\n",
       " 22,\n",
       " 19,\n",
       " 20,\n",
       " 13,\n",
       " 21,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 16,\n",
       " 12,\n",
       " 15,\n",
       " 8,\n",
       " 0,\n",
       " 24,\n",
       " 19,\n",
       " 22,\n",
       " 13,\n",
       " 12,\n",
       " 16,\n",
       " 2,\n",
       " 16,\n",
       " 20,\n",
       " 12,\n",
       " 11,\n",
       " 22,\n",
       " 20,\n",
       " 16,\n",
       " 17,\n",
       " 97,\n",
       " 0,\n",
       " 23,\n",
       " 100,\n",
       " 18,\n",
       " 13,\n",
       " 21,\n",
       " 24,\n",
       " 16,\n",
       " 18,\n",
       " 22,\n",
       " 18,\n",
       " 11,\n",
       " 24,\n",
       " 22,\n",
       " 20,\n",
       " 22,\n",
       " 26,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 31]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denormalizedTestY_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
